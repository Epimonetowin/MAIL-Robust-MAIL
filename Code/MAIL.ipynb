{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7957692,"sourceType":"datasetVersion","datasetId":4680816},{"sourceId":7957702,"sourceType":"datasetVersion","datasetId":4680825},{"sourceId":10653461,"sourceType":"datasetVersion","datasetId":6597049}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import cv2\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nsns.set_style('whitegrid')\nfrom sklearn.metrics import confusion_matrix , classification_report\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense , Flatten , Conv2D , MaxPooling2D , Dropout , Activation , BatchNormalization\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import Adam , Adamax\nfrom tensorflow.keras import regularizers\n\n#Warnings\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-09T06:00:10.470559Z","iopub.execute_input":"2025-03-09T06:00:10.470899Z","iopub.status.idle":"2025-03-09T06:00:24.898940Z","shell.execute_reply.started":"2025-03-09T06:00:10.470870Z","shell.execute_reply":"2025-03-09T06:00:24.898259Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import tensorflow as tf\n#tf.keras.mixed_precision.set_global_policy('mixed_float16')\n\nimport tensorflow as tf\nimport numpy as np\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D, BatchNormalization, ReLU, Add\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.losses import KLDivergence\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.datasets import cifar10\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.applications import DenseNet121, ResNet50V2\nfrom tensorflow.keras.layers import GlobalAveragePooling2D\nimport copy\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D, BatchNormalization, ReLU, Add\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.losses import KLDivergence\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.datasets import cifar10\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.applications import DenseNet169, MobileNetV2, ResNet50, EfficientNetB0\nfrom tensorflow.keras.layers import GlobalAveragePooling2D\nimport copy\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T06:00:24.900055Z","iopub.execute_input":"2025-03-09T06:00:24.900773Z","iopub.status.idle":"2025-03-09T06:00:24.928896Z","shell.execute_reply.started":"2025-03-09T06:00:24.900726Z","shell.execute_reply":"2025-03-09T06:00:24.928053Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train_h = np.load('/kaggle/input/ham10000-current-dataset-joy/x_train.npy')\ny_train_h = np.load('/kaggle/input/ham10000-current-dataset-joy/y_train.npy')\nX_test_h = np.load('/kaggle/input/ham10000-current-dataset-joy/x_test(1).npy')\ny_test_h = np.load('/kaggle/input/ham10000-current-dataset-joy/y_test(1).npy')\n\nX_val_h = np.load('/kaggle/input/ham10000-current-dataset-joy/x_val.npy')\ny_val_h = np.load('/kaggle/input/ham10000-current-dataset-joy/y_val.npy')\n\n\nX_train_h.shape, y_train_h.shape, X_test_h.shape, y_test_h.shape, X_val_h.shape, y_val_h.shape\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T06:00:24.930533Z","iopub.execute_input":"2025-03-09T06:00:24.930862Z","iopub.status.idle":"2025-03-09T06:00:36.943614Z","shell.execute_reply.started":"2025-03-09T06:00:24.930832Z","shell.execute_reply":"2025-03-09T06:00:36.942806Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"((8012, 128, 128, 3),\n (8012, 7),\n (1001, 128, 128, 3),\n (1001, 7),\n (1002, 128, 128, 3),\n (1002, 7))"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"random_indices = np.random.choice(1001, 810, replace=False)\n\nX_test_h1 = X_test_h[random_indices]\ny_test_h1 = y_test_h[random_indices]\n\nX_test_h1.shape, y_test_h1.shape, X_test_h.shape, y_test_h.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T06:00:36.944804Z","iopub.execute_input":"2025-03-09T06:00:36.945034Z","iopub.status.idle":"2025-03-09T06:00:36.998119Z","shell.execute_reply.started":"2025-03-09T06:00:36.945016Z","shell.execute_reply":"2025-03-09T06:00:36.997384Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"((810, 128, 128, 3), (810, 7), (1001, 128, 128, 3), (1001, 7))"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"random_indices = np.random.choice(1002, 648, replace=False)\n\nX_val_h1 = X_val_h[random_indices]\ny_val_h1 = y_val_h[random_indices]\n\nX_test_h1.shape, y_test_h1.shape, X_test_h.shape, y_test_h.shape, X_val_h1.shape, y_val_h1.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T06:00:36.998954Z","iopub.execute_input":"2025-03-09T06:00:36.999201Z","iopub.status.idle":"2025-03-09T06:00:37.043541Z","shell.execute_reply.started":"2025-03-09T06:00:36.999174Z","shell.execute_reply":"2025-03-09T06:00:37.042735Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"((810, 128, 128, 3),\n (810, 7),\n (1001, 128, 128, 3),\n (1001, 7),\n (648, 128, 128, 3),\n (648, 7))"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"#X_train_s.shape,X_test_s.shape, y_train_s.shape,y_test_s.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T06:00:37.044321Z","iopub.execute_input":"2025-03-09T06:00:37.044619Z","iopub.status.idle":"2025-03-09T06:00:37.048013Z","shell.execute_reply.started":"2025-03-09T06:00:37.044590Z","shell.execute_reply":"2025-03-09T06:00:37.047370Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train_h.shape, y_train_h.shape, X_test_h.shape, y_test_h.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T06:00:37.048895Z","iopub.execute_input":"2025-03-09T06:00:37.049203Z","iopub.status.idle":"2025-03-09T06:00:37.067183Z","shell.execute_reply.started":"2025-03-09T06:00:37.049172Z","shell.execute_reply":"2025-03-09T06:00:37.066416Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"((8012, 128, 128, 3), (8012, 7), (1001, 128, 128, 3), (1001, 7))"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"X_train_s = np.load('/kaggle/input/large-sipkamed-cervical-cancer/data_cervical_cancer_sipkamed.npy')\ny_train_s = np.load('/kaggle/input/large-sipkamed-cervical-cancer/labels_cervical_cancer_sipkamed.npy')\n\nX_train_s.shape, y_train_s.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T06:00:37.069655Z","iopub.execute_input":"2025-03-09T06:00:37.069919Z","iopub.status.idle":"2025-03-09T06:00:38.144530Z","shell.execute_reply.started":"2025-03-09T06:00:37.069899Z","shell.execute_reply":"2025-03-09T06:00:38.143561Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"((4049, 128, 128, 3), (4049, 5))"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train_s, X_test_s, y_train_s, y_test_s = train_test_split(X_train_s, y_train_s, test_size=0.2, random_state=42)\nX_train_s, X_val_s, y_train_s, y_val_s = train_test_split(X_train_s, y_train_s, test_size=0.2, random_state=42)\n\nX_train_s.shape,X_test_s.shape, y_train_s.shape,y_test_s.shape, y_val_s.shape,y_val_s.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T06:00:38.145907Z","iopub.execute_input":"2025-03-09T06:00:38.146220Z","iopub.status.idle":"2025-03-09T06:00:38.277401Z","shell.execute_reply.started":"2025-03-09T06:00:38.146194Z","shell.execute_reply":"2025-03-09T06:00:38.276636Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"((2591, 128, 128, 3),\n (810, 128, 128, 3),\n (2591, 5),\n (810, 5),\n (648, 5),\n (648, 5))"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"'''import numpy as np\nimport cv2\n\ndef rotate_image(image, angle):\n    \"\"\"\n    Rotate the image by the specified angle.\n    \"\"\"\n    center = tuple(np.array(image.shape[1::-1]) / 2)\n    rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n    rotated_image = cv2.warpAffine(image, rotation_matrix, image.shape[1::-1], flags=cv2.INTER_LINEAR)\n    return rotated_image\n\ndef translate_image(image, tx, ty):\n    \"\"\"\n    Translate the image by the specified translation parameters.\n    \"\"\"\n    translation_matrix = np.float32([[1, 0, tx], [0, 1, ty]])\n    translated_image = cv2.warpAffine(image, translation_matrix, image.shape[1::-1])\n    return translated_image\n\n# Example data\n#X_train = np.random.rand(100, 28, 28)  # Assuming 100 images of size 28x28\n#y_train = np.random.randint(0, 10, 100)  # Assuming 100 labels\n\n# Augmentation parameters\nrotation_angles = [20]\ntranslations = [(5, 5)]\n\naugmented_X_train = []\naugmented_y_train = []\n\nfor image, label in zip(X_train_s, y_train_s):\n    # Original image\n    #augmented_X_train.append(image)\n    #augmented_y_train.append(label)\n\n    # Augment with rotations\n    for angle in rotation_angles:\n        rotated_image = rotate_image(image, angle)\n        augmented_X_train.append(rotated_image)\n        augmented_y_train.append(label)\n\n    # Augment with translations\n    for tx, ty in translations:\n        translated_image = translate_image(image, tx, ty)\n        augmented_X_train.append(translated_image)\n        augmented_y_train.append(label)\n\n# Convert lists to numpy arrays\naugmented_X_train = np.array(augmented_X_train)\naugmented_y_train = np.array(augmented_y_train)\n\n# Shuffle the data\nshuffle_indices = np.random.permutation(len(augmented_X_train))\naugmented_X_train = augmented_X_train[shuffle_indices]\naugmented_y_train = augmented_y_train[shuffle_indices]\naugmented_X_train.shape, augmented_y_train.shape\n# Now, augmented_X_train and augmented_y_train contain the augmented dataset.'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T06:00:38.278302Z","iopub.execute_input":"2025-03-09T06:00:38.278600Z","iopub.status.idle":"2025-03-09T06:00:38.285978Z","shell.execute_reply.started":"2025-03-09T06:00:38.278574Z","shell.execute_reply":"2025-03-09T06:00:38.285158Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"'import numpy as np\\nimport cv2\\n\\ndef rotate_image(image, angle):\\n    \"\"\"\\n    Rotate the image by the specified angle.\\n    \"\"\"\\n    center = tuple(np.array(image.shape[1::-1]) / 2)\\n    rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\\n    rotated_image = cv2.warpAffine(image, rotation_matrix, image.shape[1::-1], flags=cv2.INTER_LINEAR)\\n    return rotated_image\\n\\ndef translate_image(image, tx, ty):\\n    \"\"\"\\n    Translate the image by the specified translation parameters.\\n    \"\"\"\\n    translation_matrix = np.float32([[1, 0, tx], [0, 1, ty]])\\n    translated_image = cv2.warpAffine(image, translation_matrix, image.shape[1::-1])\\n    return translated_image\\n\\n# Example data\\n#X_train = np.random.rand(100, 28, 28)  # Assuming 100 images of size 28x28\\n#y_train = np.random.randint(0, 10, 100)  # Assuming 100 labels\\n\\n# Augmentation parameters\\nrotation_angles = [20]\\ntranslations = [(5, 5)]\\n\\naugmented_X_train = []\\naugmented_y_train = []\\n\\nfor image, label in zip(X_train_s, y_train_s):\\n    # Original image\\n    #augmented_X_train.append(image)\\n    #augmented_y_train.append(label)\\n\\n    # Augment with rotations\\n    for angle in rotation_angles:\\n        rotated_image = rotate_image(image, angle)\\n        augmented_X_train.append(rotated_image)\\n        augmented_y_train.append(label)\\n\\n    # Augment with translations\\n    for tx, ty in translations:\\n        translated_image = translate_image(image, tx, ty)\\n        augmented_X_train.append(translated_image)\\n        augmented_y_train.append(label)\\n\\n# Convert lists to numpy arrays\\naugmented_X_train = np.array(augmented_X_train)\\naugmented_y_train = np.array(augmented_y_train)\\n\\n# Shuffle the data\\nshuffle_indices = np.random.permutation(len(augmented_X_train))\\naugmented_X_train = augmented_X_train[shuffle_indices]\\naugmented_y_train = augmented_y_train[shuffle_indices]\\naugmented_X_train.shape, augmented_y_train.shape\\n# Now, augmented_X_train and augmented_y_train contain the augmented dataset.'"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"import numpy as np\nimport cv2\n\ndef rotate_image(image, angle):\n    \"\"\"\n    Rotate the image by the specified angle.\n    \"\"\"\n    center = tuple(np.array(image.shape[1::-1]) / 2)\n    rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n    rotated_image = cv2.warpAffine(image, rotation_matrix, image.shape[1::-1], flags=cv2.INTER_LINEAR)\n    return rotated_image\n\ndef translate_image(image, tx, ty):\n    \"\"\"\n    Translate the image by the specified translation parameters.\n    \"\"\"\n    translation_matrix = np.float32([[1, 0, tx], [0, 1, ty]])\n    translated_image = cv2.warpAffine(image, translation_matrix, image.shape[1::-1])\n    return translated_image\n\ndef apply_gaussian_blur(image, kernel_size=3):\n    \"\"\"\n    Apply Gaussian Blur to the image to reduce noise and improve generalization.\n    \"\"\"\n    blurred_image = cv2.GaussianBlur(image, (kernel_size, kernel_size), 0)\n    return blurred_image\n\n# Augmentation parameters\nrotation_angles = [20]\ntranslations = [(5, 5)]\nkernel_sizes = [3]  # Gaussian Blur kernel sizes\n\naugmented_X_train = []\naugmented_y_train = []\n\nfor image, label in zip(X_train_s, y_train_s):\n    # Augment with rotations\n    for angle in rotation_angles:\n        rotated_image = rotate_image(image, angle)\n        augmented_X_train.append(rotated_image)\n        augmented_y_train.append(label)\n\n    # Augment with translations\n    for tx, ty in translations:\n        translated_image = translate_image(image, tx, ty)\n        augmented_X_train.append(translated_image)\n        augmented_y_train.append(label)\n\n    # Augment with Gaussian Blur\n    for kernel_size in kernel_sizes:\n        blurred_image = apply_gaussian_blur(image, kernel_size)\n        augmented_X_train.append(blurred_image)\n        augmented_y_train.append(label)\n\n# Convert lists to numpy arrays\naugmented_X_train = np.array(augmented_X_train)\naugmented_y_train = np.array(augmented_y_train)\n\n# Shuffle the data\nshuffle_indices = np.random.permutation(len(augmented_X_train))\naugmented_X_train = augmented_X_train[shuffle_indices]\naugmented_y_train = augmented_y_train[shuffle_indices]\naugmented_X_train.shape, augmented_y_train.shape\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T06:00:38.286738Z","iopub.execute_input":"2025-03-09T06:00:38.287020Z","iopub.status.idle":"2025-03-09T06:00:40.521886Z","shell.execute_reply.started":"2025-03-09T06:00:38.286995Z","shell.execute_reply":"2025-03-09T06:00:40.521087Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"((7773, 128, 128, 3), (7773, 5))"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"random_indices = np.random.choice(7773, 5421, replace=False)\n\naugmented_X_train = augmented_X_train[random_indices]\naugmented_y_train = augmented_y_train[random_indices]\n\naugmented_X_train.shape, augmented_y_train.shape\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T06:00:40.522685Z","iopub.execute_input":"2025-03-09T06:00:40.522994Z","iopub.status.idle":"2025-03-09T06:00:40.585501Z","shell.execute_reply.started":"2025-03-09T06:00:40.522967Z","shell.execute_reply":"2025-03-09T06:00:40.584558Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"((5421, 128, 128, 3), (5421, 5))"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"X_train_s = np.concatenate((X_train_s, augmented_X_train), axis=0)\ny_train_s = np.concatenate((y_train_s, augmented_y_train), axis=0)\nX_train_s.shape, y_train_s.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T06:00:40.586344Z","iopub.execute_input":"2025-03-09T06:00:40.586560Z","iopub.status.idle":"2025-03-09T06:00:40.728604Z","shell.execute_reply.started":"2025-03-09T06:00:40.586544Z","shell.execute_reply":"2025-03-09T06:00:40.727763Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"((8012, 128, 128, 3), (8012, 5))"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"'''X_train_s = np.concatenate((X_train_s, X_train_s, X_train_s), axis=0)\ny_train_s = np.concatenate((y_train_s, y_train_s, y_train_s), axis=0)\nX_train_s.shape, y_train_s.shape'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T06:00:40.729386Z","iopub.execute_input":"2025-03-09T06:00:40.729705Z","iopub.status.idle":"2025-03-09T06:00:40.734271Z","shell.execute_reply.started":"2025-03-09T06:00:40.729671Z","shell.execute_reply":"2025-03-09T06:00:40.733585Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"'X_train_s = np.concatenate((X_train_s, X_train_s, X_train_s), axis=0)\\ny_train_s = np.concatenate((y_train_s, y_train_s, y_train_s), axis=0)\\nX_train_s.shape, y_train_s.shape'"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"'''X_test_s1 = np.concatenate((X_test_s, X_test_s, X_test_s), axis=0)\ny_test_s1 = np.concatenate((y_test_s, y_test_s, y_test_s), axis=0)\nX_test_s1.shape, y_test_s1.shape'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T06:00:40.735059Z","iopub.execute_input":"2025-03-09T06:00:40.735360Z","iopub.status.idle":"2025-03-09T06:00:40.751082Z","shell.execute_reply.started":"2025-03-09T06:00:40.735336Z","shell.execute_reply":"2025-03-09T06:00:40.750208Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"'X_test_s1 = np.concatenate((X_test_s, X_test_s, X_test_s), axis=0)\\ny_test_s1 = np.concatenate((y_test_s, y_test_s, y_test_s), axis=0)\\nX_test_s1.shape, y_test_s1.shape'"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"X_train_s.shape, y_train_s.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T06:00:40.752186Z","iopub.execute_input":"2025-03-09T06:00:40.752495Z","iopub.status.idle":"2025-03-09T06:00:40.766472Z","shell.execute_reply.started":"2025-03-09T06:00:40.752465Z","shell.execute_reply":"2025-03-09T06:00:40.765765Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"((8012, 128, 128, 3), (8012, 5))"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"augmented_X_train.shape, augmented_y_train.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T06:00:40.767239Z","iopub.execute_input":"2025-03-09T06:00:40.767485Z","iopub.status.idle":"2025-03-09T06:00:40.784211Z","shell.execute_reply.started":"2025-03-09T06:00:40.767455Z","shell.execute_reply":"2025-03-09T06:00:40.783526Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"((5421, 128, 128, 3), (5421, 5))"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"#### Multi-branch fusion attention (MFA) module #####\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.signal import fft2d, dct\n\nclass GlobalMinPooling2D(layers.Layer):\n    def __init__(self, **kwargs):\n        super(GlobalMinPooling2D, self).__init__(**kwargs)\n\n    def call(self, inputs):\n        return tf.reduce_min(inputs, axis=[1, 2])\n\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0], input_shape[-1])\n\n    def get_config(self):\n        config = super(GlobalMinPooling2D, self).get_config()\n        return config\n\nclass DeeperGlobalLocalAttentionLayer1(layers.Layer):\n    def __init__(self, units, activation='sigmoid', dropout_rate=0.2, use_scale=True, axis=-1, **kwargs):\n        super(DeeperGlobalLocalAttentionLayer1, self).__init__(**kwargs)\n        self.units = units\n        self.activation = activation\n        self.dropout_rate = dropout_rate\n        self.use_scale = use_scale\n        self.axis = axis\n\n    def build(self, input_shape):\n        _, _, _, channels = input_shape\n        \n        \n        self.global_conv1 = layers.DepthwiseConv2D(kernel_size=(1, 1), activation=self.activation)\n        self.global_avg_pooling1 = layers.GlobalAveragePooling2D()\n        \n        self.global_conv2 = layers.DepthwiseConv2D(kernel_size=(1, 1), activation=self.activation)\n        self.global_avg_pooling2 = layers.GlobalMaxPooling2D()\n        \n        self.global_conv_3 = layers.DepthwiseConv2D(kernel_size=(1, 1), activation=self.activation)\n        self.global_avg_pooling_3 = GlobalMinPooling2D()\n        \n        \n        self.global_conv3 = layers.DepthwiseConv2D(kernel_size=(1, 1), activation=self.activation)\n        self.global_avg_pooling3 = layers.GlobalAveragePooling2D()\n        \n        self.global_conv4 = layers.DepthwiseConv2D(kernel_size=(1, 1), activation=self.activation)\n        self.global_avg_pooling4 = layers.GlobalMaxPooling2D()\n\n        self.global_conv_4 = layers.DepthwiseConv2D(kernel_size=(1, 1), activation=self.activation)\n        self.global_avg_pooling_4 = GlobalMinPooling2D()\n        \n        \n        self.concat1 = layers.Add()\n        self.concat2 = layers.Add()\n        self.concat3 = layers.Add()\n        self.concat4 = layers.Add()\n        self.concat_3 = layers.Add()\n        self.concat_4 = layers.Add()\n        \n        self.concat5 = layers.Concatenate(axis=-1)\n        \n        self.global_attention = layers.Dense(units=self.units, activation=self.activation)\n\n        #self.local_dsc = layers.DepthwiseConv2D(kernel_size=(3, 3), padding=\"same\", depth_multiplier=1)\n        self.local_conv1 = layers.DepthwiseConv2D(kernel_size=(1, 1), activation=self.activation)\n        self.local_conv2 = layers.DepthwiseConv2D(kernel_size=(1, 1), activation=self.activation)\n        \n        self.concat6 = layers.Add()\n        \n        if self.use_scale:\n            self.global_scale = self.add_weight(shape=(1, 1, 1, 1), initializer=tf.keras.initializers.HeNormal(), trainable=True, name='global_scale')\n            self.local_scale = self.add_weight(shape=(1, 1, 1, self.units), initializer=tf.keras.initializers.HeNormal(), trainable=True, name='local_scale')\n        \n        super(DeeperGlobalLocalAttentionLayer1, self).build(input_shape)\n\n    def call(self, inputs, training=None):\n        ##### Hierarchical Information Fusion Attention(HIFA) ######\n        \n        if not tf.sparse.is_sparse(inputs):\n            inputs = tf.sparse.from_dense(inputs)\n        \n        global_attention1 = self.global_conv1(inputs)\n        global_avg1 = self.global_avg_pooling1(global_attention1)\n        \n        global_attention2 = self.global_conv2(global_attention1)\n        global_avg2 = self.global_avg_pooling2(global_attention2)\n\n        global_attention_3 = self.global_conv_3(global_attention1)\n        global_avg_3 = self.global_avg_pooling_3(global_attention_3)\n        \n        \n        global_concat1 = self.concat1([global_avg1, global_avg2, global_avg_3])\n        global_sub1 = global_avg2 - global_avg1 - global_avg_3\n        global_concat1 = global_concat1 + global_sub1\n        \n        global_attention_concat1 = self.concat2([global_attention1, global_attention2, global_attention_3])\n        \n        global_sub_1 = global_attention2 - global_attention1 - global_attention_3\n\n        global_attention_concat1 = global_attention_concat1 + global_sub_1\n        \n        global_attention3 = self.global_conv3(global_attention_concat1)\n        global_avg3 = self.global_avg_pooling3(global_attention3)\n        \n        global_attention4 = self.global_conv4(global_attention3)\n        global_avg4 = self.global_avg_pooling4(global_attention4)\n\n        global_attention_4 = self.global_conv_3(global_attention3)\n        global_avg_4 = self.global_avg_pooling_3(global_attention_4)\n        \n        \n        global_concat2 = self.concat3([global_avg3, global_avg4, global_avg_4])\n        global_sub2 = global_avg4 - global_avg3 - global_avg_4\n        global_concat2 = global_concat2 + global_sub2\n\n        \n        #global_attention_concat2 = self.concat4([global_attention3, global_attention4, global_attention_4])\n        #global_sub_2 = global_attention4 - global_attention3 - global_attention_4\n\n        #global_attention_concat2 = global_attention_concat2 + global_sub_2\n\n        \n        \n        global_avg_concat = self.concat5([global_concat1, global_concat2])\n        \n        global_attention = self.global_attention(global_avg_concat)\n\n        if tf.sparse.is_sparse(global_attention):\n            global_attention = tf.sparse.to_dense(global_attention)\n            \n        #global_attention = L.Activation(\"relu\")(tf.expand_dims(tf.expand_dims(global_attention, 1), 1))\n        \n        global_attention = tf.expand_dims(tf.expand_dims(global_attention, 1), 1)\n\n        ##### frequency domain Local Information learning Attention ######\n        \n        input_shape = tf.shape(inputs)\n\n        \n        flattened_inputs = tf.reshape(inputs, [-1, input_shape[-1]])  # Flatten along the last axis\n        dct_transformed = tf.signal.dct(flattened_inputs, type=2, norm='ortho')\n        dct_transformed = tf.reshape(dct_transformed, input_shape)  # Reshape back to original dimensions\n        \n        local_attention1 = self.local_conv1(dct_transformed)\n        local_attention1 = tf.reduce_mean(local_attention1, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        local_attention2 = self.local_conv2(local_attention1)\n        local_attention2 = tf.reduce_mean(local_attention2, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n\n        '''local_attention3 = self.local_conv1(inputs)\n        local_attention3 = tf.reduce_max(local_attention3, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        local_attention4 = self.local_conv2(local_attention3)\n        local_attention4 = tf.reduce_max(local_attention4, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n\n        local_attention5 = self.local_conv1(inputs)\n        local_attention5 = tf.reduce_mean(local_attention5, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        local_attention6 = self.local_conv2(local_attention5)\n        local_attention6 = tf.reduce_mean(local_attention6, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions'''\n\n        local_attention7 = self.local_conv1(dct_transformed)\n        local_attention7 = tf.reduce_max(local_attention7, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        local_attention8 = self.local_conv2(local_attention7)\n        local_attention8 = tf.reduce_max(local_attention8, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n\n        local_attention9 = self.local_conv1(dct_transformed)\n        local_attention9 = tf.reduce_min(local_attention9, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        local_attention10 = self.local_conv2(local_attention9)\n        local_attention10 = tf.reduce_min(local_attention10, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n\n\n        \n        \n        #local_attention = self.concat6([local_attention1, local_attention2, local_attention3, local_attention4, \n         #                              local_attention5, local_attention6, local_attention7, local_attention8])\n        \n        local_attention_avg = (self.concat6([local_attention1, local_attention2])) \n        local_attention_max = self.concat6([local_attention7, local_attention8]) \n        local_attention_min = self.concat6([local_attention9, local_attention10]) \n\n        local_attention = self.concat6([local_attention_avg, local_attention_max, local_attention_min]) \n        \n        # Scale Global and Local Attention\n        if self.use_scale:\n            global_attention *= self.global_scale\n            local_attention *= self.local_scale\n\n        # Combine Global and Local Attention\n        attention = tf.sigmoid(global_attention + local_attention)\n        return attention\n\n    def get_config(self):\n        config = super(DeeperGlobalLocalAttentionLayer1, self).get_config()\n        config.update({'units': self.units, 'activation': self.activation, 'dropout_rate': self.dropout_rate,\n                       'use_scale': self.use_scale})\n        return config\n\n'''class DeeperAttentionLayer1(layers.Layer):\n    def __init__(self, units=64, use_scale=True, **kwargs):\n        super(DeeperAttentionLayer1, self).__init__(**kwargs)\n        self.units = units\n        self.use_scale = use_scale\n\n    def build(self, input_shape):\n        _, H, W, C = input_shape\n        self.alpha = self.add_weight(shape=(1, 1, 1, C), initializer='ones', trainable=True, name='alpha')\n        self.deeper_global_local_attention = DeeperGlobalLocalAttentionLayer1(units=self.units, activation='sigmoid', \n                                                                              dropout_rate=0.2,  # You can adjust the dropout rate\n                                                                              use_scale=self.use_scale)\n        super(DeeperAttentionLayer1, self).build(input_shape)\n\n    def call(self, inputs, training=None):\n        attention = self.deeper_global_local_attention(inputs, training=training)\n        attention_feature = inputs * attention * self.alpha\n        return attention_feature\n\n    def get_config(self):\n        config = super(DeeperAttentionLayer1, self).get_config()\n        config.update({'units': self.units, 'use_scale': self.use_scale})\n        return config'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T09:52:58.690566Z","iopub.execute_input":"2025-03-04T09:52:58.690877Z","iopub.status.idle":"2025-03-04T09:52:58.719151Z","shell.execute_reply.started":"2025-03-04T09:52:58.690855Z","shell.execute_reply":"2025-03-04T09:52:58.718329Z"}},"outputs":[{"execution_count":88,"output_type":"execute_result","data":{"text/plain":"\"class DeeperAttentionLayer1(layers.Layer):\\n    def __init__(self, units=64, use_scale=True, **kwargs):\\n        super(DeeperAttentionLayer1, self).__init__(**kwargs)\\n        self.units = units\\n        self.use_scale = use_scale\\n\\n    def build(self, input_shape):\\n        _, H, W, C = input_shape\\n        self.alpha = self.add_weight(shape=(1, 1, 1, C), initializer='ones', trainable=True, name='alpha')\\n        self.deeper_global_local_attention = DeeperGlobalLocalAttentionLayer1(units=self.units, activation='sigmoid', \\n                                                                              dropout_rate=0.2,  # You can adjust the dropout rate\\n                                                                              use_scale=self.use_scale)\\n        super(DeeperAttentionLayer1, self).build(input_shape)\\n\\n    def call(self, inputs, training=None):\\n        attention = self.deeper_global_local_attention(inputs, training=training)\\n        attention_feature = inputs * attention * self.alpha\\n        return attention_feature\\n\\n    def get_config(self):\\n        config = super(DeeperAttentionLayer1, self).get_config()\\n        config.update({'units': self.units, 'use_scale': self.use_scale})\\n        return config\""},"metadata":{}}],"execution_count":88},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#### Multi-branch fusion attention (MFA) module #####\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.signal import fft2d, dct\n\nclass GlobalMinPooling2D(layers.Layer):\n    def __init__(self, **kwargs):\n        super(GlobalMinPooling2D, self).__init__(**kwargs)\n\n    def call(self, inputs):\n        return tf.reduce_min(inputs, axis=[1, 2])\n\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0], input_shape[-1])\n\n    def get_config(self):\n        config = super(GlobalMinPooling2D, self).get_config()\n        return config\n\nclass DeeperGlobalLocalAttentionLayer1(layers.Layer):\n    def __init__(self, units, activation='sigmoid', dropout_rate=0.2, use_scale=True, axis=-1, **kwargs):\n        super(DeeperGlobalLocalAttentionLayer1, self).__init__(**kwargs)\n        self.units = units\n        self.activation = activation\n        self.dropout_rate = dropout_rate\n        self.use_scale = use_scale\n        self.axis = axis\n\n    def build(self, input_shape):\n        _, _, _, channels = input_shape\n        \n        \n        self.global_conv1 = layers.Conv2D(filters = channels, kernel_size=(1, 1), activation=self.activation, dilation_rate=1)\n        self.global_avg_pooling1 = layers.GlobalAveragePooling2D()\n        \n        #self.global_conv2 = layers.SeparableConv2D(filters = self.units, kernel_size=(1, 1), activation=self.activation)\n        self.global_conv2 = layers.DepthwiseConv2D(kernel_size=(3, 3), activation=self.activation, dilation_rate=1, padding=\"same\")\n\n        self.global_avg_pooling2 = layers.GlobalMaxPooling2D()\n        \n        self.global_conv_3 = layers.DepthwiseConv2D(kernel_size=(5, 5), activation=self.activation, dilation_rate=1, padding=\"same\")\n        self.global_avg_pooling_3 = GlobalMinPooling2D()\n\n        self.global_conv4 = layers.Conv2D(filters = channels, kernel_size=(5, 5), activation=self.activation, groups=8, padding=\"same\")\n        #self.global_conv5 = layers.Conv2D(filters = channels, kernel_size=(1, 1), activation=self.activation, padding=\"same\")\n        \n        \n        '''self.global_conv3 = layers.DepthwiseConv2D(kernel_size=(1, 1), activation=self.activation)\n        self.global_avg_pooling3 = layers.GlobalAveragePooling2D()\n        \n        self.global_conv4 = layers.DepthwiseConv2D(kernel_size=(1, 1), activation=self.activation)\n        self.global_avg_pooling4 = layers.GlobalMaxPooling2D()\n\n        self.global_conv_4 = layers.DepthwiseConv2D(kernel_size=(1, 1), activation=self.activation)\n        self.global_avg_pooling_4 = GlobalMinPooling2D()'''\n        \n        \n        self.concat1 = layers.Add()\n        self.concat2 = layers.Add()\n        self.concat3 = layers.Add()\n        self.concat4 = layers.Add()\n        self.concat_3 = layers.Add()\n        self.concat_4 = layers.Add()\n        \n        self.concat5 = layers.Concatenate(axis=-1)\n        \n        self.global_attention = layers.Dense(units=self.units, activation=self.activation)\n\n        #self.local_dsc = layers.DepthwiseConv2D(kernel_size=(3, 3), padding=\"same\", depth_multiplier=1)\n        self.local_conv1 = layers.DepthwiseConv2D(kernel_size=(1, 1), activation=self.activation)\n        self.local_conv2 = layers.DepthwiseConv2D(kernel_size=(1, 1), activation=self.activation)\n        self.local_conv3 = layers.DepthwiseConv2D(kernel_size=(1, 1), activation=self.activation)\n\n        self.local_conv4 = layers.DepthwiseConv2D(kernel_size=(1, 1), activation=self.activation)\n        self.batch_norm = layers.LayerNormalization()\n        \n        self.concat6 = layers.Add()\n        \n        if self.use_scale:\n            self.global_scale = self.add_weight(shape=(1, 1, 1, 1), initializer=tf.keras.initializers.HeNormal(), trainable=True, name='global_scale')\n            self.local_scale = self.add_weight(shape=(1, 1, 1, self.units), initializer=tf.keras.initializers.HeNormal(), trainable=True, name='local_scale')\n        \n        super(DeeperGlobalLocalAttentionLayer1, self).build(input_shape)\n\n    def call(self, inputs, training=None):\n        ##### Hierarchical Information Fusion Attention(HIFA) ######\n\n        input_shape = tf.shape(inputs)\n        \n        flattened_inputs = tf.reshape(inputs, [-1, input_shape[-1]])  # Flatten along the last axis\n        dct_transformed = tf.signal.dct(flattened_inputs, type=2, norm='ortho')\n        dct_transformed = tf.reshape(dct_transformed, input_shape)  # Reshape back to original dimensions\n        \n        \n        global_attention1 = self.global_conv1(dct_transformed)\n        global_avg1 = self.global_avg_pooling1(global_attention1)\n        global_avg2 = self.global_avg_pooling2(global_attention1)\n        global_avg3 = self.global_avg_pooling_3(global_attention1)\n\n        '''global_concat1 = self.concat1([global_avg1, global_avg2, global_avg3])\n        global_sub1 = global_avg2 - global_avg1 - global_avg3\n        global_concat1 = global_concat1 + global_sub1\n        '''\n\n        dct_transformed_ga = self.concat1([inputs, global_attention1]) #global_attention1 + dct_transformed\n        \n        global_attention2 = self.global_conv2(dct_transformed_ga)\n        global_avg4 = self.global_avg_pooling1(global_attention2)\n        global_avg5 = self.global_avg_pooling2(global_attention2)\n        global_avg6 = self.global_avg_pooling_3(global_attention2)\n\n        '''global_concat2 = self.concat1([global_avg4, global_avg5, global_avg6])\n        global_sub2 = global_avg5 - global_avg4 - global_avg6\n        global_concat2 = global_concat2 + global_sub2\n        '''\n\n        \n        #dct_transformed_ga = self.global_conv4(dct_transformed_ga)\n        #dct_transformed_ga = self.global_conv4(dct_transformed_ga)\n        dct_transformed_ga2 =  self.concat1([dct_transformed_ga, global_attention2]) #dct_transformed_ga + global_attention2\n\n        '''x1 = layers.Concatenate()([conv1x1, conv3x3, conv5x5])\n        x1 = layers.Conv2D(filters, kernel_size=1, groups=groups, padding=\"same\")(x1)\n\n        x = layers.Conv2D(filters=filters, kernel_size=(1, 1), strides=strides, padding='same')(x)\n        x = layers.Add()([x, x1])\n        x = layers.Activation('relu')(x)\n        '''\n        \n        global_attention3 = self.global_conv_3(dct_transformed_ga2)\n        global_avg7 = self.global_avg_pooling1(global_attention3)\n        global_avg8 = self.global_avg_pooling2(global_attention3)\n        global_avg9 = self.global_avg_pooling_3(global_attention3)\n\n        global_concat_avg = self.concat1([global_avg1, global_avg4, global_avg7])\n        global_concat_max = self.concat1([global_avg2, global_avg5, global_avg8])\n        global_concat_min = self.concat1([global_avg3, global_avg6, global_avg9])\n\n        global_concat_add = global_concat_avg + global_concat_max + global_concat_min \n        global_concat_sub = global_concat_max - global_concat_avg - global_concat_min \n        \n\n        global_avg_concat = global_concat_add + global_concat_sub  #self.concat5([global_concat_add, global_concat_sub])\n        global_attention = self.global_attention(global_avg_concat)\n        \n        global_attention = tf.expand_dims(tf.expand_dims(global_attention, 1), 1)\n\n        ##### frequency domain Local Information learning Attention ######\n        \n        input_shape = tf.shape(inputs)\n        batch_size, height, width, channels = inputs.shape\n        \n        flattened_inputs = tf.reshape(inputs, [-1, input_shape[-1]])  # Flatten along the last axis\n        dct_transformed = tf.signal.dct(flattened_inputs, type=2, norm='ortho')\n        dct_transformed = tf.reshape(dct_transformed, input_shape)  # Reshape back to original dimensions\n        \n        local_attention1 = self.local_conv1(dct_transformed)\n        local_attention1 = tf.reduce_mean(local_attention1, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        #local_max1 = tf.reduce_max(local_attention1, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        #local_min1 = tf.reduce_min(local_attention1, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        #local_attention_1 = local_avg1 + local_max1 + local_min1\n        \n        local_attention2 = self.local_conv2(dct_transformed)\n        #local_avg2 = tf.reduce_mean(local_attention2, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        local_attention2 = tf.reduce_max(local_attention2, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        #local_min2 = tf.reduce_min(local_attention2, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        #local_attention_2 = local_avg2 + local_max2 + local_min2\n\n        #local_attention2 = tf.reduce_mean(local_attention2, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n\n        local_attention3 = self.local_conv3(dct_transformed)\n        #local_avg3 = tf.reduce_mean(local_attention3, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        #local_max3 = tf.reduce_max(local_attention3, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        local_attention3 = tf.reduce_min(local_attention3, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        #local_attention_3 = local_avg3 + local_max3 + local_min3\n\n\n        \n        \n\n        '''#local_attention1 = tf.reshape(local_attention1, [batch_size, -1, self.units])\n        local_attention2 = tf.reshape(local_attention2, [batch_size, self.units, -1])\n        local_attention3 = tf.reshape(local_attention3, [batch_size, -1, self.units])\n\n        f = tf.matmul(local_attention1, local_attention2)\n        f_softmax = tf.nn.softmax(f, -1)\n        y = tf.matmul(f_softmax, local_attention3)\n        y = tf.reshape(y, [batch_size, height, width, self.units])\n\n        y = self.local_conv4(y)\n        y = self.batch_norm(y)'''\n\n\n        '''local_attention1 = tf.cast(local_attention1, tf.float32)\n        local_attention2 = tf.cast(local_attention2, tf.float32)\n        local_attention3 = tf.cast(local_attention3, tf.float32)\n\n        local_attention1_flattened = tf.reshape(local_attention1, [batch_size, height * width, channels])\n        local_attention2_flattened = tf.reshape(local_attention2, [batch_size, height * width, channels])\n        local_attention3_flattened = tf.reshape(local_attention3, [batch_size, height * width, channels])'''\n        \n        \n        \n        # Step 2: Perform the matrix multiplication and softmax operation\n        f = tf.matmul(local_attention1, local_attention2, transpose_b=True)  # Shape: (batch_size, height * width, height * width)\n        f_softmax = tf.nn.softmax(f, axis=-1)  # Softmax along the second axis (the spatial dimension)\n        \n        # Step 3: Multiply with local_attention3 and reshape\n        y = tf.matmul(f_softmax, local_attention3)  # Shape: (batch_size, height * width, channels)\n        #y = tf.reshape(y, [batch_size, height, width, channels])  # Reshaping to the original spatial dimensions\n        \n        # Step 4: Apply convolution and batch normalization\n        y = self.local_conv4(y)  # Convolution layer (you may have to define self.local_conv4 as a Conv2D layer)\n        y = self.batch_norm(y)  # Batch normalization\n        \n\n        \n\n        \n\n        '''local_attention3 = self.local_conv1(inputs)\n        local_attention3 = tf.reduce_max(local_attention3, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        local_attention4 = self.local_conv2(local_attention3)\n        local_attention4 = tf.reduce_max(local_attention4, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n\n        local_attention5 = self.local_conv1(inputs)\n        local_attention5 = tf.reduce_mean(local_attention5, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        local_attention6 = self.local_conv2(local_attention5)\n        local_attention6 = tf.reduce_mean(local_attention6, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions'''\n\n        \n\n        '''local_attention7 = self.local_conv1(dct_transformed)\n        local_attention7 = tf.reduce_max(local_attention7, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        local_attention8 = self.local_conv2(local_attention7)\n        local_attention8 = tf.reduce_max(local_attention8, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n\n        local_attention9 = self.local_conv1(dct_transformed)\n        local_attention9 = tf.reduce_min(local_attention9, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        local_attention10 = self.local_conv2(local_attention9)\n        local_attention10 = tf.reduce_min(local_attention10, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n\n\n        \n        \n        #local_attention = self.concat6([local_attention1, local_attention2, local_attention3, local_attention4, \n         #                              local_attention5, local_attention6, local_attention7, local_attention8])\n        \n        local_attention_avg = (self.concat6([local_attention1, local_attention2])) \n        local_attention_max = self.concat6([local_attention7, local_attention8]) \n        local_attention_min = self.concat6([local_attention9, local_attention10]) \n\n        local_attention = self.concat6([local_attention_avg, local_attention_max, local_attention_min]) '''\n        \n        # Scale Global and Local Attention\n        if self.use_scale:\n            global_attention *= self.global_scale\n            y *= self.local_scale\n\n        # Combine Global and Local Attention\n        attention = tf.sigmoid(global_attention + y)\n        return attention\n\n    def get_config(self):\n        config = super(DeeperGlobalLocalAttentionLayer1, self).get_config()\n        config.update({'units': self.units, 'activation': self.activation, 'dropout_rate': self.dropout_rate,\n                       'use_scale': self.use_scale})\n        return config\n\n'''class DeeperAttentionLayer1(layers.Layer):\n    def __init__(self, units=64, use_scale=True, **kwargs):\n        super(DeeperAttentionLayer1, self).__init__(**kwargs)\n        self.units = units\n        self.use_scale = use_scale\n\n    def build(self, input_shape):\n        _, H, W, C = input_shape\n        self.alpha = self.add_weight(shape=(1, 1, 1, C), initializer='ones', trainable=True, name='alpha')\n        self.deeper_global_local_attention = DeeperGlobalLocalAttentionLayer1(units=self.units, activation='sigmoid', \n                                                                              dropout_rate=0.2,  # You can adjust the dropout rate\n                                                                              use_scale=self.use_scale)\n        super(DeeperAttentionLayer1, self).build(input_shape)\n\n    def call(self, inputs, training=None):\n        attention = self.deeper_global_local_attention(inputs, training=training)\n        attention_feature = inputs * attention * self.alpha\n        return attention_feature\n\n    def get_config(self):\n        config = super(DeeperAttentionLayer1, self).get_config()\n        config.update({'units': self.units, 'use_scale': self.use_scale})\n        return config'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T06:00:40.784901Z","iopub.execute_input":"2025-03-09T06:00:40.785129Z","iopub.status.idle":"2025-03-09T06:00:40.812552Z","shell.execute_reply.started":"2025-03-09T06:00:40.785111Z","shell.execute_reply":"2025-03-09T06:00:40.811677Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"\"class DeeperAttentionLayer1(layers.Layer):\\n    def __init__(self, units=64, use_scale=True, **kwargs):\\n        super(DeeperAttentionLayer1, self).__init__(**kwargs)\\n        self.units = units\\n        self.use_scale = use_scale\\n\\n    def build(self, input_shape):\\n        _, H, W, C = input_shape\\n        self.alpha = self.add_weight(shape=(1, 1, 1, C), initializer='ones', trainable=True, name='alpha')\\n        self.deeper_global_local_attention = DeeperGlobalLocalAttentionLayer1(units=self.units, activation='sigmoid', \\n                                                                              dropout_rate=0.2,  # You can adjust the dropout rate\\n                                                                              use_scale=self.use_scale)\\n        super(DeeperAttentionLayer1, self).build(input_shape)\\n\\n    def call(self, inputs, training=None):\\n        attention = self.deeper_global_local_attention(inputs, training=training)\\n        attention_feature = inputs * attention * self.alpha\\n        return attention_feature\\n\\n    def get_config(self):\\n        config = super(DeeperAttentionLayer1, self).get_config()\\n        config.update({'units': self.units, 'use_scale': self.use_scale})\\n        return config\""},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers\n\n# Frequency Transform Layer\n'''class FrequencyTransformLayer(layers.Layer):\n    def __init__(self, transform_type='dct', **kwargs):\n        super(FrequencyTransformLayer, self).__init__(**kwargs)\n        self.transform_type = transform_type\n\n    def call(self, inputs):\n        # Ensure inputs are float32 for DCT and FFT operations\n        #inputs = tf.cast(inputs, tf.float32)\n\n        if self.transform_type == 'dct':\n            # Apply 2D DCT along the last axis\n            input_shape = tf.shape(inputs)\n            flattened_inputs = tf.reshape(inputs, [-1, input_shape[-1]])  # Flatten along the last axis\n            dct_transformed = tf.signal.dct(flattened_inputs, type=2, norm='ortho', axis = -1)\n            return tf.reshape(dct_transformed, input_shape)  # Reshape back to original dimensions\n        elif self.transform_type == 'fft':\n            # Apply 2D FFT and return the magnitude\n            fft_transformed = tf.signal.fft2d(tf.cast(inputs, tf.complex64))\n            return tf.math.abs(fft_transformed)\n        else:\n            raise ValueError(\"Unsupported transform type. Choose 'dct' or 'fft'.\")\n\n    def get_config(self):\n        config = super(FrequencyTransformLayer, self).get_config()\n        config.update({'transform_type': self.transform_type})\n        return config'''\n\n# DCT Transform Function\n'''def dct_transform(inputs):\n    # Apply DCT transform to the inputs\n    dct_input = FrequencyTransformLayer(transform_type='dct')(inputs)  # Use FrequencyTransformLayer for DCT\n    \n    # Apply global average pooling (GAP), global max pooling (GMP), and global min pooling (GMP_MIN)\n    gap = tf.reduce_mean(dct_input, axis=[1, 2], keepdims=True)\n    gmp = tf.reduce_max(dct_input, axis=[1, 2], keepdims=True)\n    gmp_min = tf.reduce_min(dct_input, axis=[1, 2], keepdims=True)\n    \n    # Combine the pooled features\n    pooled_features1 = gap + gmp + gmp_min\n    pooled_features2 = gmp - (gap - gmp_min)\n    pooled_features = pooled_features1 + pooled_features2\n    \n    return pooled_features'''\n\n\n\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\n# 1D DCT from scratch using TensorFlow\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\n# 1D DCT from scratch using TensorFlow\n'''def dct1d(x):\n    \"\"\"1D Discrete Cosine Transform.\"\"\"\n    N = tf.shape(x)[0]\n    k = tf.range(N, dtype=tf.float32)  # Create range for k\n    n = tf.range(N, dtype=tf.float32)  # Create range for n\n    # Using broadcasting to calculate the DCT\n    X = tf.reduce_sum(x * tf.cos(np.pi / N * (n + 0.5)[:, None] * k[None, :]), axis=0)\n    return X\n\n# 2D DCT from scratch using TensorFlow\ndef dct2d(x):\n    \"\"\"2D Discrete Cosine Transform (DCT). Applies DCT row-wise and column-wise.\"\"\"\n    # Apply DCT to each row (along the last axis)\n    x_dct_rows = tf.map_fn(dct1d, x, dtype=tf.float32)\n    # Apply DCT to each column (along the second last axis)\n    x_dct = tf.map_fn(lambda x: dct1d(tf.transpose(x)), x_dct_rows, dtype=tf.float32)\n    return x_dct\n\n\nimport tensorflow as tf\n\n# 1D DCT from scratch using TensorFlow\ndef dct1d(x):\n    \"\"\"1D Discrete Cosine Transform.\"\"\"\n    N = tf.shape(x)[0]\n    k = tf.range(N, dtype=tf.int32)  # Create range for k\n    n = tf.range(N, dtype=tf.int32)  # Create range for n\n    pi = tf.constant(3, dtype=tf.int32)  # Explicitly use TensorFlow constant for pi\n    \n    # Using broadcasting to calculate the DCT\n    X = tf.reduce_sum(x * tf.cos(3 / N * (n + 0.5)[:, None] * k[None, :]), axis=0)\n    return X\n\n# 2D DCT from scratch using TensorFlow\ndef dct2d(x):\n    \"\"\"2D Discrete Cosine Transform (DCT). Applies DCT row-wise and column-wise.\"\"\"\n    # Apply DCT to each row (along the last axis)\n    x_dct_rows = tf.map_fn(dct1d, x, dtype=tf.int32)\n    # Apply DCT to each column (along the second last axis)\n    x_dct = tf.map_fn(lambda x: dct1d(tf.transpose(x)), x_dct_rows, dtype=tf.int32)\n    return x_dct\n\n# Custom FrequencyTransformLayer class\nclass FrequencyTransformLayer(layers.Layer):\n    def __init__(self, transform_type='dct', **kwargs):\n        super(FrequencyTransformLayer, self).__init__(**kwargs)\n        self.transform_type = transform_type\n\n    def call(self, inputs):\n        # Ensure inputs are float32 for DCT and FFT operations\n        inputs = tf.cast(inputs, tf.float32)\n\n        if self.transform_type == 'dct':\n            # Apply 2D DCT from scratch (using custom dct2d function)\n            input_shape = tf.shape(inputs)\n            flattened_inputs = tf.reshape(inputs, [-1, input_shape[-1]])  # Flatten along the last axis\n            \n            # Perform the 2D DCT using the custom scratch implementation\n            dct_transformed = dct2d(flattened_inputs)  # Perform 2D DCT\n            \n            # Reshape back to the original dimensions\n            return tf.reshape(dct_transformed, input_shape)  # Reshape back to original dimensions\n\n        elif self.transform_type == 'fft':\n            # Apply 2D FFT and return the magnitude\n            fft_transformed = tf.signal.fft2d(tf.cast(inputs, tf.complex64))\n            return tf.math.abs(fft_transformed)\n\n        else:\n            raise ValueError(\"Unsupported transform type. Choose 'dct' or 'fft'.\")\n\n    def get_config(self):\n        config = super(FrequencyTransformLayer, self).get_config()\n        config.update({'transform_type': self.transform_type})\n        return config\n'''\n\n\n\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Layer\n\n'''class DCTTransformLayer(Layer):\n    def __init__(self, **kwargs):\n        super(DCTTransformLayer, self).__init__(**kwargs)\n        # Optionally initialize any additional parameters here\n        #self.frequency_transform = FrequencyTransformLayer(transform_type='dct')\n\n    def build(self, input_shape):\n        # This method is used for any initialization specific to the layer, such as creating weights or other components\n        pass\n\n    def call(self, inputs):\n        # Apply DCT transform to the inputs\n        #dct_input = self.frequency_transform(inputs)  # Using self to reference the FrequencyTransformLayer instance\n        \n        # Apply global average pooling (GAP), global max pooling (GMP), and global min pooling (GMP_MIN)\n        gap = tf.reduce_mean(inputs, axis=[1, 2], keepdims=True)\n        gmp = tf.reduce_max(inputs, axis=[1, 2], keepdims=True)\n        gmp_min = tf.reduce_min(inputs, axis=[1, 2], keepdims=True)\n        \n        # Combine the pooled features\n        pooled_features1 = gap + gmp + gmp_min\n        pooled_features2 = gmp - (gap - gmp_min)\n        pooled_features = pooled_features1 + pooled_features2\n        \n        return pooled_features\n\n\n\n\n# Frequency Attention\nimport tensorflow as tf\n\n\n# DCT Attention Non-Local Block\nclass DCTAttentionNonLocalBlock(layers.Layer):\n    def __init__(self, in_channels, intermediate_channels, dct_threshold=0.1):\n        super(DCTAttentionNonLocalBlock, self).__init__()\n        self.in_channels = in_channels\n        self.intermediate_channels = intermediate_channels\n        self.dct_threshold = dct_threshold\n\n\n        #self.dct_transform_layer = DCTTransformLayer()\n\n        \n        \n        # Define your layers here\n        self.theta = layers.Conv2D(intermediate_channels, kernel_size=1, use_bias=False)\n        self.phi = layers.Conv2D(intermediate_channels, kernel_size=1, use_bias=False)\n        self.g = layers.Conv2D(intermediate_channels, kernel_size=1, use_bias=False)\n        self.W = layers.Conv2D(in_channels, kernel_size=1, use_bias=False)\n        \n        self.attn_refine = tf.keras.Sequential([\n            layers.Conv2D(in_channels, kernel_size=3, padding='same', use_bias=False),\n            layers.ReLU(),\n            layers.Conv2D(in_channels, kernel_size=3, padding='same', use_bias=False)\n        ])\n        \n        self.channel_match_conv = layers.Conv2D(intermediate_channels, kernel_size=1)\n        self.reduce_channels_dct = layers.Conv2D(intermediate_channels, kernel_size=1)\n\n        self.relative_bias_dct = tf.Variable(tf.zeros((1, 1, 1, 1)), trainable=True)\n        self.relative_bias_non_local = tf.Variable(tf.zeros((1, 1, 1, 1)), trainable=True)\n\n    def call(self, inputs):\n        # Get the shape of the input\n        shape = tf.shape(inputs)\n        batch_size, height, width = shape[0], shape[1], shape[2]\n\n        # Process the inputs\n        # Global attention\n        gap = tf.reduce_mean(inputs, axis=[1, 2], keepdims=True)\n        gmp = tf.reduce_max(inputs, axis=[1, 2], keepdims=True)\n        gmp_min = tf.reduce_min(inputs, axis=[1, 2], keepdims=True)\n        \n        # Combine the pooled features\n        pooled_features1 = gap + gmp + gmp_min\n        pooled_features2 = gmp - (gap - gmp_min)\n        \n        #pooled_features = pooled_features1 + pooled_features2\n        \n        #dct_input = self.dct_transform_layer(inputs)  # Apply DCT transform\n        #dct_input = dct_transform(inputs)  # Apply DCT transform\n\n        \n        dct_attention = tf.sigmoid((pooled_features1 + pooled_features2) * self.relative_bias_dct) #frequency_attention(dct_input, self.dct_threshold) + self.relative_bias_dct\n\n        theta_x = tf.reshape(self.theta(inputs), [batch_size, height * width, self.intermediate_channels])\n        phi_x = tf.reshape(self.phi(inputs), [batch_size, height * width, self.intermediate_channels])\n        g_x = tf.reshape(self.g(inputs), [batch_size, height * width, self.intermediate_channels])\n\n        f = tf.matmul(theta_x, phi_x, transpose_b=True)\n        f_softmax = tf.nn.softmax(f, axis=-1)\n        y = tf.matmul(f_softmax, g_x)\n        y = tf.reshape(y, [batch_size, height, width, self.intermediate_channels])\n\n        refined_attention = self.attn_refine(inputs) + self.relative_bias_non_local\n        refined_attention = self.channel_match_conv(refined_attention)\n\n        scale_factor = tf.sigmoid(refined_attention)\n        #refined_dct_attention = self.reduce_channels_dct(dct_transform(dct_attention))\n        dct_attention = self.reduce_channels_dct(dct_attention)\n\n        y1 = y + (y * scale_factor) + refined_attention\n        y2 = y - (y * scale_factor) - refined_attention\n        y = tf.sigmoid(y1 + y2 + dct_attention)\n\n        y = self.W(y)\n        return tf.nn.relu(inputs + y)\n\n    def compute_output_shape(self, input_shape):\n        \"\"\"\n        Manually compute the output shape of the layer.\n        The output shape will be the same as the input shape.\n        \"\"\"\n        # The output will have the same dimensions as the input (batch_size, height, width, channels)\n        batch_size = input_shape[0]\n        height = input_shape[1]\n        width = input_shape[2]\n        channels = self.in_channels\n        return (batch_size, height, width, channels)\n'''\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\n'''# Custom Self-Attention Layer\nclass SelfAttention(layers.Layer):\n    def __init__(self, channels, **kwargs):\n        super(SelfAttention, self).__init__(**kwargs)\n        self.channels = channels\n        self.query_conv = layers.Conv2D(channels // 8, kernel_size=1)\n        self.key_conv = layers.Conv2D(channels // 8, kernel_size=1)\n        self.value_conv = layers.Conv2D(channels, kernel_size=1)\n        self.attn_conv = layers.Conv2D(channels, kernel_size=1)\n\n    def call(self, inputs):\n        # Generate Q, K, V matrices\n        query = self.query_conv(inputs)\n        key = self.key_conv(inputs)\n        value = self.value_conv(inputs)\n\n        # Perform dot product between query and key (self-attention)\n        attn_map = tf.matmul(query, key, transpose_b=True)\n        attn_map = tf.nn.softmax(attn_map, axis=-1)\n\n        # Weighted sum of value vectors\n        attn_out = tf.matmul(attn_map, value)\n\n        # Final attention output\n        attn_out = self.attn_conv(attn_out)\n        return attn_out + inputs  # Add residual connection\n'''\n\n\nfrom tensorflow.keras import layers\n\ndef multi_kernel_groupwise_conv2(x, filters, groups=16, strides=1):\n    # 1x1 Group-wise Convolution\n    conv1x1 = layers.DepthwiseConv2D(kernel_size=1, strides=strides, padding=\"same\")(x)\n\n    # 3x3 Group-wise Convolution\n    conv3x3 = layers.DepthwiseConv2D(kernel_size=5, strides=strides, padding=\"same\")(x)\n\n    # 5x5 Group-wise Convolution\n    conv5x5 = layers.DepthwiseConv2D(kernel_size=7, strides=strides, padding=\"same\")(x)\n\n    # Depthwise 3x3 Group-wise Convolution\n    #depthwise3x3 = layers.DepthwiseConv2D(kernel_size=3, strides=strides, padding=\"same\")(x)\n    \n    # Concatenate all outputs along the channel axis\n    x1 = layers.Concatenate()([conv1x1, conv3x3, conv5x5])\n    x1 = layers.Conv2D(filters, kernel_size=1, groups=groups, padding=\"same\")(x1)\n\n    x = layers.Conv2D(filters=filters, kernel_size=(1, 1), strides=strides, padding='same')(x)\n    x = layers.Add()([x, x1])\n    x = layers.Activation('relu')(x)\n    \n    return x\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\nclass MultiKernelGroupwiseConv2(layers.Layer):\n    def __init__(self, filters, groups=16, strides=1, **kwargs):\n        super(MultiKernelGroupwiseConv2, self).__init__(**kwargs)\n        self.filters = filters\n        self.groups = groups\n        self.strides = strides\n\n        # 1x1 Group-wise Convolution\n        self.conv1x1 = layers.DepthwiseConv2D(kernel_size=1, strides=strides, padding=\"same\")\n\n        # 3x3 Group-wise Convolution\n        self.conv3x3 = layers.DepthwiseConv2D(kernel_size=5, strides=strides, padding=\"same\")\n\n        # 5x5 Group-wise Convolution\n        self.conv5x5 = layers.DepthwiseConv2D(kernel_size=7, strides=strides, padding=\"same\")\n\n        # Final 1x1 Group-wise Convolution\n        self.final_conv = layers.Conv2D(filters, kernel_size=1, groups=groups, padding=\"same\")\n\n        # Shortcut Path\n        self.shortcut_conv = layers.Conv2D(filters=filters, kernel_size=(1, 1), strides=strides, padding='same')\n\n        # Activation Function\n        self.activation = layers.Activation('relu')\n\n    def call(self, inputs):\n        conv1x1 = self.conv1x1(inputs)\n        conv3x3 = self.conv3x3(inputs)\n        conv5x5 = self.conv5x5(inputs)\n\n        # Concatenate along the channel axis\n        x1 = layers.Concatenate()([conv1x1, conv3x3, conv5x5])\n        x1 = self.final_conv(x1)\n\n        # Shortcut connection\n        x = self.shortcut_conv(inputs)\n        x = layers.Add()([x, x1])\n        x = self.activation(x)\n\n        return x\n\n\n# Custom Attention Block with Global Pooling\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\nclass GlobalMinPooling2D(layers.Layer):\n    def __init__(self, **kwargs):\n        super(GlobalMinPooling2D, self).__init__(**kwargs)\n\n    def call(self, inputs):\n        return tf.reduce_min(inputs, axis=[1, 2])\n\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0], input_shape[-1])\n\n    def get_config(self):\n        config = super(GlobalMinPooling2D, self).get_config()\n        return config\n\n\nclass AttentionBlock(layers.Layer):\n    def __init__(self, channels, reduction_ratio=8, use_scale = True, **kwargs):\n        super(AttentionBlock, self).__init__(**kwargs)\n        self.channels = channels\n        #inputs1, inputs2 = inputs\n\n        # Global Pooling (Average, Max, and Min)\n        self.gap = layers.GlobalAveragePooling2D()\n        self.gmp = layers.GlobalMaxPooling2D()\n        self.gmin = GlobalMinPooling2D()  #layers.Lambda(lambda x: tf.reduce_min(x, axis=[1, 2], keepdims=False))  # Min pooling\n\n        # Efficient Channel Attention (Replaces Dense Layer)\n        \n        #self.channel_attn = layers.DepthwiseConv2D(kernel_size=1)\n        #self.channel_attn_out = layers.DepthwiseConv2D(kernel_size=1)\n\n        # Spatial Attention (Lightweight Conv)\n        #self.spatial_attn = layers.Conv2D(1, kernel_size=3, padding=\"same\", activation=\"sigmoid\")\n\n        # Batch Normalization\n        self.batch_norm = layers.BatchNormalization()\n        \n        '''self.weight1 = self.add_weight(\n            shape=(1,1,1,1), initializer=initializers.HeNormal(), trainable=True, name=\"weight1\"\n        )\n        self.weight2 = self.add_weight(\n            shape=(1,1,1,1), initializer=initializers.RandomNormal(mean=0.0, stddev=0.02), trainable=True, name=\"weight2\"\n        )'''\n        \n        #self.weight1 = self.add_weight(shape=(1, 1, 1, 1),  initializer=tf.keras.initializers.HeNormal(), trainable=True, name='weight1', \n         #                                     constraint=tf.keras.constraints.MaxNorm(2.0))\n        \n        self.weight1 = self.add_weight(shape=(1, 1, 1, 1),  initializer=tf.keras.initializers.HeNormal(), trainable=True, name='weight2', \n                                              constraint=tf.keras.constraints.MaxNorm(2.0))\n\n        self.weight2 = self.add_weight(shape=(1, 1, 1, 1),  initializer=tf.keras.initializers.HeNormal(), trainable=True, name='weight3', \n                                              constraint=tf.keras.constraints.MaxNorm(2.0))\n\n        self.weight3 = self.add_weight(shape=(1, 1, 1, 1), initializer='ones', trainable=True, name='alpha2')\n        \n            \n        self.dropout = layers.Dropout(0.25)\n        self.bn = layers.LayerNormalization()\n\n        self.use_scale = use_scale\n        self.local_attn = DeeperGlobalLocalAttentionLayer1(units=self.channels, activation='sigmoid', \n                                                                              dropout_rate=0.2,  # You can adjust the dropout rate\n                                                                              use_scale=self.use_scale)\n\n        self.multi_kernel_layer = MultiKernelGroupwiseConv2(filters=self.channels, groups=16, strides=1)\n        #output = multi_kernel_layer(input_tensor)\n\n        \n    def call(self, inputs):\n        inputs1, inputs2 = inputs\n\n        \n        # Compute Global Average, Max, and Min Pooling\n        gap_out = self.gap(inputs1)\n        gmp_out = self.gmp(inputs1)\n        gmin_out = self.gmin(inputs1)\n\n        # Combine pooling features\n        combined_pool1 = gap_out + gmp_out + gmin_out\n        combined_pool2 = gmp_out - (gap_out - gmin_out)\n        \n        # Normalize for stability\n        #combined_pool1 = tf.nn.l2_normalize(combined_pool1, axis=-1)\n        \n        combined_pool1 = tf.expand_dims(tf.expand_dims(combined_pool1, 1), 1) \n        combined_pool2 = tf.expand_dims(tf.expand_dims(combined_pool2, 1), 1)\n\n        combined_pool1 = self.bn(combined_pool1)\n        combined_pool1 = self.dropout(combined_pool1)\n\n        combined_pool2 = self.bn(combined_pool2)\n        combined_pool2 = self.dropout(combined_pool2)\n        \n        combined_pool1 = combined_pool1 * self.weight1\n        combined_pool2 = combined_pool2 * self.weight1\n        combined_pool = combined_pool1 + combined_pool2\n\n\n\n        gap_out2 = self.gap(inputs2)\n        gmp_out2 = self.gmp(inputs2)\n        gmin_out2 = self.gmin(inputs2)\n\n        # Combine pooling features\n        combined_pool3 = gap_out2 + gmp_out2 + gmin_out2\n        combined_pool4 = gmp_out2 - (gap_out2 - gmin_out2)\n        \n        # Normalize for stability\n        #combined_pool1 = tf.nn.l2_normalize(combined_pool1, axis=-1)\n        \n        combined_pool3 = tf.expand_dims(tf.expand_dims(combined_pool3, 1), 1) \n        combined_pool4 = tf.expand_dims(tf.expand_dims(combined_pool4, 1), 1)\n\n        combined_pool3 = self.bn(combined_pool3)\n        combined_pool3 = self.dropout(combined_pool3)\n\n        combined_pool4 = self.bn(combined_pool4)\n        combined_pool4 = self.dropout(combined_pool4)\n        \n        combined_pool3 = combined_pool3 * self.weight1\n        combined_pool4 = combined_pool4 * self.weight1\n        combined_pool5 = combined_pool3 + combined_pool4\n\n        \n        \n\n        #combined_pool = tf.sigmoid(combined_pool1 + combined_pool2) \n        \n        #combined_pool = inputs * combined_pool * self.weight3\n\n        # Compute multiscale information for input modalitiy = 1\n        channel_attn = self.multi_kernel_layer(inputs1)  #multi_kernel_groupwise_conv2(inputs1, filters=self.channels, groups=16)  # Strides=1 to avoid mismatch   #self.channel_attn(inputs2)\n        gap = tf.reduce_mean(channel_attn, axis=[1, 2], keepdims=True)\n        gmp = tf.reduce_max(channel_attn, axis=[1, 2], keepdims=True)\n        gmp_min = tf.reduce_min(channel_attn, axis=[1, 2], keepdims=True)\n\n        pooled_features1 = gap + gmp + gmp_min\n        pooled_features2 = gmp - (gap - gmp_min)\n\n        pooled_features = pooled_features1 + pooled_features2\n\n        channel_attn = channel_attn + pooled_features \n        channel_attn = self.bn(channel_attn)\n        channel_attn = self.dropout(channel_attn)\n        channel_attn = channel_attn * self.weight2\n\n        # Compute multiscale information for input modalitiy = 2\n        channel_attn2 = self.multi_kernel_layer(inputs2)     #multi_kernel_groupwise_conv2(inputs2, filters=self.channels, groups=16)  # Strides=1 to avoid mismatch   #self.channel_attn_out(pooled_features)  # Sigmoid attention map\n        #print('channel_attn shape:', channel_attn.shape)\n\n        gap2 = tf.reduce_mean(channel_attn2, axis=[1, 2], keepdims=True)\n        gmp2 = tf.reduce_max(channel_attn2, axis=[1, 2], keepdims=True)\n        gmp_min2 = tf.reduce_min(channel_attn2, axis=[1, 2], keepdims=True)\n\n        pooled_features3 = gap2 + gmp2 + gmp_min2\n        pooled_features4 = gmp2 - (gap2 - gmp_min2)\n\n        pooled_features5 = pooled_features3 + pooled_features4\n        \n        channel_attn2 = channel_attn2 + pooled_features5 \n        channel_attn2 = self.bn(channel_attn2)\n        channel_attn2 = self.dropout(channel_attn2)\n        channel_attn2 = channel_attn2 * self.weight2\n\n\n        ## Cross-modal attention\n        cross_attention1 = tf.sigmoid(combined_pool + channel_attn2)\n        cross_attention2 = tf.sigmoid(combined_pool5 + channel_attn)\n\n        multi_modal_attn = tf.sigmoid(cross_attention1 + cross_attention2)\n        \n        local1 = self.local_attn(inputs1) \n        local2 = self.local_attn(inputs2)\n\n        \n        \n        \n        # Apply Cross Attention Maps\n        attention1 =  tf.sigmoid(multi_modal_attn + local2) \n        attention2 =  tf.sigmoid(multi_modal_attn + local1) \n        \n        att1 = inputs1 * attention2 * self.weight3\n        att2 = inputs2 * attention1 * self.weight3\n        \n        print('output attention 1 shape:', att1.shape)\n        print('output attention 2 shape:', att2.shape)\n\n        return att1, att2\n\n\n# Residual Attention Block (Using Spatial Attention)\n'''class ResidualAttentionBlock(layers.Layer):\n    def __init__(self, in_channels, **kwargs):\n        super(ResidualAttentionBlock, self).__init__(**kwargs)\n        self.in_channels = in_channels\n        self.attn_block = AttentionBlock(in_channels)\n\n        # Convolution layers for feature refinement\n        #self.dwc = layers.DepthwiseConv2D(kernel_size=(3, 3), padding=\"same\", depth_multiplier=1)\n        self.conv1 = layers.Conv2D(in_channels, kernel_size=1, padding=\"same\")\n        self.relu = layers.ReLU()\n        self.conv2 = layers.Conv2D(in_channels, kernel_size=1, padding=\"same\")\n\n    def call(self, inputs):\n        # Apply attention mechanism\n        attn_out = self.attn_block(inputs)\n\n        # Refining features\n        #x = self.dwc(attn_out)\n        x = self.conv1(attn_out)\n        x = self.relu(x)\n        #x = self.dwc(x)\n        x = self.conv2(x)\n\n        # Add residual connection\n        return inputs * tf.sigmoid(x + attn_out)  # Residual connection for better feature learning\n'''\n\n\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\n'''def multi_kernel_groupwise_conv1(x, filters, groups=8, strides=2):\n    \"\"\" Advanced Multi-Kernel Groupwise Convolution Block (Without Attention, Optimized) \"\"\"\n\n    # Multi-Kernel Parallel Convolutions with Different Receptive Fields\n    conv1x1 = layers.Conv2D(filters // 4, kernel_size=1, groups=groups, padding=\"same\")(x)\n    conv3x3 = layers.Conv2D(filters // 4, kernel_size=3, padding=\"same\")(x)\n    conv5x5 = layers.Conv2D(filters // 4, kernel_size=5, padding=\"same\")(x)\n\n    # Dilated Convolutions for Larger Receptive Field (Helps Capture Global Context)\n    conv_dilated = layers.Conv2D(filters // 4, kernel_size=3, dilation_rate=2, padding=\"same\")(x)\n\n    # Feature Fusion via Concatenation\n    x1 = layers.Concatenate()([conv1x1, conv3x3, conv5x5, conv_dilated])\n\n    # Channel Shuffle for Better Feature Mixing (Inspired by ShuffleNet)\n    def channel_shuffle(x, groups):\n        batch, height, width, channels = x.shape\n        x = tf.reshape(x, [-1, height, width, groups, channels // groups])\n        x = tf.transpose(x, [0, 1, 2, 4, 3])\n        x = tf.reshape(x, [-1, height, width, channels])\n        return x\n\n    x1 = layers.Lambda(lambda x: channel_shuffle(x, groups))(x1)\n\n    # Depthwise + Grouped Convolutions Hybrid (Efficient Feature Extraction)\n    x1 = layers.DepthwiseConv2D(kernel_size=3, padding=\"same\")(x1)\n    x1 = layers.Conv2D(filters, kernel_size=1, groups=groups, padding=\"same\")(x1)\n\n    # Strided Convolution for Downsampling Instead of Extra 1x1 Conv\n    x = layers.Conv2D(filters, kernel_size=5, strides=strides, padding=\"same\")(x)\n\n    # Residual Connection for Gradient Flow\n    x = layers.Add()([x, x1])\n    x = layers.Activation(\"relu\")(x)\n    \n    return x'''\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\n'''def multi_kernel_groupwise_conv1(x, filters, groups=8, strides=2):\n    \"\"\" Advanced Multi-Kernel Groupwise Convolution Block (Without Attention, Optimized) \"\"\"\n\n    # Multi-Kernel Parallel Convolutions with Different Receptive Fields\n    conv1x1 = layers.Conv2D(filters // 4, kernel_size=1, groups=groups, padding=\"same\")(x)\n    conv3x3 = layers.DepthwiseConv2D(kernel_size=3, padding=\"same\")(x)\n    conv5x5 = layers.DepthwiseConv2D(kernel_size=5, padding=\"same\")(x)\n\n    # Dilated Convolutions for Larger Receptive Field\n    conv_dilated = layers.DepthwiseConv2D(kernel_size=3, dilation_rate=2, padding=\"same\")(x)\n\n    # Feature Fusion via Concatenation\n    x1 = layers.Concatenate()([conv1x1, conv3x3, conv5x5, conv_dilated])\n\n    # Channel Shuffle for Better Feature Mixing\n    def channel_shuffle(x, groups):\n        batch, height, width, channels = x.shape\n        x = tf.reshape(x, [-1, height, width, groups, channels // groups])\n        x = tf.transpose(x, [0, 1, 2, 4, 3])\n        x = tf.reshape(x, [-1, height, width, channels])\n        return x\n\n    x1 = layers.Lambda(lambda x: channel_shuffle(x, groups))(x1)\n\n    # Depthwise + Grouped Convolutions Hybrid\n    x1 = layers.DepthwiseConv2D(kernel_size=3, padding=\"same\")(x1)\n    x1 = layers.Conv2D(filters, kernel_size=1, groups=groups, padding=\"same\")(x1)\n\n    # **FIX: Ensure x1 is downsampled before addition**\n    x1 = layers.DepthwiseConv2D(kernel_size=3, strides=strides, padding=\"same\")(x1)\n\n    # Strided Convolution for Downsampling\n    x = layers.DepthwiseConv2D(kernel_size=3, strides=strides, padding=\"same\")(x)\n\n    # **Now x and x1 have the same shape before addition**\n    x = layers.Add()([x, x1])\n    x = layers.Activation(\"relu\")(x)\n    \n    return x'''\n\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\ndef multi_kernel_groupwise_conv1(x, filters, groups=8, strides=2):\n    \"\"\" Advanced Multi-Kernel Groupwise Convolution Block (Without Attention, Optimized) \"\"\"\n\n    # Multi-Kernel Parallel Convolutions with Different Receptive Fields\n    conv1x1 = layers.Conv2D(filters // 4, kernel_size=1, padding=\"same\")(x)\n    conv3x3 = layers.DepthwiseConv2D(kernel_size=3, padding=\"same\")(x)\n    conv5x5 = layers.DepthwiseConv2D(kernel_size=5, padding=\"same\")(x)\n    conv_dilated = layers.DepthwiseConv2D(kernel_size=3, dilation_rate=2, padding=\"same\")(x)\n\n    # Feature Fusion via Concatenation\n    x1 = layers.Concatenate()([conv1x1, conv3x3, conv5x5, conv_dilated])\n\n    # Channel Shuffle for Better Feature Mixing\n    def channel_shuffle(x, groups):\n        batch, height, width, channels = tf.unstack(tf.shape(x))\n        x = tf.reshape(x, [-1, height, width, groups, channels // groups])\n        x = tf.transpose(x, [0, 1, 2, 4, 3])\n        x = tf.reshape(x, [-1, height, width, channels])\n        return x\n\n    x1 = layers.Lambda(lambda x: channel_shuffle(x, groups))(x1)\n\n    # Depthwise + Grouped Convolutions Hybrid\n    x1 = layers.DepthwiseConv2D(kernel_size=3, padding=\"same\")(x1)\n    x1 = layers.Conv2D(filters, kernel_size=1, padding=\"same\")(x1)\n\n    # Downsampling x1\n    x1 = layers.DepthwiseConv2D(kernel_size=3, strides=strides, padding=\"same\")(x1)\n\n    # **Fix: Ensure x has the same number of channels as x1**\n    x = layers.Conv2D(filters, kernel_size=1, padding=\"same\")(x)\n    x = layers.DepthwiseConv2D(kernel_size=3, strides=strides, padding=\"same\")(x)\n\n    # Residual Connection\n    x = layers.Add()([x, x1])\n    x = layers.Activation(\"relu\")(x)\n    \n    return x\n\n\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\ndef multi_kernel_groupwise_conv(x, filters, groups=8, strides=1, use_se=True):\n    # 1x1 Group-wise Convolution (Efficient Channel Mixing)\n    conv1x1 = layers.Conv2D(filters, kernel_size=1, groups=groups, strides=strides, padding=\"same\", use_bias=False)(x)\n    conv1x1 = layers.BatchNormalization()(conv1x1)\n\n    # Depthwise Convolutions (Multi-scale receptive fields)\n    conv3x3 = layers.DepthwiseConv2D(kernel_size=5, strides=strides, padding=\"same\", use_bias=False)(x)\n    conv3x3 = layers.BatchNormalization()(conv3x3)\n\n    conv5x5 = layers.DepthwiseConv2D(kernel_size=7, strides=strides, padding=\"same\", use_bias=False)(x)\n    conv5x5 = layers.BatchNormalization()(conv5x5)\n\n    # Concatenation and 1x1 Fusion\n    x1 = layers.Concatenate()([conv1x1, conv3x3, conv5x5])\n    x1 = layers.Conv2D(filters, kernel_size=1, groups=groups, padding=\"same\", use_bias=False)(x1)\n    x1 = layers.BatchNormalization()(x1)\n\n    # Optional: Squeeze-and-Excitation (SE) block for adaptive recalibration\n    if use_se:\n        se1 = layers.GlobalAveragePooling2D()(x1)\n        se2 = layers.GlobalMaxPooling2D()(x1)\n        se = se1 + se2\n        se = layers.Dense(filters // 16, activation='relu', use_bias=False)(se)\n        se = layers.Dense(filters, activation='sigmoid', use_bias=False)(se)\n        se = layers.Reshape((1, 1, filters))(se)\n        x1 = layers.Multiply()([x1, se])\n\n    # Residual Connection\n    x = layers.Conv2D(filters=filters, kernel_size=1, strides=strides, padding='same', use_bias=False)(x)\n    x = layers.BatchNormalization()(x)\n\n    x = layers.Add()([x, x1])\n    x = layers.Activation('gelu')(x)  # GELU activation for better convergence\n\n    return x\n\n\n\n# Usage example\n'''def RGSA(x, filters, strides=(1, 1), use_projection=False):\n    shortcut = x  # Save the original input for residual connection\n\n    x = multi_kernel_groupwise_conv(x, filters=filters, groups=16, strides=strides)\n    \n    # Normalization and Activation\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation('relu')(x)\n\n    # Second Depthwise and Pointwise Convolution\n    \n    x = layers.Conv2D(filters, kernel_size=(3, 3), padding=\"same\")(x)\n    x = layers.BatchNormalization()(x)\n\n    # Adjust Shortcut if Needed\n    if strides != (1, 1) or use_projection:\n        shortcut = layers.Conv2D(filters=filters, kernel_size=(1, 1), strides=strides, padding='same')(shortcut)\n        shortcut = layers.BatchNormalization()(shortcut)\n\n    # Residual Connection\n    x = layers.Add()([x, shortcut])\n\n    # Final Activation\n    x = layers.Activation('relu')(x)\n\n    return x'''\n\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\n'''def RGSA(x, filters, strides=(1, 1), use_projection=False):\n    shortcut = x  # Save the original input for residual connection\n\n    # Multi-Kernel Groupwise Convolution\n    x = multi_kernel_groupwise_conv(x, filters=filters, groups=16, strides=strides)\n\n    # Normalization and Activation\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation('relu')(x)\n\n    # Second Depthwise and Pointwise Convolution\n    #x = layers.DepthwiseConv2D(kernel_size=(3, 3), padding=\"same\")(x)  # Depthwise conv for spatial filtering\n    x = multi_kernel_groupwise_conv2(x, filters=filters, groups=16, strides=strides)\n    x = layers.Conv2D(filters, kernel_size=(1, 1), padding=\"same\")(x)  # Pointwise conv for channel mixing\n    x = layers.BatchNormalization()(x)\n\n    # Adjust Shortcut if Needed\n    if strides != (1, 1) or use_projection:\n        shortcut = layers.DepthwiseConv2D(kernel_size=(3, 3), strides=strides, padding='same')(shortcut)  # Efficient downsampling\n        shortcut = layers.Conv2D(filters, kernel_size=(1, 1), padding=\"same\")(shortcut)  # Channel projection\n        shortcut = layers.BatchNormalization()(shortcut)\n\n    # Residual Connection\n    x = layers.Add()([x, shortcut])\n\n    # Final Activation\n    x = layers.Activation('relu')(x)\n\n    return x'''\n\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\ndef RGSA(x, filters, strides=(1, 1), use_projection=False):\n    shortcut = x  # Save the original input for residual connection\n\n    # Multi-Kernel Groupwise Convolution (First Layer)\n    x = multi_kernel_groupwise_conv(x, filters=filters, groups=16, strides=strides)\n\n    # Normalization and Activation\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation('relu')(x)\n\n    # Second Depthwise and Pointwise Convolution\n    x = multi_kernel_groupwise_conv(x, filters=filters, groups=16, strides=(1, 1))  # Strides=1 to avoid mismatch\n    x = layers.Conv2D(filters, kernel_size=(1, 1), padding=\"same\")(x)  # Pointwise conv for channel mixing\n    x = layers.BatchNormalization()(x)\n\n    # Adjust Shortcut if Needed (Ensure Matching Shape)\n    if strides != (1, 1) or use_projection:\n        shortcut = layers.Conv2D(filters, kernel_size=(1, 1), strides=strides, padding=\"same\")(shortcut)  # Downsampling shortcut\n        shortcut = layers.BatchNormalization()(shortcut)\n\n    # Residual Connection (Ensure Same Shape)\n    x = layers.Add()([x, shortcut])\n\n    # Final Activation\n    x = layers.Activation('relu')(x)\n\n    return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T06:00:40.813450Z","iopub.execute_input":"2025-03-09T06:00:40.813747Z","iopub.status.idle":"2025-03-09T06:00:41.061157Z","shell.execute_reply.started":"2025-03-09T06:00:40.813713Z","shell.execute_reply":"2025-03-09T06:00:41.060231Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"def info_fusion(x_1, x_2, filter):\n    x11 = multi_kernel_groupwise_conv1(x_1, filters=filter, groups=16, strides=(2,2))\n    x21 = multi_kernel_groupwise_conv1(x_2, filters=filter, groups=16, strides=(2,2))\n\n    print('128 x11, x21 shape:',x11.shape, x21.shape)\n\n    '''x11 = multi_kernel_groupwise_conv(x11, filters=256, groups=16, strides=(2,2))\n    x21 = multi_kernel_groupwise_conv(x21, filters=256, groups=16, strides=(2,2))\n\n    print('256 x11, x21 shape:',x11.shape, x21.shape)\n\n    x11 = multi_kernel_groupwise_conv(x11, filters=512, groups=16, strides=(2,2))\n    x21 = multi_kernel_groupwise_conv(x21, filters=512, groups=16, strides=(2,2))\n\n    print('512 x11, x21 shape:',x11.shape, x21.shape)'''\n    return x11, x21\n\n\ndef residual_GLC_branch1(inputs1, inputs2):\n    \n    x1 = Conv2D(filters=64, kernel_size=(7, 7), strides=(2, 2), padding='same')(inputs1)\n    #x1 = DeeperAttentionLayer1(units=64, use_scale=True)(x1) ## MFA ####\n    #x1 = AttentionBlock(64)(x1)\n    x1 = BatchNormalization()(x1)\n    x1 = tf.keras.layers.Activation('relu')(x1)\n    x1 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x1)\n    \n    x2 = Conv2D(filters=64, kernel_size=(7, 7), strides=(2, 2), padding='same')(inputs2)\n    #x2 = DeeperAttentionLayer1(units=64, use_scale=True)(x2) ## MFA ####\n    #x2 = AttentionBlock(64)(x2)\n    x2 = BatchNormalization()(x2)\n    x2 = tf.keras.layers.Activation('relu')(x2)\n    x2 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x2)\n    \n\n    x1 = RGSA(x1, filters=64)\n    x1 = tf.keras.layers.Dropout(0.25)(x1, training = True)  ## MCD ####\n    #x1 = DeeperAttentionLayer1(units=64, use_scale=True)(x1) ## MFA ####\n    #x1 = AttentionBlock(64)(x1)\n\n    x2 = RGSA(x2, filters=64)\n    x2 = tf.keras.layers.Dropout(0.25)(x2, training = True)  ## MCD ####\n    #x2 = DeeperAttentionLayer1(units=64, use_scale=True)(x2)\n    #x2 = AttentionBlock(64)(x2)\n    \n    x_1, x_2 = AttentionBlock(channels = 64)([x1, x2])  ## MIFA ####\n\n    print('64 x_1, x_2 shape:',x_1.shape, x_2.shape)\n\n    \n    \n    x1 = RGSA(x_1, filters=64)\n    x1 = tf.keras.layers.Dropout(0.25)(x1, training = True)  ## MCD ####\n    #x1 = DeeperAttentionLayer1(units=64, use_scale=True)(x1) ## MFA ####\n    #x1 = AttentionBlock(64)(x1)\n    \n    x2 = RGSA(x_2, filters=64)\n    x2 = tf.keras.layers.Dropout(0.25)(x2, training = True)  ## MCD ####\n    #x2 = DeeperAttentionLayer1(units=64, use_scale=True)(x2)\n    #x2 = AttentionBlock(64)(x2)\n\n    x_11, x_22 = AttentionBlock(channels = 64)([x1, x2])  ## MIFA ####\n    print('x_11, x_22 shape:',x_11.shape, x_22.shape)\n\n    \n    x11, x21 = info_fusion(x_11, x_22, 128)\n    x11, x21 = info_fusion(x11, x21, 256)\n    x11, x21 = info_fusion(x11, x21, 512)\n    \n    \n    \n    x1 = RGSA(x_11, filters=128, strides=(2, 2), use_projection=True)\n    x1 = tf.keras.layers.Dropout(0.25)(x1, training = True)  ## MCD ####\n    #x1 = DeeperAttentionLayer1(units=128, use_scale=True)(x1) ## MFA ####\n    #x1 = AttentionBlock(128)(x1)\n\n    x2 = RGSA(x_22, filters=128, strides=(2, 2), use_projection=True)\n    x2 = tf.keras.layers.Dropout(0.25)(x2, training = True)  ## MCD ####\n    #x2 = DeeperAttentionLayer1(units=128, use_scale=True)(x2)\n    #x2 = AttentionBlock(128)(x2)\n\n    x_111, x_222 = AttentionBlock(channels = 128)([x1, x2])  ## MIFA ####\n    print('x_111, x_222 shape:',x_111.shape, x_222.shape)\n    \n    x1 = RGSA(x_111, filters=128)\n    x1 = tf.keras.layers.Dropout(0.25)(x1, training = True)  ## MCD ####\n    #x1 = DeeperAttentionLayer1(units=128, use_scale=True)(x1)\n    #x1 = AttentionBlock(128)(x1)\n  \n    x2 = RGSA(x_222, filters=128)\n    x2 = tf.keras.layers.Dropout(0.25)(x2, training = True)  ## MCD ####\n    #x2 = DeeperAttentionLayer1(units=128, use_scale=True)(x2)\n    #x2 = AttentionBlock(128)(x2)\n\n    x_1111, x_2222 = AttentionBlock(channels = 128)([x1, x2])  ## MIFA ####\n    print('x_1111, x_2222 shape:',x_1111.shape, x_2222.shape)\n\n\n    x111, x211 = info_fusion(x_1111, x_2222, 256)\n    x111, x211 = info_fusion(x111, x211, 512)\n    \n    \n    x1 = RGSA(x_1111, filters=256, strides=(2, 2), use_projection=True)\n    x1 = tf.keras.layers.Dropout(0.25)(x1, training = True)  ## MCD ####\n    #x1 = DeeperAttentionLayer1(units=256, use_scale=True)(x1)\n    #x1 = AttentionBlock(256)(x1)\n    \n    x2 = RGSA(x_2222, filters=256, strides=(2, 2), use_projection=True)\n    x2 = tf.keras.layers.Dropout(0.25)(x2, training = True)  ## MCD ####\n    #x2 = DeeperAttentionLayer1(units=256, use_scale=True)(x2)\n    #x2 = AttentionBlock(256)(x2)\n\n    x_11111, x_22222 = AttentionBlock(channels = 256)([x1, x2])  ## MIFA ####\n    print('x_11111, x_22222 shape:',x_11111.shape, x_22222.shape)\n    \n    \n    x1 = RGSA(x_11111, filters=256)\n    x1 = tf.keras.layers.Dropout(0.25)(x1, training = True)  ## MCD ####\n    #x1 = DeeperAttentionLayer1(units=256, use_scale=True)(x1)\n    #x1 = AttentionBlock(256)(x1)\n    \n    x2 = RGSA(x_22222, filters=256)\n    x2 = tf.keras.layers.Dropout(0.25)(x2, training = True)  ## MCD ####\n    #x2 = DeeperAttentionLayer1(units=256, use_scale=True)(x2)\n    #x2 = AttentionBlock(256)(x2)\n    \n    x_12, x_21 = AttentionBlock(channels = 256)([x1, x2])  ## MIFA ####\n    print('x_12, x_21 shape:',x_12.shape, x_21.shape)\n\n    x1111, x2111 = info_fusion(x_12, x_21, 512)\n    \n\n    x1 = RGSA(x_12, filters=512, strides=(2, 2), use_projection=True)\n    #x1 = DeeperAttentionLayer1(units=512, use_scale=True)(x1)\n    #x1 = AttentionBlock(512)(x1)\n    \n    \n    x2 = RGSA(x_21, filters=512, strides=(2, 2), use_projection=True)\n    #x2 = DeeperAttentionLayer1(units=512, use_scale=True)(x2)\n    #x2 = AttentionBlock(512)(x2)\n\n    x_112, x_211 = AttentionBlock(channels = 512)([x1, x2])  ## MIFA ####\n    print('x_112, x_211 shape:',x_112.shape, x_211.shape)\n    \n    x1 = RGSA(x_112, filters=512)\n    x2 = RGSA(x_211, filters=512)\n    x_1112, x_2111 = AttentionBlock(channels = 512)([x1, x2])\n    print('x_1112, x_2111 shape:',x_1112.shape, x_2111.shape)\n\n    # x11, x21, x111, x211, x1111, x2111, x_1112, x_2111\n\n    x__1 = tf.keras.layers.Concatenate(axis=-1)([x11, x111, x1111, x_1112])\n    x__1 = tf.keras.layers.Dropout(0.25)(x__1, training = True)  ## MCD ####\n    x__2 = tf.keras.layers.Concatenate(axis=-1)([x21, x211, x2111, x_2111])\n    x__2 = tf.keras.layers.Dropout(0.25)(x__2, training = True)  ## MCD ####\n    \n    return x__1, x__2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T06:00:41.063320Z","iopub.execute_input":"2025-03-09T06:00:41.063631Z","iopub.status.idle":"2025-03-09T06:00:41.084213Z","shell.execute_reply.started":"2025-03-09T06:00:41.063606Z","shell.execute_reply":"2025-03-09T06:00:41.083230Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"#def build_resnet18(input_shape=(128, 128, 3), num_classes=2):\ninput_shape=(128, 128, 3)\ninputs1 = Input(shape=input_shape)\ninputs2 = Input(shape=input_shape)\n\nimport tensorflow.keras.layers as L\n\n#input_data = Input(shape=input_shape, name='input_data')\n# Initial convolutional layer\n\nx1, x2 = residual_GLC_branch1(inputs1, inputs2)\n#print('x:',x.shape)\n\ncon = tf.keras.layers.Concatenate(axis=-1)([x1, x2])\n\ncon = tf.keras.layers.Dropout(0.25)(con, training = True)  ## MCD ####\n\nx = GlobalAveragePooling2D()(con)\n#print('GlobalAveragePooling2D x:',x.shape)\n\noutputs1 = Dense(5, activation='softmax')(x)\noutputs2 = Dense(7, activation='softmax')(x)\n\n# Create the model\nmodel = Model([inputs1, inputs2], [outputs1, outputs2])\n#return model\nprint(model.summary())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T06:00:44.295029Z","iopub.execute_input":"2025-03-09T06:00:44.295348Z","iopub.status.idle":"2025-03-09T06:01:00.463630Z","shell.execute_reply.started":"2025-03-09T06:00:44.295325Z","shell.execute_reply":"2025-03-09T06:01:00.462688Z"}},"outputs":[{"name":"stdout","text":"output attention 1 shape: (None, 32, 32, 64)\noutput attention 2 shape: (None, 32, 32, 64)\noutput attention 1 shape: (None, 32, 32, 64)\noutput attention 2 shape: (None, 32, 32, 64)\n64 x_1, x_2 shape: (None, 32, 32, 64) (None, 32, 32, 64)\noutput attention 1 shape: (None, 32, 32, 64)\noutput attention 2 shape: (None, 32, 32, 64)\noutput attention 1 shape: (None, 32, 32, 64)\noutput attention 2 shape: (None, 32, 32, 64)\nx_11, x_22 shape: (None, 32, 32, 64) (None, 32, 32, 64)\n128 x11, x21 shape: (None, 16, 16, 128) (None, 16, 16, 128)\n128 x11, x21 shape: (None, 8, 8, 256) (None, 8, 8, 256)\n128 x11, x21 shape: (None, 4, 4, 512) (None, 4, 4, 512)\noutput attention 1 shape: (None, 16, 16, 128)\noutput attention 2 shape: (None, 16, 16, 128)\noutput attention 1 shape: (None, 16, 16, 128)\noutput attention 2 shape: (None, 16, 16, 128)\nx_111, x_222 shape: (None, 16, 16, 128) (None, 16, 16, 128)\noutput attention 1 shape: (None, 16, 16, 128)\noutput attention 2 shape: (None, 16, 16, 128)\noutput attention 1 shape: (None, 16, 16, 128)\noutput attention 2 shape: (None, 16, 16, 128)\nx_1111, x_2222 shape: (None, 16, 16, 128) (None, 16, 16, 128)\n128 x11, x21 shape: (None, 8, 8, 256) (None, 8, 8, 256)\n128 x11, x21 shape: (None, 4, 4, 512) (None, 4, 4, 512)\noutput attention 1 shape: (None, 8, 8, 256)\noutput attention 2 shape: (None, 8, 8, 256)\noutput attention 1 shape: (None, 8, 8, 256)\noutput attention 2 shape: (None, 8, 8, 256)\nx_11111, x_22222 shape: (None, 8, 8, 256) (None, 8, 8, 256)\noutput attention 1 shape: (None, 8, 8, 256)\noutput attention 2 shape: (None, 8, 8, 256)\noutput attention 1 shape: (None, 8, 8, 256)\noutput attention 2 shape: (None, 8, 8, 256)\nx_12, x_21 shape: (None, 8, 8, 256) (None, 8, 8, 256)\n128 x11, x21 shape: (None, 4, 4, 512) (None, 4, 4, 512)\noutput attention 1 shape: (None, 4, 4, 512)\noutput attention 2 shape: (None, 4, 4, 512)\noutput attention 1 shape: (None, 4, 4, 512)\noutput attention 2 shape: (None, 4, 4, 512)\nx_112, x_211 shape: (None, 4, 4, 512) (None, 4, 4, 512)\noutput attention 1 shape: (None, 4, 4, 512)\noutput attention 2 shape: (None, 4, 4, 512)\noutput attention 1 shape: (None, 4, 4, 512)\noutput attention 2 shape: (None, 4, 4, 512)\nx_1112, x_2111 shape: (None, 4, 4, 512) (None, 4, 4, 512)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\n\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m\n\n input_layer (\u001b[38;5;33mInputLayer\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  -                      \n\n input_layer_1              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  -                      \n (\u001b[38;5;33mInputLayer\u001b[0m)                                                                              \n\n conv2d (\u001b[38;5;33mConv2D\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m9,472\u001b[0m  input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n\n conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m9,472\u001b[0m  input_layer_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n batch_normalization        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  conv2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_1      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  conv2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n activation (\u001b[38;5;33mActivation\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization[\u001b[38;5;34m0\u001b[0m \n\n activation_1 (\u001b[38;5;33mActivation\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n\n max_pooling2d              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  activation[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n\n max_pooling2d_1            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  activation_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n\n conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  max_pooling2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n depthwise_conv2d           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m1,600\u001b[0m  max_pooling2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_1         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m3,136\u001b[0m  max_pooling2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n conv2d_9 (\u001b[38;5;33mConv2D\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  max_pooling2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n\n depthwise_conv2d_4         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m1,600\u001b[0m  max_pooling2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_5         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m3,136\u001b[0m  max_pooling2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n batch_normalization_2      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  conv2d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_3      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  depthwise_conv2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_4      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  depthwise_conv2d_1[\u001b[38;5;34m0\u001b[0m] \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_14     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  conv2d_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_15     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  depthwise_conv2d_4[\u001b[38;5;34m0\u001b[0m] \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_16     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  depthwise_conv2d_5[\u001b[38;5;34m0\u001b[0m] \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n concatenate (\u001b[38;5;33mConcatenate\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m192\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_2 \n                                                                    batch_normalization_3 \n                                                                    batch_normalization_4 \n\n concatenate_2              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m192\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      batch_normalization_1 \n                                                                    batch_normalization_1 \n\n conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m768\u001b[0m  concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n\n conv2d_10 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m768\u001b[0m  concatenate_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n batch_normalization_5      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  conv2d_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_17     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  conv2d_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n global_average_pooling2d   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  batch_normalization_5 \n (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n\n global_max_pooling2d       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  batch_normalization_5 \n (\u001b[38;5;33mGlobalMaxPooling2D\u001b[0m)                                                                      \n\n global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n\n global_max_pooling2d_2     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mGlobalMaxPooling2D\u001b[0m)                                                                      \n\n add (\u001b[38;5;33mAdd\u001b[0m)                  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  global_average_poolin \n                                                                    global_max_pooling2d[\u001b[38;5;34m\u001b[0m \n\n add_5 (\u001b[38;5;33mAdd\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n dense (\u001b[38;5;33mDense\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)                          \u001b[38;5;34m256\u001b[0m  add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              \n\n dense_4 (\u001b[38;5;33mDense\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)                          \u001b[38;5;34m256\u001b[0m  add_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            \n\n dense_1 (\u001b[38;5;33mDense\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                         \u001b[38;5;34m256\u001b[0m  dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            \n\n dense_5 (\u001b[38;5;33mDense\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                         \u001b[38;5;34m256\u001b[0m  dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n\n conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m4,096\u001b[0m  max_pooling2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n reshape (\u001b[38;5;33mReshape\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)                     \u001b[38;5;34m0\u001b[0m  dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n\n conv2d_11 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m4,096\u001b[0m  max_pooling2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n\n reshape_2 (\u001b[38;5;33mReshape\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)                     \u001b[38;5;34m0\u001b[0m  dense_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n\n batch_normalization_6      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  conv2d_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n multiply (\u001b[38;5;33mMultiply\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_5 \n                                                                    reshape[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n\n batch_normalization_18     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  conv2d_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n multiply_2 (\u001b[38;5;33mMultiply\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n                                                                    reshape_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n\n add_1 (\u001b[38;5;33mAdd\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_6 \n                                                                    multiply[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n add_6 (\u001b[38;5;33mAdd\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n                                                                    multiply_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n activation_2 (\u001b[38;5;33mActivation\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            \n\n activation_6 (\u001b[38;5;33mActivation\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  add_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            \n\n batch_normalization_7      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  activation_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_19     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  activation_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n activation_3 (\u001b[38;5;33mActivation\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_7 \n\n activation_7 (\u001b[38;5;33mActivation\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n\n conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  activation_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n\n depthwise_conv2d_2         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m1,600\u001b[0m  activation_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_3         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m3,136\u001b[0m  activation_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n conv2d_12 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  activation_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n\n depthwise_conv2d_6         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m1,600\u001b[0m  activation_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_7         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m3,136\u001b[0m  activation_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n batch_normalization_8      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  conv2d_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_9      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  depthwise_conv2d_2[\u001b[38;5;34m0\u001b[0m] \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_10     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  depthwise_conv2d_3[\u001b[38;5;34m0\u001b[0m] \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_20     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  conv2d_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_21     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  depthwise_conv2d_6[\u001b[38;5;34m0\u001b[0m] \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_22     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  depthwise_conv2d_7[\u001b[38;5;34m0\u001b[0m] \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n concatenate_1              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m192\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_8 \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      batch_normalization_9 \n                                                                    batch_normalization_1 \n\n concatenate_3              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m192\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_2 \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      batch_normalization_2 \n                                                                    batch_normalization_2 \n\n conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m768\u001b[0m  concatenate_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n conv2d_13 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m768\u001b[0m  concatenate_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n batch_normalization_11     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  conv2d_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_23     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  conv2d_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n\n global_max_pooling2d_1     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mGlobalMaxPooling2D\u001b[0m)                                                                      \n\n global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  batch_normalization_2 \n (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n\n global_max_pooling2d_3     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  batch_normalization_2 \n (\u001b[38;5;33mGlobalMaxPooling2D\u001b[0m)                                                                      \n\n add_2 (\u001b[38;5;33mAdd\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n add_7 (\u001b[38;5;33mAdd\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n dense_2 (\u001b[38;5;33mDense\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)                          \u001b[38;5;34m256\u001b[0m  add_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            \n\n dense_6 (\u001b[38;5;33mDense\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)                          \u001b[38;5;34m256\u001b[0m  add_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            \n\n dense_3 (\u001b[38;5;33mDense\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                         \u001b[38;5;34m256\u001b[0m  dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n\n dense_7 (\u001b[38;5;33mDense\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                         \u001b[38;5;34m256\u001b[0m  dense_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n\n conv2d_7 (\u001b[38;5;33mConv2D\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m4,096\u001b[0m  activation_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n\n reshape_1 (\u001b[38;5;33mReshape\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)                     \u001b[38;5;34m0\u001b[0m  dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n\n conv2d_14 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m4,096\u001b[0m  activation_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n\n reshape_3 (\u001b[38;5;33mReshape\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)                     \u001b[38;5;34m0\u001b[0m  dense_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n\n batch_normalization_12     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  conv2d_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n multiply_1 (\u001b[38;5;33mMultiply\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n                                                                    reshape_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n\n batch_normalization_24     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  conv2d_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n multiply_3 (\u001b[38;5;33mMultiply\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_2 \n                                                                    reshape_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n\n add_3 (\u001b[38;5;33mAdd\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n                                                                    multiply_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n add_8 (\u001b[38;5;33mAdd\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_2 \n                                                                    multiply_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n activation_4 (\u001b[38;5;33mActivation\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  add_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            \n\n activation_8 (\u001b[38;5;33mActivation\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  add_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            \n\n conv2d_8 (\u001b[38;5;33mConv2D\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m4,160\u001b[0m  activation_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n\n conv2d_15 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m4,160\u001b[0m  activation_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n\n batch_normalization_13     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  conv2d_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_25     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  conv2d_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n add_4 (\u001b[38;5;33mAdd\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n                                                                    max_pooling2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n add_9 (\u001b[38;5;33mAdd\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_2 \n                                                                    max_pooling2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n\n activation_5 (\u001b[38;5;33mActivation\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  add_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            \n\n activation_9 (\u001b[38;5;33mActivation\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  add_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            \n\n dropout (\u001b[38;5;33mDropout\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  activation_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n\n dropout_1 (\u001b[38;5;33mDropout\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  activation_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n\n attention_block            [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m),            \u001b[38;5;34m21,444\u001b[0m  dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],         \n (\u001b[38;5;33mAttentionBlock\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)]                     dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n\n conv2d_20 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  attention_block[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n\n depthwise_conv2d_17        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m1,600\u001b[0m  attention_block[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_18        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m3,136\u001b[0m  attention_block[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n conv2d_27 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  attention_block[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m]  \n\n depthwise_conv2d_21        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m1,600\u001b[0m  attention_block[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m]  \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_22        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m3,136\u001b[0m  attention_block[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m]  \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n batch_normalization_27     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  conv2d_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_28     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  depthwise_conv2d_17[\u001b[38;5;34m0\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_29     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  depthwise_conv2d_18[\u001b[38;5;34m0\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_39     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  conv2d_27[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_40     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  depthwise_conv2d_21[\u001b[38;5;34m0\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_41     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  depthwise_conv2d_22[\u001b[38;5;34m0\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n concatenate_10             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m192\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_2 \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      batch_normalization_2 \n                                                                    batch_normalization_2 \n\n concatenate_12             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m192\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_3 \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      batch_normalization_4 \n                                                                    batch_normalization_4 \n\n conv2d_21 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m768\u001b[0m  concatenate_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n conv2d_28 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m768\u001b[0m  concatenate_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n batch_normalization_30     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  conv2d_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_42     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  conv2d_28[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  batch_normalization_3 \n (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n\n global_max_pooling2d_6     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  batch_normalization_3 \n (\u001b[38;5;33mGlobalMaxPooling2D\u001b[0m)                                                                      \n\n global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  batch_normalization_4 \n (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n\n global_max_pooling2d_8     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  batch_normalization_4 \n (\u001b[38;5;33mGlobalMaxPooling2D\u001b[0m)                                                                      \n\n add_22 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n add_27 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n dense_9 (\u001b[38;5;33mDense\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)                          \u001b[38;5;34m256\u001b[0m  add_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n\n dense_13 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)                          \u001b[38;5;34m256\u001b[0m  add_27[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n\n dense_10 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                         \u001b[38;5;34m256\u001b[0m  dense_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n\n dense_14 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                         \u001b[38;5;34m256\u001b[0m  dense_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n conv2d_22 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m4,096\u001b[0m  attention_block[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n\n reshape_4 (\u001b[38;5;33mReshape\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)                     \u001b[38;5;34m0\u001b[0m  dense_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n conv2d_29 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m4,096\u001b[0m  attention_block[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m]  \n\n reshape_6 (\u001b[38;5;33mReshape\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)                     \u001b[38;5;34m0\u001b[0m  dense_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n batch_normalization_31     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  conv2d_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n multiply_4 (\u001b[38;5;33mMultiply\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_3 \n                                                                    reshape_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n\n batch_normalization_43     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  conv2d_29[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n multiply_6 (\u001b[38;5;33mMultiply\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_4 \n                                                                    reshape_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n\n add_23 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_3 \n                                                                    multiply_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n add_28 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_4 \n                                                                    multiply_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n activation_11              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  add_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_15              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  add_28[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n batch_normalization_32     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  activation_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_44     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  activation_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n activation_12              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_3 \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_16              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_4 \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n conv2d_23 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  activation_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n depthwise_conv2d_19        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m1,600\u001b[0m  activation_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_20        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m3,136\u001b[0m  activation_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n conv2d_30 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  activation_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n depthwise_conv2d_23        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m1,600\u001b[0m  activation_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_24        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m3,136\u001b[0m  activation_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n batch_normalization_33     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  conv2d_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_34     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  depthwise_conv2d_19[\u001b[38;5;34m0\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_35     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  depthwise_conv2d_20[\u001b[38;5;34m0\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_45     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  conv2d_30[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_46     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  depthwise_conv2d_23[\u001b[38;5;34m0\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_47     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  depthwise_conv2d_24[\u001b[38;5;34m0\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n concatenate_11             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m192\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_3 \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      batch_normalization_3 \n                                                                    batch_normalization_3 \n\n concatenate_13             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m192\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_4 \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      batch_normalization_4 \n                                                                    batch_normalization_4 \n\n conv2d_24 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m768\u001b[0m  concatenate_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n conv2d_31 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m768\u001b[0m  concatenate_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n batch_normalization_36     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  conv2d_24[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_48     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  conv2d_31[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  batch_normalization_3 \n (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n\n global_max_pooling2d_7     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  batch_normalization_3 \n (\u001b[38;5;33mGlobalMaxPooling2D\u001b[0m)                                                                      \n\n global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  batch_normalization_4 \n (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n\n global_max_pooling2d_9     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  batch_normalization_4 \n (\u001b[38;5;33mGlobalMaxPooling2D\u001b[0m)                                                                      \n\n add_24 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n add_29 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n dense_11 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)                          \u001b[38;5;34m256\u001b[0m  add_24[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n\n dense_15 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)                          \u001b[38;5;34m256\u001b[0m  add_29[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n\n dense_12 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                         \u001b[38;5;34m256\u001b[0m  dense_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n dense_16 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                         \u001b[38;5;34m256\u001b[0m  dense_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n conv2d_25 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m4,096\u001b[0m  activation_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n reshape_5 (\u001b[38;5;33mReshape\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)                     \u001b[38;5;34m0\u001b[0m  dense_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n conv2d_32 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m4,096\u001b[0m  activation_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n reshape_7 (\u001b[38;5;33mReshape\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)                     \u001b[38;5;34m0\u001b[0m  dense_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n batch_normalization_37     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  conv2d_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n multiply_5 (\u001b[38;5;33mMultiply\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_3 \n                                                                    reshape_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n\n batch_normalization_49     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  conv2d_32[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n multiply_7 (\u001b[38;5;33mMultiply\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_4 \n                                                                    reshape_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n\n add_25 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_3 \n                                                                    multiply_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n add_30 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_4 \n                                                                    multiply_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n activation_13              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  add_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_17              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  add_30[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n conv2d_26 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m4,160\u001b[0m  activation_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n conv2d_33 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m4,160\u001b[0m  activation_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n batch_normalization_38     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  conv2d_26[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_50     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  conv2d_33[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n add_26 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_3 \n                                                                    attention_block[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n\n add_31 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_5 \n                                                                    attention_block[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m]  \n\n activation_14              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  add_26[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_18              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  add_31[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n dropout_3 (\u001b[38;5;33mDropout\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  activation_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n dropout_4 (\u001b[38;5;33mDropout\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  activation_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n attention_block_1          [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m),            \u001b[38;5;34m21,444\u001b[0m  dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       \n (\u001b[38;5;33mAttentionBlock\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)]                     dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n\n conv2d_56 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  attention_block_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n depthwise_conv2d_70        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m1,600\u001b[0m  attention_block_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_71        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m3,136\u001b[0m  attention_block_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n conv2d_64 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  attention_block_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n depthwise_conv2d_74        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m1,600\u001b[0m  attention_block_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_75        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m3,136\u001b[0m  attention_block_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n batch_normalization_52     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_56[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_53     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  depthwise_conv2d_70[\u001b[38;5;34m0\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_54     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  depthwise_conv2d_71[\u001b[38;5;34m0\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_65     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_64[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_66     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  depthwise_conv2d_74[\u001b[38;5;34m0\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_67     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  depthwise_conv2d_75[\u001b[38;5;34m0\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n concatenate_26             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m256\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_5 \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      batch_normalization_5 \n                                                                    batch_normalization_5 \n\n concatenate_28             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m256\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_6 \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      batch_normalization_6 \n                                                                    batch_normalization_6 \n\n conv2d_57 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m2,048\u001b[0m  concatenate_26[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n conv2d_65 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m2,048\u001b[0m  concatenate_28[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n batch_normalization_55     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_57[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_68     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_65[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_5 \n (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n\n global_max_pooling2d_12    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_5 \n (\u001b[38;5;33mGlobalMaxPooling2D\u001b[0m)                                                                      \n\n global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_6 \n (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n\n global_max_pooling2d_14    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_6 \n (\u001b[38;5;33mGlobalMaxPooling2D\u001b[0m)                                                                      \n\n add_50 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n add_55 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n dense_18 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)                        \u001b[38;5;34m1,024\u001b[0m  add_50[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n\n dense_22 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)                        \u001b[38;5;34m1,024\u001b[0m  add_55[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n\n dense_19 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                      \u001b[38;5;34m1,024\u001b[0m  dense_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n dense_23 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                      \u001b[38;5;34m1,024\u001b[0m  dense_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n conv2d_58 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m8,192\u001b[0m  attention_block_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n reshape_8 (\u001b[38;5;33mReshape\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  dense_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n conv2d_66 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m8,192\u001b[0m  attention_block_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n reshape_10 (\u001b[38;5;33mReshape\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  dense_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n batch_normalization_56     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_58[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n multiply_8 (\u001b[38;5;33mMultiply\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_5 \n                                                                    reshape_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n\n batch_normalization_69     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_66[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n multiply_10 (\u001b[38;5;33mMultiply\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_6 \n                                                                    reshape_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n add_51 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_5 \n                                                                    multiply_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n add_56 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_6 \n                                                                    multiply_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n\n activation_26              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  add_51[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_30              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  add_56[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n batch_normalization_57     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  activation_26[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_70     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  activation_30[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n activation_27              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_5 \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_31              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_7 \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n conv2d_59 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m1,024\u001b[0m  activation_27[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n depthwise_conv2d_72        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m3,200\u001b[0m  activation_27[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_73        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m6,272\u001b[0m  activation_27[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n conv2d_67 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m1,024\u001b[0m  activation_31[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n depthwise_conv2d_76        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m3,200\u001b[0m  activation_31[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_77        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m6,272\u001b[0m  activation_31[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n batch_normalization_58     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_59[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_59     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  depthwise_conv2d_72[\u001b[38;5;34m0\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_60     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  depthwise_conv2d_73[\u001b[38;5;34m0\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_71     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_67[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_72     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  depthwise_conv2d_76[\u001b[38;5;34m0\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_73     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  depthwise_conv2d_77[\u001b[38;5;34m0\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n concatenate_27             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m384\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_5 \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      batch_normalization_5 \n                                                                    batch_normalization_6 \n\n concatenate_29             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m384\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_7 \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      batch_normalization_7 \n                                                                    batch_normalization_7 \n\n conv2d_60 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m3,072\u001b[0m  concatenate_27[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n conv2d_68 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m3,072\u001b[0m  concatenate_29[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n batch_normalization_61     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_60[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_74     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_68[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_6 \n (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n\n global_max_pooling2d_13    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_6 \n (\u001b[38;5;33mGlobalMaxPooling2D\u001b[0m)                                                                      \n\n global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_7 \n (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n\n global_max_pooling2d_15    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_7 \n (\u001b[38;5;33mGlobalMaxPooling2D\u001b[0m)                                                                      \n\n add_52 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n add_57 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n dense_20 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)                        \u001b[38;5;34m1,024\u001b[0m  add_52[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n\n dense_24 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)                        \u001b[38;5;34m1,024\u001b[0m  add_57[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n\n dense_21 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                      \u001b[38;5;34m1,024\u001b[0m  dense_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n dense_25 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                      \u001b[38;5;34m1,024\u001b[0m  dense_24[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n conv2d_61 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)             \u001b[38;5;34m16,384\u001b[0m  activation_27[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n reshape_9 (\u001b[38;5;33mReshape\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  dense_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n conv2d_69 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)             \u001b[38;5;34m16,384\u001b[0m  activation_31[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n reshape_11 (\u001b[38;5;33mReshape\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  dense_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n batch_normalization_62     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_61[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n multiply_9 (\u001b[38;5;33mMultiply\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_6 \n                                                                    reshape_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n\n batch_normalization_75     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_69[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n multiply_11 (\u001b[38;5;33mMultiply\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_7 \n                                                                    reshape_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n add_53 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_6 \n                                                                    multiply_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n add_58 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_7 \n                                                                    multiply_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n\n activation_28              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  add_53[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_32              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  add_58[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n conv2d_62 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)             \u001b[38;5;34m16,512\u001b[0m  activation_28[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n conv2d_63 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m8,320\u001b[0m  attention_block_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n conv2d_70 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)             \u001b[38;5;34m16,512\u001b[0m  activation_32[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n conv2d_71 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m8,320\u001b[0m  attention_block_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n batch_normalization_63     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_62[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_64     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_63[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_76     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_70[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_77     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_71[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n add_54 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_6 \n                                                                    batch_normalization_6 \n\n add_59 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_7 \n                                                                    batch_normalization_7 \n\n activation_29              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  add_54[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_33              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  add_59[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n dropout_6 (\u001b[38;5;33mDropout\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  activation_29[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n dropout_7 (\u001b[38;5;33mDropout\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  activation_33[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n attention_block_2          [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m),           \u001b[38;5;34m68,996\u001b[0m  dropout_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       \n (\u001b[38;5;33mAttentionBlock\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)]                    dropout_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n\n conv2d_76 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m1,024\u001b[0m  attention_block_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n depthwise_conv2d_87        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m3,200\u001b[0m  attention_block_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_88        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m6,272\u001b[0m  attention_block_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n conv2d_83 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m1,024\u001b[0m  attention_block_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n depthwise_conv2d_91        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m3,200\u001b[0m  attention_block_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_92        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m6,272\u001b[0m  attention_block_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n batch_normalization_79     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_76[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_80     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  depthwise_conv2d_87[\u001b[38;5;34m0\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_81     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  depthwise_conv2d_88[\u001b[38;5;34m0\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_91     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_83[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_92     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  depthwise_conv2d_91[\u001b[38;5;34m0\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_93     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  depthwise_conv2d_92[\u001b[38;5;34m0\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n concatenate_36             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m384\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_7 \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      batch_normalization_8 \n                                                                    batch_normalization_8 \n\n concatenate_38             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m384\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_9 \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      batch_normalization_9 \n                                                                    batch_normalization_9 \n\n conv2d_77 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m3,072\u001b[0m  concatenate_36[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n conv2d_84 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m3,072\u001b[0m  concatenate_38[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n batch_normalization_82     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_77[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_94     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_84[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_8 \n (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n\n global_max_pooling2d_18    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_8 \n (\u001b[38;5;33mGlobalMaxPooling2D\u001b[0m)                                                                      \n\n global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_9 \n (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n\n global_max_pooling2d_20    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_9 \n (\u001b[38;5;33mGlobalMaxPooling2D\u001b[0m)                                                                      \n\n add_72 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n add_77 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n dense_27 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)                        \u001b[38;5;34m1,024\u001b[0m  add_72[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n\n dense_31 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)                        \u001b[38;5;34m1,024\u001b[0m  add_77[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n\n dense_28 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                      \u001b[38;5;34m1,024\u001b[0m  dense_27[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n dense_32 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                      \u001b[38;5;34m1,024\u001b[0m  dense_31[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n conv2d_78 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)             \u001b[38;5;34m16,384\u001b[0m  attention_block_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n reshape_12 (\u001b[38;5;33mReshape\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  dense_28[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n conv2d_85 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)             \u001b[38;5;34m16,384\u001b[0m  attention_block_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n reshape_14 (\u001b[38;5;33mReshape\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  dense_32[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n batch_normalization_83     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_78[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n multiply_12 (\u001b[38;5;33mMultiply\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_8 \n                                                                    reshape_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n batch_normalization_95     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_85[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n multiply_14 (\u001b[38;5;33mMultiply\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_9 \n                                                                    reshape_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n add_73 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_8 \n                                                                    multiply_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n\n add_78 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_9 \n                                                                    multiply_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n\n activation_35              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  add_73[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_39              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  add_78[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n batch_normalization_84     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  activation_35[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_96     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  activation_39[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n activation_36              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_8 \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_40              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_9 \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n conv2d_79 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m1,024\u001b[0m  activation_36[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n depthwise_conv2d_89        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m3,200\u001b[0m  activation_36[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_90        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m6,272\u001b[0m  activation_36[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n conv2d_86 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m1,024\u001b[0m  activation_40[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n depthwise_conv2d_93        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m3,200\u001b[0m  activation_40[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_94        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m6,272\u001b[0m  activation_40[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n batch_normalization_85     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_79[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_86     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  depthwise_conv2d_89[\u001b[38;5;34m0\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_87     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  depthwise_conv2d_90[\u001b[38;5;34m0\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_97     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_86[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_98     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  depthwise_conv2d_93[\u001b[38;5;34m0\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_99     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  depthwise_conv2d_94[\u001b[38;5;34m0\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n concatenate_37             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m384\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_8 \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      batch_normalization_8 \n                                                                    batch_normalization_8 \n\n concatenate_39             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m384\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_9 \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      batch_normalization_9 \n                                                                    batch_normalization_9 \n\n conv2d_80 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m3,072\u001b[0m  concatenate_37[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n conv2d_87 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m3,072\u001b[0m  concatenate_39[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n batch_normalization_88     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_80[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_100    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_87[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_8 \n (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n\n global_max_pooling2d_19    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_8 \n (\u001b[38;5;33mGlobalMaxPooling2D\u001b[0m)                                                                      \n\n global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n\n global_max_pooling2d_21    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mGlobalMaxPooling2D\u001b[0m)                                                                      \n\n add_74 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n add_79 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n dense_29 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)                        \u001b[38;5;34m1,024\u001b[0m  add_74[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n\n dense_33 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)                        \u001b[38;5;34m1,024\u001b[0m  add_79[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n\n dense_30 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                      \u001b[38;5;34m1,024\u001b[0m  dense_29[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n dense_34 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                      \u001b[38;5;34m1,024\u001b[0m  dense_33[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n conv2d_81 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)             \u001b[38;5;34m16,384\u001b[0m  activation_36[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n reshape_13 (\u001b[38;5;33mReshape\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  dense_30[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n conv2d_88 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)             \u001b[38;5;34m16,384\u001b[0m  activation_40[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n reshape_15 (\u001b[38;5;33mReshape\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  dense_34[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n batch_normalization_89     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_81[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n multiply_13 (\u001b[38;5;33mMultiply\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_8 \n                                                                    reshape_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n batch_normalization_101    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_88[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n multiply_15 (\u001b[38;5;33mMultiply\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n                                                                    reshape_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n add_75 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_8 \n                                                                    multiply_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n\n add_80 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n                                                                    multiply_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n\n activation_37              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  add_75[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_41              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  add_80[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n conv2d_82 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)             \u001b[38;5;34m16,512\u001b[0m  activation_37[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n conv2d_89 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)             \u001b[38;5;34m16,512\u001b[0m  activation_41[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n batch_normalization_90     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_82[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_102    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_89[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n add_76 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_9 \n                                                                    attention_block_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n add_81 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n                                                                    attention_block_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n activation_38              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  add_76[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_42              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  add_81[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n dropout_9 (\u001b[38;5;33mDropout\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  activation_38[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n dropout_10 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  activation_42[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n attention_block_3          [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m),           \u001b[38;5;34m68,996\u001b[0m  dropout_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       \n (\u001b[38;5;33mAttentionBlock\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)]                    dropout_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n conv2d_106 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  attention_block_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n depthwise_conv2d_128       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m3,200\u001b[0m  attention_block_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_129       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m6,272\u001b[0m  attention_block_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n conv2d_114 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  attention_block_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n depthwise_conv2d_132       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m3,200\u001b[0m  attention_block_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_133       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m6,272\u001b[0m  attention_block_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n batch_normalization_104    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  conv2d_106[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_105    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m512\u001b[0m  depthwise_conv2d_128[\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_106    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m512\u001b[0m  depthwise_conv2d_129[\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_117    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  conv2d_114[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_118    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m512\u001b[0m  depthwise_conv2d_132[\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_119    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m512\u001b[0m  depthwise_conv2d_133[\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n concatenate_50             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      batch_normalization_1 \n                                                                    batch_normalization_1 \n\n concatenate_52             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      batch_normalization_1 \n                                                                    batch_normalization_1 \n\n conv2d_107 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m8,192\u001b[0m  concatenate_50[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n conv2d_115 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m8,192\u001b[0m  concatenate_52[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n batch_normalization_107    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  conv2d_107[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_120    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  conv2d_115[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n\n global_max_pooling2d_24    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mGlobalMaxPooling2D\u001b[0m)                                                                      \n\n global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n\n global_max_pooling2d_26    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mGlobalMaxPooling2D\u001b[0m)                                                                      \n\n add_98 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n add_103 (\u001b[38;5;33mAdd\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n dense_36 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)                       \u001b[38;5;34m4,096\u001b[0m  add_98[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n\n dense_40 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)                       \u001b[38;5;34m4,096\u001b[0m  add_103[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n\n dense_37 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                      \u001b[38;5;34m4,096\u001b[0m  dense_36[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n dense_41 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                      \u001b[38;5;34m4,096\u001b[0m  dense_40[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n conv2d_108 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)               \u001b[38;5;34m32,768\u001b[0m  attention_block_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n reshape_16 (\u001b[38;5;33mReshape\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  dense_37[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n conv2d_116 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)               \u001b[38;5;34m32,768\u001b[0m  attention_block_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n reshape_18 (\u001b[38;5;33mReshape\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  dense_41[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n batch_normalization_108    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  conv2d_108[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n multiply_16 (\u001b[38;5;33mMultiply\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n                                                                    reshape_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n batch_normalization_121    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  conv2d_116[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n multiply_18 (\u001b[38;5;33mMultiply\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n                                                                    reshape_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n add_99 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n                                                                    multiply_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n\n add_104 (\u001b[38;5;33mAdd\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n                                                                    multiply_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n\n activation_48              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_99[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_52              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_104[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n batch_normalization_109    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  activation_48[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_122    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  activation_52[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n activation_49              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_53              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n conv2d_109 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m4,096\u001b[0m  activation_49[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n depthwise_conv2d_130       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m6,400\u001b[0m  activation_49[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_131       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)               \u001b[38;5;34m12,544\u001b[0m  activation_49[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n conv2d_117 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m4,096\u001b[0m  activation_53[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n depthwise_conv2d_134       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m6,400\u001b[0m  activation_53[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_135       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)               \u001b[38;5;34m12,544\u001b[0m  activation_53[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n batch_normalization_110    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  conv2d_109[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_111    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  depthwise_conv2d_130[\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_112    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  depthwise_conv2d_131[\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_123    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  conv2d_117[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_124    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  depthwise_conv2d_134[\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_125    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  depthwise_conv2d_135[\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n concatenate_51             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m768\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      batch_normalization_1 \n                                                                    batch_normalization_1 \n\n concatenate_53             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m768\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      batch_normalization_1 \n                                                                    batch_normalization_1 \n\n conv2d_110 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)               \u001b[38;5;34m12,288\u001b[0m  concatenate_51[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n conv2d_118 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)               \u001b[38;5;34m12,288\u001b[0m  concatenate_53[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n batch_normalization_113    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  conv2d_110[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_126    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  conv2d_118[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n\n global_max_pooling2d_25    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mGlobalMaxPooling2D\u001b[0m)                                                                      \n\n global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n\n global_max_pooling2d_27    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mGlobalMaxPooling2D\u001b[0m)                                                                      \n\n add_100 (\u001b[38;5;33mAdd\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n add_105 (\u001b[38;5;33mAdd\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n dense_38 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)                       \u001b[38;5;34m4,096\u001b[0m  add_100[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n\n dense_42 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)                       \u001b[38;5;34m4,096\u001b[0m  add_105[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n\n dense_39 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                      \u001b[38;5;34m4,096\u001b[0m  dense_38[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n dense_43 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                      \u001b[38;5;34m4,096\u001b[0m  dense_42[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n conv2d_111 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)               \u001b[38;5;34m65,536\u001b[0m  activation_49[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n reshape_17 (\u001b[38;5;33mReshape\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  dense_39[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n conv2d_119 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)               \u001b[38;5;34m65,536\u001b[0m  activation_53[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n reshape_19 (\u001b[38;5;33mReshape\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  dense_43[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n batch_normalization_114    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  conv2d_111[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n multiply_17 (\u001b[38;5;33mMultiply\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n                                                                    reshape_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n batch_normalization_127    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  conv2d_119[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n multiply_19 (\u001b[38;5;33mMultiply\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n                                                                    reshape_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n add_101 (\u001b[38;5;33mAdd\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n                                                                    multiply_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n\n add_106 (\u001b[38;5;33mAdd\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n                                                                    multiply_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n\n activation_50              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_101[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_54              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_106[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n conv2d_112 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)               \u001b[38;5;34m65,792\u001b[0m  activation_50[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n conv2d_113 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)               \u001b[38;5;34m33,024\u001b[0m  attention_block_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n conv2d_120 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)               \u001b[38;5;34m65,792\u001b[0m  activation_54[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n conv2d_121 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)               \u001b[38;5;34m33,024\u001b[0m  attention_block_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n batch_normalization_115    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  conv2d_112[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_116    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  conv2d_113[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_128    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  conv2d_120[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_129    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  conv2d_121[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n add_102 (\u001b[38;5;33mAdd\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n                                                                    batch_normalization_1 \n\n add_107 (\u001b[38;5;33mAdd\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n                                                                    batch_normalization_1 \n\n activation_51              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_102[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_55              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_107[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n dropout_12 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  activation_51[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n dropout_13 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  activation_55[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n attention_block_4          [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m),            \u001b[38;5;34m242,436\u001b[0m  dropout_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n (\u001b[38;5;33mAttentionBlock\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)]                      dropout_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n conv2d_126 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m4,096\u001b[0m  attention_block_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n depthwise_conv2d_145       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m6,400\u001b[0m  attention_block_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_146       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)               \u001b[38;5;34m12,544\u001b[0m  attention_block_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n conv2d_133 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m4,096\u001b[0m  attention_block_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n depthwise_conv2d_149       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m6,400\u001b[0m  attention_block_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_150       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)               \u001b[38;5;34m12,544\u001b[0m  attention_block_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n batch_normalization_131    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  conv2d_126[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_132    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  depthwise_conv2d_145[\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_133    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  depthwise_conv2d_146[\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_143    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  conv2d_133[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_144    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  depthwise_conv2d_149[\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_145    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  depthwise_conv2d_150[\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n concatenate_60             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m768\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      batch_normalization_1 \n                                                                    batch_normalization_1 \n\n concatenate_62             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m768\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      batch_normalization_1 \n                                                                    batch_normalization_1 \n\n conv2d_127 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)               \u001b[38;5;34m12,288\u001b[0m  concatenate_60[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n conv2d_134 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)               \u001b[38;5;34m12,288\u001b[0m  concatenate_62[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n batch_normalization_134    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  conv2d_127[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_146    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  conv2d_134[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n\n global_max_pooling2d_30    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mGlobalMaxPooling2D\u001b[0m)                                                                      \n\n global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n\n global_max_pooling2d_32    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mGlobalMaxPooling2D\u001b[0m)                                                                      \n\n add_120 (\u001b[38;5;33mAdd\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n add_125 (\u001b[38;5;33mAdd\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n dense_45 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)                       \u001b[38;5;34m4,096\u001b[0m  add_120[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n\n dense_49 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)                       \u001b[38;5;34m4,096\u001b[0m  add_125[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n\n dense_46 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                      \u001b[38;5;34m4,096\u001b[0m  dense_45[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n dense_50 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                      \u001b[38;5;34m4,096\u001b[0m  dense_49[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n conv2d_128 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)               \u001b[38;5;34m65,536\u001b[0m  attention_block_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n reshape_20 (\u001b[38;5;33mReshape\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  dense_46[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n conv2d_135 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)               \u001b[38;5;34m65,536\u001b[0m  attention_block_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n reshape_22 (\u001b[38;5;33mReshape\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  dense_50[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n batch_normalization_135    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  conv2d_128[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n multiply_20 (\u001b[38;5;33mMultiply\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n                                                                    reshape_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n batch_normalization_147    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  conv2d_135[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n multiply_22 (\u001b[38;5;33mMultiply\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n                                                                    reshape_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n add_121 (\u001b[38;5;33mAdd\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n                                                                    multiply_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n\n add_126 (\u001b[38;5;33mAdd\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n                                                                    multiply_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n\n activation_57              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_121[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_61              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_126[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n batch_normalization_136    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  activation_57[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_148    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  activation_61[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n activation_58              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_62              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n conv2d_129 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m4,096\u001b[0m  activation_58[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n depthwise_conv2d_147       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m6,400\u001b[0m  activation_58[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_148       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)               \u001b[38;5;34m12,544\u001b[0m  activation_58[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n conv2d_136 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m4,096\u001b[0m  activation_62[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n depthwise_conv2d_151       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m6,400\u001b[0m  activation_62[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_152       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)               \u001b[38;5;34m12,544\u001b[0m  activation_62[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n batch_normalization_137    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  conv2d_129[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_138    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  depthwise_conv2d_147[\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_139    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  depthwise_conv2d_148[\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_149    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  conv2d_136[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_150    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  depthwise_conv2d_151[\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_151    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  depthwise_conv2d_152[\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n concatenate_61             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m768\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      batch_normalization_1 \n                                                                    batch_normalization_1 \n\n concatenate_63             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m768\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      batch_normalization_1 \n                                                                    batch_normalization_1 \n\n conv2d_130 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)               \u001b[38;5;34m12,288\u001b[0m  concatenate_61[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n conv2d_137 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)               \u001b[38;5;34m12,288\u001b[0m  concatenate_63[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n batch_normalization_140    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  conv2d_130[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_152    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  conv2d_137[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n\n global_max_pooling2d_31    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mGlobalMaxPooling2D\u001b[0m)                                                                      \n\n global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n\n global_max_pooling2d_33    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mGlobalMaxPooling2D\u001b[0m)                                                                      \n\n add_122 (\u001b[38;5;33mAdd\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n add_127 (\u001b[38;5;33mAdd\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n dense_47 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)                       \u001b[38;5;34m4,096\u001b[0m  add_122[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n\n dense_51 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)                       \u001b[38;5;34m4,096\u001b[0m  add_127[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n\n dense_48 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                      \u001b[38;5;34m4,096\u001b[0m  dense_47[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n dense_52 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                      \u001b[38;5;34m4,096\u001b[0m  dense_51[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n conv2d_131 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)               \u001b[38;5;34m65,536\u001b[0m  activation_58[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n reshape_21 (\u001b[38;5;33mReshape\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  dense_48[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n conv2d_138 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)               \u001b[38;5;34m65,536\u001b[0m  activation_62[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n reshape_23 (\u001b[38;5;33mReshape\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  dense_52[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n batch_normalization_141    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  conv2d_131[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n multiply_21 (\u001b[38;5;33mMultiply\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n                                                                    reshape_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n batch_normalization_153    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  conv2d_138[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n multiply_23 (\u001b[38;5;33mMultiply\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n                                                                    reshape_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n add_123 (\u001b[38;5;33mAdd\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n                                                                    multiply_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n\n add_128 (\u001b[38;5;33mAdd\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n                                                                    multiply_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n\n activation_59              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_123[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_63              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_128[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n conv2d_132 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)               \u001b[38;5;34m65,792\u001b[0m  activation_59[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n conv2d_139 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)               \u001b[38;5;34m65,792\u001b[0m  activation_63[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n batch_normalization_142    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  conv2d_132[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_154    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  conv2d_139[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n add_124 (\u001b[38;5;33mAdd\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n                                                                    attention_block_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n add_129 (\u001b[38;5;33mAdd\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n                                                                    attention_block_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n activation_60              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_124[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_64              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_129[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n dropout_15 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  activation_60[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n dropout_16 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  activation_64[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n attention_block_5          [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m),            \u001b[38;5;34m242,436\u001b[0m  dropout_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n (\u001b[38;5;33mAttentionBlock\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)]                      dropout_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n conv2d_150 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m8,192\u001b[0m  attention_block_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n depthwise_conv2d_174       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m6,400\u001b[0m  attention_block_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_175       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m)               \u001b[38;5;34m12,544\u001b[0m  attention_block_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n conv2d_158 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m8,192\u001b[0m  attention_block_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n depthwise_conv2d_178       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m6,400\u001b[0m  attention_block_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_179       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m)               \u001b[38;5;34m12,544\u001b[0m  attention_block_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n batch_normalization_156    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  conv2d_150[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_157    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  depthwise_conv2d_174[\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_158    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  depthwise_conv2d_175[\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_169    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  conv2d_158[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_170    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  depthwise_conv2d_178[\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_171    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  depthwise_conv2d_179[\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n concatenate_72             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      batch_normalization_1 \n                                                                    batch_normalization_1 \n\n concatenate_74             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      batch_normalization_1 \n                                                                    batch_normalization_1 \n\n conv2d_151 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)               \u001b[38;5;34m32,768\u001b[0m  concatenate_72[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n conv2d_159 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)               \u001b[38;5;34m32,768\u001b[0m  concatenate_74[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n batch_normalization_159    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  conv2d_151[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_172    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  conv2d_159[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n\n global_max_pooling2d_36    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mGlobalMaxPooling2D\u001b[0m)                                                                      \n\n global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n\n global_max_pooling2d_38    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mGlobalMaxPooling2D\u001b[0m)                                                                      \n\n add_144 (\u001b[38;5;33mAdd\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n add_149 (\u001b[38;5;33mAdd\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n dense_54 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                      \u001b[38;5;34m16,384\u001b[0m  add_144[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n\n dense_58 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                      \u001b[38;5;34m16,384\u001b[0m  add_149[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n\n dense_55 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                     \u001b[38;5;34m16,384\u001b[0m  dense_54[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n dense_59 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                     \u001b[38;5;34m16,384\u001b[0m  dense_58[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n conv2d_152 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)              \u001b[38;5;34m131,072\u001b[0m  attention_block_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n reshape_24 (\u001b[38;5;33mReshape\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  dense_55[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n conv2d_160 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)              \u001b[38;5;34m131,072\u001b[0m  attention_block_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n reshape_26 (\u001b[38;5;33mReshape\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  dense_59[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n batch_normalization_160    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  conv2d_152[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n multiply_24 (\u001b[38;5;33mMultiply\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n                                                                    reshape_24[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n batch_normalization_173    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  conv2d_160[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n multiply_26 (\u001b[38;5;33mMultiply\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n                                                                    reshape_26[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n add_145 (\u001b[38;5;33mAdd\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n                                                                    multiply_24[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n\n add_150 (\u001b[38;5;33mAdd\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n                                                                    multiply_26[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n\n activation_68              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_145[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_72              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_150[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n batch_normalization_161    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  activation_68[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_174    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  activation_72[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n activation_69              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_73              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n conv2d_153 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)               \u001b[38;5;34m16,384\u001b[0m  activation_69[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n depthwise_conv2d_176       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)               \u001b[38;5;34m12,800\u001b[0m  activation_69[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_177       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)               \u001b[38;5;34m25,088\u001b[0m  activation_69[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n conv2d_161 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)               \u001b[38;5;34m16,384\u001b[0m  activation_73[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n depthwise_conv2d_180       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)               \u001b[38;5;34m12,800\u001b[0m  activation_73[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_181       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)               \u001b[38;5;34m25,088\u001b[0m  activation_73[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n batch_normalization_162    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  conv2d_153[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_163    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  depthwise_conv2d_176[\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_164    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  depthwise_conv2d_177[\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_175    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  conv2d_161[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_176    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  depthwise_conv2d_180[\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_177    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  depthwise_conv2d_181[\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n concatenate_73             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m1536\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      batch_normalization_1 \n                                                                    batch_normalization_1 \n\n concatenate_75             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m1536\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      batch_normalization_1 \n                                                                    batch_normalization_1 \n\n conv2d_154 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)               \u001b[38;5;34m49,152\u001b[0m  concatenate_73[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n conv2d_162 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)               \u001b[38;5;34m49,152\u001b[0m  concatenate_75[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n batch_normalization_165    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  conv2d_154[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_178    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  conv2d_162[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n\n global_max_pooling2d_37    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mGlobalMaxPooling2D\u001b[0m)                                                                      \n\n global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n\n global_max_pooling2d_39    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mGlobalMaxPooling2D\u001b[0m)                                                                      \n\n add_146 (\u001b[38;5;33mAdd\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n add_151 (\u001b[38;5;33mAdd\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n dense_56 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                      \u001b[38;5;34m16,384\u001b[0m  add_146[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n\n dense_60 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                      \u001b[38;5;34m16,384\u001b[0m  add_151[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n\n dense_57 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                     \u001b[38;5;34m16,384\u001b[0m  dense_56[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n dense_61 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                     \u001b[38;5;34m16,384\u001b[0m  dense_60[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n conv2d_155 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)              \u001b[38;5;34m262,144\u001b[0m  activation_69[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n reshape_25 (\u001b[38;5;33mReshape\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  dense_57[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n conv2d_163 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)              \u001b[38;5;34m262,144\u001b[0m  activation_73[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n reshape_27 (\u001b[38;5;33mReshape\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  dense_61[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n batch_normalization_166    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  conv2d_155[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n multiply_25 (\u001b[38;5;33mMultiply\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n                                                                    reshape_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n batch_normalization_179    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  conv2d_163[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n multiply_27 (\u001b[38;5;33mMultiply\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n                                                                    reshape_27[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n add_147 (\u001b[38;5;33mAdd\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n                                                                    multiply_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n\n add_152 (\u001b[38;5;33mAdd\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n                                                                    multiply_27[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n\n activation_70              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_147[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_74              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_152[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n conv2d_156 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)              \u001b[38;5;34m262,656\u001b[0m  activation_70[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n conv2d_157 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)              \u001b[38;5;34m131,584\u001b[0m  attention_block_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n conv2d_164 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)              \u001b[38;5;34m262,656\u001b[0m  activation_74[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n conv2d_165 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)              \u001b[38;5;34m131,584\u001b[0m  attention_block_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n batch_normalization_167    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  conv2d_156[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_168    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  conv2d_157[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_180    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  conv2d_164[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_181    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  conv2d_165[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n add_148 (\u001b[38;5;33mAdd\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n                                                                    batch_normalization_1 \n\n add_153 (\u001b[38;5;33mAdd\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n                                                                    batch_normalization_1 \n\n activation_71              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_148[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_75              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_153[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n attention_block_6          [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m),            \u001b[38;5;34m902,660\u001b[0m  activation_71[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   \n (\u001b[38;5;33mAttentionBlock\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)]                      activation_75[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n conv2d_170 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)               \u001b[38;5;34m16,384\u001b[0m  attention_block_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n depthwise_conv2d_191       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)               \u001b[38;5;34m12,800\u001b[0m  attention_block_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_192       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)               \u001b[38;5;34m25,088\u001b[0m  attention_block_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n conv2d_177 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)               \u001b[38;5;34m16,384\u001b[0m  attention_block_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n depthwise_conv2d_195       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)               \u001b[38;5;34m12,800\u001b[0m  attention_block_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_196       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)               \u001b[38;5;34m25,088\u001b[0m  attention_block_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n batch_normalization_183    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  conv2d_170[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_184    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  depthwise_conv2d_191[\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_185    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  depthwise_conv2d_192[\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_195    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  conv2d_177[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_196    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  depthwise_conv2d_195[\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_197    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  depthwise_conv2d_196[\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n concatenate_82             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m1536\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      batch_normalization_1 \n                                                                    batch_normalization_1 \n\n concatenate_84             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m1536\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      batch_normalization_1 \n                                                                    batch_normalization_1 \n\n conv2d_171 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)               \u001b[38;5;34m49,152\u001b[0m  concatenate_82[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n conv2d_178 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)               \u001b[38;5;34m49,152\u001b[0m  concatenate_84[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n batch_normalization_186    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  conv2d_171[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_198    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  conv2d_178[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n\n global_max_pooling2d_42    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mGlobalMaxPooling2D\u001b[0m)                                                                      \n\n global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n\n global_max_pooling2d_44    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mGlobalMaxPooling2D\u001b[0m)                                                                      \n\n add_166 (\u001b[38;5;33mAdd\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n add_171 (\u001b[38;5;33mAdd\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n dense_63 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                      \u001b[38;5;34m16,384\u001b[0m  add_166[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n\n dense_67 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                      \u001b[38;5;34m16,384\u001b[0m  add_171[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n\n dense_64 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                     \u001b[38;5;34m16,384\u001b[0m  dense_63[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n dense_68 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                     \u001b[38;5;34m16,384\u001b[0m  dense_67[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n conv2d_38 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)               \u001b[38;5;34m2,080\u001b[0m  attention_block_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n depthwise_conv2d_34        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m640\u001b[0m  attention_block_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_35        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m1,664\u001b[0m  attention_block_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_36        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m640\u001b[0m  attention_block_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n conv2d_172 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)              \u001b[38;5;34m262,144\u001b[0m  attention_block_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n reshape_28 (\u001b[38;5;33mReshape\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  dense_64[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n conv2d_179 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)              \u001b[38;5;34m262,144\u001b[0m  attention_block_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n reshape_30 (\u001b[38;5;33mReshape\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  dense_68[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n conv2d_41 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)               \u001b[38;5;34m2,080\u001b[0m  attention_block_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n depthwise_conv2d_40        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m640\u001b[0m  attention_block_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_41        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m1,664\u001b[0m  attention_block_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_42        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m640\u001b[0m  attention_block_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n concatenate_20             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m224\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  conv2d_38[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      depthwise_conv2d_34[\u001b[38;5;34m0\u001b[0m \n                                                                    depthwise_conv2d_35[\u001b[38;5;34m0\u001b[0m \n                                                                    depthwise_conv2d_36[\u001b[38;5;34m0\u001b[0m \n\n batch_normalization_187    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  conv2d_172[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n multiply_28 (\u001b[38;5;33mMultiply\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n                                                                    reshape_28[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n batch_normalization_199    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  conv2d_179[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n multiply_30 (\u001b[38;5;33mMultiply\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n                                                                    reshape_30[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n concatenate_21             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m224\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  conv2d_41[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      depthwise_conv2d_40[\u001b[38;5;34m0\u001b[0m \n                                                                    depthwise_conv2d_41[\u001b[38;5;34m0\u001b[0m \n                                                                    depthwise_conv2d_42[\u001b[38;5;34m0\u001b[0m \n\n lambda (\u001b[38;5;33mLambda\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m224\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  concatenate_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n add_167 (\u001b[38;5;33mAdd\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n                                                                    multiply_28[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n\n add_172 (\u001b[38;5;33mAdd\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n                                                                    multiply_30[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n\n lambda_1 (\u001b[38;5;33mLambda\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m224\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  concatenate_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n depthwise_conv2d_37        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m224\u001b[0m)              \u001b[38;5;34m2,240\u001b[0m  lambda[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n activation_77              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_167[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_81              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_172[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n depthwise_conv2d_43        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m224\u001b[0m)              \u001b[38;5;34m2,240\u001b[0m  lambda_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n conv2d_40 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m8,320\u001b[0m  attention_block_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n conv2d_39 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)             \u001b[38;5;34m28,800\u001b[0m  depthwise_conv2d_37[\u001b[38;5;34m0\u001b[0m \n\n batch_normalization_188    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  activation_77[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_200    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  activation_81[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n conv2d_43 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m8,320\u001b[0m  attention_block_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n conv2d_42 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)             \u001b[38;5;34m28,800\u001b[0m  depthwise_conv2d_43[\u001b[38;5;34m0\u001b[0m \n\n depthwise_conv2d_39        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m1,280\u001b[0m  conv2d_40[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_38        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m1,280\u001b[0m  conv2d_39[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n activation_78              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_82              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_2 \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n depthwise_conv2d_45        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m1,280\u001b[0m  conv2d_43[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_44        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m1,280\u001b[0m  conv2d_42[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n add_44 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  depthwise_conv2d_39[\u001b[38;5;34m0\u001b[0m \n                                                                    depthwise_conv2d_38[\u001b[38;5;34m0\u001b[0m \n\n conv2d_173 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)               \u001b[38;5;34m16,384\u001b[0m  activation_78[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n depthwise_conv2d_193       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)               \u001b[38;5;34m12,800\u001b[0m  activation_78[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_194       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)               \u001b[38;5;34m25,088\u001b[0m  activation_78[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n conv2d_180 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)               \u001b[38;5;34m16,384\u001b[0m  activation_82[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n depthwise_conv2d_197       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)               \u001b[38;5;34m12,800\u001b[0m  activation_82[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_198       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)               \u001b[38;5;34m25,088\u001b[0m  activation_82[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n add_45 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  depthwise_conv2d_45[\u001b[38;5;34m0\u001b[0m \n                                                                    depthwise_conv2d_44[\u001b[38;5;34m0\u001b[0m \n\n activation_20              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  add_44[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n batch_normalization_189    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  conv2d_173[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_190    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  depthwise_conv2d_193[\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_191    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  depthwise_conv2d_194[\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_201    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  conv2d_180[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_202    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  depthwise_conv2d_197[\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_203    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  depthwise_conv2d_198[\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n activation_21              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  add_45[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n conv2d_44 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m8,256\u001b[0m  activation_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n depthwise_conv2d_46        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m1,280\u001b[0m  activation_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_47        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m3,328\u001b[0m  activation_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_48        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m1,280\u001b[0m  activation_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n conv2d_94 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m8,256\u001b[0m  attention_block_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n depthwise_conv2d_104       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m1,280\u001b[0m  attention_block_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_105       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m3,328\u001b[0m  attention_block_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_106       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m1,280\u001b[0m  attention_block_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n concatenate_83             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m1536\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      batch_normalization_1 \n                                                                    batch_normalization_1 \n\n concatenate_85             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m1536\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_2 \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      batch_normalization_2 \n                                                                    batch_normalization_2 \n\n conv2d_47 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m8,256\u001b[0m  activation_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n depthwise_conv2d_52        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m1,280\u001b[0m  activation_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_53        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m3,328\u001b[0m  activation_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_54        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m1,280\u001b[0m  activation_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n conv2d_97 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m8,256\u001b[0m  attention_block_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n depthwise_conv2d_110       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m1,280\u001b[0m  attention_block_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_111       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m3,328\u001b[0m  attention_block_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_112       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m1,280\u001b[0m  attention_block_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n concatenate_22             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m448\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  conv2d_44[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      depthwise_conv2d_46[\u001b[38;5;34m0\u001b[0m \n                                                                    depthwise_conv2d_47[\u001b[38;5;34m0\u001b[0m \n                                                                    depthwise_conv2d_48[\u001b[38;5;34m0\u001b[0m \n\n concatenate_46             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m448\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  conv2d_94[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      depthwise_conv2d_104[\u001b[38;5;34m\u001b[0m \n                                                                    depthwise_conv2d_105[\u001b[38;5;34m\u001b[0m \n                                                                    depthwise_conv2d_106[\u001b[38;5;34m\u001b[0m \n\n conv2d_174 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)               \u001b[38;5;34m49,152\u001b[0m  concatenate_83[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n conv2d_181 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)               \u001b[38;5;34m49,152\u001b[0m  concatenate_85[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n concatenate_23             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m448\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  conv2d_47[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      depthwise_conv2d_52[\u001b[38;5;34m0\u001b[0m \n                                                                    depthwise_conv2d_53[\u001b[38;5;34m0\u001b[0m \n                                                                    depthwise_conv2d_54[\u001b[38;5;34m0\u001b[0m \n\n concatenate_47             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m448\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  conv2d_97[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      depthwise_conv2d_110[\u001b[38;5;34m\u001b[0m \n                                                                    depthwise_conv2d_111[\u001b[38;5;34m\u001b[0m \n                                                                    depthwise_conv2d_112[\u001b[38;5;34m\u001b[0m \n\n lambda_2 (\u001b[38;5;33mLambda\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m448\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  concatenate_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n lambda_6 (\u001b[38;5;33mLambda\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m448\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  concatenate_46[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n batch_normalization_192    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  conv2d_174[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_204    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  conv2d_181[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n lambda_3 (\u001b[38;5;33mLambda\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m448\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  concatenate_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n lambda_7 (\u001b[38;5;33mLambda\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m448\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  concatenate_47[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n depthwise_conv2d_49        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m448\u001b[0m)              \u001b[38;5;34m4,480\u001b[0m  lambda_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_107       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m448\u001b[0m)              \u001b[38;5;34m4,480\u001b[0m  lambda_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n\n global_max_pooling2d_43    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mGlobalMaxPooling2D\u001b[0m)                                                                      \n\n global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_2 \n (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n\n global_max_pooling2d_45    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_2 \n (\u001b[38;5;33mGlobalMaxPooling2D\u001b[0m)                                                                      \n\n depthwise_conv2d_55        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m448\u001b[0m)              \u001b[38;5;34m4,480\u001b[0m  lambda_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_113       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m448\u001b[0m)              \u001b[38;5;34m4,480\u001b[0m  lambda_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n conv2d_46 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m256\u001b[0m)             \u001b[38;5;34m33,024\u001b[0m  activation_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n conv2d_45 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m256\u001b[0m)            \u001b[38;5;34m114,944\u001b[0m  depthwise_conv2d_49[\u001b[38;5;34m0\u001b[0m \n\n conv2d_96 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m256\u001b[0m)             \u001b[38;5;34m33,024\u001b[0m  attention_block_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n conv2d_95 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m256\u001b[0m)            \u001b[38;5;34m114,944\u001b[0m  depthwise_conv2d_107[\u001b[38;5;34m\u001b[0m \n\n add_168 (\u001b[38;5;33mAdd\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n add_173 (\u001b[38;5;33mAdd\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n conv2d_49 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m256\u001b[0m)             \u001b[38;5;34m33,024\u001b[0m  activation_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n conv2d_48 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m256\u001b[0m)            \u001b[38;5;34m114,944\u001b[0m  depthwise_conv2d_55[\u001b[38;5;34m0\u001b[0m \n\n conv2d_99 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m256\u001b[0m)             \u001b[38;5;34m33,024\u001b[0m  attention_block_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n conv2d_98 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m256\u001b[0m)            \u001b[38;5;34m114,944\u001b[0m  depthwise_conv2d_113[\u001b[38;5;34m\u001b[0m \n\n depthwise_conv2d_51        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m2,560\u001b[0m  conv2d_46[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_50        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m2,560\u001b[0m  conv2d_45[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_109       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m2,560\u001b[0m  conv2d_96[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_108       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m2,560\u001b[0m  conv2d_95[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n dense_65 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                      \u001b[38;5;34m16,384\u001b[0m  add_168[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n\n dense_69 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                      \u001b[38;5;34m16,384\u001b[0m  add_173[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n\n depthwise_conv2d_57        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m2,560\u001b[0m  conv2d_49[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_56        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m2,560\u001b[0m  conv2d_48[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_115       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m2,560\u001b[0m  conv2d_99[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_114       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m2,560\u001b[0m  conv2d_98[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n add_46 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  depthwise_conv2d_51[\u001b[38;5;34m0\u001b[0m \n                                                                    depthwise_conv2d_50[\u001b[38;5;34m0\u001b[0m \n\n add_94 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  depthwise_conv2d_109[\u001b[38;5;34m\u001b[0m \n                                                                    depthwise_conv2d_108[\u001b[38;5;34m\u001b[0m \n\n dense_66 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                     \u001b[38;5;34m16,384\u001b[0m  dense_65[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n dense_70 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                     \u001b[38;5;34m16,384\u001b[0m  dense_69[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n add_47 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  depthwise_conv2d_57[\u001b[38;5;34m0\u001b[0m \n                                                                    depthwise_conv2d_56[\u001b[38;5;34m0\u001b[0m \n\n add_95 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  depthwise_conv2d_115[\u001b[38;5;34m\u001b[0m \n                                                                    depthwise_conv2d_114[\u001b[38;5;34m\u001b[0m \n\n activation_22              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_46[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_44              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_94[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n conv2d_175 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)              \u001b[38;5;34m262,144\u001b[0m  activation_78[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n reshape_29 (\u001b[38;5;33mReshape\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  dense_66[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n conv2d_182 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)              \u001b[38;5;34m262,144\u001b[0m  activation_82[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n reshape_31 (\u001b[38;5;33mReshape\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  dense_70[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n activation_23              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_47[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_45              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_95[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n conv2d_50 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)               \u001b[38;5;34m32,896\u001b[0m  activation_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n depthwise_conv2d_58        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m2,560\u001b[0m  activation_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_59        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m6,656\u001b[0m  activation_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_60        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m2,560\u001b[0m  activation_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n conv2d_100 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)               \u001b[38;5;34m32,896\u001b[0m  activation_44[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n depthwise_conv2d_116       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m2,560\u001b[0m  activation_44[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_117       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m6,656\u001b[0m  activation_44[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_118       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m2,560\u001b[0m  activation_44[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n conv2d_144 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)               \u001b[38;5;34m32,896\u001b[0m  attention_block_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n depthwise_conv2d_162       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m2,560\u001b[0m  attention_block_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_163       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m6,656\u001b[0m  attention_block_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_164       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m2,560\u001b[0m  attention_block_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n batch_normalization_193    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  conv2d_175[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n multiply_29 (\u001b[38;5;33mMultiply\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n                                                                    reshape_29[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n batch_normalization_205    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  conv2d_182[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n multiply_31 (\u001b[38;5;33mMultiply\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_2 \n                                                                    reshape_31[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n conv2d_53 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)               \u001b[38;5;34m32,896\u001b[0m  activation_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n depthwise_conv2d_64        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m2,560\u001b[0m  activation_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_65        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m6,656\u001b[0m  activation_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_66        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m2,560\u001b[0m  activation_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n conv2d_103 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)               \u001b[38;5;34m32,896\u001b[0m  activation_45[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n depthwise_conv2d_122       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m2,560\u001b[0m  activation_45[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_123       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m6,656\u001b[0m  activation_45[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_124       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m2,560\u001b[0m  activation_45[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n conv2d_147 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)               \u001b[38;5;34m32,896\u001b[0m  attention_block_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n depthwise_conv2d_168       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m2,560\u001b[0m  attention_block_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_169       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m6,656\u001b[0m  attention_block_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_170       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m2,560\u001b[0m  attention_block_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n concatenate_24             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m896\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  conv2d_50[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      depthwise_conv2d_58[\u001b[38;5;34m0\u001b[0m \n                                                                    depthwise_conv2d_59[\u001b[38;5;34m0\u001b[0m \n                                                                    depthwise_conv2d_60[\u001b[38;5;34m0\u001b[0m \n\n concatenate_48             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m896\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  conv2d_100[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      depthwise_conv2d_116[\u001b[38;5;34m\u001b[0m \n                                                                    depthwise_conv2d_117[\u001b[38;5;34m\u001b[0m \n                                                                    depthwise_conv2d_118[\u001b[38;5;34m\u001b[0m \n\n concatenate_70             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m896\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  conv2d_144[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      depthwise_conv2d_162[\u001b[38;5;34m\u001b[0m \n                                                                    depthwise_conv2d_163[\u001b[38;5;34m\u001b[0m \n                                                                    depthwise_conv2d_164[\u001b[38;5;34m\u001b[0m \n\n add_169 (\u001b[38;5;33mAdd\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n                                                                    multiply_29[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n\n add_174 (\u001b[38;5;33mAdd\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_2 \n                                                                    multiply_31[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n\n concatenate_25             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m896\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  conv2d_53[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      depthwise_conv2d_64[\u001b[38;5;34m0\u001b[0m \n                                                                    depthwise_conv2d_65[\u001b[38;5;34m0\u001b[0m \n                                                                    depthwise_conv2d_66[\u001b[38;5;34m0\u001b[0m \n\n concatenate_49             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m896\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  conv2d_103[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      depthwise_conv2d_122[\u001b[38;5;34m\u001b[0m \n                                                                    depthwise_conv2d_123[\u001b[38;5;34m\u001b[0m \n                                                                    depthwise_conv2d_124[\u001b[38;5;34m\u001b[0m \n\n concatenate_71             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m896\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  conv2d_147[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      depthwise_conv2d_168[\u001b[38;5;34m\u001b[0m \n                                                                    depthwise_conv2d_169[\u001b[38;5;34m\u001b[0m \n                                                                    depthwise_conv2d_170[\u001b[38;5;34m\u001b[0m \n\n lambda_4 (\u001b[38;5;33mLambda\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m896\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  concatenate_24[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n lambda_8 (\u001b[38;5;33mLambda\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m896\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  concatenate_48[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n lambda_10 (\u001b[38;5;33mLambda\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m896\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  concatenate_70[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n activation_79              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_169[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_83              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_174[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n lambda_5 (\u001b[38;5;33mLambda\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m896\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  concatenate_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n lambda_9 (\u001b[38;5;33mLambda\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m896\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  concatenate_49[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n lambda_11 (\u001b[38;5;33mLambda\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m896\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  concatenate_71[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n depthwise_conv2d_61        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m896\u001b[0m)                \u001b[38;5;34m8,960\u001b[0m  lambda_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_119       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m896\u001b[0m)                \u001b[38;5;34m8,960\u001b[0m  lambda_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_165       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m896\u001b[0m)                \u001b[38;5;34m8,960\u001b[0m  lambda_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n conv2d_176 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)              \u001b[38;5;34m262,656\u001b[0m  activation_79[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n conv2d_183 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)              \u001b[38;5;34m262,656\u001b[0m  activation_83[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n depthwise_conv2d_67        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m896\u001b[0m)                \u001b[38;5;34m8,960\u001b[0m  lambda_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_125       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m896\u001b[0m)                \u001b[38;5;34m8,960\u001b[0m  lambda_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_171       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m896\u001b[0m)                \u001b[38;5;34m8,960\u001b[0m  lambda_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n conv2d_52 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m512\u001b[0m)              \u001b[38;5;34m131,584\u001b[0m  activation_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n conv2d_51 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m512\u001b[0m)              \u001b[38;5;34m459,264\u001b[0m  depthwise_conv2d_61[\u001b[38;5;34m0\u001b[0m \n\n conv2d_102 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m512\u001b[0m)              \u001b[38;5;34m131,584\u001b[0m  activation_44[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n conv2d_101 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m512\u001b[0m)              \u001b[38;5;34m459,264\u001b[0m  depthwise_conv2d_119[\u001b[38;5;34m\u001b[0m \n\n conv2d_146 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m512\u001b[0m)              \u001b[38;5;34m131,584\u001b[0m  attention_block_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n conv2d_145 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m512\u001b[0m)              \u001b[38;5;34m459,264\u001b[0m  depthwise_conv2d_165[\u001b[38;5;34m\u001b[0m \n\n batch_normalization_194    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  conv2d_176[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_206    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  conv2d_183[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n conv2d_55 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m512\u001b[0m)              \u001b[38;5;34m131,584\u001b[0m  activation_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n conv2d_54 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m512\u001b[0m)              \u001b[38;5;34m459,264\u001b[0m  depthwise_conv2d_67[\u001b[38;5;34m0\u001b[0m \n\n conv2d_105 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m512\u001b[0m)              \u001b[38;5;34m131,584\u001b[0m  activation_45[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n conv2d_104 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m512\u001b[0m)              \u001b[38;5;34m459,264\u001b[0m  depthwise_conv2d_125[\u001b[38;5;34m\u001b[0m \n\n conv2d_149 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m512\u001b[0m)              \u001b[38;5;34m131,584\u001b[0m  attention_block_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n conv2d_148 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m512\u001b[0m)              \u001b[38;5;34m459,264\u001b[0m  depthwise_conv2d_171[\u001b[38;5;34m\u001b[0m \n\n depthwise_conv2d_63        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m5,120\u001b[0m  conv2d_52[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_62        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m5,120\u001b[0m  conv2d_51[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_121       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m5,120\u001b[0m  conv2d_102[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_120       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m5,120\u001b[0m  conv2d_101[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_167       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m5,120\u001b[0m  conv2d_146[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_166       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m5,120\u001b[0m  conv2d_145[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n add_170 (\u001b[38;5;33mAdd\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n                                                                    attention_block_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n add_175 (\u001b[38;5;33mAdd\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_2 \n                                                                    attention_block_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n depthwise_conv2d_69        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m5,120\u001b[0m  conv2d_55[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_68        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m5,120\u001b[0m  conv2d_54[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_127       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m5,120\u001b[0m  conv2d_105[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_126       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m5,120\u001b[0m  conv2d_104[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_173       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m5,120\u001b[0m  conv2d_149[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_172       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m5,120\u001b[0m  conv2d_148[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n add_48 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  depthwise_conv2d_63[\u001b[38;5;34m0\u001b[0m \n                                                                    depthwise_conv2d_62[\u001b[38;5;34m0\u001b[0m \n\n add_96 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  depthwise_conv2d_121[\u001b[38;5;34m\u001b[0m \n                                                                    depthwise_conv2d_120[\u001b[38;5;34m\u001b[0m \n\n add_142 (\u001b[38;5;33mAdd\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  depthwise_conv2d_167[\u001b[38;5;34m\u001b[0m \n                                                                    depthwise_conv2d_166[\u001b[38;5;34m\u001b[0m \n\n activation_80              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_170[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_84              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_175[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n add_49 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  depthwise_conv2d_69[\u001b[38;5;34m0\u001b[0m \n                                                                    depthwise_conv2d_68[\u001b[38;5;34m0\u001b[0m \n\n add_97 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  depthwise_conv2d_127[\u001b[38;5;34m\u001b[0m \n                                                                    depthwise_conv2d_126[\u001b[38;5;34m\u001b[0m \n\n add_143 (\u001b[38;5;33mAdd\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  depthwise_conv2d_173[\u001b[38;5;34m\u001b[0m \n                                                                    depthwise_conv2d_172[\u001b[38;5;34m\u001b[0m \n\n activation_24              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_48[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_46              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_96[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_66              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_142[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n attention_block_7          [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m),            \u001b[38;5;34m902,660\u001b[0m  activation_80[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   \n (\u001b[38;5;33mAttentionBlock\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)]                      activation_84[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n activation_25              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_49[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_47              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_97[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_67              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_143[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n concatenate_92             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m2048\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  activation_24[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      activation_46[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   \n                                                                    activation_66[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   \n                                                                    attention_block_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n concatenate_93             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m2048\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  activation_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      activation_47[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   \n                                                                    activation_67[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   \n                                                                    attention_block_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n dropout_20 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m2048\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  concatenate_92[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n dropout_21 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m2048\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  concatenate_93[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n concatenate_94             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4096\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  dropout_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      dropout_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n dropout_22 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4096\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  concatenate_94[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m)                         \u001b[38;5;34m0\u001b[0m  dropout_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n\n dense_72 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)                       \u001b[38;5;34m20,485\u001b[0m  global_average_poolin \n\n dense_73 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)                       \u001b[38;5;34m28,679\u001b[0m  global_average_poolin \n\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n<span style=\"font-weight: bold\"> Layer (type)              </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">        Param # </span><span style=\"font-weight: bold\"> Connected to           </span>\n\n input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                      \n\n input_layer_1              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                      \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                                              \n\n conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">9,472</span>  input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n\n conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">9,472</span>  input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n batch_normalization        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_1      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n activation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n activation_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n\n max_pooling2d              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  activation[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n\n max_pooling2d_1            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  activation_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n\n conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  max_pooling2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n depthwise_conv2d           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">1,600</span>  max_pooling2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_1         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">3,136</span>  max_pooling2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n conv2d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  max_pooling2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n\n depthwise_conv2d_4         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">1,600</span>  max_pooling2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_5         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">3,136</span>  max_pooling2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n batch_normalization_2      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_3      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  depthwise_conv2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_4      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  depthwise_conv2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_14     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_15     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  depthwise_conv2d_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_16     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  depthwise_conv2d_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n concatenate (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_2 \n                                                                    batch_normalization_3 \n                                                                    batch_normalization_4 \n\n concatenate_2              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      batch_normalization_1 \n                                                                    batch_normalization_1 \n\n conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>  concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n\n conv2d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>  concatenate_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n batch_normalization_5      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_17     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n global_average_pooling2d   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_5 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n\n global_max_pooling2d       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_5 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling2D</span>)                                                                      \n\n global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n\n global_max_pooling2d_2     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling2D</span>)                                                                      \n\n add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n                                                                    global_max_pooling2d[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n add_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              \n\n dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  add_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            \n\n dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            \n\n dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n\n conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span>  max_pooling2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n\n conv2d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span>  max_pooling2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n\n reshape_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n\n batch_normalization_6      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n multiply (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_5 \n                                                                    reshape[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n\n batch_normalization_18     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n multiply_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n                                                                    reshape_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n\n add_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_6 \n                                                                    multiply[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n add_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n                                                                    multiply_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n activation_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            \n\n activation_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            \n\n batch_normalization_7      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  activation_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_19     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  activation_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n activation_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_7 \n\n activation_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n\n conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  activation_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n\n depthwise_conv2d_2         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">1,600</span>  activation_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_3         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">3,136</span>  activation_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n conv2d_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  activation_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n\n depthwise_conv2d_6         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">1,600</span>  activation_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_7         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">3,136</span>  activation_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n batch_normalization_8      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_9      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  depthwise_conv2d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_10     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  depthwise_conv2d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_20     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_21     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  depthwise_conv2d_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_22     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  depthwise_conv2d_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n concatenate_1              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_8 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      batch_normalization_9 \n                                                                    batch_normalization_1 \n\n concatenate_3              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_2 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      batch_normalization_2 \n                                                                    batch_normalization_2 \n\n conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>  concatenate_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n conv2d_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>  concatenate_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n batch_normalization_11     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_23     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n\n global_max_pooling2d_1     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling2D</span>)                                                                      \n\n global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_2 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n\n global_max_pooling2d_3     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_2 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling2D</span>)                                                                      \n\n add_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n add_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  add_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            \n\n dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  add_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            \n\n dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n\n dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  dense_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n\n conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span>  activation_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n\n reshape_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n\n conv2d_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span>  activation_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n\n reshape_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n\n batch_normalization_12     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n multiply_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n                                                                    reshape_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n\n batch_normalization_24     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n multiply_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_2 \n                                                                    reshape_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n\n add_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n                                                                    multiply_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n add_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_2 \n                                                                    multiply_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n activation_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            \n\n activation_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            \n\n conv2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span>  activation_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n\n conv2d_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span>  activation_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n\n batch_normalization_13     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_25     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n add_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n                                                                    max_pooling2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n add_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_2 \n                                                                    max_pooling2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n\n activation_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            \n\n activation_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            \n\n dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  activation_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n\n dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  activation_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n\n attention_block            [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>),            <span style=\"color: #00af00; text-decoration-color: #00af00\">21,444</span>  dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],         \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AttentionBlock</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)]                     dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n\n conv2d_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  attention_block[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n\n depthwise_conv2d_17        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">1,600</span>  attention_block[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_18        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">3,136</span>  attention_block[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n conv2d_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  attention_block[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>]  \n\n depthwise_conv2d_21        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">1,600</span>  attention_block[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>]  \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_22        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">3,136</span>  attention_block[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>]  \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n batch_normalization_27     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_28     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  depthwise_conv2d_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_29     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  depthwise_conv2d_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_39     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_40     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  depthwise_conv2d_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_41     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  depthwise_conv2d_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n concatenate_10             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_2 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      batch_normalization_2 \n                                                                    batch_normalization_2 \n\n concatenate_12             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_3 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      batch_normalization_4 \n                                                                    batch_normalization_4 \n\n conv2d_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>  concatenate_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n conv2d_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>  concatenate_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n batch_normalization_30     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_42     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_28[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_3 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n\n global_max_pooling2d_6     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_3 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling2D</span>)                                                                      \n\n global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_4 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n\n global_max_pooling2d_8     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_4 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling2D</span>)                                                                      \n\n add_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n add_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  add_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n\n dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  add_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n\n dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  dense_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n\n dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  dense_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n conv2d_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span>  attention_block[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n\n reshape_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n conv2d_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span>  attention_block[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>]  \n\n reshape_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n batch_normalization_31     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n multiply_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_3 \n                                                                    reshape_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n\n batch_normalization_43     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_29[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n multiply_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_4 \n                                                                    reshape_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n\n add_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_3 \n                                                                    multiply_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n add_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_4 \n                                                                    multiply_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n activation_11              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_15              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_28[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n batch_normalization_32     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  activation_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_44     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  activation_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n activation_12              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_3 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_16              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_4 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n conv2d_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  activation_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n depthwise_conv2d_19        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">1,600</span>  activation_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_20        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">3,136</span>  activation_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n conv2d_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  activation_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n depthwise_conv2d_23        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">1,600</span>  activation_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_24        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">3,136</span>  activation_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n batch_normalization_33     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_34     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  depthwise_conv2d_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_35     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  depthwise_conv2d_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_45     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_30[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_46     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  depthwise_conv2d_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_47     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  depthwise_conv2d_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n concatenate_11             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_3 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      batch_normalization_3 \n                                                                    batch_normalization_3 \n\n concatenate_13             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_4 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      batch_normalization_4 \n                                                                    batch_normalization_4 \n\n conv2d_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>  concatenate_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n conv2d_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>  concatenate_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n batch_normalization_36     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_48     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_31[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_3 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n\n global_max_pooling2d_7     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_3 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling2D</span>)                                                                      \n\n global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_4 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n\n global_max_pooling2d_9     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_4 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling2D</span>)                                                                      \n\n add_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n add_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  add_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n\n dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  add_29[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n\n dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  dense_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  dense_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n conv2d_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span>  activation_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n reshape_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n conv2d_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span>  activation_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n reshape_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n batch_normalization_37     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n multiply_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_3 \n                                                                    reshape_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n\n batch_normalization_49     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_32[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n multiply_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_4 \n                                                                    reshape_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n\n add_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_3 \n                                                                    multiply_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n add_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_4 \n                                                                    multiply_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n activation_13              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_17              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_30[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n conv2d_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span>  activation_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n conv2d_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span>  activation_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n batch_normalization_38     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_50     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_33[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n add_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_3 \n                                                                    attention_block[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n\n add_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_5 \n                                                                    attention_block[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>]  \n\n activation_14              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_18              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_31[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  activation_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  activation_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n attention_block_1          [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>),            <span style=\"color: #00af00; text-decoration-color: #00af00\">21,444</span>  dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AttentionBlock</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)]                     dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n\n conv2d_56 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  attention_block_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n depthwise_conv2d_70        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">1,600</span>  attention_block_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_71        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">3,136</span>  attention_block_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n conv2d_64 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  attention_block_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n depthwise_conv2d_74        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">1,600</span>  attention_block_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_75        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">3,136</span>  attention_block_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n batch_normalization_52     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_56[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_53     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  depthwise_conv2d_70[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_54     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  depthwise_conv2d_71[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_65     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_64[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_66     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  depthwise_conv2d_74[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_67     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  depthwise_conv2d_75[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n concatenate_26             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_5 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      batch_normalization_5 \n                                                                    batch_normalization_5 \n\n concatenate_28             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_6 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      batch_normalization_6 \n                                                                    batch_normalization_6 \n\n conv2d_57 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  concatenate_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n conv2d_65 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  concatenate_28[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n batch_normalization_55     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_57[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_68     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_65[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_5 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n\n global_max_pooling2d_12    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_5 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling2D</span>)                                                                      \n\n global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_6 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n\n global_max_pooling2d_14    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_6 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling2D</span>)                                                                      \n\n add_50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n add_55 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n dense_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                        <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  add_50[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n\n dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                        <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  add_55[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n\n dense_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  dense_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n dense_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  dense_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n conv2d_58 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">8,192</span>  attention_block_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n reshape_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n conv2d_66 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">8,192</span>  attention_block_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n reshape_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n batch_normalization_56     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_58[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n multiply_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_5 \n                                                                    reshape_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n\n batch_normalization_69     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_66[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n multiply_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_6 \n                                                                    reshape_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n add_51 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_5 \n                                                                    multiply_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n add_56 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_6 \n                                                                    multiply_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n\n activation_26              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_51[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_30              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_56[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n batch_normalization_57     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  activation_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_70     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  activation_30[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n activation_27              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_5 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_31              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_7 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n conv2d_59 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  activation_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n depthwise_conv2d_72        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">3,200</span>  activation_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_73        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">6,272</span>  activation_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n conv2d_67 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  activation_31[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n depthwise_conv2d_76        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">3,200</span>  activation_31[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_77        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">6,272</span>  activation_31[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n batch_normalization_58     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_59[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_59     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  depthwise_conv2d_72[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_60     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  depthwise_conv2d_73[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_71     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_67[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_72     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  depthwise_conv2d_76[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_73     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  depthwise_conv2d_77[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n concatenate_27             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_5 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      batch_normalization_5 \n                                                                    batch_normalization_6 \n\n concatenate_29             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_7 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      batch_normalization_7 \n                                                                    batch_normalization_7 \n\n conv2d_60 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">3,072</span>  concatenate_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n conv2d_68 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">3,072</span>  concatenate_29[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n batch_normalization_61     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_60[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_74     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_68[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_6 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n\n global_max_pooling2d_13    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_6 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling2D</span>)                                                                      \n\n global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_7 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n\n global_max_pooling2d_15    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_7 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling2D</span>)                                                                      \n\n add_52 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n add_57 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n dense_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                        <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  add_52[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n\n dense_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                        <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  add_57[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n\n dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  dense_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n dense_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  dense_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n conv2d_61 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">16,384</span>  activation_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n reshape_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n conv2d_69 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">16,384</span>  activation_31[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n reshape_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n batch_normalization_62     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_61[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n multiply_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_6 \n                                                                    reshape_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n\n batch_normalization_75     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_69[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n multiply_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_7 \n                                                                    reshape_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n add_53 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_6 \n                                                                    multiply_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n add_58 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_7 \n                                                                    multiply_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n\n activation_28              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_53[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_32              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_58[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n conv2d_62 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span>  activation_28[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n conv2d_63 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span>  attention_block_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n conv2d_70 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span>  activation_32[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n conv2d_71 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span>  attention_block_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n batch_normalization_63     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_62[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_64     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_63[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_76     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_70[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_77     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_71[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n add_54 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_6 \n                                                                    batch_normalization_6 \n\n add_59 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_7 \n                                                                    batch_normalization_7 \n\n activation_29              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_54[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_33              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_59[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  activation_29[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  activation_33[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n attention_block_2          [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>),           <span style=\"color: #00af00; text-decoration-color: #00af00\">68,996</span>  dropout_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AttentionBlock</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)]                    dropout_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n\n conv2d_76 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  attention_block_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n depthwise_conv2d_87        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">3,200</span>  attention_block_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_88        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">6,272</span>  attention_block_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n conv2d_83 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  attention_block_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n depthwise_conv2d_91        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">3,200</span>  attention_block_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_92        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">6,272</span>  attention_block_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n batch_normalization_79     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_76[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_80     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  depthwise_conv2d_87[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_81     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  depthwise_conv2d_88[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_91     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_83[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_92     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  depthwise_conv2d_91[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_93     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  depthwise_conv2d_92[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n concatenate_36             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_7 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      batch_normalization_8 \n                                                                    batch_normalization_8 \n\n concatenate_38             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_9 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      batch_normalization_9 \n                                                                    batch_normalization_9 \n\n conv2d_77 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">3,072</span>  concatenate_36[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n conv2d_84 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">3,072</span>  concatenate_38[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n batch_normalization_82     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_77[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_94     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_84[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_8 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n\n global_max_pooling2d_18    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_8 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling2D</span>)                                                                      \n\n global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_9 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n\n global_max_pooling2d_20    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_9 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling2D</span>)                                                                      \n\n add_72 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n add_77 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n dense_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                        <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  add_72[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n\n dense_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                        <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  add_77[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n\n dense_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  dense_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n dense_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  dense_31[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n conv2d_78 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">16,384</span>  attention_block_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n reshape_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_28[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n conv2d_85 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">16,384</span>  attention_block_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n reshape_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_32[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n batch_normalization_83     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_78[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n multiply_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_8 \n                                                                    reshape_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n batch_normalization_95     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_85[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n multiply_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_9 \n                                                                    reshape_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n add_73 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_8 \n                                                                    multiply_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n\n add_78 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_9 \n                                                                    multiply_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n\n activation_35              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_73[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_39              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_78[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n batch_normalization_84     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  activation_35[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_96     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  activation_39[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n activation_36              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_8 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_40              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_9 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n conv2d_79 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  activation_36[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n depthwise_conv2d_89        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">3,200</span>  activation_36[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_90        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">6,272</span>  activation_36[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n conv2d_86 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  activation_40[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n depthwise_conv2d_93        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">3,200</span>  activation_40[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_94        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">6,272</span>  activation_40[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n batch_normalization_85     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_79[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_86     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  depthwise_conv2d_89[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_87     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  depthwise_conv2d_90[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_97     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_86[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_98     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  depthwise_conv2d_93[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_99     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  depthwise_conv2d_94[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n concatenate_37             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_8 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      batch_normalization_8 \n                                                                    batch_normalization_8 \n\n concatenate_39             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_9 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      batch_normalization_9 \n                                                                    batch_normalization_9 \n\n conv2d_80 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">3,072</span>  concatenate_37[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n conv2d_87 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">3,072</span>  concatenate_39[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n batch_normalization_88     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_80[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_100    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_87[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_8 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n\n global_max_pooling2d_19    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_8 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling2D</span>)                                                                      \n\n global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n\n global_max_pooling2d_21    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling2D</span>)                                                                      \n\n add_74 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n add_79 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n dense_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                        <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  add_74[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n\n dense_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                        <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  add_79[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n\n dense_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  dense_29[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n dense_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  dense_33[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n conv2d_81 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">16,384</span>  activation_36[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n reshape_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_30[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n conv2d_88 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">16,384</span>  activation_40[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n reshape_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_34[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n batch_normalization_89     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_81[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n multiply_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_8 \n                                                                    reshape_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n batch_normalization_101    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_88[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n multiply_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n                                                                    reshape_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n add_75 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_8 \n                                                                    multiply_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n\n add_80 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n                                                                    multiply_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n\n activation_37              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_75[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_41              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_80[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n conv2d_82 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span>  activation_37[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n conv2d_89 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span>  activation_41[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n batch_normalization_90     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_82[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_102    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_89[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n add_76 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_9 \n                                                                    attention_block_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n add_81 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n                                                                    attention_block_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n activation_38              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_76[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_42              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_81[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  activation_38[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n dropout_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  activation_42[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n attention_block_3          [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>),           <span style=\"color: #00af00; text-decoration-color: #00af00\">68,996</span>  dropout_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AttentionBlock</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)]                    dropout_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n conv2d_106 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  attention_block_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n depthwise_conv2d_128       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">3,200</span>  attention_block_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_129       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">6,272</span>  attention_block_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n conv2d_114 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  attention_block_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n depthwise_conv2d_132       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">3,200</span>  attention_block_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_133       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">6,272</span>  attention_block_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n batch_normalization_104    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_106[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_105    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  depthwise_conv2d_128[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_106    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  depthwise_conv2d_129[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_117    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_114[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_118    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  depthwise_conv2d_132[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_119    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  depthwise_conv2d_133[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n concatenate_50             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      batch_normalization_1 \n                                                                    batch_normalization_1 \n\n concatenate_52             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      batch_normalization_1 \n                                                                    batch_normalization_1 \n\n conv2d_107 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">8,192</span>  concatenate_50[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n conv2d_115 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">8,192</span>  concatenate_52[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n batch_normalization_107    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_107[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_120    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_115[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n\n global_max_pooling2d_24    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling2D</span>)                                                                      \n\n global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n\n global_max_pooling2d_26    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling2D</span>)                                                                      \n\n add_98 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n add_103 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n dense_36 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                       <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span>  add_98[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n\n dense_40 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                       <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span>  add_103[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n\n dense_37 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span>  dense_36[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n dense_41 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span>  dense_40[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n conv2d_108 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">32,768</span>  attention_block_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n reshape_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_37[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n conv2d_116 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">32,768</span>  attention_block_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n reshape_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_41[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n batch_normalization_108    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_108[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n multiply_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n                                                                    reshape_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n batch_normalization_121    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_116[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n multiply_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n                                                                    reshape_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n add_99 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n                                                                    multiply_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n\n add_104 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n                                                                    multiply_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n\n activation_48              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_99[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_52              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_104[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n batch_normalization_109    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  activation_48[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_122    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  activation_52[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n activation_49              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_53              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n conv2d_109 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span>  activation_49[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n depthwise_conv2d_130       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">6,400</span>  activation_49[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_131       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">12,544</span>  activation_49[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n conv2d_117 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span>  activation_53[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n depthwise_conv2d_134       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">6,400</span>  activation_53[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_135       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">12,544</span>  activation_53[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n batch_normalization_110    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_109[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_111    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  depthwise_conv2d_130[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_112    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  depthwise_conv2d_131[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_123    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_117[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_124    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  depthwise_conv2d_134[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_125    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  depthwise_conv2d_135[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n concatenate_51             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      batch_normalization_1 \n                                                                    batch_normalization_1 \n\n concatenate_53             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      batch_normalization_1 \n                                                                    batch_normalization_1 \n\n conv2d_110 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">12,288</span>  concatenate_51[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n conv2d_118 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">12,288</span>  concatenate_53[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n batch_normalization_113    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_110[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_126    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_118[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n\n global_max_pooling2d_25    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling2D</span>)                                                                      \n\n global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n\n global_max_pooling2d_27    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling2D</span>)                                                                      \n\n add_100 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n add_105 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n dense_38 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                       <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span>  add_100[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n\n dense_42 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                       <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span>  add_105[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n\n dense_39 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span>  dense_38[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n dense_43 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span>  dense_42[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n conv2d_111 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">65,536</span>  activation_49[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n reshape_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_39[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n conv2d_119 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">65,536</span>  activation_53[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n reshape_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_43[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n batch_normalization_114    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_111[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n multiply_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n                                                                    reshape_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n batch_normalization_127    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_119[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n multiply_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n                                                                    reshape_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n add_101 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n                                                                    multiply_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n\n add_106 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n                                                                    multiply_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n\n activation_50              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_101[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_54              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_106[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n conv2d_112 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span>  activation_50[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n conv2d_113 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span>  attention_block_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n conv2d_120 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span>  activation_54[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n conv2d_121 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span>  attention_block_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n batch_normalization_115    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_112[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_116    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_113[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_128    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_120[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_129    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_121[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n add_102 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n                                                                    batch_normalization_1 \n\n add_107 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n                                                                    batch_normalization_1 \n\n activation_51              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_102[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_55              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_107[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n dropout_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  activation_51[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n dropout_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  activation_55[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n attention_block_4          [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),            <span style=\"color: #00af00; text-decoration-color: #00af00\">242,436</span>  dropout_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AttentionBlock</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]                      dropout_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n conv2d_126 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span>  attention_block_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n depthwise_conv2d_145       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">6,400</span>  attention_block_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_146       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">12,544</span>  attention_block_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n conv2d_133 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span>  attention_block_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n depthwise_conv2d_149       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">6,400</span>  attention_block_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_150       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">12,544</span>  attention_block_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n batch_normalization_131    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_126[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_132    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  depthwise_conv2d_145[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_133    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  depthwise_conv2d_146[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_143    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_133[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_144    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  depthwise_conv2d_149[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_145    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  depthwise_conv2d_150[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n concatenate_60             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      batch_normalization_1 \n                                                                    batch_normalization_1 \n\n concatenate_62             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      batch_normalization_1 \n                                                                    batch_normalization_1 \n\n conv2d_127 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">12,288</span>  concatenate_60[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n conv2d_134 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">12,288</span>  concatenate_62[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n batch_normalization_134    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_127[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_146    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_134[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n\n global_max_pooling2d_30    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling2D</span>)                                                                      \n\n global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n\n global_max_pooling2d_32    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling2D</span>)                                                                      \n\n add_120 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n add_125 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n dense_45 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                       <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span>  add_120[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n\n dense_49 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                       <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span>  add_125[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n\n dense_46 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span>  dense_45[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n dense_50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span>  dense_49[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n conv2d_128 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">65,536</span>  attention_block_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n reshape_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_46[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n conv2d_135 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">65,536</span>  attention_block_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n reshape_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_50[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n batch_normalization_135    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_128[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n multiply_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n                                                                    reshape_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n batch_normalization_147    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_135[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n multiply_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n                                                                    reshape_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n add_121 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n                                                                    multiply_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n\n add_126 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n                                                                    multiply_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n\n activation_57              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_121[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_61              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_126[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n batch_normalization_136    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  activation_57[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_148    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  activation_61[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n activation_58              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_62              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n conv2d_129 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span>  activation_58[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n depthwise_conv2d_147       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">6,400</span>  activation_58[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_148       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">12,544</span>  activation_58[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n conv2d_136 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span>  activation_62[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n depthwise_conv2d_151       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">6,400</span>  activation_62[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_152       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">12,544</span>  activation_62[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n batch_normalization_137    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_129[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_138    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  depthwise_conv2d_147[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_139    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  depthwise_conv2d_148[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_149    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_136[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_150    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  depthwise_conv2d_151[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_151    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  depthwise_conv2d_152[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n concatenate_61             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      batch_normalization_1 \n                                                                    batch_normalization_1 \n\n concatenate_63             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      batch_normalization_1 \n                                                                    batch_normalization_1 \n\n conv2d_130 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">12,288</span>  concatenate_61[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n conv2d_137 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">12,288</span>  concatenate_63[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n batch_normalization_140    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_130[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_152    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_137[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n\n global_max_pooling2d_31    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling2D</span>)                                                                      \n\n global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n\n global_max_pooling2d_33    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling2D</span>)                                                                      \n\n add_122 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n add_127 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n dense_47 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                       <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span>  add_122[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n\n dense_51 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                       <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span>  add_127[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n\n dense_48 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span>  dense_47[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n dense_52 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span>  dense_51[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n conv2d_131 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">65,536</span>  activation_58[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n reshape_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_48[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n conv2d_138 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">65,536</span>  activation_62[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n reshape_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_52[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n batch_normalization_141    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_131[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n multiply_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n                                                                    reshape_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n batch_normalization_153    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_138[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n multiply_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n                                                                    reshape_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n add_123 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n                                                                    multiply_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n\n add_128 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n                                                                    multiply_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n\n activation_59              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_123[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_63              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_128[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n conv2d_132 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span>  activation_59[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n conv2d_139 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span>  activation_63[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n batch_normalization_142    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_132[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_154    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_139[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n add_124 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n                                                                    attention_block_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n add_129 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n                                                                    attention_block_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n activation_60              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_124[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_64              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_129[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n dropout_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  activation_60[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n dropout_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  activation_64[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n attention_block_5          [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),            <span style=\"color: #00af00; text-decoration-color: #00af00\">242,436</span>  dropout_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AttentionBlock</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]                      dropout_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n conv2d_150 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">8,192</span>  attention_block_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n depthwise_conv2d_174       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">6,400</span>  attention_block_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_175       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">12,544</span>  attention_block_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n conv2d_158 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">8,192</span>  attention_block_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n depthwise_conv2d_178       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">6,400</span>  attention_block_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_179       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">12,544</span>  attention_block_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n batch_normalization_156    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  conv2d_150[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_157    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  depthwise_conv2d_174[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_158    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  depthwise_conv2d_175[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_169    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  conv2d_158[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_170    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  depthwise_conv2d_178[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_171    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  depthwise_conv2d_179[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n concatenate_72             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      batch_normalization_1 \n                                                                    batch_normalization_1 \n\n concatenate_74             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      batch_normalization_1 \n                                                                    batch_normalization_1 \n\n conv2d_151 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">32,768</span>  concatenate_72[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n conv2d_159 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">32,768</span>  concatenate_74[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n batch_normalization_159    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  conv2d_151[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_172    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  conv2d_159[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n\n global_max_pooling2d_36    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling2D</span>)                                                                      \n\n global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n\n global_max_pooling2d_38    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling2D</span>)                                                                      \n\n add_144 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n add_149 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n dense_54 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">16,384</span>  add_144[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n\n dense_58 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">16,384</span>  add_149[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n\n dense_55 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,384</span>  dense_54[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n dense_59 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,384</span>  dense_58[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n conv2d_152 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">131,072</span>  attention_block_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n reshape_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_55[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n conv2d_160 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">131,072</span>  attention_block_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n reshape_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_59[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n batch_normalization_160    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  conv2d_152[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n multiply_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n                                                                    reshape_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n batch_normalization_173    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  conv2d_160[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n multiply_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n                                                                    reshape_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n add_145 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n                                                                    multiply_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n\n add_150 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n                                                                    multiply_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n\n activation_68              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_145[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_72              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_150[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n batch_normalization_161    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  activation_68[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_174    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  activation_72[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n activation_69              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_73              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n conv2d_153 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">16,384</span>  activation_69[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n depthwise_conv2d_176       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">12,800</span>  activation_69[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_177       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">25,088</span>  activation_69[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n conv2d_161 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">16,384</span>  activation_73[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n depthwise_conv2d_180       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">12,800</span>  activation_73[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_181       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">25,088</span>  activation_73[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n batch_normalization_162    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  conv2d_153[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_163    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  depthwise_conv2d_176[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_164    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  depthwise_conv2d_177[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_175    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  conv2d_161[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_176    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  depthwise_conv2d_180[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_177    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  depthwise_conv2d_181[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n concatenate_73             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1536</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      batch_normalization_1 \n                                                                    batch_normalization_1 \n\n concatenate_75             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1536</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      batch_normalization_1 \n                                                                    batch_normalization_1 \n\n conv2d_154 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">49,152</span>  concatenate_73[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n conv2d_162 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">49,152</span>  concatenate_75[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n batch_normalization_165    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  conv2d_154[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_178    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  conv2d_162[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n\n global_max_pooling2d_37    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling2D</span>)                                                                      \n\n global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n\n global_max_pooling2d_39    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling2D</span>)                                                                      \n\n add_146 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n add_151 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n dense_56 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">16,384</span>  add_146[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n\n dense_60 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">16,384</span>  add_151[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n\n dense_57 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,384</span>  dense_56[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n dense_61 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,384</span>  dense_60[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n conv2d_155 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">262,144</span>  activation_69[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n reshape_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_57[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n conv2d_163 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">262,144</span>  activation_73[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n reshape_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_61[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n batch_normalization_166    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  conv2d_155[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n multiply_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n                                                                    reshape_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n batch_normalization_179    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  conv2d_163[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n multiply_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n                                                                    reshape_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n add_147 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n                                                                    multiply_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n\n add_152 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n                                                                    multiply_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n\n activation_70              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_147[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_74              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_152[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n conv2d_156 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span>  activation_70[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n conv2d_157 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span>  attention_block_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n conv2d_164 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span>  activation_74[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n conv2d_165 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span>  attention_block_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n batch_normalization_167    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  conv2d_156[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_168    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  conv2d_157[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_180    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  conv2d_164[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_181    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  conv2d_165[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n add_148 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n                                                                    batch_normalization_1 \n\n add_153 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n                                                                    batch_normalization_1 \n\n activation_71              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_148[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_75              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_153[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n attention_block_6          [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>),            <span style=\"color: #00af00; text-decoration-color: #00af00\">902,660</span>  activation_71[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AttentionBlock</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)]                      activation_75[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n conv2d_170 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">16,384</span>  attention_block_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n depthwise_conv2d_191       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">12,800</span>  attention_block_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_192       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">25,088</span>  attention_block_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n conv2d_177 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">16,384</span>  attention_block_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n depthwise_conv2d_195       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">12,800</span>  attention_block_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_196       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">25,088</span>  attention_block_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n batch_normalization_183    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  conv2d_170[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_184    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  depthwise_conv2d_191[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_185    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  depthwise_conv2d_192[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_195    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  conv2d_177[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_196    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  depthwise_conv2d_195[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_197    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  depthwise_conv2d_196[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n concatenate_82             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1536</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      batch_normalization_1 \n                                                                    batch_normalization_1 \n\n concatenate_84             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1536</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      batch_normalization_1 \n                                                                    batch_normalization_1 \n\n conv2d_171 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">49,152</span>  concatenate_82[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n conv2d_178 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">49,152</span>  concatenate_84[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n batch_normalization_186    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  conv2d_171[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_198    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  conv2d_178[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n\n global_max_pooling2d_42    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling2D</span>)                                                                      \n\n global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n\n global_max_pooling2d_44    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling2D</span>)                                                                      \n\n add_166 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n add_171 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n dense_63 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">16,384</span>  add_166[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n\n dense_67 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">16,384</span>  add_171[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n\n dense_64 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,384</span>  dense_63[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n dense_68 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,384</span>  dense_67[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n conv2d_38 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span>  attention_block_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n depthwise_conv2d_34        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span>  attention_block_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_35        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">1,664</span>  attention_block_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_36        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span>  attention_block_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n conv2d_172 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">262,144</span>  attention_block_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n reshape_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_64[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n conv2d_179 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">262,144</span>  attention_block_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n reshape_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_68[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n conv2d_41 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span>  attention_block_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n depthwise_conv2d_40        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span>  attention_block_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_41        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">1,664</span>  attention_block_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_42        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span>  attention_block_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n concatenate_20             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv2d_38[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      depthwise_conv2d_34[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n                                                                    depthwise_conv2d_35[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n                                                                    depthwise_conv2d_36[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n batch_normalization_187    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  conv2d_172[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n multiply_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n                                                                    reshape_28[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n batch_normalization_199    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  conv2d_179[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n multiply_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n                                                                    reshape_30[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n concatenate_21             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv2d_41[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      depthwise_conv2d_40[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n                                                                    depthwise_conv2d_41[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n                                                                    depthwise_conv2d_42[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n lambda (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  concatenate_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n add_167 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n                                                                    multiply_28[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n\n add_172 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n                                                                    multiply_30[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n\n lambda_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  concatenate_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n depthwise_conv2d_37        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">2,240</span>  lambda[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n activation_77              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_167[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_81              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_172[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n depthwise_conv2d_43        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">2,240</span>  lambda_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n conv2d_40 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span>  attention_block_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n conv2d_39 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">28,800</span>  depthwise_conv2d_37[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n batch_normalization_188    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  activation_77[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_200    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  activation_81[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n conv2d_43 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span>  attention_block_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n conv2d_42 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">28,800</span>  depthwise_conv2d_43[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n depthwise_conv2d_39        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280</span>  conv2d_40[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_38        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280</span>  conv2d_39[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n activation_78              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_82              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_2 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n depthwise_conv2d_45        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280</span>  conv2d_43[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_44        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280</span>  conv2d_42[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n add_44 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  depthwise_conv2d_39[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n                                                                    depthwise_conv2d_38[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n conv2d_173 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">16,384</span>  activation_78[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n depthwise_conv2d_193       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">12,800</span>  activation_78[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_194       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">25,088</span>  activation_78[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n conv2d_180 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">16,384</span>  activation_82[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n depthwise_conv2d_197       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">12,800</span>  activation_82[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_198       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">25,088</span>  activation_82[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n add_45 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  depthwise_conv2d_45[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n                                                                    depthwise_conv2d_44[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n activation_20              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_44[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n batch_normalization_189    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  conv2d_173[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_190    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  depthwise_conv2d_193[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_191    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  depthwise_conv2d_194[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_201    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  conv2d_180[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_202    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  depthwise_conv2d_197[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_203    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  depthwise_conv2d_198[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n activation_21              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_45[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n conv2d_44 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span>  activation_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n depthwise_conv2d_46        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280</span>  activation_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_47        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">3,328</span>  activation_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_48        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280</span>  activation_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n conv2d_94 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span>  attention_block_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n depthwise_conv2d_104       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280</span>  attention_block_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_105       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">3,328</span>  attention_block_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_106       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280</span>  attention_block_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n concatenate_83             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1536</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      batch_normalization_1 \n                                                                    batch_normalization_1 \n\n concatenate_85             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1536</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_2 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      batch_normalization_2 \n                                                                    batch_normalization_2 \n\n conv2d_47 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span>  activation_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n depthwise_conv2d_52        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280</span>  activation_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_53        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">3,328</span>  activation_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_54        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280</span>  activation_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n conv2d_97 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span>  attention_block_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n depthwise_conv2d_110       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280</span>  attention_block_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_111       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">3,328</span>  attention_block_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_112       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280</span>  attention_block_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n concatenate_22             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv2d_44[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      depthwise_conv2d_46[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n                                                                    depthwise_conv2d_47[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n                                                                    depthwise_conv2d_48[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n concatenate_46             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv2d_94[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      depthwise_conv2d_104[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n                                                                    depthwise_conv2d_105[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n                                                                    depthwise_conv2d_106[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n conv2d_174 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">49,152</span>  concatenate_83[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n conv2d_181 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">49,152</span>  concatenate_85[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n concatenate_23             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv2d_47[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      depthwise_conv2d_52[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n                                                                    depthwise_conv2d_53[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n                                                                    depthwise_conv2d_54[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n concatenate_47             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv2d_97[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      depthwise_conv2d_110[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n                                                                    depthwise_conv2d_111[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n                                                                    depthwise_conv2d_112[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n lambda_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  concatenate_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n lambda_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  concatenate_46[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n batch_normalization_192    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  conv2d_174[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_204    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  conv2d_181[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n lambda_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  concatenate_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n lambda_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  concatenate_47[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n depthwise_conv2d_49        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">4,480</span>  lambda_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_107       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">4,480</span>  lambda_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n\n global_max_pooling2d_43    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling2D</span>)                                                                      \n\n global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_2 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n\n global_max_pooling2d_45    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_2 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling2D</span>)                                                                      \n\n depthwise_conv2d_55        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">4,480</span>  lambda_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_113       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">4,480</span>  lambda_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n conv2d_46 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span>  activation_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n conv2d_45 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">114,944</span>  depthwise_conv2d_49[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n conv2d_96 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span>  attention_block_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n conv2d_95 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">114,944</span>  depthwise_conv2d_107[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n add_168 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n add_173 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n conv2d_49 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span>  activation_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n conv2d_48 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">114,944</span>  depthwise_conv2d_55[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n conv2d_99 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span>  attention_block_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n conv2d_98 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">114,944</span>  depthwise_conv2d_113[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n depthwise_conv2d_51        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560</span>  conv2d_46[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_50        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560</span>  conv2d_45[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_109       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560</span>  conv2d_96[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_108       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560</span>  conv2d_95[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n dense_65 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">16,384</span>  add_168[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n\n dense_69 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">16,384</span>  add_173[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n\n depthwise_conv2d_57        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560</span>  conv2d_49[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_56        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560</span>  conv2d_48[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_115       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560</span>  conv2d_99[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_114       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560</span>  conv2d_98[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n add_46 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  depthwise_conv2d_51[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n                                                                    depthwise_conv2d_50[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n add_94 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  depthwise_conv2d_109[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n                                                                    depthwise_conv2d_108[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n dense_66 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,384</span>  dense_65[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n dense_70 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,384</span>  dense_69[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n add_47 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  depthwise_conv2d_57[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n                                                                    depthwise_conv2d_56[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n add_95 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  depthwise_conv2d_115[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n                                                                    depthwise_conv2d_114[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n activation_22              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_46[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_44              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_94[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n conv2d_175 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">262,144</span>  activation_78[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n reshape_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_66[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n conv2d_182 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">262,144</span>  activation_82[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n reshape_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_70[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n activation_23              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_47[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_45              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_95[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n conv2d_50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span>  activation_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n depthwise_conv2d_58        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560</span>  activation_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_59        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">6,656</span>  activation_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_60        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560</span>  activation_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n conv2d_100 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span>  activation_44[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n depthwise_conv2d_116       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560</span>  activation_44[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_117       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">6,656</span>  activation_44[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_118       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560</span>  activation_44[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n conv2d_144 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span>  attention_block_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n depthwise_conv2d_162       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560</span>  attention_block_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_163       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">6,656</span>  attention_block_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_164       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560</span>  attention_block_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n batch_normalization_193    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  conv2d_175[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n multiply_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n                                                                    reshape_29[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n batch_normalization_205    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  conv2d_182[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n multiply_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_2 \n                                                                    reshape_31[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n conv2d_53 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span>  activation_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n depthwise_conv2d_64        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560</span>  activation_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_65        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">6,656</span>  activation_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_66        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560</span>  activation_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n conv2d_103 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span>  activation_45[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n depthwise_conv2d_122       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560</span>  activation_45[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_123       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">6,656</span>  activation_45[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_124       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560</span>  activation_45[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n conv2d_147 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span>  attention_block_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n depthwise_conv2d_168       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560</span>  attention_block_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_169       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">6,656</span>  attention_block_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_170       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560</span>  attention_block_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n concatenate_24             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv2d_50[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      depthwise_conv2d_58[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n                                                                    depthwise_conv2d_59[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n                                                                    depthwise_conv2d_60[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n concatenate_48             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv2d_100[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      depthwise_conv2d_116[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n                                                                    depthwise_conv2d_117[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n                                                                    depthwise_conv2d_118[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n concatenate_70             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv2d_144[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      depthwise_conv2d_162[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n                                                                    depthwise_conv2d_163[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n                                                                    depthwise_conv2d_164[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n add_169 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n                                                                    multiply_29[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n\n add_174 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_2 \n                                                                    multiply_31[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n\n concatenate_25             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv2d_53[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      depthwise_conv2d_64[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n                                                                    depthwise_conv2d_65[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n                                                                    depthwise_conv2d_66[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n concatenate_49             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv2d_103[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      depthwise_conv2d_122[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n                                                                    depthwise_conv2d_123[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n                                                                    depthwise_conv2d_124[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n concatenate_71             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv2d_147[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      depthwise_conv2d_168[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n                                                                    depthwise_conv2d_169[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n                                                                    depthwise_conv2d_170[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n lambda_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  concatenate_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n lambda_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  concatenate_48[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n lambda_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  concatenate_70[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n activation_79              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_169[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_83              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_174[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n lambda_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  concatenate_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n lambda_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  concatenate_49[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n lambda_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  concatenate_71[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n depthwise_conv2d_61        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">8,960</span>  lambda_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_119       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">8,960</span>  lambda_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_165       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">8,960</span>  lambda_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n conv2d_176 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span>  activation_79[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n conv2d_183 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span>  activation_83[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n depthwise_conv2d_67        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">8,960</span>  lambda_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_125       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">8,960</span>  lambda_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_171       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">8,960</span>  lambda_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n conv2d_52 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span>  activation_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n conv2d_51 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">459,264</span>  depthwise_conv2d_61[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n conv2d_102 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span>  activation_44[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n conv2d_101 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">459,264</span>  depthwise_conv2d_119[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n conv2d_146 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span>  attention_block_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n conv2d_145 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">459,264</span>  depthwise_conv2d_165[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n batch_normalization_194    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  conv2d_176[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_206    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  conv2d_183[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n conv2d_55 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span>  activation_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n conv2d_54 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">459,264</span>  depthwise_conv2d_67[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n conv2d_105 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span>  activation_45[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n conv2d_104 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">459,264</span>  depthwise_conv2d_125[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n conv2d_149 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span>  attention_block_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n conv2d_148 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">459,264</span>  depthwise_conv2d_171[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n depthwise_conv2d_63        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">5,120</span>  conv2d_52[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_62        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">5,120</span>  conv2d_51[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_121       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">5,120</span>  conv2d_102[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_120       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">5,120</span>  conv2d_101[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_167       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">5,120</span>  conv2d_146[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_166       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">5,120</span>  conv2d_145[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n add_170 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n                                                                    attention_block_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n add_175 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_2 \n                                                                    attention_block_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n depthwise_conv2d_69        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">5,120</span>  conv2d_55[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_68        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">5,120</span>  conv2d_54[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_127       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">5,120</span>  conv2d_105[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_126       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">5,120</span>  conv2d_104[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_173       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">5,120</span>  conv2d_149[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_172       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">5,120</span>  conv2d_148[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n add_48 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  depthwise_conv2d_63[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n                                                                    depthwise_conv2d_62[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n add_96 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  depthwise_conv2d_121[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n                                                                    depthwise_conv2d_120[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n add_142 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  depthwise_conv2d_167[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n                                                                    depthwise_conv2d_166[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n activation_80              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_170[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_84              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_175[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n add_49 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  depthwise_conv2d_69[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n                                                                    depthwise_conv2d_68[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n add_97 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  depthwise_conv2d_127[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n                                                                    depthwise_conv2d_126[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n add_143 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  depthwise_conv2d_173[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n                                                                    depthwise_conv2d_172[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n activation_24              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_48[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_46              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_96[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_66              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_142[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n attention_block_7          [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>),            <span style=\"color: #00af00; text-decoration-color: #00af00\">902,660</span>  activation_80[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AttentionBlock</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)]                      activation_84[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n activation_25              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_49[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_47              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_97[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_67              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_143[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n concatenate_92             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  activation_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      activation_46[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   \n                                                                    activation_66[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   \n                                                                    attention_block_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n concatenate_93             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  activation_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      activation_47[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   \n                                                                    activation_67[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   \n                                                                    attention_block_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n dropout_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  concatenate_92[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n dropout_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  concatenate_93[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n concatenate_94             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dropout_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      dropout_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n dropout_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  concatenate_94[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dropout_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n\n dense_72 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)                       <span style=\"color: #00af00; text-decoration-color: #00af00\">20,485</span>  global_average_poolin \n\n dense_73 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)                       <span style=\"color: #00af00; text-decoration-color: #00af00\">28,679</span>  global_average_poolin \n\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m13,098,604\u001b[0m (49.97 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">13,098,604</span> (49.97 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m13,006,188\u001b[0m (49.61 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">13,006,188</span> (49.61 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m92,416\u001b[0m (361.00 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">92,416</span> (361.00 KB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"None\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\n\ninitial_gamma = 0.5\nlearning_rate = 1e-2\noptimizer = Adam(learning_rate=0.001)\n#opt = Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.9, epsilon=None, amsgrad=False)\n# Compile the model with the custom optimizer\nmodel.compile(optimizer=optimizer,\n              loss=['categorical_crossentropy', 'categorical_crossentropy'],\n              loss_weights=[initial_gamma, (1 -  initial_gamma)],\n              metrics=['accuracy', 'accuracy'])\n\n\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\ndef checkpoint_callback():\n\n    checkpoint_filepath = 'best1_model_cer_skin_lung.keras'\n\n    model_checkpoint_callback= ModelCheckpoint(filepath=checkpoint_filepath,\n                           save_weights_only=False,\n                           #frequency='epoch',\n                           monitor='val_loss',\n                           save_best_only=True,\n                            mode='min',\n                           verbose=1)\n\n    return model_checkpoint_callback\n\ndef early_stopping(patience):\n    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', restore_best_weights=True, patience=60, verbose=1)\n    return es_callback\n\n\n\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n                              patience=50, verbose = 1, min_lr=0.00001)\n\ncheckpoint_callback = checkpoint_callback()\n\nearly_stopping = early_stopping(patience=100)\ncallbacks = [checkpoint_callback, early_stopping, reduce_lr]\n            \n\n# Fit the model with callbacks\nhistory = model.fit([X_train_s, X_train_h], [y_train_s, y_train_h],\n                    epochs=200,\n                    validation_data=([X_val_s, X_val_h1], [y_val_s, y_val_h1]), verbose=1,\n                    shuffle=True,\n                    callbacks=callbacks) # UpdateGammaCallback","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T06:01:00.464819Z","iopub.execute_input":"2025-03-09T06:01:00.465131Z","iopub.status.idle":"2025-03-09T11:41:23.669945Z","shell.execute_reply.started":"2025-03-09T06:01:00.465102Z","shell.execute_reply":"2025-03-09T11:41:23.668926Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/200\noutput attention 1 shape: (None, 32, 32, 64)\noutput attention 2 shape: (None, 32, 32, 64)\noutput attention 1 shape: (None, 32, 32, 64)\noutput attention 2 shape: (None, 32, 32, 64)\noutput attention 1 shape: (None, 16, 16, 128)\noutput attention 2 shape: (None, 16, 16, 128)\noutput attention 1 shape: (None, 16, 16, 128)\noutput attention 2 shape: (None, 16, 16, 128)\noutput attention 1 shape: (None, 8, 8, 256)\noutput attention 2 shape: (None, 8, 8, 256)\noutput attention 1 shape: (None, 8, 8, 256)\noutput attention 2 shape: (None, 8, 8, 256)\noutput attention 1 shape: (None, 4, 4, 512)\noutput attention 2 shape: (None, 4, 4, 512)\noutput attention 1 shape: (None, 4, 4, 512)\noutput attention 2 shape: (None, 4, 4, 512)\noutput attention 1 shape: (None, 32, 32, 64)\noutput attention 2 shape: (None, 32, 32, 64)\noutput attention 1 shape: (None, 32, 32, 64)\noutput attention 2 shape: (None, 32, 32, 64)\noutput attention 1 shape: (None, 16, 16, 128)\noutput attention 2 shape: (None, 16, 16, 128)\noutput attention 1 shape: (None, 16, 16, 128)\noutput attention 2 shape: (None, 16, 16, 128)\noutput attention 1 shape: (None, 8, 8, 256)\noutput attention 2 shape: (None, 8, 8, 256)\noutput attention 1 shape: (None, 8, 8, 256)\noutput attention 2 shape: (None, 8, 8, 256)\noutput attention 1 shape: (None, 4, 4, 512)\noutput attention 2 shape: (None, 4, 4, 512)\noutput attention 1 shape: (None, 4, 4, 512)\noutput attention 2 shape: (None, 4, 4, 512)\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - dense_72_accuracy: 0.4592 - dense_72_loss: 0.6274 - dense_73_accuracy: 0.6397 - dense_73_loss: 0.5488 - loss: 1.1762   output attention 1 shape: (None, 32, 32, 64)\noutput attention 2 shape: (None, 32, 32, 64)\noutput attention 1 shape: (None, 32, 32, 64)\noutput attention 2 shape: (None, 32, 32, 64)\noutput attention 1 shape: (None, 16, 16, 128)\noutput attention 2 shape: (None, 16, 16, 128)\noutput attention 1 shape: (None, 16, 16, 128)\noutput attention 2 shape: (None, 16, 16, 128)\noutput attention 1 shape: (None, 8, 8, 256)\noutput attention 2 shape: (None, 8, 8, 256)\noutput attention 1 shape: (None, 8, 8, 256)\noutput attention 2 shape: (None, 8, 8, 256)\noutput attention 1 shape: (None, 4, 4, 512)\noutput attention 2 shape: (None, 4, 4, 512)\noutput attention 1 shape: (None, 4, 4, 512)\noutput attention 2 shape: (None, 4, 4, 512)\n\nEpoch 1: val_loss improved from inf to 1.25313, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m880s\u001b[0m 2s/step - dense_72_accuracy: 0.4598 - dense_72_loss: 0.6269 - dense_73_accuracy: 0.6398 - dense_73_loss: 0.5486 - loss: 1.1754 - val_dense_72_accuracy: 0.4784 - val_dense_72_loss: 0.6143 - val_dense_73_accuracy: 0.6497 - val_dense_73_loss: 0.6423 - val_loss: 1.2531 - learning_rate: 0.0010\nEpoch 2/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 403ms/step - dense_72_accuracy: 0.7714 - dense_72_loss: 0.3165 - dense_73_accuracy: 0.6833 - dense_73_loss: 0.4472 - loss: 0.7638\nEpoch 2: val_loss improved from 1.25313 to 1.21923, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 426ms/step - dense_72_accuracy: 0.7715 - dense_72_loss: 0.3165 - dense_73_accuracy: 0.6833 - dense_73_loss: 0.4472 - loss: 0.7637 - val_dense_72_accuracy: 0.6343 - val_dense_72_loss: 0.5245 - val_dense_73_accuracy: 0.4907 - val_dense_73_loss: 0.6766 - val_loss: 1.2192 - learning_rate: 0.0010\nEpoch 3/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 402ms/step - dense_72_accuracy: 0.8381 - dense_72_loss: 0.2381 - dense_73_accuracy: 0.6981 - dense_73_loss: 0.4182 - loss: 0.6564\nEpoch 3: val_loss did not improve from 1.21923\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 410ms/step - dense_72_accuracy: 0.8381 - dense_72_loss: 0.2381 - dense_73_accuracy: 0.6981 - dense_73_loss: 0.4182 - loss: 0.6563 - val_dense_72_accuracy: 0.6528 - val_dense_72_loss: 0.6552 - val_dense_73_accuracy: 0.5340 - val_dense_73_loss: 0.6075 - val_loss: 1.2486 - learning_rate: 0.0010\nEpoch 4/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 393ms/step - dense_72_accuracy: 0.8668 - dense_72_loss: 0.1860 - dense_73_accuracy: 0.7011 - dense_73_loss: 0.4097 - loss: 0.5957\nEpoch 4: val_loss did not improve from 1.21923\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 401ms/step - dense_72_accuracy: 0.8668 - dense_72_loss: 0.1860 - dense_73_accuracy: 0.7011 - dense_73_loss: 0.4097 - loss: 0.5957 - val_dense_72_accuracy: 0.7701 - val_dense_72_loss: 0.4325 - val_dense_73_accuracy: 0.1852 - val_dense_73_loss: 1.3993 - val_loss: 1.8165 - learning_rate: 0.0010\nEpoch 5/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 399ms/step - dense_72_accuracy: 0.8930 - dense_72_loss: 0.1581 - dense_73_accuracy: 0.7159 - dense_73_loss: 0.3894 - loss: 0.5475\nEpoch 5: val_loss improved from 1.21923 to 1.05890, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 422ms/step - dense_72_accuracy: 0.8930 - dense_72_loss: 0.1581 - dense_73_accuracy: 0.7159 - dense_73_loss: 0.3894 - loss: 0.5474 - val_dense_72_accuracy: 0.8472 - val_dense_72_loss: 0.2806 - val_dense_73_accuracy: 0.4753 - val_dense_73_loss: 0.7707 - val_loss: 1.0589 - learning_rate: 0.0010\nEpoch 6/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 394ms/step - dense_72_accuracy: 0.9093 - dense_72_loss: 0.1356 - dense_73_accuracy: 0.7288 - dense_73_loss: 0.3589 - loss: 0.4945\nEpoch 6: val_loss improved from 1.05890 to 0.68412, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 417ms/step - dense_72_accuracy: 0.9093 - dense_72_loss: 0.1356 - dense_73_accuracy: 0.7288 - dense_73_loss: 0.3590 - loss: 0.4946 - val_dense_72_accuracy: 0.9151 - val_dense_72_loss: 0.1254 - val_dense_73_accuracy: 0.6080 - val_dense_73_loss: 0.5518 - val_loss: 0.6841 - learning_rate: 0.0010\nEpoch 7/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 391ms/step - dense_72_accuracy: 0.9215 - dense_72_loss: 0.1190 - dense_73_accuracy: 0.7412 - dense_73_loss: 0.3542 - loss: 0.4732\nEpoch 7: val_loss improved from 0.68412 to 0.61248, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 414ms/step - dense_72_accuracy: 0.9216 - dense_72_loss: 0.1190 - dense_73_accuracy: 0.7412 - dense_73_loss: 0.3542 - loss: 0.4731 - val_dense_72_accuracy: 0.8889 - val_dense_72_loss: 0.1720 - val_dense_73_accuracy: 0.7022 - val_dense_73_loss: 0.4413 - val_loss: 0.6125 - learning_rate: 0.0010\nEpoch 8/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 397ms/step - dense_72_accuracy: 0.9261 - dense_72_loss: 0.1048 - dense_73_accuracy: 0.7394 - dense_73_loss: 0.3460 - loss: 0.4508\nEpoch 8: val_loss did not improve from 0.61248\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 405ms/step - dense_72_accuracy: 0.9261 - dense_72_loss: 0.1048 - dense_73_accuracy: 0.7394 - dense_73_loss: 0.3460 - loss: 0.4508 - val_dense_72_accuracy: 0.8611 - val_dense_72_loss: 0.3223 - val_dense_73_accuracy: 0.6389 - val_dense_73_loss: 0.5398 - val_loss: 0.8710 - learning_rate: 0.0010\nEpoch 9/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 408ms/step - dense_72_accuracy: 0.9453 - dense_72_loss: 0.0834 - dense_73_accuracy: 0.7532 - dense_73_loss: 0.3246 - loss: 0.4080\nEpoch 9: val_loss did not improve from 0.61248\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 416ms/step - dense_72_accuracy: 0.9453 - dense_72_loss: 0.0835 - dense_73_accuracy: 0.7533 - dense_73_loss: 0.3246 - loss: 0.4081 - val_dense_72_accuracy: 0.7809 - val_dense_72_loss: 0.4885 - val_dense_73_accuracy: 0.7022 - val_dense_73_loss: 0.4429 - val_loss: 0.9109 - learning_rate: 0.0010\nEpoch 10/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 389ms/step - dense_72_accuracy: 0.9483 - dense_72_loss: 0.0806 - dense_73_accuracy: 0.7591 - dense_73_loss: 0.3293 - loss: 0.4100\nEpoch 10: val_loss did not improve from 0.61248\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 397ms/step - dense_72_accuracy: 0.9483 - dense_72_loss: 0.0806 - dense_73_accuracy: 0.7591 - dense_73_loss: 0.3293 - loss: 0.4099 - val_dense_72_accuracy: 0.8071 - val_dense_72_loss: 0.3394 - val_dense_73_accuracy: 0.7284 - val_dense_73_loss: 0.3838 - val_loss: 0.7142 - learning_rate: 0.0010\nEpoch 11/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 400ms/step - dense_72_accuracy: 0.9578 - dense_72_loss: 0.0636 - dense_73_accuracy: 0.7623 - dense_73_loss: 0.3190 - loss: 0.3826\nEpoch 11: val_loss did not improve from 0.61248\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 407ms/step - dense_72_accuracy: 0.9578 - dense_72_loss: 0.0636 - dense_73_accuracy: 0.7623 - dense_73_loss: 0.3190 - loss: 0.3826 - val_dense_72_accuracy: 0.8380 - val_dense_72_loss: 0.2518 - val_dense_73_accuracy: 0.4969 - val_dense_73_loss: 0.7524 - val_loss: 1.0139 - learning_rate: 0.0010\nEpoch 12/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 399ms/step - dense_72_accuracy: 0.9594 - dense_72_loss: 0.0555 - dense_73_accuracy: 0.7717 - dense_73_loss: 0.3125 - loss: 0.3680\nEpoch 12: val_loss did not improve from 0.61248\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 407ms/step - dense_72_accuracy: 0.9594 - dense_72_loss: 0.0555 - dense_73_accuracy: 0.7717 - dense_73_loss: 0.3125 - loss: 0.3680 - val_dense_72_accuracy: 0.8364 - val_dense_72_loss: 0.3922 - val_dense_73_accuracy: 0.6867 - val_dense_73_loss: 0.5981 - val_loss: 0.9976 - learning_rate: 0.0010\nEpoch 13/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 389ms/step - dense_72_accuracy: 0.9643 - dense_72_loss: 0.0546 - dense_73_accuracy: 0.7811 - dense_73_loss: 0.3051 - loss: 0.3596\nEpoch 13: val_loss improved from 0.61248 to 0.51397, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 413ms/step - dense_72_accuracy: 0.9643 - dense_72_loss: 0.0546 - dense_73_accuracy: 0.7811 - dense_73_loss: 0.3051 - loss: 0.3596 - val_dense_72_accuracy: 0.8827 - val_dense_72_loss: 0.1849 - val_dense_73_accuracy: 0.7577 - val_dense_73_loss: 0.3212 - val_loss: 0.5140 - learning_rate: 0.0010\nEpoch 14/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 400ms/step - dense_72_accuracy: 0.9642 - dense_72_loss: 0.0502 - dense_73_accuracy: 0.7813 - dense_73_loss: 0.2919 - loss: 0.3422\nEpoch 14: val_loss did not improve from 0.51397\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 407ms/step - dense_72_accuracy: 0.9642 - dense_72_loss: 0.0502 - dense_73_accuracy: 0.7813 - dense_73_loss: 0.2919 - loss: 0.3422 - val_dense_72_accuracy: 0.9213 - val_dense_72_loss: 0.1386 - val_dense_73_accuracy: 0.7022 - val_dense_73_loss: 0.4656 - val_loss: 0.6171 - learning_rate: 0.0010\nEpoch 15/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 408ms/step - dense_72_accuracy: 0.9689 - dense_72_loss: 0.0478 - dense_73_accuracy: 0.7877 - dense_73_loss: 0.2871 - loss: 0.3349\nEpoch 15: val_loss did not improve from 0.51397\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 415ms/step - dense_72_accuracy: 0.9688 - dense_72_loss: 0.0478 - dense_73_accuracy: 0.7877 - dense_73_loss: 0.2871 - loss: 0.3349 - val_dense_72_accuracy: 0.9105 - val_dense_72_loss: 0.1295 - val_dense_73_accuracy: 0.7238 - val_dense_73_loss: 0.4042 - val_loss: 0.5329 - learning_rate: 0.0010\nEpoch 16/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 399ms/step - dense_72_accuracy: 0.9673 - dense_72_loss: 0.0481 - dense_73_accuracy: 0.7994 - dense_73_loss: 0.2697 - loss: 0.3177\nEpoch 16: val_loss did not improve from 0.51397\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 407ms/step - dense_72_accuracy: 0.9673 - dense_72_loss: 0.0481 - dense_73_accuracy: 0.7994 - dense_73_loss: 0.2697 - loss: 0.3178 - val_dense_72_accuracy: 0.9429 - val_dense_72_loss: 0.1010 - val_dense_73_accuracy: 0.7207 - val_dense_73_loss: 0.4378 - val_loss: 0.5284 - learning_rate: 0.0010\nEpoch 17/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 394ms/step - dense_72_accuracy: 0.9732 - dense_72_loss: 0.0393 - dense_73_accuracy: 0.7922 - dense_73_loss: 0.2757 - loss: 0.3150\nEpoch 17: val_loss did not improve from 0.51397\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 402ms/step - dense_72_accuracy: 0.9732 - dense_72_loss: 0.0393 - dense_73_accuracy: 0.7922 - dense_73_loss: 0.2757 - loss: 0.3150 - val_dense_72_accuracy: 0.8318 - val_dense_72_loss: 0.3277 - val_dense_73_accuracy: 0.6975 - val_dense_73_loss: 0.3909 - val_loss: 0.7352 - learning_rate: 0.0010\nEpoch 18/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 391ms/step - dense_72_accuracy: 0.9686 - dense_72_loss: 0.0496 - dense_73_accuracy: 0.8115 - dense_73_loss: 0.2627 - loss: 0.3123\nEpoch 18: val_loss did not improve from 0.51397\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 399ms/step - dense_72_accuracy: 0.9686 - dense_72_loss: 0.0496 - dense_73_accuracy: 0.8115 - dense_73_loss: 0.2627 - loss: 0.3123 - val_dense_72_accuracy: 0.9167 - val_dense_72_loss: 0.1580 - val_dense_73_accuracy: 0.6698 - val_dense_73_loss: 0.4459 - val_loss: 0.6042 - learning_rate: 0.0010\nEpoch 19/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 400ms/step - dense_72_accuracy: 0.9740 - dense_72_loss: 0.0360 - dense_73_accuracy: 0.8136 - dense_73_loss: 0.2545 - loss: 0.2905\nEpoch 19: val_loss did not improve from 0.51397\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 408ms/step - dense_72_accuracy: 0.9740 - dense_72_loss: 0.0360 - dense_73_accuracy: 0.8136 - dense_73_loss: 0.2545 - loss: 0.2905 - val_dense_72_accuracy: 0.9383 - val_dense_72_loss: 0.1138 - val_dense_73_accuracy: 0.6420 - val_dense_73_loss: 0.6910 - val_loss: 0.8135 - learning_rate: 0.0010\nEpoch 20/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 400ms/step - dense_72_accuracy: 0.9736 - dense_72_loss: 0.0383 - dense_73_accuracy: 0.8201 - dense_73_loss: 0.2417 - loss: 0.2799\nEpoch 20: val_loss improved from 0.51397 to 0.50896, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 432ms/step - dense_72_accuracy: 0.9736 - dense_72_loss: 0.0383 - dense_73_accuracy: 0.8201 - dense_73_loss: 0.2417 - loss: 0.2800 - val_dense_72_accuracy: 0.9367 - val_dense_72_loss: 0.1025 - val_dense_73_accuracy: 0.7176 - val_dense_73_loss: 0.4054 - val_loss: 0.5090 - learning_rate: 0.0010\nEpoch 21/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 399ms/step - dense_72_accuracy: 0.9765 - dense_72_loss: 0.0328 - dense_73_accuracy: 0.8256 - dense_73_loss: 0.2405 - loss: 0.2732\nEpoch 21: val_loss did not improve from 0.50896\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 407ms/step - dense_72_accuracy: 0.9765 - dense_72_loss: 0.0328 - dense_73_accuracy: 0.8255 - dense_73_loss: 0.2405 - loss: 0.2733 - val_dense_72_accuracy: 0.8873 - val_dense_72_loss: 0.2083 - val_dense_73_accuracy: 0.7855 - val_dense_73_loss: 0.3174 - val_loss: 0.5302 - learning_rate: 0.0010\nEpoch 22/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 394ms/step - dense_72_accuracy: 0.9790 - dense_72_loss: 0.0316 - dense_73_accuracy: 0.8281 - dense_73_loss: 0.2308 - loss: 0.2624\nEpoch 22: val_loss improved from 0.50896 to 0.46936, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 417ms/step - dense_72_accuracy: 0.9790 - dense_72_loss: 0.0316 - dense_73_accuracy: 0.8281 - dense_73_loss: 0.2309 - loss: 0.2625 - val_dense_72_accuracy: 0.9182 - val_dense_72_loss: 0.1223 - val_dense_73_accuracy: 0.7531 - val_dense_73_loss: 0.3524 - val_loss: 0.4694 - learning_rate: 0.0010\nEpoch 23/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 399ms/step - dense_72_accuracy: 0.9817 - dense_72_loss: 0.0277 - dense_73_accuracy: 0.8399 - dense_73_loss: 0.2248 - loss: 0.2525\nEpoch 23: val_loss improved from 0.46936 to 0.39951, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 422ms/step - dense_72_accuracy: 0.9817 - dense_72_loss: 0.0277 - dense_73_accuracy: 0.8398 - dense_73_loss: 0.2248 - loss: 0.2525 - val_dense_72_accuracy: 0.9290 - val_dense_72_loss: 0.1214 - val_dense_73_accuracy: 0.7932 - val_dense_73_loss: 0.2760 - val_loss: 0.3995 - learning_rate: 0.0010\nEpoch 24/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 399ms/step - dense_72_accuracy: 0.9833 - dense_72_loss: 0.0265 - dense_73_accuracy: 0.8342 - dense_73_loss: 0.2158 - loss: 0.2423\nEpoch 24: val_loss did not improve from 0.39951\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 407ms/step - dense_72_accuracy: 0.9833 - dense_72_loss: 0.0265 - dense_73_accuracy: 0.8342 - dense_73_loss: 0.2158 - loss: 0.2423 - val_dense_72_accuracy: 0.9198 - val_dense_72_loss: 0.1441 - val_dense_73_accuracy: 0.7654 - val_dense_73_loss: 0.3044 - val_loss: 0.4534 - learning_rate: 0.0010\nEpoch 25/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 399ms/step - dense_72_accuracy: 0.9826 - dense_72_loss: 0.0237 - dense_73_accuracy: 0.8477 - dense_73_loss: 0.2096 - loss: 0.2333\nEpoch 25: val_loss did not improve from 0.39951\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 407ms/step - dense_72_accuracy: 0.9826 - dense_72_loss: 0.0237 - dense_73_accuracy: 0.8477 - dense_73_loss: 0.2097 - loss: 0.2333 - val_dense_72_accuracy: 0.9367 - val_dense_72_loss: 0.1167 - val_dense_73_accuracy: 0.7299 - val_dense_73_loss: 0.4012 - val_loss: 0.5287 - learning_rate: 0.0010\nEpoch 26/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 392ms/step - dense_72_accuracy: 0.9888 - dense_72_loss: 0.0184 - dense_73_accuracy: 0.8546 - dense_73_loss: 0.1988 - loss: 0.2173\nEpoch 26: val_loss did not improve from 0.39951\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 399ms/step - dense_72_accuracy: 0.9888 - dense_72_loss: 0.0184 - dense_73_accuracy: 0.8546 - dense_73_loss: 0.1988 - loss: 0.2173 - val_dense_72_accuracy: 0.9336 - val_dense_72_loss: 0.1175 - val_dense_73_accuracy: 0.7500 - val_dense_73_loss: 0.3327 - val_loss: 0.4582 - learning_rate: 0.0010\nEpoch 27/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 391ms/step - dense_72_accuracy: 0.9809 - dense_72_loss: 0.0283 - dense_73_accuracy: 0.8572 - dense_73_loss: 0.1970 - loss: 0.2253\nEpoch 27: val_loss did not improve from 0.39951\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 399ms/step - dense_72_accuracy: 0.9809 - dense_72_loss: 0.0283 - dense_73_accuracy: 0.8572 - dense_73_loss: 0.1970 - loss: 0.2252 - val_dense_72_accuracy: 0.9198 - val_dense_72_loss: 0.1604 - val_dense_73_accuracy: 0.7886 - val_dense_73_loss: 0.2797 - val_loss: 0.4447 - learning_rate: 0.0010\nEpoch 28/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 393ms/step - dense_72_accuracy: 0.9798 - dense_72_loss: 0.0328 - dense_73_accuracy: 0.8785 - dense_73_loss: 0.1691 - loss: 0.2019\nEpoch 28: val_loss improved from 0.39951 to 0.37719, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 416ms/step - dense_72_accuracy: 0.9799 - dense_72_loss: 0.0328 - dense_73_accuracy: 0.8785 - dense_73_loss: 0.1692 - loss: 0.2020 - val_dense_72_accuracy: 0.9522 - val_dense_72_loss: 0.0856 - val_dense_73_accuracy: 0.7963 - val_dense_73_loss: 0.2820 - val_loss: 0.3772 - learning_rate: 0.0010\nEpoch 29/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 399ms/step - dense_72_accuracy: 0.9862 - dense_72_loss: 0.0206 - dense_73_accuracy: 0.8642 - dense_73_loss: 0.1743 - loss: 0.1949\nEpoch 29: val_loss did not improve from 0.37719\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 407ms/step - dense_72_accuracy: 0.9862 - dense_72_loss: 0.0207 - dense_73_accuracy: 0.8642 - dense_73_loss: 0.1743 - loss: 0.1949 - val_dense_72_accuracy: 0.8920 - val_dense_72_loss: 0.1747 - val_dense_73_accuracy: 0.8395 - val_dense_73_loss: 0.2242 - val_loss: 0.3983 - learning_rate: 0.0010\nEpoch 30/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 391ms/step - dense_72_accuracy: 0.9835 - dense_72_loss: 0.0254 - dense_73_accuracy: 0.8749 - dense_73_loss: 0.1669 - loss: 0.1922\nEpoch 30: val_loss improved from 0.37719 to 0.26160, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 422ms/step - dense_72_accuracy: 0.9835 - dense_72_loss: 0.0254 - dense_73_accuracy: 0.8749 - dense_73_loss: 0.1669 - loss: 0.1922 - val_dense_72_accuracy: 0.9367 - val_dense_72_loss: 0.1346 - val_dense_73_accuracy: 0.9059 - val_dense_73_loss: 0.1265 - val_loss: 0.2616 - learning_rate: 0.0010\nEpoch 31/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 393ms/step - dense_72_accuracy: 0.9881 - dense_72_loss: 0.0179 - dense_73_accuracy: 0.8818 - dense_73_loss: 0.1564 - loss: 0.1743\nEpoch 31: val_loss did not improve from 0.26160\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 401ms/step - dense_72_accuracy: 0.9880 - dense_72_loss: 0.0179 - dense_73_accuracy: 0.8818 - dense_73_loss: 0.1564 - loss: 0.1744 - val_dense_72_accuracy: 0.5710 - val_dense_72_loss: 2.0105 - val_dense_73_accuracy: 0.7623 - val_dense_73_loss: 0.3084 - val_loss: 2.3450 - learning_rate: 0.0010\nEpoch 32/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 390ms/step - dense_72_accuracy: 0.9847 - dense_72_loss: 0.0216 - dense_73_accuracy: 0.8817 - dense_73_loss: 0.1538 - loss: 0.1754\nEpoch 32: val_loss did not improve from 0.26160\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 398ms/step - dense_72_accuracy: 0.9847 - dense_72_loss: 0.0216 - dense_73_accuracy: 0.8818 - dense_73_loss: 0.1538 - loss: 0.1755 - val_dense_72_accuracy: 0.9336 - val_dense_72_loss: 0.1413 - val_dense_73_accuracy: 0.8627 - val_dense_73_loss: 0.1745 - val_loss: 0.3207 - learning_rate: 0.0010\nEpoch 33/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 395ms/step - dense_72_accuracy: 0.9881 - dense_72_loss: 0.0176 - dense_73_accuracy: 0.9007 - dense_73_loss: 0.1333 - loss: 0.1509\nEpoch 33: val_loss did not improve from 0.26160\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 403ms/step - dense_72_accuracy: 0.9881 - dense_72_loss: 0.0176 - dense_73_accuracy: 0.9006 - dense_73_loss: 0.1334 - loss: 0.1510 - val_dense_72_accuracy: 0.9367 - val_dense_72_loss: 0.1504 - val_dense_73_accuracy: 0.7809 - val_dense_73_loss: 0.3902 - val_loss: 0.5548 - learning_rate: 0.0010\nEpoch 34/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 399ms/step - dense_72_accuracy: 0.9840 - dense_72_loss: 0.0262 - dense_73_accuracy: 0.8952 - dense_73_loss: 0.1486 - loss: 0.1748\nEpoch 34: val_loss did not improve from 0.26160\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 407ms/step - dense_72_accuracy: 0.9840 - dense_72_loss: 0.0262 - dense_73_accuracy: 0.8952 - dense_73_loss: 0.1486 - loss: 0.1748 - val_dense_72_accuracy: 0.7438 - val_dense_72_loss: 0.8384 - val_dense_73_accuracy: 0.8889 - val_dense_73_loss: 0.1705 - val_loss: 1.0172 - learning_rate: 0.0010\nEpoch 35/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 390ms/step - dense_72_accuracy: 0.9896 - dense_72_loss: 0.0159 - dense_73_accuracy: 0.9109 - dense_73_loss: 0.1242 - loss: 0.1401\nEpoch 35: val_loss did not improve from 0.26160\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 398ms/step - dense_72_accuracy: 0.9896 - dense_72_loss: 0.0159 - dense_73_accuracy: 0.9108 - dense_73_loss: 0.1242 - loss: 0.1401 - val_dense_72_accuracy: 0.9167 - val_dense_72_loss: 0.1708 - val_dense_73_accuracy: 0.8951 - val_dense_73_loss: 0.1513 - val_loss: 0.3276 - learning_rate: 0.0010\nEpoch 36/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 407ms/step - dense_72_accuracy: 0.9916 - dense_72_loss: 0.0113 - dense_73_accuracy: 0.9132 - dense_73_loss: 0.1162 - loss: 0.1275\nEpoch 36: val_loss did not improve from 0.26160\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 415ms/step - dense_72_accuracy: 0.9916 - dense_72_loss: 0.0113 - dense_73_accuracy: 0.9131 - dense_73_loss: 0.1163 - loss: 0.1276 - val_dense_72_accuracy: 0.9090 - val_dense_72_loss: 0.2269 - val_dense_73_accuracy: 0.8611 - val_dense_73_loss: 0.2036 - val_loss: 0.4220 - learning_rate: 0.0010\nEpoch 37/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 399ms/step - dense_72_accuracy: 0.9906 - dense_72_loss: 0.0145 - dense_73_accuracy: 0.9215 - dense_73_loss: 0.1157 - loss: 0.1301\nEpoch 37: val_loss did not improve from 0.26160\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 407ms/step - dense_72_accuracy: 0.9906 - dense_72_loss: 0.0144 - dense_73_accuracy: 0.9215 - dense_73_loss: 0.1157 - loss: 0.1301 - val_dense_72_accuracy: 0.9182 - val_dense_72_loss: 0.1749 - val_dense_73_accuracy: 0.8796 - val_dense_73_loss: 0.1827 - val_loss: 0.3606 - learning_rate: 0.0010\nEpoch 38/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 398ms/step - dense_72_accuracy: 0.9883 - dense_72_loss: 0.0164 - dense_73_accuracy: 0.9244 - dense_73_loss: 0.1055 - loss: 0.1219\nEpoch 38: val_loss did not improve from 0.26160\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 406ms/step - dense_72_accuracy: 0.9883 - dense_72_loss: 0.0164 - dense_73_accuracy: 0.9244 - dense_73_loss: 0.1056 - loss: 0.1220 - val_dense_72_accuracy: 0.8827 - val_dense_72_loss: 0.2083 - val_dense_73_accuracy: 0.7346 - val_dense_73_loss: 0.4463 - val_loss: 0.6600 - learning_rate: 0.0010\nEpoch 39/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 399ms/step - dense_72_accuracy: 0.9886 - dense_72_loss: 0.0176 - dense_73_accuracy: 0.9236 - dense_73_loss: 0.1073 - loss: 0.1249\nEpoch 39: val_loss did not improve from 0.26160\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 406ms/step - dense_72_accuracy: 0.9886 - dense_72_loss: 0.0176 - dense_73_accuracy: 0.9236 - dense_73_loss: 0.1073 - loss: 0.1249 - val_dense_72_accuracy: 0.9182 - val_dense_72_loss: 0.1598 - val_dense_73_accuracy: 0.8889 - val_dense_73_loss: 0.1709 - val_loss: 0.3366 - learning_rate: 0.0010\nEpoch 40/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 393ms/step - dense_72_accuracy: 0.9886 - dense_72_loss: 0.0175 - dense_73_accuracy: 0.9325 - dense_73_loss: 0.0942 - loss: 0.1118\nEpoch 40: val_loss did not improve from 0.26160\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 401ms/step - dense_72_accuracy: 0.9886 - dense_72_loss: 0.0175 - dense_73_accuracy: 0.9325 - dense_73_loss: 0.0943 - loss: 0.1118 - val_dense_72_accuracy: 0.9367 - val_dense_72_loss: 0.1388 - val_dense_73_accuracy: 0.8380 - val_dense_73_loss: 0.2238 - val_loss: 0.3610 - learning_rate: 0.0010\nEpoch 41/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 400ms/step - dense_72_accuracy: 0.9887 - dense_72_loss: 0.0197 - dense_73_accuracy: 0.9382 - dense_73_loss: 0.0898 - loss: 0.1095\nEpoch 41: val_loss did not improve from 0.26160\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 407ms/step - dense_72_accuracy: 0.9887 - dense_72_loss: 0.0197 - dense_73_accuracy: 0.9382 - dense_73_loss: 0.0899 - loss: 0.1096 - val_dense_72_accuracy: 0.9429 - val_dense_72_loss: 0.1100 - val_dense_73_accuracy: 0.8472 - val_dense_73_loss: 0.2592 - val_loss: 0.3794 - learning_rate: 0.0010\nEpoch 42/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 407ms/step - dense_72_accuracy: 0.9891 - dense_72_loss: 0.0165 - dense_73_accuracy: 0.9327 - dense_73_loss: 0.0908 - loss: 0.1073\nEpoch 42: val_loss did not improve from 0.26160\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 415ms/step - dense_72_accuracy: 0.9890 - dense_72_loss: 0.0165 - dense_73_accuracy: 0.9327 - dense_73_loss: 0.0908 - loss: 0.1073 - val_dense_72_accuracy: 0.9491 - val_dense_72_loss: 0.0938 - val_dense_73_accuracy: 0.8488 - val_dense_73_loss: 0.2025 - val_loss: 0.2964 - learning_rate: 0.0010\nEpoch 43/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 393ms/step - dense_72_accuracy: 0.9892 - dense_72_loss: 0.0171 - dense_73_accuracy: 0.9424 - dense_73_loss: 0.0817 - loss: 0.0988\nEpoch 43: val_loss did not improve from 0.26160\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 401ms/step - dense_72_accuracy: 0.9892 - dense_72_loss: 0.0171 - dense_73_accuracy: 0.9424 - dense_73_loss: 0.0817 - loss: 0.0988 - val_dense_72_accuracy: 0.9028 - val_dense_72_loss: 0.2165 - val_dense_73_accuracy: 0.8796 - val_dense_73_loss: 0.1790 - val_loss: 0.4082 - learning_rate: 0.0010\nEpoch 44/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 399ms/step - dense_72_accuracy: 0.9919 - dense_72_loss: 0.0122 - dense_73_accuracy: 0.9411 - dense_73_loss: 0.0770 - loss: 0.0892\nEpoch 44: val_loss did not improve from 0.26160\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 407ms/step - dense_72_accuracy: 0.9919 - dense_72_loss: 0.0122 - dense_73_accuracy: 0.9411 - dense_73_loss: 0.0770 - loss: 0.0892 - val_dense_72_accuracy: 0.9352 - val_dense_72_loss: 0.1450 - val_dense_73_accuracy: 0.7762 - val_dense_73_loss: 0.5341 - val_loss: 0.6802 - learning_rate: 0.0010\nEpoch 45/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 399ms/step - dense_72_accuracy: 0.9939 - dense_72_loss: 0.0100 - dense_73_accuracy: 0.9398 - dense_73_loss: 0.0836 - loss: 0.0937\nEpoch 45: val_loss did not improve from 0.26160\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 407ms/step - dense_72_accuracy: 0.9939 - dense_72_loss: 0.0100 - dense_73_accuracy: 0.9398 - dense_73_loss: 0.0837 - loss: 0.0937 - val_dense_72_accuracy: 0.9228 - val_dense_72_loss: 0.1953 - val_dense_73_accuracy: 0.8981 - val_dense_73_loss: 0.1470 - val_loss: 0.3519 - learning_rate: 0.0010\nEpoch 46/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 400ms/step - dense_72_accuracy: 0.9905 - dense_72_loss: 0.0137 - dense_73_accuracy: 0.9457 - dense_73_loss: 0.0778 - loss: 0.0915\nEpoch 46: val_loss did not improve from 0.26160\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 408ms/step - dense_72_accuracy: 0.9905 - dense_72_loss: 0.0137 - dense_73_accuracy: 0.9457 - dense_73_loss: 0.0778 - loss: 0.0915 - val_dense_72_accuracy: 0.9151 - val_dense_72_loss: 0.1826 - val_dense_73_accuracy: 0.9460 - val_dense_73_loss: 0.0917 - val_loss: 0.2808 - learning_rate: 0.0010\nEpoch 47/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 408ms/step - dense_72_accuracy: 0.9955 - dense_72_loss: 0.0081 - dense_73_accuracy: 0.9487 - dense_73_loss: 0.0685 - loss: 0.0766\nEpoch 47: val_loss did not improve from 0.26160\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 416ms/step - dense_72_accuracy: 0.9955 - dense_72_loss: 0.0081 - dense_73_accuracy: 0.9487 - dense_73_loss: 0.0686 - loss: 0.0767 - val_dense_72_accuracy: 0.9491 - val_dense_72_loss: 0.1021 - val_dense_73_accuracy: 0.8519 - val_dense_73_loss: 0.2445 - val_loss: 0.3523 - learning_rate: 0.0010\nEpoch 48/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 397ms/step - dense_72_accuracy: 0.9881 - dense_72_loss: 0.0163 - dense_73_accuracy: 0.9456 - dense_73_loss: 0.0745 - loss: 0.0908\nEpoch 48: val_loss did not improve from 0.26160\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 405ms/step - dense_72_accuracy: 0.9881 - dense_72_loss: 0.0162 - dense_73_accuracy: 0.9456 - dense_73_loss: 0.0745 - loss: 0.0907 - val_dense_72_accuracy: 0.9552 - val_dense_72_loss: 0.0886 - val_dense_73_accuracy: 0.8611 - val_dense_73_loss: 0.2534 - val_loss: 0.3350 - learning_rate: 0.0010\nEpoch 49/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 396ms/step - dense_72_accuracy: 0.9943 - dense_72_loss: 0.0097 - dense_73_accuracy: 0.9540 - dense_73_loss: 0.0648 - loss: 0.0745\nEpoch 49: val_loss improved from 0.26160 to 0.17235, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 419ms/step - dense_72_accuracy: 0.9943 - dense_72_loss: 0.0097 - dense_73_accuracy: 0.9539 - dense_73_loss: 0.0648 - loss: 0.0746 - val_dense_72_accuracy: 0.9568 - val_dense_72_loss: 0.0934 - val_dense_73_accuracy: 0.9475 - val_dense_73_loss: 0.0801 - val_loss: 0.1723 - learning_rate: 0.0010\nEpoch 50/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 392ms/step - dense_72_accuracy: 0.9922 - dense_72_loss: 0.0129 - dense_73_accuracy: 0.9525 - dense_73_loss: 0.0663 - loss: 0.0792\nEpoch 50: val_loss did not improve from 0.17235\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 399ms/step - dense_72_accuracy: 0.9922 - dense_72_loss: 0.0129 - dense_73_accuracy: 0.9524 - dense_73_loss: 0.0663 - loss: 0.0792 - val_dense_72_accuracy: 0.9537 - val_dense_72_loss: 0.1101 - val_dense_73_accuracy: 0.8796 - val_dense_73_loss: 0.1960 - val_loss: 0.3127 - learning_rate: 0.0010\nEpoch 51/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 391ms/step - dense_72_accuracy: 0.9904 - dense_72_loss: 0.0125 - dense_73_accuracy: 0.9571 - dense_73_loss: 0.0613 - loss: 0.0738\nEpoch 51: val_loss did not improve from 0.17235\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 399ms/step - dense_72_accuracy: 0.9904 - dense_72_loss: 0.0125 - dense_73_accuracy: 0.9570 - dense_73_loss: 0.0613 - loss: 0.0738 - val_dense_72_accuracy: 0.9475 - val_dense_72_loss: 0.1013 - val_dense_73_accuracy: 0.9259 - val_dense_73_loss: 0.1282 - val_loss: 0.2341 - learning_rate: 0.0010\nEpoch 52/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 392ms/step - dense_72_accuracy: 0.9933 - dense_72_loss: 0.0092 - dense_73_accuracy: 0.9576 - dense_73_loss: 0.0570 - loss: 0.0662\nEpoch 52: val_loss did not improve from 0.17235\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 400ms/step - dense_72_accuracy: 0.9933 - dense_72_loss: 0.0092 - dense_73_accuracy: 0.9576 - dense_73_loss: 0.0570 - loss: 0.0663 - val_dense_72_accuracy: 0.9367 - val_dense_72_loss: 0.1551 - val_dense_73_accuracy: 0.9012 - val_dense_73_loss: 0.1833 - val_loss: 0.3476 - learning_rate: 0.0010\nEpoch 53/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 400ms/step - dense_72_accuracy: 0.9853 - dense_72_loss: 0.0208 - dense_73_accuracy: 0.9576 - dense_73_loss: 0.0607 - loss: 0.0815\nEpoch 53: val_loss improved from 0.17235 to 0.15569, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 423ms/step - dense_72_accuracy: 0.9853 - dense_72_loss: 0.0208 - dense_73_accuracy: 0.9575 - dense_73_loss: 0.0607 - loss: 0.0815 - val_dense_72_accuracy: 0.9537 - val_dense_72_loss: 0.0740 - val_dense_73_accuracy: 0.9429 - val_dense_73_loss: 0.0763 - val_loss: 0.1557 - learning_rate: 0.0010\nEpoch 54/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 391ms/step - dense_72_accuracy: 0.9949 - dense_72_loss: 0.0077 - dense_73_accuracy: 0.9632 - dense_73_loss: 0.0495 - loss: 0.0573\nEpoch 54: val_loss did not improve from 0.15569\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 399ms/step - dense_72_accuracy: 0.9948 - dense_72_loss: 0.0077 - dense_73_accuracy: 0.9631 - dense_73_loss: 0.0496 - loss: 0.0573 - val_dense_72_accuracy: 0.9352 - val_dense_72_loss: 0.1395 - val_dense_73_accuracy: 0.8951 - val_dense_73_loss: 0.1419 - val_loss: 0.2914 - learning_rate: 0.0010\nEpoch 55/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 392ms/step - dense_72_accuracy: 0.9954 - dense_72_loss: 0.0073 - dense_73_accuracy: 0.9640 - dense_73_loss: 0.0533 - loss: 0.0606\nEpoch 55: val_loss did not improve from 0.15569\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 400ms/step - dense_72_accuracy: 0.9954 - dense_72_loss: 0.0073 - dense_73_accuracy: 0.9640 - dense_73_loss: 0.0533 - loss: 0.0606 - val_dense_72_accuracy: 0.9275 - val_dense_72_loss: 0.1306 - val_dense_73_accuracy: 0.9213 - val_dense_73_loss: 0.1043 - val_loss: 0.2434 - learning_rate: 0.0010\nEpoch 56/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 395ms/step - dense_72_accuracy: 0.9895 - dense_72_loss: 0.0152 - dense_73_accuracy: 0.9588 - dense_73_loss: 0.0567 - loss: 0.0719\nEpoch 56: val_loss did not improve from 0.15569\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 403ms/step - dense_72_accuracy: 0.9895 - dense_72_loss: 0.0152 - dense_73_accuracy: 0.9588 - dense_73_loss: 0.0568 - loss: 0.0719 - val_dense_72_accuracy: 0.9676 - val_dense_72_loss: 0.0799 - val_dense_73_accuracy: 0.9090 - val_dense_73_loss: 0.1171 - val_loss: 0.2020 - learning_rate: 0.0010\nEpoch 57/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 399ms/step - dense_72_accuracy: 0.9949 - dense_72_loss: 0.0072 - dense_73_accuracy: 0.9682 - dense_73_loss: 0.0445 - loss: 0.0517\nEpoch 57: val_loss did not improve from 0.15569\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 407ms/step - dense_72_accuracy: 0.9949 - dense_72_loss: 0.0072 - dense_73_accuracy: 0.9682 - dense_73_loss: 0.0445 - loss: 0.0517 - val_dense_72_accuracy: 0.9136 - val_dense_72_loss: 0.2099 - val_dense_73_accuracy: 0.9352 - val_dense_73_loss: 0.0940 - val_loss: 0.3147 - learning_rate: 0.0010\nEpoch 58/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 400ms/step - dense_72_accuracy: 0.9917 - dense_72_loss: 0.0118 - dense_73_accuracy: 0.9630 - dense_73_loss: 0.0509 - loss: 0.0627\nEpoch 58: val_loss did not improve from 0.15569\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 407ms/step - dense_72_accuracy: 0.9917 - dense_72_loss: 0.0118 - dense_73_accuracy: 0.9630 - dense_73_loss: 0.0509 - loss: 0.0627 - val_dense_72_accuracy: 0.9429 - val_dense_72_loss: 0.1516 - val_dense_73_accuracy: 0.9367 - val_dense_73_loss: 0.0850 - val_loss: 0.2451 - learning_rate: 0.0010\nEpoch 59/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 399ms/step - dense_72_accuracy: 0.9946 - dense_72_loss: 0.0078 - dense_73_accuracy: 0.9651 - dense_73_loss: 0.0490 - loss: 0.0568\nEpoch 59: val_loss did not improve from 0.15569\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 407ms/step - dense_72_accuracy: 0.9946 - dense_72_loss: 0.0078 - dense_73_accuracy: 0.9651 - dense_73_loss: 0.0490 - loss: 0.0568 - val_dense_72_accuracy: 0.9414 - val_dense_72_loss: 0.1107 - val_dense_73_accuracy: 0.8627 - val_dense_73_loss: 0.2496 - val_loss: 0.3732 - learning_rate: 0.0010\nEpoch 60/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 400ms/step - dense_72_accuracy: 0.9944 - dense_72_loss: 0.0098 - dense_73_accuracy: 0.9594 - dense_73_loss: 0.0602 - loss: 0.0700\nEpoch 60: val_loss did not improve from 0.15569\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 408ms/step - dense_72_accuracy: 0.9944 - dense_72_loss: 0.0098 - dense_73_accuracy: 0.9594 - dense_73_loss: 0.0602 - loss: 0.0700 - val_dense_72_accuracy: 0.9198 - val_dense_72_loss: 0.1328 - val_dense_73_accuracy: 0.8673 - val_dense_73_loss: 0.2192 - val_loss: 0.3540 - learning_rate: 0.0010\nEpoch 61/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 396ms/step - dense_72_accuracy: 0.9951 - dense_72_loss: 0.0090 - dense_73_accuracy: 0.9681 - dense_73_loss: 0.0422 - loss: 0.0512\nEpoch 61: val_loss did not improve from 0.15569\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 404ms/step - dense_72_accuracy: 0.9951 - dense_72_loss: 0.0090 - dense_73_accuracy: 0.9681 - dense_73_loss: 0.0423 - loss: 0.0512 - val_dense_72_accuracy: 0.9367 - val_dense_72_loss: 0.1676 - val_dense_73_accuracy: 0.8704 - val_dense_73_loss: 0.2185 - val_loss: 0.3991 - learning_rate: 0.0010\nEpoch 62/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 397ms/step - dense_72_accuracy: 0.9867 - dense_72_loss: 0.0194 - dense_73_accuracy: 0.9566 - dense_73_loss: 0.0595 - loss: 0.0789\nEpoch 62: val_loss did not improve from 0.15569\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 405ms/step - dense_72_accuracy: 0.9867 - dense_72_loss: 0.0193 - dense_73_accuracy: 0.9566 - dense_73_loss: 0.0595 - loss: 0.0789 - val_dense_72_accuracy: 0.9213 - val_dense_72_loss: 0.1741 - val_dense_73_accuracy: 0.9660 - val_dense_73_loss: 0.0587 - val_loss: 0.2274 - learning_rate: 0.0010\nEpoch 63/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 390ms/step - dense_72_accuracy: 0.9969 - dense_72_loss: 0.0046 - dense_73_accuracy: 0.9744 - dense_73_loss: 0.0362 - loss: 0.0407\nEpoch 63: val_loss did not improve from 0.15569\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 397ms/step - dense_72_accuracy: 0.9969 - dense_72_loss: 0.0046 - dense_73_accuracy: 0.9744 - dense_73_loss: 0.0362 - loss: 0.0408 - val_dense_72_accuracy: 0.9105 - val_dense_72_loss: 0.1472 - val_dense_73_accuracy: 0.9383 - val_dense_73_loss: 0.1011 - val_loss: 0.2560 - learning_rate: 0.0010\nEpoch 64/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 402ms/step - dense_72_accuracy: 0.9957 - dense_72_loss: 0.0068 - dense_73_accuracy: 0.9723 - dense_73_loss: 0.0409 - loss: 0.0477\nEpoch 64: val_loss did not improve from 0.15569\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 409ms/step - dense_72_accuracy: 0.9957 - dense_72_loss: 0.0068 - dense_73_accuracy: 0.9723 - dense_73_loss: 0.0409 - loss: 0.0477 - val_dense_72_accuracy: 0.8889 - val_dense_72_loss: 0.2479 - val_dense_73_accuracy: 0.9475 - val_dense_73_loss: 0.0744 - val_loss: 0.3338 - learning_rate: 0.0010\nEpoch 65/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 391ms/step - dense_72_accuracy: 0.9918 - dense_72_loss: 0.0132 - dense_73_accuracy: 0.9701 - dense_73_loss: 0.0463 - loss: 0.0595\nEpoch 65: val_loss did not improve from 0.15569\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 399ms/step - dense_72_accuracy: 0.9918 - dense_72_loss: 0.0132 - dense_73_accuracy: 0.9701 - dense_73_loss: 0.0463 - loss: 0.0595 - val_dense_72_accuracy: 0.9352 - val_dense_72_loss: 0.1413 - val_dense_73_accuracy: 0.9090 - val_dense_73_loss: 0.1357 - val_loss: 0.2867 - learning_rate: 0.0010\nEpoch 66/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 394ms/step - dense_72_accuracy: 0.9951 - dense_72_loss: 0.0078 - dense_73_accuracy: 0.9692 - dense_73_loss: 0.0450 - loss: 0.0528\nEpoch 66: val_loss did not improve from 0.15569\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 402ms/step - dense_72_accuracy: 0.9951 - dense_72_loss: 0.0078 - dense_73_accuracy: 0.9692 - dense_73_loss: 0.0450 - loss: 0.0528 - val_dense_72_accuracy: 0.9568 - val_dense_72_loss: 0.0872 - val_dense_73_accuracy: 0.9429 - val_dense_73_loss: 0.0879 - val_loss: 0.1787 - learning_rate: 0.0010\nEpoch 67/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 401ms/step - dense_72_accuracy: 0.9963 - dense_72_loss: 0.0069 - dense_73_accuracy: 0.9789 - dense_73_loss: 0.0320 - loss: 0.0390\nEpoch 67: val_loss did not improve from 0.15569\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 409ms/step - dense_72_accuracy: 0.9963 - dense_72_loss: 0.0069 - dense_73_accuracy: 0.9789 - dense_73_loss: 0.0321 - loss: 0.0390 - val_dense_72_accuracy: 0.9244 - val_dense_72_loss: 0.1417 - val_dense_73_accuracy: 0.9059 - val_dense_73_loss: 0.1701 - val_loss: 0.2954 - learning_rate: 0.0010\nEpoch 68/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 399ms/step - dense_72_accuracy: 0.9943 - dense_72_loss: 0.0082 - dense_73_accuracy: 0.9718 - dense_73_loss: 0.0394 - loss: 0.0476\nEpoch 68: val_loss did not improve from 0.15569\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 407ms/step - dense_72_accuracy: 0.9943 - dense_72_loss: 0.0082 - dense_73_accuracy: 0.9718 - dense_73_loss: 0.0394 - loss: 0.0477 - val_dense_72_accuracy: 0.9522 - val_dense_72_loss: 0.0943 - val_dense_73_accuracy: 0.9321 - val_dense_73_loss: 0.0782 - val_loss: 0.1783 - learning_rate: 0.0010\nEpoch 69/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 398ms/step - dense_72_accuracy: 0.9950 - dense_72_loss: 0.0071 - dense_73_accuracy: 0.9753 - dense_73_loss: 0.0370 - loss: 0.0441\nEpoch 69: val_loss did not improve from 0.15569\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 406ms/step - dense_72_accuracy: 0.9950 - dense_72_loss: 0.0071 - dense_73_accuracy: 0.9753 - dense_73_loss: 0.0370 - loss: 0.0441 - val_dense_72_accuracy: 0.9475 - val_dense_72_loss: 0.1525 - val_dense_73_accuracy: 0.9244 - val_dense_73_loss: 0.1170 - val_loss: 0.2740 - learning_rate: 0.0010\nEpoch 70/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 408ms/step - dense_72_accuracy: 0.9975 - dense_72_loss: 0.0078 - dense_73_accuracy: 0.9680 - dense_73_loss: 0.0460 - loss: 0.0538\nEpoch 70: val_loss did not improve from 0.15569\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 416ms/step - dense_72_accuracy: 0.9975 - dense_72_loss: 0.0078 - dense_73_accuracy: 0.9680 - dense_73_loss: 0.0460 - loss: 0.0538 - val_dense_72_accuracy: 0.9383 - val_dense_72_loss: 0.1027 - val_dense_73_accuracy: 0.8225 - val_dense_73_loss: 0.3208 - val_loss: 0.4357 - learning_rate: 0.0010\nEpoch 71/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 393ms/step - dense_72_accuracy: 0.9955 - dense_72_loss: 0.0085 - dense_73_accuracy: 0.9691 - dense_73_loss: 0.0471 - loss: 0.0556\nEpoch 71: val_loss did not improve from 0.15569\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 401ms/step - dense_72_accuracy: 0.9955 - dense_72_loss: 0.0085 - dense_73_accuracy: 0.9691 - dense_73_loss: 0.0471 - loss: 0.0556 - val_dense_72_accuracy: 0.9321 - val_dense_72_loss: 0.1309 - val_dense_73_accuracy: 0.8904 - val_dense_73_loss: 0.1873 - val_loss: 0.3219 - learning_rate: 0.0010\nEpoch 72/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 390ms/step - dense_72_accuracy: 0.9935 - dense_72_loss: 0.0107 - dense_73_accuracy: 0.9689 - dense_73_loss: 0.0430 - loss: 0.0537\nEpoch 72: val_loss did not improve from 0.15569\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 398ms/step - dense_72_accuracy: 0.9935 - dense_72_loss: 0.0107 - dense_73_accuracy: 0.9689 - dense_73_loss: 0.0430 - loss: 0.0537 - val_dense_72_accuracy: 0.9275 - val_dense_72_loss: 0.1434 - val_dense_73_accuracy: 0.9660 - val_dense_73_loss: 0.0549 - val_loss: 0.1996 - learning_rate: 0.0010\nEpoch 73/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 388ms/step - dense_72_accuracy: 0.9943 - dense_72_loss: 0.0077 - dense_73_accuracy: 0.9800 - dense_73_loss: 0.0315 - loss: 0.0392\nEpoch 73: val_loss did not improve from 0.15569\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 396ms/step - dense_72_accuracy: 0.9943 - dense_72_loss: 0.0077 - dense_73_accuracy: 0.9800 - dense_73_loss: 0.0315 - loss: 0.0392 - val_dense_72_accuracy: 0.9336 - val_dense_72_loss: 0.1384 - val_dense_73_accuracy: 0.9367 - val_dense_73_loss: 0.0945 - val_loss: 0.2392 - learning_rate: 0.0010\nEpoch 74/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 392ms/step - dense_72_accuracy: 0.9961 - dense_72_loss: 0.0072 - dense_73_accuracy: 0.9770 - dense_73_loss: 0.0354 - loss: 0.0426\nEpoch 74: val_loss did not improve from 0.15569\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 399ms/step - dense_72_accuracy: 0.9961 - dense_72_loss: 0.0072 - dense_73_accuracy: 0.9770 - dense_73_loss: 0.0354 - loss: 0.0426 - val_dense_72_accuracy: 0.9506 - val_dense_72_loss: 0.0968 - val_dense_73_accuracy: 0.8102 - val_dense_73_loss: 0.3306 - val_loss: 0.4409 - learning_rate: 0.0010\nEpoch 75/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 398ms/step - dense_72_accuracy: 0.9956 - dense_72_loss: 0.0068 - dense_73_accuracy: 0.9753 - dense_73_loss: 0.0370 - loss: 0.0438\nEpoch 75: val_loss did not improve from 0.15569\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 406ms/step - dense_72_accuracy: 0.9956 - dense_72_loss: 0.0068 - dense_73_accuracy: 0.9753 - dense_73_loss: 0.0370 - loss: 0.0439 - val_dense_72_accuracy: 0.9522 - val_dense_72_loss: 0.0956 - val_dense_73_accuracy: 0.9552 - val_dense_73_loss: 0.0662 - val_loss: 0.1670 - learning_rate: 0.0010\nEpoch 76/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 399ms/step - dense_72_accuracy: 0.9890 - dense_72_loss: 0.0201 - dense_73_accuracy: 0.9802 - dense_73_loss: 0.0301 - loss: 0.0501\nEpoch 76: val_loss did not improve from 0.15569\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 407ms/step - dense_72_accuracy: 0.9890 - dense_72_loss: 0.0201 - dense_73_accuracy: 0.9802 - dense_73_loss: 0.0301 - loss: 0.0501 - val_dense_72_accuracy: 0.9352 - val_dense_72_loss: 0.1813 - val_dense_73_accuracy: 0.8086 - val_dense_73_loss: 0.4733 - val_loss: 0.6778 - learning_rate: 0.0010\nEpoch 77/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 399ms/step - dense_72_accuracy: 0.9933 - dense_72_loss: 0.0091 - dense_73_accuracy: 0.9803 - dense_73_loss: 0.0276 - loss: 0.0367\nEpoch 77: val_loss did not improve from 0.15569\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 407ms/step - dense_72_accuracy: 0.9933 - dense_72_loss: 0.0091 - dense_73_accuracy: 0.9803 - dense_73_loss: 0.0276 - loss: 0.0367 - val_dense_72_accuracy: 0.9444 - val_dense_72_loss: 0.1245 - val_dense_73_accuracy: 0.9074 - val_dense_73_loss: 0.1871 - val_loss: 0.3228 - learning_rate: 0.0010\nEpoch 78/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 399ms/step - dense_72_accuracy: 0.9933 - dense_72_loss: 0.0119 - dense_73_accuracy: 0.9726 - dense_73_loss: 0.0404 - loss: 0.0523\nEpoch 78: val_loss did not improve from 0.15569\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 406ms/step - dense_72_accuracy: 0.9933 - dense_72_loss: 0.0119 - dense_73_accuracy: 0.9726 - dense_73_loss: 0.0404 - loss: 0.0523 - val_dense_72_accuracy: 0.9367 - val_dense_72_loss: 0.1302 - val_dense_73_accuracy: 0.9414 - val_dense_73_loss: 0.0884 - val_loss: 0.2042 - learning_rate: 0.0010\nEpoch 79/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 399ms/step - dense_72_accuracy: 0.9921 - dense_72_loss: 0.0143 - dense_73_accuracy: 0.9712 - dense_73_loss: 0.0398 - loss: 0.0541\nEpoch 79: val_loss did not improve from 0.15569\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 407ms/step - dense_72_accuracy: 0.9921 - dense_72_loss: 0.0143 - dense_73_accuracy: 0.9712 - dense_73_loss: 0.0398 - loss: 0.0540 - val_dense_72_accuracy: 0.9444 - val_dense_72_loss: 0.1285 - val_dense_73_accuracy: 0.9660 - val_dense_73_loss: 0.0459 - val_loss: 0.1746 - learning_rate: 0.0010\nEpoch 80/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 399ms/step - dense_72_accuracy: 0.9971 - dense_72_loss: 0.0039 - dense_73_accuracy: 0.9785 - dense_73_loss: 0.0298 - loss: 0.0336\nEpoch 80: val_loss did not improve from 0.15569\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 406ms/step - dense_72_accuracy: 0.9971 - dense_72_loss: 0.0039 - dense_73_accuracy: 0.9785 - dense_73_loss: 0.0298 - loss: 0.0336 - val_dense_72_accuracy: 0.9630 - val_dense_72_loss: 0.0835 - val_dense_73_accuracy: 0.9491 - val_dense_73_loss: 0.0774 - val_loss: 0.1668 - learning_rate: 0.0010\nEpoch 81/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 392ms/step - dense_72_accuracy: 0.9957 - dense_72_loss: 0.0078 - dense_73_accuracy: 0.9789 - dense_73_loss: 0.0275 - loss: 0.0353\nEpoch 81: val_loss did not improve from 0.15569\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 400ms/step - dense_72_accuracy: 0.9957 - dense_72_loss: 0.0078 - dense_73_accuracy: 0.9788 - dense_73_loss: 0.0276 - loss: 0.0354 - val_dense_72_accuracy: 0.9398 - val_dense_72_loss: 0.1235 - val_dense_73_accuracy: 0.9614 - val_dense_73_loss: 0.0501 - val_loss: 0.1800 - learning_rate: 0.0010\nEpoch 82/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 406ms/step - dense_72_accuracy: 0.9951 - dense_72_loss: 0.0079 - dense_73_accuracy: 0.9719 - dense_73_loss: 0.0429 - loss: 0.0508\nEpoch 82: val_loss did not improve from 0.15569\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 414ms/step - dense_72_accuracy: 0.9951 - dense_72_loss: 0.0079 - dense_73_accuracy: 0.9719 - dense_73_loss: 0.0429 - loss: 0.0508 - val_dense_72_accuracy: 0.9475 - val_dense_72_loss: 0.1021 - val_dense_73_accuracy: 0.8997 - val_dense_73_loss: 0.1546 - val_loss: 0.2658 - learning_rate: 0.0010\nEpoch 83/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 388ms/step - dense_72_accuracy: 0.9961 - dense_72_loss: 0.0052 - dense_73_accuracy: 0.9802 - dense_73_loss: 0.0305 - loss: 0.0358\nEpoch 83: val_loss did not improve from 0.15569\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 396ms/step - dense_72_accuracy: 0.9961 - dense_72_loss: 0.0052 - dense_73_accuracy: 0.9802 - dense_73_loss: 0.0305 - loss: 0.0358 - val_dense_72_accuracy: 0.9213 - val_dense_72_loss: 0.2032 - val_dense_73_accuracy: 0.9275 - val_dense_73_loss: 0.1312 - val_loss: 0.3463 - learning_rate: 0.0010\nEpoch 84/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 390ms/step - dense_72_accuracy: 0.9967 - dense_72_loss: 0.0056 - dense_73_accuracy: 0.9764 - dense_73_loss: 0.0334 - loss: 0.0390\nEpoch 84: val_loss did not improve from 0.15569\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 398ms/step - dense_72_accuracy: 0.9967 - dense_72_loss: 0.0056 - dense_73_accuracy: 0.9764 - dense_73_loss: 0.0334 - loss: 0.0390 - val_dense_72_accuracy: 0.9383 - val_dense_72_loss: 0.1182 - val_dense_73_accuracy: 0.9475 - val_dense_73_loss: 0.0667 - val_loss: 0.1888 - learning_rate: 0.0010\nEpoch 85/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 390ms/step - dense_72_accuracy: 0.9888 - dense_72_loss: 0.0147 - dense_73_accuracy: 0.9828 - dense_73_loss: 0.0249 - loss: 0.0396\nEpoch 85: val_loss did not improve from 0.15569\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 397ms/step - dense_72_accuracy: 0.9888 - dense_72_loss: 0.0147 - dense_73_accuracy: 0.9828 - dense_73_loss: 0.0249 - loss: 0.0396 - val_dense_72_accuracy: 0.9429 - val_dense_72_loss: 0.1453 - val_dense_73_accuracy: 0.9244 - val_dense_73_loss: 0.1108 - val_loss: 0.2655 - learning_rate: 0.0010\nEpoch 86/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 390ms/step - dense_72_accuracy: 0.9962 - dense_72_loss: 0.0053 - dense_73_accuracy: 0.9811 - dense_73_loss: 0.0284 - loss: 0.0337\nEpoch 86: val_loss did not improve from 0.15569\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 398ms/step - dense_72_accuracy: 0.9962 - dense_72_loss: 0.0053 - dense_73_accuracy: 0.9811 - dense_73_loss: 0.0284 - loss: 0.0337 - val_dense_72_accuracy: 0.9429 - val_dense_72_loss: 0.1276 - val_dense_73_accuracy: 0.9290 - val_dense_73_loss: 0.1148 - val_loss: 0.2509 - learning_rate: 0.0010\nEpoch 87/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 410ms/step - dense_72_accuracy: 0.9943 - dense_72_loss: 0.0068 - dense_73_accuracy: 0.9775 - dense_73_loss: 0.0326 - loss: 0.0394\nEpoch 87: val_loss did not improve from 0.15569\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 418ms/step - dense_72_accuracy: 0.9942 - dense_72_loss: 0.0068 - dense_73_accuracy: 0.9775 - dense_73_loss: 0.0326 - loss: 0.0394 - val_dense_72_accuracy: 0.9475 - val_dense_72_loss: 0.1367 - val_dense_73_accuracy: 0.9475 - val_dense_73_loss: 0.0820 - val_loss: 0.2257 - learning_rate: 0.0010\nEpoch 88/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 404ms/step - dense_72_accuracy: 0.9953 - dense_72_loss: 0.0086 - dense_73_accuracy: 0.9795 - dense_73_loss: 0.0260 - loss: 0.0346\nEpoch 88: val_loss did not improve from 0.15569\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 412ms/step - dense_72_accuracy: 0.9954 - dense_72_loss: 0.0086 - dense_73_accuracy: 0.9795 - dense_73_loss: 0.0260 - loss: 0.0346 - val_dense_72_accuracy: 0.9537 - val_dense_72_loss: 0.0830 - val_dense_73_accuracy: 0.8318 - val_dense_73_loss: 0.4334 - val_loss: 0.5274 - learning_rate: 0.0010\nEpoch 89/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 405ms/step - dense_72_accuracy: 0.9968 - dense_72_loss: 0.0049 - dense_73_accuracy: 0.9758 - dense_73_loss: 0.0387 - loss: 0.0436\nEpoch 89: val_loss did not improve from 0.15569\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 414ms/step - dense_72_accuracy: 0.9968 - dense_72_loss: 0.0049 - dense_73_accuracy: 0.9758 - dense_73_loss: 0.0386 - loss: 0.0436 - val_dense_72_accuracy: 0.9367 - val_dense_72_loss: 0.1398 - val_dense_73_accuracy: 0.9599 - val_dense_73_loss: 0.0564 - val_loss: 0.2033 - learning_rate: 0.0010\nEpoch 90/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 394ms/step - dense_72_accuracy: 0.9965 - dense_72_loss: 0.0055 - dense_73_accuracy: 0.9794 - dense_73_loss: 0.0281 - loss: 0.0336\nEpoch 90: val_loss did not improve from 0.15569\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 402ms/step - dense_72_accuracy: 0.9965 - dense_72_loss: 0.0055 - dense_73_accuracy: 0.9794 - dense_73_loss: 0.0281 - loss: 0.0336 - val_dense_72_accuracy: 0.9398 - val_dense_72_loss: 0.1287 - val_dense_73_accuracy: 0.9676 - val_dense_73_loss: 0.0611 - val_loss: 0.1969 - learning_rate: 0.0010\nEpoch 91/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 396ms/step - dense_72_accuracy: 0.9973 - dense_72_loss: 0.0037 - dense_73_accuracy: 0.9828 - dense_73_loss: 0.0238 - loss: 0.0275\nEpoch 91: val_loss did not improve from 0.15569\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 404ms/step - dense_72_accuracy: 0.9973 - dense_72_loss: 0.0037 - dense_73_accuracy: 0.9828 - dense_73_loss: 0.0238 - loss: 0.0275 - val_dense_72_accuracy: 0.8904 - val_dense_72_loss: 0.2586 - val_dense_73_accuracy: 0.9830 - val_dense_73_loss: 0.0279 - val_loss: 0.2968 - learning_rate: 0.0010\nEpoch 92/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 394ms/step - dense_72_accuracy: 0.9928 - dense_72_loss: 0.0087 - dense_73_accuracy: 0.9778 - dense_73_loss: 0.0367 - loss: 0.0454\nEpoch 92: val_loss did not improve from 0.15569\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 401ms/step - dense_72_accuracy: 0.9928 - dense_72_loss: 0.0087 - dense_73_accuracy: 0.9778 - dense_73_loss: 0.0366 - loss: 0.0454 - val_dense_72_accuracy: 0.9383 - val_dense_72_loss: 0.1577 - val_dense_73_accuracy: 0.9522 - val_dense_73_loss: 0.0751 - val_loss: 0.2413 - learning_rate: 0.0010\nEpoch 93/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 410ms/step - dense_72_accuracy: 0.9963 - dense_72_loss: 0.0064 - dense_73_accuracy: 0.9845 - dense_73_loss: 0.0229 - loss: 0.0293\nEpoch 93: val_loss did not improve from 0.15569\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 418ms/step - dense_72_accuracy: 0.9963 - dense_72_loss: 0.0064 - dense_73_accuracy: 0.9845 - dense_73_loss: 0.0229 - loss: 0.0293 - val_dense_72_accuracy: 0.9105 - val_dense_72_loss: 0.2016 - val_dense_73_accuracy: 0.9290 - val_dense_73_loss: 0.1107 - val_loss: 0.3231 - learning_rate: 0.0010\nEpoch 94/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 405ms/step - dense_72_accuracy: 0.9938 - dense_72_loss: 0.0076 - dense_73_accuracy: 0.9838 - dense_73_loss: 0.0209 - loss: 0.0284\nEpoch 94: val_loss did not improve from 0.15569\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 413ms/step - dense_72_accuracy: 0.9938 - dense_72_loss: 0.0076 - dense_73_accuracy: 0.9838 - dense_73_loss: 0.0209 - loss: 0.0284 - val_dense_72_accuracy: 0.9398 - val_dense_72_loss: 0.1486 - val_dense_73_accuracy: 0.8904 - val_dense_73_loss: 0.2190 - val_loss: 0.3610 - learning_rate: 0.0010\nEpoch 95/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 393ms/step - dense_72_accuracy: 0.9961 - dense_72_loss: 0.0057 - dense_73_accuracy: 0.9745 - dense_73_loss: 0.0380 - loss: 0.0436\nEpoch 95: val_loss did not improve from 0.15569\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 401ms/step - dense_72_accuracy: 0.9961 - dense_72_loss: 0.0057 - dense_73_accuracy: 0.9745 - dense_73_loss: 0.0380 - loss: 0.0436 - val_dense_72_accuracy: 0.9398 - val_dense_72_loss: 0.1527 - val_dense_73_accuracy: 0.9028 - val_dense_73_loss: 0.1520 - val_loss: 0.3127 - learning_rate: 0.0010\nEpoch 96/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 404ms/step - dense_72_accuracy: 0.9979 - dense_72_loss: 0.0032 - dense_73_accuracy: 0.9794 - dense_73_loss: 0.0321 - loss: 0.0353\nEpoch 96: val_loss did not improve from 0.15569\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 411ms/step - dense_72_accuracy: 0.9979 - dense_72_loss: 0.0032 - dense_73_accuracy: 0.9794 - dense_73_loss: 0.0321 - loss: 0.0353 - val_dense_72_accuracy: 0.9367 - val_dense_72_loss: 0.1215 - val_dense_73_accuracy: 0.9660 - val_dense_73_loss: 0.0451 - val_loss: 0.1728 - learning_rate: 0.0010\nEpoch 97/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 394ms/step - dense_72_accuracy: 0.9977 - dense_72_loss: 0.0045 - dense_73_accuracy: 0.9789 - dense_73_loss: 0.0305 - loss: 0.0351\nEpoch 97: val_loss did not improve from 0.15569\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 402ms/step - dense_72_accuracy: 0.9977 - dense_72_loss: 0.0045 - dense_73_accuracy: 0.9789 - dense_73_loss: 0.0305 - loss: 0.0351 - val_dense_72_accuracy: 0.9568 - val_dense_72_loss: 0.1195 - val_dense_73_accuracy: 0.8364 - val_dense_73_loss: 0.3250 - val_loss: 0.4609 - learning_rate: 0.0010\nEpoch 98/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 390ms/step - dense_72_accuracy: 0.9970 - dense_72_loss: 0.0041 - dense_73_accuracy: 0.9853 - dense_73_loss: 0.0237 - loss: 0.0277\nEpoch 98: val_loss did not improve from 0.15569\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 398ms/step - dense_72_accuracy: 0.9970 - dense_72_loss: 0.0041 - dense_73_accuracy: 0.9853 - dense_73_loss: 0.0237 - loss: 0.0277 - val_dense_72_accuracy: 0.9522 - val_dense_72_loss: 0.1224 - val_dense_73_accuracy: 0.9012 - val_dense_73_loss: 0.2014 - val_loss: 0.3355 - learning_rate: 0.0010\nEpoch 99/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 401ms/step - dense_72_accuracy: 0.9966 - dense_72_loss: 0.0059 - dense_73_accuracy: 0.9744 - dense_73_loss: 0.0345 - loss: 0.0404\nEpoch 99: val_loss did not improve from 0.15569\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 409ms/step - dense_72_accuracy: 0.9966 - dense_72_loss: 0.0059 - dense_73_accuracy: 0.9744 - dense_73_loss: 0.0345 - loss: 0.0404 - val_dense_72_accuracy: 0.9460 - val_dense_72_loss: 0.1297 - val_dense_73_accuracy: 0.9028 - val_dense_73_loss: 0.2086 - val_loss: 0.3454 - learning_rate: 0.0010\nEpoch 100/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 395ms/step - dense_72_accuracy: 0.9968 - dense_72_loss: 0.0055 - dense_73_accuracy: 0.9825 - dense_73_loss: 0.0248 - loss: 0.0303\nEpoch 100: val_loss did not improve from 0.15569\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 403ms/step - dense_72_accuracy: 0.9968 - dense_72_loss: 0.0055 - dense_73_accuracy: 0.9825 - dense_73_loss: 0.0248 - loss: 0.0303 - val_dense_72_accuracy: 0.9475 - val_dense_72_loss: 0.1172 - val_dense_73_accuracy: 0.9491 - val_dense_73_loss: 0.0652 - val_loss: 0.1891 - learning_rate: 0.0010\nEpoch 101/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 394ms/step - dense_72_accuracy: 0.9968 - dense_72_loss: 0.0041 - dense_73_accuracy: 0.9841 - dense_73_loss: 0.0256 - loss: 0.0297\nEpoch 101: val_loss did not improve from 0.15569\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 402ms/step - dense_72_accuracy: 0.9968 - dense_72_loss: 0.0041 - dense_73_accuracy: 0.9841 - dense_73_loss: 0.0256 - loss: 0.0297 - val_dense_72_accuracy: 0.9522 - val_dense_72_loss: 0.1196 - val_dense_73_accuracy: 0.9660 - val_dense_73_loss: 0.0559 - val_loss: 0.1751 - learning_rate: 0.0010\nEpoch 102/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 400ms/step - dense_72_accuracy: 0.9966 - dense_72_loss: 0.0048 - dense_73_accuracy: 0.9792 - dense_73_loss: 0.0287 - loss: 0.0335\nEpoch 102: val_loss did not improve from 0.15569\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 408ms/step - dense_72_accuracy: 0.9966 - dense_72_loss: 0.0048 - dense_73_accuracy: 0.9792 - dense_73_loss: 0.0287 - loss: 0.0335 - val_dense_72_accuracy: 0.9398 - val_dense_72_loss: 0.1311 - val_dense_73_accuracy: 0.9769 - val_dense_73_loss: 0.0280 - val_loss: 0.1648 - learning_rate: 0.0010\nEpoch 103/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 391ms/step - dense_72_accuracy: 0.9962 - dense_72_loss: 0.0053 - dense_73_accuracy: 0.9849 - dense_73_loss: 0.0212 - loss: 0.0265\nEpoch 103: val_loss did not improve from 0.15569\n\nEpoch 103: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 399ms/step - dense_72_accuracy: 0.9962 - dense_72_loss: 0.0053 - dense_73_accuracy: 0.9849 - dense_73_loss: 0.0212 - loss: 0.0265 - val_dense_72_accuracy: 0.9522 - val_dense_72_loss: 0.1085 - val_dense_73_accuracy: 0.9321 - val_dense_73_loss: 0.1456 - val_loss: 0.2545 - learning_rate: 0.0010\nEpoch 104/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 390ms/step - dense_72_accuracy: 0.9984 - dense_72_loss: 0.0025 - dense_73_accuracy: 0.9903 - dense_73_loss: 0.0141 - loss: 0.0166\nEpoch 104: val_loss improved from 0.15569 to 0.12898, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 413ms/step - dense_72_accuracy: 0.9984 - dense_72_loss: 0.0025 - dense_73_accuracy: 0.9903 - dense_73_loss: 0.0141 - loss: 0.0166 - val_dense_72_accuracy: 0.9568 - val_dense_72_loss: 0.1143 - val_dense_73_accuracy: 0.9954 - val_dense_73_loss: 0.0110 - val_loss: 0.1290 - learning_rate: 1.0000e-04\nEpoch 105/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 403ms/step - dense_72_accuracy: 0.9996 - dense_72_loss: 0.0011 - dense_73_accuracy: 0.9939 - dense_73_loss: 0.0092 - loss: 0.0103\nEpoch 105: val_loss improved from 0.12898 to 0.12197, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 427ms/step - dense_72_accuracy: 0.9996 - dense_72_loss: 0.0011 - dense_73_accuracy: 0.9939 - dense_73_loss: 0.0092 - loss: 0.0103 - val_dense_72_accuracy: 0.9583 - val_dense_72_loss: 0.1096 - val_dense_73_accuracy: 0.9954 - val_dense_73_loss: 0.0090 - val_loss: 0.1220 - learning_rate: 1.0000e-04\nEpoch 106/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 404ms/step - dense_72_accuracy: 0.9988 - dense_72_loss: 0.0018 - dense_73_accuracy: 0.9936 - dense_73_loss: 0.0084 - loss: 0.0102\nEpoch 106: val_loss improved from 0.12197 to 0.11939, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 427ms/step - dense_72_accuracy: 0.9988 - dense_72_loss: 0.0018 - dense_73_accuracy: 0.9936 - dense_73_loss: 0.0084 - loss: 0.0102 - val_dense_72_accuracy: 0.9568 - val_dense_72_loss: 0.1069 - val_dense_73_accuracy: 0.9938 - val_dense_73_loss: 0.0092 - val_loss: 0.1194 - learning_rate: 1.0000e-04\nEpoch 107/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 395ms/step - dense_72_accuracy: 0.9978 - dense_72_loss: 0.0028 - dense_73_accuracy: 0.9971 - dense_73_loss: 0.0056 - loss: 0.0085\nEpoch 107: val_loss improved from 0.11939 to 0.11691, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 418ms/step - dense_72_accuracy: 0.9978 - dense_72_loss: 0.0028 - dense_73_accuracy: 0.9970 - dense_73_loss: 0.0057 - loss: 0.0085 - val_dense_72_accuracy: 0.9614 - val_dense_72_loss: 0.1046 - val_dense_73_accuracy: 0.9954 - val_dense_73_loss: 0.0087 - val_loss: 0.1169 - learning_rate: 1.0000e-04\nEpoch 108/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 394ms/step - dense_72_accuracy: 0.9997 - dense_72_loss: 4.9096e-04 - dense_73_accuracy: 0.9952 - dense_73_loss: 0.0060 - loss: 0.0065\nEpoch 108: val_loss did not improve from 0.11691\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 402ms/step - dense_72_accuracy: 0.9997 - dense_72_loss: 4.9140e-04 - dense_73_accuracy: 0.9952 - dense_73_loss: 0.0060 - loss: 0.0065 - val_dense_72_accuracy: 0.9630 - val_dense_72_loss: 0.1065 - val_dense_73_accuracy: 0.9907 - val_dense_73_loss: 0.0099 - val_loss: 0.1204 - learning_rate: 1.0000e-04\nEpoch 109/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 399ms/step - dense_72_accuracy: 0.9997 - dense_72_loss: 0.0010 - dense_73_accuracy: 0.9953 - dense_73_loss: 0.0056 - loss: 0.0066\nEpoch 109: val_loss did not improve from 0.11691\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 406ms/step - dense_72_accuracy: 0.9997 - dense_72_loss: 0.0010 - dense_73_accuracy: 0.9953 - dense_73_loss: 0.0056 - loss: 0.0066 - val_dense_72_accuracy: 0.9583 - val_dense_72_loss: 0.1119 - val_dense_73_accuracy: 0.9938 - val_dense_73_loss: 0.0080 - val_loss: 0.1240 - learning_rate: 1.0000e-04\nEpoch 110/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 394ms/step - dense_72_accuracy: 0.9999 - dense_72_loss: 3.8606e-04 - dense_73_accuracy: 0.9959 - dense_73_loss: 0.0060 - loss: 0.0064\nEpoch 110: val_loss did not improve from 0.11691\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 402ms/step - dense_72_accuracy: 0.9999 - dense_72_loss: 3.8607e-04 - dense_73_accuracy: 0.9959 - dense_73_loss: 0.0060 - loss: 0.0064 - val_dense_72_accuracy: 0.9583 - val_dense_72_loss: 0.1159 - val_dense_73_accuracy: 0.9907 - val_dense_73_loss: 0.0122 - val_loss: 0.1326 - learning_rate: 1.0000e-04\nEpoch 111/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 404ms/step - dense_72_accuracy: 0.9984 - dense_72_loss: 0.0015 - dense_73_accuracy: 0.9964 - dense_73_loss: 0.0052 - loss: 0.0067\nEpoch 111: val_loss did not improve from 0.11691\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 412ms/step - dense_72_accuracy: 0.9984 - dense_72_loss: 0.0015 - dense_73_accuracy: 0.9964 - dense_73_loss: 0.0052 - loss: 0.0067 - val_dense_72_accuracy: 0.9568 - val_dense_72_loss: 0.1165 - val_dense_73_accuracy: 0.9938 - val_dense_73_loss: 0.0076 - val_loss: 0.1281 - learning_rate: 1.0000e-04\nEpoch 112/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 404ms/step - dense_72_accuracy: 0.9997 - dense_72_loss: 7.5991e-04 - dense_73_accuracy: 0.9993 - dense_73_loss: 0.0022 - loss: 0.0030\nEpoch 112: val_loss did not improve from 0.11691\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 412ms/step - dense_72_accuracy: 0.9997 - dense_72_loss: 7.6045e-04 - dense_73_accuracy: 0.9993 - dense_73_loss: 0.0022 - loss: 0.0030 - val_dense_72_accuracy: 0.9552 - val_dense_72_loss: 0.1165 - val_dense_73_accuracy: 0.9938 - val_dense_73_loss: 0.0075 - val_loss: 0.1279 - learning_rate: 1.0000e-04\nEpoch 113/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 403ms/step - dense_72_accuracy: 0.9996 - dense_72_loss: 6.6859e-04 - dense_73_accuracy: 0.9979 - dense_73_loss: 0.0037 - loss: 0.0044\nEpoch 113: val_loss did not improve from 0.11691\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 411ms/step - dense_72_accuracy: 0.9996 - dense_72_loss: 6.6855e-04 - dense_73_accuracy: 0.9979 - dense_73_loss: 0.0037 - loss: 0.0044 - val_dense_72_accuracy: 0.9599 - val_dense_72_loss: 0.1168 - val_dense_73_accuracy: 0.9954 - val_dense_73_loss: 0.0066 - val_loss: 0.1273 - learning_rate: 1.0000e-04\nEpoch 114/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 390ms/step - dense_72_accuracy: 0.9997 - dense_72_loss: 5.9190e-04 - dense_73_accuracy: 0.9969 - dense_73_loss: 0.0064 - loss: 0.0070\nEpoch 114: val_loss did not improve from 0.11691\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 398ms/step - dense_72_accuracy: 0.9997 - dense_72_loss: 5.9149e-04 - dense_73_accuracy: 0.9969 - dense_73_loss: 0.0064 - loss: 0.0070 - val_dense_72_accuracy: 0.9537 - val_dense_72_loss: 0.1142 - val_dense_73_accuracy: 0.9954 - val_dense_73_loss: 0.0067 - val_loss: 0.1247 - learning_rate: 1.0000e-04\nEpoch 115/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 391ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 3.6748e-04 - dense_73_accuracy: 0.9976 - dense_73_loss: 0.0042 - loss: 0.0046\nEpoch 115: val_loss did not improve from 0.11691\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 398ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 3.6716e-04 - dense_73_accuracy: 0.9976 - dense_73_loss: 0.0042 - loss: 0.0046 - val_dense_72_accuracy: 0.9568 - val_dense_72_loss: 0.1155 - val_dense_73_accuracy: 0.9954 - val_dense_73_loss: 0.0072 - val_loss: 0.1268 - learning_rate: 1.0000e-04\nEpoch 116/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 402ms/step - dense_72_accuracy: 0.9998 - dense_72_loss: 4.5885e-04 - dense_73_accuracy: 0.9973 - dense_73_loss: 0.0038 - loss: 0.0043\nEpoch 116: val_loss improved from 0.11691 to 0.11139, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 425ms/step - dense_72_accuracy: 0.9998 - dense_72_loss: 4.5918e-04 - dense_73_accuracy: 0.9973 - dense_73_loss: 0.0038 - loss: 0.0043 - val_dense_72_accuracy: 0.9630 - val_dense_72_loss: 0.1044 - val_dense_73_accuracy: 0.9969 - val_dense_73_loss: 0.0034 - val_loss: 0.1114 - learning_rate: 1.0000e-04\nEpoch 117/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 395ms/step - dense_72_accuracy: 0.9991 - dense_72_loss: 0.0014 - dense_73_accuracy: 0.9976 - dense_73_loss: 0.0041 - loss: 0.0055\nEpoch 117: val_loss did not improve from 0.11139\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 403ms/step - dense_72_accuracy: 0.9991 - dense_72_loss: 0.0014 - dense_73_accuracy: 0.9976 - dense_73_loss: 0.0041 - loss: 0.0055 - val_dense_72_accuracy: 0.9614 - val_dense_72_loss: 0.1053 - val_dense_73_accuracy: 0.9969 - val_dense_73_loss: 0.0039 - val_loss: 0.1120 - learning_rate: 1.0000e-04\nEpoch 118/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 395ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 1.2989e-04 - dense_73_accuracy: 0.9983 - dense_73_loss: 0.0033 - loss: 0.0034\nEpoch 118: val_loss did not improve from 0.11139\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 403ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 1.2996e-04 - dense_73_accuracy: 0.9983 - dense_73_loss: 0.0033 - loss: 0.0034 - val_dense_72_accuracy: 0.9645 - val_dense_72_loss: 0.1086 - val_dense_73_accuracy: 0.9985 - val_dense_73_loss: 0.0037 - val_loss: 0.1155 - learning_rate: 1.0000e-04\nEpoch 119/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 400ms/step - dense_72_accuracy: 0.9996 - dense_72_loss: 0.0011 - dense_73_accuracy: 0.9985 - dense_73_loss: 0.0031 - loss: 0.0041\nEpoch 119: val_loss did not improve from 0.11139\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 408ms/step - dense_72_accuracy: 0.9996 - dense_72_loss: 0.0011 - dense_73_accuracy: 0.9985 - dense_73_loss: 0.0031 - loss: 0.0041 - val_dense_72_accuracy: 0.9552 - val_dense_72_loss: 0.1269 - val_dense_73_accuracy: 0.9985 - val_dense_73_loss: 0.0036 - val_loss: 0.1337 - learning_rate: 1.0000e-04\nEpoch 120/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 400ms/step - dense_72_accuracy: 0.9991 - dense_72_loss: 0.0011 - dense_73_accuracy: 0.9983 - dense_73_loss: 0.0031 - loss: 0.0042\nEpoch 120: val_loss did not improve from 0.11139\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 407ms/step - dense_72_accuracy: 0.9991 - dense_72_loss: 0.0010 - dense_73_accuracy: 0.9983 - dense_73_loss: 0.0031 - loss: 0.0041 - val_dense_72_accuracy: 0.9552 - val_dense_72_loss: 0.1259 - val_dense_73_accuracy: 0.9985 - val_dense_73_loss: 0.0025 - val_loss: 0.1330 - learning_rate: 1.0000e-04\nEpoch 121/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 399ms/step - dense_72_accuracy: 0.9995 - dense_72_loss: 4.7374e-04 - dense_73_accuracy: 0.9996 - dense_73_loss: 0.0017 - loss: 0.0022\nEpoch 121: val_loss did not improve from 0.11139\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 407ms/step - dense_72_accuracy: 0.9995 - dense_72_loss: 4.7363e-04 - dense_73_accuracy: 0.9996 - dense_73_loss: 0.0017 - loss: 0.0022 - val_dense_72_accuracy: 0.9599 - val_dense_72_loss: 0.1153 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0033 - val_loss: 0.1226 - learning_rate: 1.0000e-04\nEpoch 122/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 404ms/step - dense_72_accuracy: 0.9995 - dense_72_loss: 5.7655e-04 - dense_73_accuracy: 0.9987 - dense_73_loss: 0.0022 - loss: 0.0028\nEpoch 122: val_loss did not improve from 0.11139\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 411ms/step - dense_72_accuracy: 0.9995 - dense_72_loss: 5.7583e-04 - dense_73_accuracy: 0.9987 - dense_73_loss: 0.0022 - loss: 0.0028 - val_dense_72_accuracy: 0.9583 - val_dense_72_loss: 0.1219 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0032 - val_loss: 0.1297 - learning_rate: 1.0000e-04\nEpoch 123/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 403ms/step - dense_72_accuracy: 0.9993 - dense_72_loss: 3.8447e-04 - dense_73_accuracy: 0.9986 - dense_73_loss: 0.0021 - loss: 0.0024\nEpoch 123: val_loss did not improve from 0.11139\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 411ms/step - dense_72_accuracy: 0.9993 - dense_72_loss: 3.8397e-04 - dense_73_accuracy: 0.9986 - dense_73_loss: 0.0021 - loss: 0.0024 - val_dense_72_accuracy: 0.9614 - val_dense_72_loss: 0.1186 - val_dense_73_accuracy: 0.9954 - val_dense_73_loss: 0.0067 - val_loss: 0.1298 - learning_rate: 1.0000e-04\nEpoch 124/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 394ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 1.0833e-04 - dense_73_accuracy: 0.9985 - dense_73_loss: 0.0024 - loss: 0.0026\nEpoch 124: val_loss did not improve from 0.11139\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 402ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 1.0819e-04 - dense_73_accuracy: 0.9985 - dense_73_loss: 0.0024 - loss: 0.0026 - val_dense_72_accuracy: 0.9614 - val_dense_72_loss: 0.1168 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0019 - val_loss: 0.1230 - learning_rate: 1.0000e-04\nEpoch 125/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 391ms/step - dense_72_accuracy: 0.9994 - dense_72_loss: 3.6654e-04 - dense_73_accuracy: 0.9988 - dense_73_loss: 0.0023 - loss: 0.0027\nEpoch 125: val_loss did not improve from 0.11139\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 399ms/step - dense_72_accuracy: 0.9994 - dense_72_loss: 3.6601e-04 - dense_73_accuracy: 0.9988 - dense_73_loss: 0.0023 - loss: 0.0027 - val_dense_72_accuracy: 0.9568 - val_dense_72_loss: 0.1245 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0016 - val_loss: 0.1306 - learning_rate: 1.0000e-04\nEpoch 126/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 395ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 2.5001e-04 - dense_73_accuracy: 0.9992 - dense_73_loss: 0.0020 - loss: 0.0022\nEpoch 126: val_loss did not improve from 0.11139\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 403ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 2.4954e-04 - dense_73_accuracy: 0.9991 - dense_73_loss: 0.0020 - loss: 0.0022 - val_dense_72_accuracy: 0.9660 - val_dense_72_loss: 0.1147 - val_dense_73_accuracy: 0.9985 - val_dense_73_loss: 0.0026 - val_loss: 0.1214 - learning_rate: 1.0000e-04\nEpoch 127/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 410ms/step - dense_72_accuracy: 0.9999 - dense_72_loss: 1.9088e-04 - dense_73_accuracy: 0.9994 - dense_73_loss: 0.0022 - loss: 0.0024\nEpoch 127: val_loss did not improve from 0.11139\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 418ms/step - dense_72_accuracy: 0.9999 - dense_72_loss: 1.9116e-04 - dense_73_accuracy: 0.9994 - dense_73_loss: 0.0022 - loss: 0.0024 - val_dense_72_accuracy: 0.9552 - val_dense_72_loss: 0.1288 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0019 - val_loss: 0.1353 - learning_rate: 1.0000e-04\nEpoch 128/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 394ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 1.1086e-04 - dense_73_accuracy: 0.9993 - dense_73_loss: 0.0017 - loss: 0.0018\nEpoch 128: val_loss did not improve from 0.11139\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 402ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 1.1092e-04 - dense_73_accuracy: 0.9993 - dense_73_loss: 0.0017 - loss: 0.0018 - val_dense_72_accuracy: 0.9583 - val_dense_72_loss: 0.1292 - val_dense_73_accuracy: 0.9954 - val_dense_73_loss: 0.0046 - val_loss: 0.1386 - learning_rate: 1.0000e-04\nEpoch 129/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 400ms/step - dense_72_accuracy: 0.9998 - dense_72_loss: 5.5563e-04 - dense_73_accuracy: 0.9980 - dense_73_loss: 0.0026 - loss: 0.0031\nEpoch 129: val_loss did not improve from 0.11139\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 408ms/step - dense_72_accuracy: 0.9998 - dense_72_loss: 5.5704e-04 - dense_73_accuracy: 0.9980 - dense_73_loss: 0.0026 - loss: 0.0031 - val_dense_72_accuracy: 0.9583 - val_dense_72_loss: 0.1202 - val_dense_73_accuracy: 0.9985 - val_dense_73_loss: 0.0023 - val_loss: 0.1261 - learning_rate: 1.0000e-04\nEpoch 130/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 396ms/step - dense_72_accuracy: 0.9997 - dense_72_loss: 4.0745e-04 - dense_73_accuracy: 0.9982 - dense_73_loss: 0.0025 - loss: 0.0029\nEpoch 130: val_loss did not improve from 0.11139\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 403ms/step - dense_72_accuracy: 0.9997 - dense_72_loss: 4.0754e-04 - dense_73_accuracy: 0.9982 - dense_73_loss: 0.0025 - loss: 0.0029 - val_dense_72_accuracy: 0.9583 - val_dense_72_loss: 0.1253 - val_dense_73_accuracy: 0.9985 - val_dense_73_loss: 0.0021 - val_loss: 0.1320 - learning_rate: 1.0000e-04\nEpoch 131/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 389ms/step - dense_72_accuracy: 0.9995 - dense_72_loss: 5.9829e-04 - dense_73_accuracy: 0.9985 - dense_73_loss: 0.0025 - loss: 0.0031\nEpoch 131: val_loss did not improve from 0.11139\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 396ms/step - dense_72_accuracy: 0.9995 - dense_72_loss: 5.9853e-04 - dense_73_accuracy: 0.9985 - dense_73_loss: 0.0025 - loss: 0.0031 - val_dense_72_accuracy: 0.9660 - val_dense_72_loss: 0.1063 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0023 - val_loss: 0.1125 - learning_rate: 1.0000e-04\nEpoch 132/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 401ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 1.6655e-04 - dense_73_accuracy: 0.9985 - dense_73_loss: 0.0036 - loss: 0.0038\nEpoch 132: val_loss improved from 0.11139 to 0.11136, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 424ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 1.6700e-04 - dense_73_accuracy: 0.9985 - dense_73_loss: 0.0036 - loss: 0.0038 - val_dense_72_accuracy: 0.9599 - val_dense_72_loss: 0.1037 - val_dense_73_accuracy: 0.9985 - val_dense_73_loss: 0.0038 - val_loss: 0.1114 - learning_rate: 1.0000e-04\nEpoch 133/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 388ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 2.1786e-04 - dense_73_accuracy: 0.9982 - dense_73_loss: 0.0020 - loss: 0.0022\nEpoch 133: val_loss improved from 0.11136 to 0.10910, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 412ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 2.1790e-04 - dense_73_accuracy: 0.9982 - dense_73_loss: 0.0020 - loss: 0.0022 - val_dense_72_accuracy: 0.9660 - val_dense_72_loss: 0.1012 - val_dense_73_accuracy: 0.9969 - val_dense_73_loss: 0.0042 - val_loss: 0.1091 - learning_rate: 1.0000e-04\nEpoch 134/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 390ms/step - dense_72_accuracy: 0.9999 - dense_72_loss: 4.7212e-04 - dense_73_accuracy: 0.9988 - dense_73_loss: 0.0024 - loss: 0.0029\nEpoch 134: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 398ms/step - dense_72_accuracy: 0.9999 - dense_72_loss: 4.7218e-04 - dense_73_accuracy: 0.9988 - dense_73_loss: 0.0024 - loss: 0.0029 - val_dense_72_accuracy: 0.9645 - val_dense_72_loss: 0.1059 - val_dense_73_accuracy: 0.9954 - val_dense_73_loss: 0.0043 - val_loss: 0.1142 - learning_rate: 1.0000e-04\nEpoch 135/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 392ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 1.4186e-04 - dense_73_accuracy: 0.9981 - dense_73_loss: 0.0031 - loss: 0.0032\nEpoch 135: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 400ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 1.4172e-04 - dense_73_accuracy: 0.9981 - dense_73_loss: 0.0031 - loss: 0.0032 - val_dense_72_accuracy: 0.9630 - val_dense_72_loss: 0.1120 - val_dense_73_accuracy: 0.9938 - val_dense_73_loss: 0.0045 - val_loss: 0.1208 - learning_rate: 1.0000e-04\nEpoch 136/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 389ms/step - dense_72_accuracy: 0.9997 - dense_72_loss: 4.9753e-04 - dense_73_accuracy: 0.9992 - dense_73_loss: 0.0017 - loss: 0.0022\nEpoch 136: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 396ms/step - dense_72_accuracy: 0.9997 - dense_72_loss: 4.9774e-04 - dense_73_accuracy: 0.9992 - dense_73_loss: 0.0017 - loss: 0.0022 - val_dense_72_accuracy: 0.9614 - val_dense_72_loss: 0.1116 - val_dense_73_accuracy: 0.9985 - val_dense_73_loss: 0.0026 - val_loss: 0.1179 - learning_rate: 1.0000e-04\nEpoch 137/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 398ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 1.6547e-04 - dense_73_accuracy: 0.9986 - dense_73_loss: 0.0022 - loss: 0.0024\nEpoch 137: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 406ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 1.6640e-04 - dense_73_accuracy: 0.9986 - dense_73_loss: 0.0023 - loss: 0.0024 - val_dense_72_accuracy: 0.9568 - val_dense_72_loss: 0.1180 - val_dense_73_accuracy: 0.9985 - val_dense_73_loss: 0.0028 - val_loss: 0.1251 - learning_rate: 1.0000e-04\nEpoch 138/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 396ms/step - dense_72_accuracy: 0.9995 - dense_72_loss: 0.0011 - dense_73_accuracy: 0.9988 - dense_73_loss: 0.0027 - loss: 0.0038\nEpoch 138: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 404ms/step - dense_72_accuracy: 0.9995 - dense_72_loss: 0.0011 - dense_73_accuracy: 0.9988 - dense_73_loss: 0.0027 - loss: 0.0038 - val_dense_72_accuracy: 0.9522 - val_dense_72_loss: 0.1263 - val_dense_73_accuracy: 0.9985 - val_dense_73_loss: 0.0027 - val_loss: 0.1336 - learning_rate: 1.0000e-04\nEpoch 139/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 395ms/step - dense_72_accuracy: 0.9998 - dense_72_loss: 4.3045e-04 - dense_73_accuracy: 0.9978 - dense_73_loss: 0.0022 - loss: 0.0026\nEpoch 139: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 403ms/step - dense_72_accuracy: 0.9998 - dense_72_loss: 4.3018e-04 - dense_73_accuracy: 0.9978 - dense_73_loss: 0.0022 - loss: 0.0026 - val_dense_72_accuracy: 0.9552 - val_dense_72_loss: 0.1283 - val_dense_73_accuracy: 0.9985 - val_dense_73_loss: 0.0024 - val_loss: 0.1352 - learning_rate: 1.0000e-04\nEpoch 140/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 394ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 1.3459e-04 - dense_73_accuracy: 0.9984 - dense_73_loss: 0.0022 - loss: 0.0024\nEpoch 140: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 402ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 1.3487e-04 - dense_73_accuracy: 0.9984 - dense_73_loss: 0.0022 - loss: 0.0024 - val_dense_72_accuracy: 0.9537 - val_dense_72_loss: 0.1207 - val_dense_73_accuracy: 0.9985 - val_dense_73_loss: 0.0026 - val_loss: 0.1269 - learning_rate: 1.0000e-04\nEpoch 141/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 390ms/step - dense_72_accuracy: 0.9998 - dense_72_loss: 1.7472e-04 - dense_73_accuracy: 0.9992 - dense_73_loss: 0.0014 - loss: 0.0016\nEpoch 141: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 398ms/step - dense_72_accuracy: 0.9998 - dense_72_loss: 1.7485e-04 - dense_73_accuracy: 0.9992 - dense_73_loss: 0.0014 - loss: 0.0016 - val_dense_72_accuracy: 0.9552 - val_dense_72_loss: 0.1324 - val_dense_73_accuracy: 0.9969 - val_dense_73_loss: 0.0049 - val_loss: 0.1423 - learning_rate: 1.0000e-04\nEpoch 142/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 398ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 1.7356e-04 - dense_73_accuracy: 0.9996 - dense_73_loss: 0.0011 - loss: 0.0013\nEpoch 142: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 406ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 1.7347e-04 - dense_73_accuracy: 0.9996 - dense_73_loss: 0.0011 - loss: 0.0013 - val_dense_72_accuracy: 0.9614 - val_dense_72_loss: 0.1149 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0032 - val_loss: 0.1222 - learning_rate: 1.0000e-04\nEpoch 143/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 398ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 1.6779e-04 - dense_73_accuracy: 0.9989 - dense_73_loss: 0.0014 - loss: 0.0015\nEpoch 143: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 406ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 1.6793e-04 - dense_73_accuracy: 0.9989 - dense_73_loss: 0.0014 - loss: 0.0015 - val_dense_72_accuracy: 0.9630 - val_dense_72_loss: 0.1220 - val_dense_73_accuracy: 0.9985 - val_dense_73_loss: 0.0036 - val_loss: 0.1299 - learning_rate: 1.0000e-04\nEpoch 144/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 390ms/step - dense_72_accuracy: 0.9999 - dense_72_loss: 1.6330e-04 - dense_73_accuracy: 0.9992 - dense_73_loss: 0.0013 - loss: 0.0015\nEpoch 144: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 397ms/step - dense_72_accuracy: 0.9999 - dense_72_loss: 1.6324e-04 - dense_73_accuracy: 0.9992 - dense_73_loss: 0.0013 - loss: 0.0015 - val_dense_72_accuracy: 0.9630 - val_dense_72_loss: 0.1148 - val_dense_73_accuracy: 0.9969 - val_dense_73_loss: 0.0035 - val_loss: 0.1224 - learning_rate: 1.0000e-04\nEpoch 145/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 393ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 1.6980e-04 - dense_73_accuracy: 0.9984 - dense_73_loss: 0.0030 - loss: 0.0031\nEpoch 145: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 401ms/step - dense_72_accuracy: 0.9999 - dense_72_loss: 1.7037e-04 - dense_73_accuracy: 0.9984 - dense_73_loss: 0.0030 - loss: 0.0031 - val_dense_72_accuracy: 0.9614 - val_dense_72_loss: 0.1184 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0023 - val_loss: 0.1237 - learning_rate: 1.0000e-04\nEpoch 146/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 398ms/step - dense_72_accuracy: 0.9996 - dense_72_loss: 2.4247e-04 - dense_73_accuracy: 0.9988 - dense_73_loss: 0.0014 - loss: 0.0017\nEpoch 146: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 406ms/step - dense_72_accuracy: 0.9996 - dense_72_loss: 2.4306e-04 - dense_73_accuracy: 0.9988 - dense_73_loss: 0.0014 - loss: 0.0017 - val_dense_72_accuracy: 0.9599 - val_dense_72_loss: 0.1152 - val_dense_73_accuracy: 0.9985 - val_dense_73_loss: 0.0023 - val_loss: 0.1214 - learning_rate: 1.0000e-04\nEpoch 147/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 390ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 1.0121e-04 - dense_73_accuracy: 0.9981 - dense_73_loss: 0.0022 - loss: 0.0023\nEpoch 147: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 397ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 1.0128e-04 - dense_73_accuracy: 0.9981 - dense_73_loss: 0.0022 - loss: 0.0023 - val_dense_72_accuracy: 0.9614 - val_dense_72_loss: 0.1140 - val_dense_73_accuracy: 0.9938 - val_dense_73_loss: 0.0074 - val_loss: 0.1252 - learning_rate: 1.0000e-04\nEpoch 148/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 397ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 1.3315e-04 - dense_73_accuracy: 0.9994 - dense_73_loss: 9.6095e-04 - loss: 0.0011\nEpoch 148: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 405ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 1.3362e-04 - dense_73_accuracy: 0.9994 - dense_73_loss: 9.6099e-04 - loss: 0.0011 - val_dense_72_accuracy: 0.9583 - val_dense_72_loss: 0.1053 - val_dense_73_accuracy: 0.9969 - val_dense_73_loss: 0.0030 - val_loss: 0.1110 - learning_rate: 1.0000e-04\nEpoch 149/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 389ms/step - dense_72_accuracy: 0.9991 - dense_72_loss: 0.0015 - dense_73_accuracy: 0.9998 - dense_73_loss: 8.4825e-04 - loss: 0.0023\nEpoch 149: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 397ms/step - dense_72_accuracy: 0.9991 - dense_72_loss: 0.0015 - dense_73_accuracy: 0.9998 - dense_73_loss: 8.4817e-04 - loss: 0.0023 - val_dense_72_accuracy: 0.9645 - val_dense_72_loss: 0.1177 - val_dense_73_accuracy: 0.9985 - val_dense_73_loss: 0.0024 - val_loss: 0.1245 - learning_rate: 1.0000e-04\nEpoch 150/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 398ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 5.8467e-05 - dense_73_accuracy: 0.9975 - dense_73_loss: 0.0035 - loss: 0.0036\nEpoch 150: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 406ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 5.8579e-05 - dense_73_accuracy: 0.9975 - dense_73_loss: 0.0035 - loss: 0.0036 - val_dense_72_accuracy: 0.9583 - val_dense_72_loss: 0.1192 - val_dense_73_accuracy: 0.9892 - val_dense_73_loss: 0.0123 - val_loss: 0.1363 - learning_rate: 1.0000e-04\nEpoch 151/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 391ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 1.6263e-04 - dense_73_accuracy: 0.9993 - dense_73_loss: 0.0012 - loss: 0.0013\nEpoch 151: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 399ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 1.6264e-04 - dense_73_accuracy: 0.9993 - dense_73_loss: 0.0012 - loss: 0.0013 - val_dense_72_accuracy: 0.9599 - val_dense_72_loss: 0.1181 - val_dense_73_accuracy: 0.9985 - val_dense_73_loss: 0.0019 - val_loss: 0.1243 - learning_rate: 1.0000e-04\nEpoch 152/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 399ms/step - dense_72_accuracy: 0.9998 - dense_72_loss: 3.7786e-04 - dense_73_accuracy: 0.9986 - dense_73_loss: 0.0019 - loss: 0.0023\nEpoch 152: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 406ms/step - dense_72_accuracy: 0.9998 - dense_72_loss: 3.7854e-04 - dense_73_accuracy: 0.9986 - dense_73_loss: 0.0019 - loss: 0.0023 - val_dense_72_accuracy: 0.9599 - val_dense_72_loss: 0.1243 - val_dense_73_accuracy: 0.9954 - val_dense_73_loss: 0.0047 - val_loss: 0.1337 - learning_rate: 1.0000e-04\nEpoch 153/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 392ms/step - dense_72_accuracy: 0.9999 - dense_72_loss: 1.2860e-04 - dense_73_accuracy: 0.9995 - dense_73_loss: 0.0011 - loss: 0.0013\nEpoch 153: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 400ms/step - dense_72_accuracy: 0.9999 - dense_72_loss: 1.2910e-04 - dense_73_accuracy: 0.9995 - dense_73_loss: 0.0011 - loss: 0.0013 - val_dense_72_accuracy: 0.9599 - val_dense_72_loss: 0.1260 - val_dense_73_accuracy: 0.9969 - val_dense_73_loss: 0.0048 - val_loss: 0.1356 - learning_rate: 1.0000e-04\nEpoch 154/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 392ms/step - dense_72_accuracy: 0.9998 - dense_72_loss: 2.5990e-04 - dense_73_accuracy: 0.9997 - dense_73_loss: 0.0011 - loss: 0.0014\nEpoch 154: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 399ms/step - dense_72_accuracy: 0.9998 - dense_72_loss: 2.6015e-04 - dense_73_accuracy: 0.9997 - dense_73_loss: 0.0011 - loss: 0.0014 - val_dense_72_accuracy: 0.9583 - val_dense_72_loss: 0.1135 - val_dense_73_accuracy: 0.9969 - val_dense_73_loss: 0.0036 - val_loss: 0.1214 - learning_rate: 1.0000e-04\nEpoch 155/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 403ms/step - dense_72_accuracy: 0.9997 - dense_72_loss: 2.4641e-04 - dense_73_accuracy: 0.9985 - dense_73_loss: 0.0024 - loss: 0.0027\nEpoch 155: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 410ms/step - dense_72_accuracy: 0.9997 - dense_72_loss: 2.4678e-04 - dense_73_accuracy: 0.9985 - dense_73_loss: 0.0024 - loss: 0.0027 - val_dense_72_accuracy: 0.9645 - val_dense_72_loss: 0.1078 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0018 - val_loss: 0.1136 - learning_rate: 1.0000e-04\nEpoch 156/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 390ms/step - dense_72_accuracy: 0.9999 - dense_72_loss: 3.9227e-04 - dense_73_accuracy: 0.9988 - dense_73_loss: 0.0016 - loss: 0.0020\nEpoch 156: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 398ms/step - dense_72_accuracy: 0.9999 - dense_72_loss: 3.9264e-04 - dense_73_accuracy: 0.9988 - dense_73_loss: 0.0016 - loss: 0.0020 - val_dense_72_accuracy: 0.9645 - val_dense_72_loss: 0.1139 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0012 - val_loss: 0.1193 - learning_rate: 1.0000e-04\nEpoch 157/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 392ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 6.8992e-05 - dense_73_accuracy: 0.9993 - dense_73_loss: 0.0012 - loss: 0.0013\nEpoch 157: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 400ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 7.3389e-05 - dense_73_accuracy: 0.9993 - dense_73_loss: 0.0012 - loss: 0.0013 - val_dense_72_accuracy: 0.9645 - val_dense_72_loss: 0.1127 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0019 - val_loss: 0.1189 - learning_rate: 1.0000e-04\nEpoch 158/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 390ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 4.8671e-05 - dense_73_accuracy: 0.9989 - dense_73_loss: 0.0018 - loss: 0.0018\nEpoch 159: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 398ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 4.8712e-05 - dense_73_accuracy: 0.9989 - dense_73_loss: 0.0018 - loss: 0.0018 - val_dense_72_accuracy: 0.9599 - val_dense_72_loss: 0.1136 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0013 - val_loss: 0.1192 - learning_rate: 1.0000e-04\nEpoch 160/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 389ms/step - dense_72_accuracy: 0.9999 - dense_72_loss: 1.1603e-04 - dense_73_accuracy: 0.9999 - dense_73_loss: 7.4228e-04 - loss: 8.5831e-04\nEpoch 160: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 397ms/step - dense_72_accuracy: 0.9999 - dense_72_loss: 1.1616e-04 - dense_73_accuracy: 0.9999 - dense_73_loss: 7.4238e-04 - loss: 8.5855e-04 - val_dense_72_accuracy: 0.9583 - val_dense_72_loss: 0.1150 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 9.3792e-04 - val_loss: 0.1202 - learning_rate: 1.0000e-04\nEpoch 161/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 398ms/step - dense_72_accuracy: 0.9997 - dense_72_loss: 7.1130e-04 - dense_73_accuracy: 0.9992 - dense_73_loss: 0.0013 - loss: 0.0021\nEpoch 161: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 405ms/step - dense_72_accuracy: 0.9997 - dense_72_loss: 7.1069e-04 - dense_73_accuracy: 0.9992 - dense_73_loss: 0.0013 - loss: 0.0021 - val_dense_72_accuracy: 0.9630 - val_dense_72_loss: 0.1119 - val_dense_73_accuracy: 0.9985 - val_dense_73_loss: 0.0019 - val_loss: 0.1177 - learning_rate: 1.0000e-04\nEpoch 162/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 390ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 1.9739e-04 - dense_73_accuracy: 0.9989 - dense_73_loss: 0.0014 - loss: 0.0016\nEpoch 162: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 398ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 1.9714e-04 - dense_73_accuracy: 0.9989 - dense_73_loss: 0.0014 - loss: 0.0016 - val_dense_72_accuracy: 0.9599 - val_dense_72_loss: 0.1201 - val_dense_73_accuracy: 0.9954 - val_dense_73_loss: 0.0059 - val_loss: 0.1306 - learning_rate: 1.0000e-04\nEpoch 163/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 393ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 8.3725e-05 - dense_73_accuracy: 0.9988 - dense_73_loss: 0.0020 - loss: 0.0021\nEpoch 163: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 401ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 8.3793e-05 - dense_73_accuracy: 0.9988 - dense_73_loss: 0.0020 - loss: 0.0021 - val_dense_72_accuracy: 0.9552 - val_dense_72_loss: 0.1292 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0015 - val_loss: 0.1354 - learning_rate: 1.0000e-04\nEpoch 164/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 390ms/step - dense_72_accuracy: 0.9999 - dense_72_loss: 1.3901e-04 - dense_73_accuracy: 0.9996 - dense_73_loss: 0.0011 - loss: 0.0012\nEpoch 164: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 397ms/step - dense_72_accuracy: 0.9999 - dense_72_loss: 1.3966e-04 - dense_73_accuracy: 0.9996 - dense_73_loss: 0.0011 - loss: 0.0012 - val_dense_72_accuracy: 0.9583 - val_dense_72_loss: 0.1219 - val_dense_73_accuracy: 0.9985 - val_dense_73_loss: 0.0038 - val_loss: 0.1303 - learning_rate: 1.0000e-04\nEpoch 165/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 391ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 4.6658e-05 - dense_73_accuracy: 0.9990 - dense_73_loss: 0.0012 - loss: 0.0013\nEpoch 165: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 399ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 4.6767e-05 - dense_73_accuracy: 0.9990 - dense_73_loss: 0.0012 - loss: 0.0013 - val_dense_72_accuracy: 0.9568 - val_dense_72_loss: 0.1277 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0011 - val_loss: 0.1335 - learning_rate: 1.0000e-04\nEpoch 166/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 398ms/step - dense_72_accuracy: 0.9999 - dense_72_loss: 2.3654e-04 - dense_73_accuracy: 0.9991 - dense_73_loss: 0.0015 - loss: 0.0018\nEpoch 166: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 405ms/step - dense_72_accuracy: 0.9999 - dense_72_loss: 2.3810e-04 - dense_73_accuracy: 0.9991 - dense_73_loss: 0.0015 - loss: 0.0018 - val_dense_72_accuracy: 0.9630 - val_dense_72_loss: 0.1168 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 7.4843e-04 - val_loss: 0.1217 - learning_rate: 1.0000e-04\nEpoch 167/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 388ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 6.8175e-05 - dense_73_accuracy: 0.9992 - dense_73_loss: 0.0013 - loss: 0.0013\nEpoch 167: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 395ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 6.8249e-05 - dense_73_accuracy: 0.9992 - dense_73_loss: 0.0013 - loss: 0.0013 - val_dense_72_accuracy: 0.9583 - val_dense_72_loss: 0.1261 - val_dense_73_accuracy: 0.9985 - val_dense_73_loss: 0.0030 - val_loss: 0.1338 - learning_rate: 1.0000e-04\nEpoch 168/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 389ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 6.3294e-05 - dense_73_accuracy: 0.9990 - dense_73_loss: 0.0014 - loss: 0.0015\nEpoch 168: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 396ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 6.3338e-05 - dense_73_accuracy: 0.9990 - dense_73_loss: 0.0014 - loss: 0.0015 - val_dense_72_accuracy: 0.9568 - val_dense_72_loss: 0.1275 - val_dense_73_accuracy: 0.9923 - val_dense_73_loss: 0.0090 - val_loss: 0.1414 - learning_rate: 1.0000e-04\nEpoch 169/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 391ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 6.6290e-05 - dense_73_accuracy: 0.9992 - dense_73_loss: 0.0010 - loss: 0.0011\nEpoch 169: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 399ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 6.6153e-05 - dense_73_accuracy: 0.9992 - dense_73_loss: 0.0010 - loss: 0.0011 - val_dense_72_accuracy: 0.9583 - val_dense_72_loss: 0.1293 - val_dense_73_accuracy: 0.9985 - val_dense_73_loss: 0.0017 - val_loss: 0.1358 - learning_rate: 1.0000e-04\nEpoch 170/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 398ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 1.4508e-04 - dense_73_accuracy: 0.9994 - dense_73_loss: 0.0012 - loss: 0.0013\nEpoch 170: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 405ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 1.4523e-04 - dense_73_accuracy: 0.9994 - dense_73_loss: 0.0012 - loss: 0.0013 - val_dense_72_accuracy: 0.9552 - val_dense_72_loss: 0.1285 - val_dense_73_accuracy: 0.9969 - val_dense_73_loss: 0.0027 - val_loss: 0.1359 - learning_rate: 1.0000e-04\nEpoch 171/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 389ms/step - dense_72_accuracy: 0.9999 - dense_72_loss: 2.5434e-04 - dense_73_accuracy: 0.9994 - dense_73_loss: 6.6963e-04 - loss: 9.2281e-04\nEpoch 171: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 396ms/step - dense_72_accuracy: 0.9999 - dense_72_loss: 2.5432e-04 - dense_73_accuracy: 0.9994 - dense_73_loss: 6.7227e-04 - loss: 9.2428e-04 - val_dense_72_accuracy: 0.9614 - val_dense_72_loss: 0.1183 - val_dense_73_accuracy: 0.9954 - val_dense_73_loss: 0.0034 - val_loss: 0.1261 - learning_rate: 1.0000e-04\nEpoch 172/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 397ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 5.4461e-05 - dense_73_accuracy: 0.9992 - dense_73_loss: 0.0016 - loss: 0.0016\nEpoch 172: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 405ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 5.4554e-05 - dense_73_accuracy: 0.9992 - dense_73_loss: 0.0016 - loss: 0.0016 - val_dense_72_accuracy: 0.9645 - val_dense_72_loss: 0.1162 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0014 - val_loss: 0.1220 - learning_rate: 1.0000e-04\nEpoch 173/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 398ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 4.6583e-05 - dense_73_accuracy: 0.9983 - dense_73_loss: 0.0022 - loss: 0.0023\nEpoch 173: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 405ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 4.6533e-05 - dense_73_accuracy: 0.9983 - dense_73_loss: 0.0022 - loss: 0.0023 - val_dense_72_accuracy: 0.9599 - val_dense_72_loss: 0.1238 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0014 - val_loss: 0.1299 - learning_rate: 1.0000e-04\nEpoch 174/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 390ms/step - dense_72_accuracy: 0.9994 - dense_72_loss: 3.8627e-04 - dense_73_accuracy: 0.9991 - dense_73_loss: 0.0029 - loss: 0.0033\nEpoch 174: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 397ms/step - dense_72_accuracy: 0.9994 - dense_72_loss: 3.8541e-04 - dense_73_accuracy: 0.9991 - dense_73_loss: 0.0029 - loss: 0.0033 - val_dense_72_accuracy: 0.9630 - val_dense_72_loss: 0.1199 - val_dense_73_accuracy: 0.9985 - val_dense_73_loss: 0.0018 - val_loss: 0.1262 - learning_rate: 1.0000e-04\nEpoch 175/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 396ms/step - dense_72_accuracy: 0.9999 - dense_72_loss: 3.0952e-04 - dense_73_accuracy: 0.9984 - dense_73_loss: 0.0016 - loss: 0.0019\nEpoch 175: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 403ms/step - dense_72_accuracy: 0.9999 - dense_72_loss: 3.0995e-04 - dense_73_accuracy: 0.9984 - dense_73_loss: 0.0016 - loss: 0.0019 - val_dense_72_accuracy: 0.9583 - val_dense_72_loss: 0.1199 - val_dense_73_accuracy: 0.9969 - val_dense_73_loss: 0.0032 - val_loss: 0.1277 - learning_rate: 1.0000e-04\nEpoch 176/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 397ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 8.4640e-05 - dense_73_accuracy: 0.9994 - dense_73_loss: 9.5804e-04 - loss: 0.0010\nEpoch 176: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 405ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 8.4966e-05 - dense_73_accuracy: 0.9994 - dense_73_loss: 9.5767e-04 - loss: 0.0010 - val_dense_72_accuracy: 0.9583 - val_dense_72_loss: 0.1247 - val_dense_73_accuracy: 0.9985 - val_dense_73_loss: 0.0021 - val_loss: 0.1315 - learning_rate: 1.0000e-04\nEpoch 177/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 390ms/step - dense_72_accuracy: 0.9996 - dense_72_loss: 4.6273e-04 - dense_73_accuracy: 0.9996 - dense_73_loss: 8.8904e-04 - loss: 0.0014\nEpoch 177: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 398ms/step - dense_72_accuracy: 0.9996 - dense_72_loss: 4.6239e-04 - dense_73_accuracy: 0.9996 - dense_73_loss: 8.9183e-04 - loss: 0.0014 - val_dense_72_accuracy: 0.9630 - val_dense_72_loss: 0.1102 - val_dense_73_accuracy: 0.9969 - val_dense_73_loss: 0.0024 - val_loss: 0.1168 - learning_rate: 1.0000e-04\nEpoch 178/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 397ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 1.4291e-04 - dense_73_accuracy: 0.9993 - dense_73_loss: 0.0012 - loss: 0.0013\nEpoch 178: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 405ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 1.4305e-04 - dense_73_accuracy: 0.9993 - dense_73_loss: 0.0012 - loss: 0.0013 - val_dense_72_accuracy: 0.9645 - val_dense_72_loss: 0.1131 - val_dense_73_accuracy: 0.9985 - val_dense_73_loss: 0.0028 - val_loss: 0.1201 - learning_rate: 1.0000e-04\nEpoch 179/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 389ms/step - dense_72_accuracy: 0.9998 - dense_72_loss: 2.9829e-04 - dense_73_accuracy: 0.9999 - dense_73_loss: 5.6390e-04 - loss: 8.6220e-04\nEpoch 179: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 397ms/step - dense_72_accuracy: 0.9998 - dense_72_loss: 2.9889e-04 - dense_73_accuracy: 0.9999 - dense_73_loss: 5.6427e-04 - loss: 8.6317e-04 - val_dense_72_accuracy: 0.9645 - val_dense_72_loss: 0.1216 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0011 - val_loss: 0.1272 - learning_rate: 1.0000e-04\nEpoch 180/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 387ms/step - dense_72_accuracy: 0.9999 - dense_72_loss: 2.6731e-04 - dense_73_accuracy: 0.9994 - dense_73_loss: 6.8933e-04 - loss: 9.5665e-04\nEpoch 180: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 395ms/step - dense_72_accuracy: 0.9999 - dense_72_loss: 2.6753e-04 - dense_73_accuracy: 0.9994 - dense_73_loss: 6.9035e-04 - loss: 9.5790e-04 - val_dense_72_accuracy: 0.9614 - val_dense_72_loss: 0.1222 - val_dense_73_accuracy: 0.9985 - val_dense_73_loss: 0.0022 - val_loss: 0.1286 - learning_rate: 1.0000e-04\nEpoch 181/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 388ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 1.1828e-04 - dense_73_accuracy: 0.9989 - dense_73_loss: 0.0017 - loss: 0.0018\nEpoch 181: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 395ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 1.1820e-04 - dense_73_accuracy: 0.9989 - dense_73_loss: 0.0017 - loss: 0.0018 - val_dense_72_accuracy: 0.9645 - val_dense_72_loss: 0.1200 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0017 - val_loss: 0.1261 - learning_rate: 1.0000e-04\nEpoch 182/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 389ms/step - dense_72_accuracy: 0.9999 - dense_72_loss: 1.5694e-04 - dense_73_accuracy: 0.9997 - dense_73_loss: 0.0011 - loss: 0.0013\nEpoch 182: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 397ms/step - dense_72_accuracy: 0.9999 - dense_72_loss: 1.5752e-04 - dense_73_accuracy: 0.9997 - dense_73_loss: 0.0011 - loss: 0.0013 - val_dense_72_accuracy: 0.9630 - val_dense_72_loss: 0.1163 - val_dense_73_accuracy: 0.9954 - val_dense_73_loss: 0.0095 - val_loss: 0.1304 - learning_rate: 1.0000e-04\nEpoch 183/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 389ms/step - dense_72_accuracy: 0.9994 - dense_72_loss: 9.3214e-04 - dense_73_accuracy: 0.9984 - dense_73_loss: 0.0030 - loss: 0.0039\nEpoch 183: val_loss did not improve from 0.10910\n\nEpoch 183: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 397ms/step - dense_72_accuracy: 0.9994 - dense_72_loss: 9.3020e-04 - dense_73_accuracy: 0.9984 - dense_73_loss: 0.0029 - loss: 0.0039 - val_dense_72_accuracy: 0.9630 - val_dense_72_loss: 0.1164 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0011 - val_loss: 0.1217 - learning_rate: 1.0000e-04\nEpoch 184/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 398ms/step - dense_72_accuracy: 0.9999 - dense_72_loss: 1.2092e-04 - dense_73_accuracy: 0.9985 - dense_73_loss: 0.0021 - loss: 0.0023\nEpoch 184: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 405ms/step - dense_72_accuracy: 0.9999 - dense_72_loss: 1.2102e-04 - dense_73_accuracy: 0.9985 - dense_73_loss: 0.0021 - loss: 0.0023 - val_dense_72_accuracy: 0.9660 - val_dense_72_loss: 0.1140 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 9.6080e-04 - val_loss: 0.1191 - learning_rate: 1.0000e-05\nEpoch 185/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 389ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 9.3666e-06 - dense_73_accuracy: 0.9988 - dense_73_loss: 0.0013 - loss: 0.0013\nEpoch 185: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 397ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 9.3824e-06 - dense_73_accuracy: 0.9988 - dense_73_loss: 0.0013 - loss: 0.0013 - val_dense_72_accuracy: 0.9645 - val_dense_72_loss: 0.1139 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0010 - val_loss: 0.1191 - learning_rate: 1.0000e-05\nEpoch 186/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 392ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 2.2693e-05 - dense_73_accuracy: 0.9995 - dense_73_loss: 4.9738e-04 - loss: 5.2007e-04\nEpoch 186: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 399ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 2.2692e-05 - dense_73_accuracy: 0.9995 - dense_73_loss: 4.9716e-04 - loss: 5.1983e-04 - val_dense_72_accuracy: 0.9645 - val_dense_72_loss: 0.1136 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 9.3687e-04 - val_loss: 0.1187 - learning_rate: 1.0000e-05\nEpoch 187/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 397ms/step - dense_72_accuracy: 0.9999 - dense_72_loss: 1.3128e-04 - dense_73_accuracy: 0.9996 - dense_73_loss: 0.0013 - loss: 0.0015\nEpoch 187: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 405ms/step - dense_72_accuracy: 0.9999 - dense_72_loss: 1.3120e-04 - dense_73_accuracy: 0.9996 - dense_73_loss: 0.0013 - loss: 0.0015 - val_dense_72_accuracy: 0.9645 - val_dense_72_loss: 0.1130 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 9.1414e-04 - val_loss: 0.1181 - learning_rate: 1.0000e-05\nEpoch 188/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 398ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 3.6492e-05 - dense_73_accuracy: 0.9986 - dense_73_loss: 0.0020 - loss: 0.0020\nEpoch 188: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 406ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 3.6577e-05 - dense_73_accuracy: 0.9986 - dense_73_loss: 0.0020 - loss: 0.0020 - val_dense_72_accuracy: 0.9660 - val_dense_72_loss: 0.1128 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0010 - val_loss: 0.1179 - learning_rate: 1.0000e-05\nEpoch 189/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 389ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 3.3632e-05 - dense_73_accuracy: 0.9998 - dense_73_loss: 8.7105e-04 - loss: 9.0469e-04\nEpoch 189: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 396ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 3.3982e-05 - dense_73_accuracy: 0.9998 - dense_73_loss: 8.7190e-04 - loss: 9.0589e-04 - val_dense_72_accuracy: 0.9645 - val_dense_72_loss: 0.1117 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 9.9563e-04 - val_loss: 0.1167 - learning_rate: 1.0000e-05\nEpoch 190/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 406ms/step - dense_72_accuracy: 0.9998 - dense_72_loss: 2.3240e-04 - dense_73_accuracy: 0.9996 - dense_73_loss: 0.0010 - loss: 0.0012    \nEpoch 190: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 414ms/step - dense_72_accuracy: 0.9998 - dense_72_loss: 2.3236e-04 - dense_73_accuracy: 0.9996 - dense_73_loss: 0.0010 - loss: 0.0012 - val_dense_72_accuracy: 0.9614 - val_dense_72_loss: 0.1100 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 9.9985e-04 - val_loss: 0.1150 - learning_rate: 1.0000e-05\nEpoch 191/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 389ms/step - dense_72_accuracy: 0.9999 - dense_72_loss: 1.0033e-04 - dense_73_accuracy: 0.9991 - dense_73_loss: 9.5981e-04 - loss: 0.0011\nEpoch 191: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 397ms/step - dense_72_accuracy: 0.9999 - dense_72_loss: 1.0043e-04 - dense_73_accuracy: 0.9991 - dense_73_loss: 9.5863e-04 - loss: 0.0011 - val_dense_72_accuracy: 0.9630 - val_dense_72_loss: 0.1091 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 9.7593e-04 - val_loss: 0.1140 - learning_rate: 1.0000e-05\nEpoch 192/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 397ms/step - dense_72_accuracy: 0.9997 - dense_72_loss: 3.7208e-04 - dense_73_accuracy: 0.9996 - dense_73_loss: 5.9026e-04 - loss: 9.6235e-04\nEpoch 192: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 404ms/step - dense_72_accuracy: 0.9997 - dense_72_loss: 3.7142e-04 - dense_73_accuracy: 0.9996 - dense_73_loss: 5.9179e-04 - loss: 9.6322e-04 - val_dense_72_accuracy: 0.9645 - val_dense_72_loss: 0.1107 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0011 - val_loss: 0.1158 - learning_rate: 1.0000e-05\nEpoch 193/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 390ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 3.7300e-05 - dense_73_accuracy: 0.9998 - dense_73_loss: 7.2526e-04 - loss: 7.6257e-04\nEpoch 193: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 397ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 3.7462e-05 - dense_73_accuracy: 0.9998 - dense_73_loss: 7.2590e-04 - loss: 7.6338e-04 - val_dense_72_accuracy: 0.9645 - val_dense_72_loss: 0.1118 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 9.5134e-04 - val_loss: 0.1168 - learning_rate: 1.0000e-05\nEpoch 193: early stopping\nRestoring model weights from the end of the best epoch: 133.\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"model.evaluate([X_test_s, X_test_h1], [y_test_s, y_test_h1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T11:41:23.671998Z","iopub.execute_input":"2025-03-09T11:41:23.672285Z","iopub.status.idle":"2025-03-09T11:41:48.123930Z","shell.execute_reply.started":"2025-03-09T11:41:23.672260Z","shell.execute_reply":"2025-03-09T11:41:48.122993Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 951ms/step - dense_72_accuracy: 0.9628 - dense_72_loss: 0.1242 - dense_73_accuracy: 0.9931 - dense_73_loss: 0.0079 - loss: 0.1323\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"[0.13188616931438446,\n 0.11716188490390778,\n 0.011239324696362019,\n 0.9629629850387573,\n 0.9913580417633057]"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"history = model.fit([X_train_s, X_train_h], [y_train_s, y_train_h],\n                    epochs=200,\n                    validation_data=([X_val_s, X_val_h1], [y_val_s, y_val_h1]), verbose=1,\n                    shuffle=True,\n                    callbacks=callbacks) # UpdateGammaCallback","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T11:50:40.118862Z","iopub.execute_input":"2025-03-09T11:50:40.119188Z","iopub.status.idle":"2025-03-09T13:31:24.065768Z","shell.execute_reply.started":"2025-03-09T11:50:40.119128Z","shell.execute_reply":"2025-03-09T13:31:24.064769Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 389ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 8.6749e-05 - dense_73_accuracy: 0.9995 - dense_73_loss: 0.0012 - loss: 0.0013\nEpoch 1: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 400ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 8.6859e-05 - dense_73_accuracy: 0.9995 - dense_73_loss: 0.0012 - loss: 0.0013 - val_dense_72_accuracy: 0.9660 - val_dense_72_loss: 0.1030 - val_dense_73_accuracy: 0.9985 - val_dense_73_loss: 0.0040 - val_loss: 0.1108 - learning_rate: 1.0000e-05\nEpoch 2/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 389ms/step - dense_72_accuracy: 0.9997 - dense_72_loss: 4.7505e-04 - dense_73_accuracy: 0.9992 - dense_73_loss: 0.0019 - loss: 0.0024\nEpoch 2: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 396ms/step - dense_72_accuracy: 0.9997 - dense_72_loss: 4.7397e-04 - dense_73_accuracy: 0.9992 - dense_73_loss: 0.0019 - loss: 0.0024 - val_dense_72_accuracy: 0.9660 - val_dense_72_loss: 0.1048 - val_dense_73_accuracy: 0.9985 - val_dense_73_loss: 0.0031 - val_loss: 0.1118 - learning_rate: 1.0000e-05\nEpoch 3/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 398ms/step - dense_72_accuracy: 0.9998 - dense_72_loss: 3.2866e-04 - dense_73_accuracy: 0.9997 - dense_73_loss: 0.0011 - loss: 0.0015\nEpoch 3: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 406ms/step - dense_72_accuracy: 0.9998 - dense_72_loss: 3.2823e-04 - dense_73_accuracy: 0.9997 - dense_73_loss: 0.0011 - loss: 0.0015 - val_dense_72_accuracy: 0.9691 - val_dense_72_loss: 0.1029 - val_dense_73_accuracy: 0.9969 - val_dense_73_loss: 0.0031 - val_loss: 0.1098 - learning_rate: 1.0000e-05\nEpoch 4/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 389ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 1.2984e-04 - dense_73_accuracy: 0.9990 - dense_73_loss: 0.0011 - loss: 0.0012\nEpoch 4: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 397ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 1.2984e-04 - dense_73_accuracy: 0.9990 - dense_73_loss: 0.0011 - loss: 0.0012 - val_dense_72_accuracy: 0.9691 - val_dense_72_loss: 0.1040 - val_dense_73_accuracy: 0.9985 - val_dense_73_loss: 0.0028 - val_loss: 0.1106 - learning_rate: 1.0000e-05\nEpoch 5/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 388ms/step - dense_72_accuracy: 0.9998 - dense_72_loss: 2.7353e-04 - dense_73_accuracy: 0.9990 - dense_73_loss: 0.0014 - loss: 0.0017\nEpoch 5: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 396ms/step - dense_72_accuracy: 0.9998 - dense_72_loss: 2.7360e-04 - dense_73_accuracy: 0.9990 - dense_73_loss: 0.0014 - loss: 0.0017 - val_dense_72_accuracy: 0.9645 - val_dense_72_loss: 0.1058 - val_dense_73_accuracy: 0.9969 - val_dense_73_loss: 0.0028 - val_loss: 0.1125 - learning_rate: 1.0000e-05\nEpoch 6/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 389ms/step - dense_72_accuracy: 0.9997 - dense_72_loss: 6.1669e-04 - dense_73_accuracy: 0.9989 - dense_73_loss: 0.0013 - loss: 0.0020\nEpoch 6: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 397ms/step - dense_72_accuracy: 0.9997 - dense_72_loss: 6.1706e-04 - dense_73_accuracy: 0.9989 - dense_73_loss: 0.0013 - loss: 0.0020 - val_dense_72_accuracy: 0.9660 - val_dense_72_loss: 0.1051 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0024 - val_loss: 0.1113 - learning_rate: 1.0000e-05\nEpoch 7/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 389ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 9.3702e-05 - dense_73_accuracy: 0.9995 - dense_73_loss: 0.0011 - loss: 0.0012\nEpoch 7: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 396ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 1.0041e-04 - dense_73_accuracy: 0.9995 - dense_73_loss: 0.0011 - loss: 0.0012 - val_dense_72_accuracy: 0.9676 - val_dense_72_loss: 0.1063 - val_dense_73_accuracy: 0.9985 - val_dense_73_loss: 0.0024 - val_loss: 0.1126 - learning_rate: 1.0000e-05\nEpoch 8/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 398ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 1.0476e-04 - dense_73_accuracy: 0.9988 - dense_73_loss: 0.0020 - loss: 0.0021\nEpoch 8: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 406ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 1.0474e-04 - dense_73_accuracy: 0.9988 - dense_73_loss: 0.0019 - loss: 0.0021 - val_dense_72_accuracy: 0.9676 - val_dense_72_loss: 0.1054 - val_dense_73_accuracy: 0.9985 - val_dense_73_loss: 0.0023 - val_loss: 0.1116 - learning_rate: 1.0000e-05\nEpoch 9/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 396ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 5.2706e-05 - dense_73_accuracy: 0.9992 - dense_73_loss: 0.0013 - loss: 0.0013\nEpoch 10: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 404ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 5.2755e-05 - dense_73_accuracy: 0.9992 - dense_73_loss: 0.0013 - loss: 0.0013 - val_dense_72_accuracy: 0.9676 - val_dense_72_loss: 0.1040 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0021 - val_loss: 0.1100 - learning_rate: 1.0000e-05\nEpoch 11/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 389ms/step - dense_72_accuracy: 0.9999 - dense_72_loss: 1.2713e-04 - dense_73_accuracy: 0.9996 - dense_73_loss: 0.0015 - loss: 0.0016\nEpoch 11: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 396ms/step - dense_72_accuracy: 0.9999 - dense_72_loss: 1.2721e-04 - dense_73_accuracy: 0.9996 - dense_73_loss: 0.0015 - loss: 0.0016 - val_dense_72_accuracy: 0.9645 - val_dense_72_loss: 0.1053 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0021 - val_loss: 0.1111 - learning_rate: 1.0000e-05\nEpoch 12/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 389ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 1.5726e-04 - dense_73_accuracy: 0.9997 - dense_73_loss: 0.0011 - loss: 0.0012\nEpoch 12: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 397ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 1.5716e-04 - dense_73_accuracy: 0.9997 - dense_73_loss: 0.0011 - loss: 0.0012 - val_dense_72_accuracy: 0.9645 - val_dense_72_loss: 0.1079 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0020 - val_loss: 0.1139 - learning_rate: 1.0000e-05\nEpoch 13/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 389ms/step - dense_72_accuracy: 0.9998 - dense_72_loss: 2.7371e-04 - dense_73_accuracy: 0.9995 - dense_73_loss: 9.2553e-04 - loss: 0.0012\nEpoch 13: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 396ms/step - dense_72_accuracy: 0.9998 - dense_72_loss: 2.7367e-04 - dense_73_accuracy: 0.9995 - dense_73_loss: 9.2561e-04 - loss: 0.0012 - val_dense_72_accuracy: 0.9630 - val_dense_72_loss: 0.1106 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0020 - val_loss: 0.1167 - learning_rate: 1.0000e-05\nEpoch 14/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 397ms/step - dense_72_accuracy: 0.9999 - dense_72_loss: 1.5886e-04 - dense_73_accuracy: 0.9990 - dense_73_loss: 0.0016 - loss: 0.0017\nEpoch 14: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 405ms/step - dense_72_accuracy: 0.9999 - dense_72_loss: 1.5875e-04 - dense_73_accuracy: 0.9990 - dense_73_loss: 0.0016 - loss: 0.0017 - val_dense_72_accuracy: 0.9645 - val_dense_72_loss: 0.1104 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0018 - val_loss: 0.1162 - learning_rate: 1.0000e-05\nEpoch 15/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 388ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 1.3864e-04 - dense_73_accuracy: 0.9993 - dense_73_loss: 0.0014 - loss: 0.0015\nEpoch 15: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 396ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 1.3933e-04 - dense_73_accuracy: 0.9993 - dense_73_loss: 0.0014 - loss: 0.0015 - val_dense_72_accuracy: 0.9645 - val_dense_72_loss: 0.1100 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0019 - val_loss: 0.1159 - learning_rate: 1.0000e-05\nEpoch 16/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 389ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 9.8733e-05 - dense_73_accuracy: 0.9996 - dense_73_loss: 8.7834e-04 - loss: 9.7708e-04\nEpoch 16: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 397ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 9.8630e-05 - dense_73_accuracy: 0.9996 - dense_73_loss: 8.7806e-04 - loss: 9.7671e-04 - val_dense_72_accuracy: 0.9614 - val_dense_72_loss: 0.1113 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0018 - val_loss: 0.1172 - learning_rate: 1.0000e-05\nEpoch 17/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 400ms/step - dense_72_accuracy: 0.9994 - dense_72_loss: 3.5402e-04 - dense_73_accuracy: 0.9985 - dense_73_loss: 0.0018 - loss: 0.0022\nEpoch 17: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 408ms/step - dense_72_accuracy: 0.9994 - dense_72_loss: 3.5310e-04 - dense_73_accuracy: 0.9985 - dense_73_loss: 0.0018 - loss: 0.0022 - val_dense_72_accuracy: 0.9630 - val_dense_72_loss: 0.1104 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0017 - val_loss: 0.1161 - learning_rate: 1.0000e-05\nEpoch 18/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 396ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 6.5995e-05 - dense_73_accuracy: 0.9992 - dense_73_loss: 0.0011 - loss: 0.0012\nEpoch 18: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 404ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 6.6169e-05 - dense_73_accuracy: 0.9992 - dense_73_loss: 0.0011 - loss: 0.0012 - val_dense_72_accuracy: 0.9630 - val_dense_72_loss: 0.1120 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0018 - val_loss: 0.1179 - learning_rate: 1.0000e-05\nEpoch 19/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 391ms/step - dense_72_accuracy: 0.9997 - dense_72_loss: 2.0956e-04 - dense_73_accuracy: 0.9986 - dense_73_loss: 0.0013 - loss: 0.0015\nEpoch 19: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 399ms/step - dense_72_accuracy: 0.9997 - dense_72_loss: 2.0918e-04 - dense_73_accuracy: 0.9986 - dense_73_loss: 0.0013 - loss: 0.0015 - val_dense_72_accuracy: 0.9645 - val_dense_72_loss: 0.1115 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0017 - val_loss: 0.1173 - learning_rate: 1.0000e-05\nEpoch 20/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 398ms/step - dense_72_accuracy: 0.9998 - dense_72_loss: 1.4201e-04 - dense_73_accuracy: 0.9993 - dense_73_loss: 0.0012 - loss: 0.0014\nEpoch 20: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 406ms/step - dense_72_accuracy: 0.9998 - dense_72_loss: 1.4229e-04 - dense_73_accuracy: 0.9993 - dense_73_loss: 0.0012 - loss: 0.0014 - val_dense_72_accuracy: 0.9614 - val_dense_72_loss: 0.1135 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0016 - val_loss: 0.1193 - learning_rate: 1.0000e-05\nEpoch 21/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 398ms/step - dense_72_accuracy: 0.9999 - dense_72_loss: 9.1346e-05 - dense_73_accuracy: 0.9988 - dense_73_loss: 0.0018 - loss: 0.0019\nEpoch 21: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 406ms/step - dense_72_accuracy: 0.9999 - dense_72_loss: 9.1860e-05 - dense_73_accuracy: 0.9988 - dense_73_loss: 0.0018 - loss: 0.0019 - val_dense_72_accuracy: 0.9630 - val_dense_72_loss: 0.1103 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0016 - val_loss: 0.1159 - learning_rate: 1.0000e-05\nEpoch 22/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 396ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 1.9667e-04 - dense_73_accuracy: 0.9994 - dense_73_loss: 0.0014 - loss: 0.0016\nEpoch 22: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 404ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 1.9642e-04 - dense_73_accuracy: 0.9994 - dense_73_loss: 0.0014 - loss: 0.0016 - val_dense_72_accuracy: 0.9630 - val_dense_72_loss: 0.1120 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0015 - val_loss: 0.1177 - learning_rate: 1.0000e-05\nEpoch 23/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 394ms/step - dense_72_accuracy: 0.9998 - dense_72_loss: 1.5356e-04 - dense_73_accuracy: 0.9992 - dense_73_loss: 0.0012 - loss: 0.0014\nEpoch 23: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 402ms/step - dense_72_accuracy: 0.9998 - dense_72_loss: 1.5348e-04 - dense_73_accuracy: 0.9992 - dense_73_loss: 0.0013 - loss: 0.0014 - val_dense_72_accuracy: 0.9630 - val_dense_72_loss: 0.1111 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0015 - val_loss: 0.1166 - learning_rate: 1.0000e-05\nEpoch 24/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 391ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 1.1079e-04 - dense_73_accuracy: 0.9994 - dense_73_loss: 0.0019 - loss: 0.0020\nEpoch 24: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 399ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 1.1113e-04 - dense_73_accuracy: 0.9994 - dense_73_loss: 0.0019 - loss: 0.0020 - val_dense_72_accuracy: 0.9630 - val_dense_72_loss: 0.1141 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0015 - val_loss: 0.1197 - learning_rate: 1.0000e-05\nEpoch 25/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 395ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 1.3972e-04 - dense_73_accuracy: 0.9993 - dense_73_loss: 0.0013 - loss: 0.0015\nEpoch 25: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 403ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 1.3960e-04 - dense_73_accuracy: 0.9993 - dense_73_loss: 0.0013 - loss: 0.0015 - val_dense_72_accuracy: 0.9630 - val_dense_72_loss: 0.1143 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0014 - val_loss: 0.1199 - learning_rate: 1.0000e-05\nEpoch 26/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 399ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 5.4848e-05 - dense_73_accuracy: 0.9996 - dense_73_loss: 8.7562e-04 - loss: 9.2976e-04\nEpoch 26: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 407ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 5.5950e-05 - dense_73_accuracy: 0.9996 - dense_73_loss: 8.7659e-04 - loss: 9.3114e-04 - val_dense_72_accuracy: 0.9630 - val_dense_72_loss: 0.1130 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0013 - val_loss: 0.1184 - learning_rate: 1.0000e-05\nEpoch 27/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 401ms/step - dense_72_accuracy: 0.9999 - dense_72_loss: 1.8016e-04 - dense_73_accuracy: 0.9988 - dense_73_loss: 0.0016 - loss: 0.0018\nEpoch 27: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 408ms/step - dense_72_accuracy: 0.9999 - dense_72_loss: 1.8105e-04 - dense_73_accuracy: 0.9988 - dense_73_loss: 0.0016 - loss: 0.0018 - val_dense_72_accuracy: 0.9614 - val_dense_72_loss: 0.1148 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0012 - val_loss: 0.1203 - learning_rate: 1.0000e-05\nEpoch 28/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 390ms/step - dense_72_accuracy: 0.9999 - dense_72_loss: 3.2307e-04 - dense_73_accuracy: 0.9995 - dense_73_loss: 0.0012 - loss: 0.0016\nEpoch 28: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 398ms/step - dense_72_accuracy: 0.9999 - dense_72_loss: 3.2340e-04 - dense_73_accuracy: 0.9995 - dense_73_loss: 0.0012 - loss: 0.0016 - val_dense_72_accuracy: 0.9614 - val_dense_72_loss: 0.1160 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0013 - val_loss: 0.1216 - learning_rate: 1.0000e-05\nEpoch 29/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 391ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 7.8161e-05 - dense_73_accuracy: 0.9995 - dense_73_loss: 0.0012 - loss: 0.0013\nEpoch 29: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 399ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 7.8085e-05 - dense_73_accuracy: 0.9995 - dense_73_loss: 0.0012 - loss: 0.0013 - val_dense_72_accuracy: 0.9645 - val_dense_72_loss: 0.1150 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0013 - val_loss: 0.1205 - learning_rate: 1.0000e-05\nEpoch 30/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 391ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 7.4943e-05 - dense_73_accuracy: 0.9993 - dense_73_loss: 8.2007e-04 - loss: 8.9502e-04\nEpoch 30: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 399ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 7.4905e-05 - dense_73_accuracy: 0.9993 - dense_73_loss: 8.2009e-04 - loss: 8.9501e-04 - val_dense_72_accuracy: 0.9660 - val_dense_72_loss: 0.1149 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0014 - val_loss: 0.1205 - learning_rate: 1.0000e-05\nEpoch 31/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 399ms/step - dense_72_accuracy: 0.9996 - dense_72_loss: 4.8086e-04 - dense_73_accuracy: 0.9998 - dense_73_loss: 5.0669e-04 - loss: 9.8756e-04\nEpoch 31: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 407ms/step - dense_72_accuracy: 0.9996 - dense_72_loss: 4.7983e-04 - dense_73_accuracy: 0.9998 - dense_73_loss: 5.0729e-04 - loss: 9.8714e-04 - val_dense_72_accuracy: 0.9630 - val_dense_72_loss: 0.1143 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0014 - val_loss: 0.1199 - learning_rate: 1.0000e-05\nEpoch 32/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 402ms/step - dense_72_accuracy: 0.9999 - dense_72_loss: 1.5213e-04 - dense_73_accuracy: 0.9997 - dense_73_loss: 6.7944e-04 - loss: 8.3158e-04\nEpoch 32: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 410ms/step - dense_72_accuracy: 0.9999 - dense_72_loss: 1.5223e-04 - dense_73_accuracy: 0.9997 - dense_73_loss: 6.8027e-04 - loss: 8.3252e-04 - val_dense_72_accuracy: 0.9630 - val_dense_72_loss: 0.1108 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0013 - val_loss: 0.1161 - learning_rate: 1.0000e-05\nEpoch 33/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 395ms/step - dense_72_accuracy: 0.9997 - dense_72_loss: 3.4430e-04 - dense_73_accuracy: 0.9996 - dense_73_loss: 8.8228e-04 - loss: 0.0012\nEpoch 33: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 403ms/step - dense_72_accuracy: 0.9997 - dense_72_loss: 3.4393e-04 - dense_73_accuracy: 0.9996 - dense_73_loss: 8.8325e-04 - loss: 0.0012 - val_dense_72_accuracy: 0.9660 - val_dense_72_loss: 0.1135 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0015 - val_loss: 0.1193 - learning_rate: 1.0000e-05\nEpoch 34/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 389ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 9.7917e-05 - dense_73_accuracy: 0.9998 - dense_73_loss: 7.6347e-04 - loss: 8.6139e-04\nEpoch 34: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 397ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 9.7853e-05 - dense_73_accuracy: 0.9998 - dense_73_loss: 7.6472e-04 - loss: 8.6259e-04 - val_dense_72_accuracy: 0.9676 - val_dense_72_loss: 0.1126 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0015 - val_loss: 0.1182 - learning_rate: 1.0000e-05\nEpoch 35/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 390ms/step - dense_72_accuracy: 0.9991 - dense_72_loss: 0.0023 - dense_73_accuracy: 0.9994 - dense_73_loss: 0.0013 - loss: 0.0036\nEpoch 35: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 397ms/step - dense_72_accuracy: 0.9991 - dense_72_loss: 0.0023 - dense_73_accuracy: 0.9994 - dense_73_loss: 0.0013 - loss: 0.0036 - val_dense_72_accuracy: 0.9614 - val_dense_72_loss: 0.1111 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0014 - val_loss: 0.1165 - learning_rate: 1.0000e-05\nEpoch 36/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 393ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 4.4968e-05 - dense_73_accuracy: 0.9994 - dense_73_loss: 8.5528e-04 - loss: 9.0014e-04\nEpoch 36: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 400ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 4.5021e-05 - dense_73_accuracy: 0.9994 - dense_73_loss: 8.5729e-04 - loss: 9.0209e-04 - val_dense_72_accuracy: 0.9660 - val_dense_72_loss: 0.1107 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0015 - val_loss: 0.1162 - learning_rate: 1.0000e-05\nEpoch 37/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 397ms/step - dense_72_accuracy: 0.9999 - dense_72_loss: 2.1166e-04 - dense_73_accuracy: 0.9997 - dense_73_loss: 7.0502e-04 - loss: 9.1643e-04\nEpoch 37: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 405ms/step - dense_72_accuracy: 0.9999 - dense_72_loss: 2.1157e-04 - dense_73_accuracy: 0.9997 - dense_73_loss: 7.0524e-04 - loss: 9.1632e-04 - val_dense_72_accuracy: 0.9645 - val_dense_72_loss: 0.1115 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0016 - val_loss: 0.1171 - learning_rate: 1.0000e-05\nEpoch 38/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 388ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 6.5015e-05 - dense_73_accuracy: 0.9987 - dense_73_loss: 0.0016 - loss: 0.0017\nEpoch 38: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 396ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 6.5409e-05 - dense_73_accuracy: 0.9987 - dense_73_loss: 0.0016 - loss: 0.0017 - val_dense_72_accuracy: 0.9676 - val_dense_72_loss: 0.1111 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0015 - val_loss: 0.1166 - learning_rate: 1.0000e-05\nEpoch 39/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 398ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 1.5347e-04 - dense_73_accuracy: 0.9987 - dense_73_loss: 0.0012 - loss: 0.0014\nEpoch 39: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 405ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 1.5548e-04 - dense_73_accuracy: 0.9987 - dense_73_loss: 0.0012 - loss: 0.0014 - val_dense_72_accuracy: 0.9676 - val_dense_72_loss: 0.1079 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0015 - val_loss: 0.1133 - learning_rate: 1.0000e-05\nEpoch 40/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 388ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 4.8969e-05 - dense_73_accuracy: 0.9992 - dense_73_loss: 0.0013 - loss: 0.0013\nEpoch 40: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 395ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 4.9212e-05 - dense_73_accuracy: 0.9992 - dense_73_loss: 0.0013 - loss: 0.0013 - val_dense_72_accuracy: 0.9676 - val_dense_72_loss: 0.1093 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0013 - val_loss: 0.1146 - learning_rate: 1.0000e-05\nEpoch 41/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 398ms/step - dense_72_accuracy: 0.9999 - dense_72_loss: 2.9735e-04 - dense_73_accuracy: 0.9988 - dense_73_loss: 0.0014 - loss: 0.0017\nEpoch 41: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 405ms/step - dense_72_accuracy: 0.9999 - dense_72_loss: 2.9781e-04 - dense_73_accuracy: 0.9988 - dense_73_loss: 0.0014 - loss: 0.0017 - val_dense_72_accuracy: 0.9691 - val_dense_72_loss: 0.1090 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0013 - val_loss: 0.1142 - learning_rate: 1.0000e-05\nEpoch 42/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 389ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 1.2213e-04 - dense_73_accuracy: 0.9987 - dense_73_loss: 0.0019 - loss: 0.0021\nEpoch 42: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 397ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 1.2251e-04 - dense_73_accuracy: 0.9987 - dense_73_loss: 0.0019 - loss: 0.0021 - val_dense_72_accuracy: 0.9676 - val_dense_72_loss: 0.1117 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0013 - val_loss: 0.1170 - learning_rate: 1.0000e-05\nEpoch 43/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 388ms/step - dense_72_accuracy: 0.9995 - dense_72_loss: 3.4154e-04 - dense_73_accuracy: 0.9994 - dense_73_loss: 0.0011 - loss: 0.0014\nEpoch 43: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 395ms/step - dense_72_accuracy: 0.9995 - dense_72_loss: 3.4131e-04 - dense_73_accuracy: 0.9994 - dense_73_loss: 0.0011 - loss: 0.0014 - val_dense_72_accuracy: 0.9660 - val_dense_72_loss: 0.1133 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0013 - val_loss: 0.1186 - learning_rate: 1.0000e-05\nEpoch 44/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 397ms/step - dense_72_accuracy: 0.9999 - dense_72_loss: 1.4609e-04 - dense_73_accuracy: 0.9994 - dense_73_loss: 0.0013 - loss: 0.0015\nEpoch 44: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 405ms/step - dense_72_accuracy: 0.9999 - dense_72_loss: 1.4613e-04 - dense_73_accuracy: 0.9994 - dense_73_loss: 0.0013 - loss: 0.0015 - val_dense_72_accuracy: 0.9660 - val_dense_72_loss: 0.1137 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0014 - val_loss: 0.1191 - learning_rate: 1.0000e-05\nEpoch 45/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 387ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 4.1445e-05 - dense_73_accuracy: 0.9994 - dense_73_loss: 7.0642e-04 - loss: 7.4787e-04\nEpoch 45: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 395ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 4.1546e-05 - dense_73_accuracy: 0.9994 - dense_73_loss: 7.0702e-04 - loss: 7.4858e-04 - val_dense_72_accuracy: 0.9630 - val_dense_72_loss: 0.1143 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0014 - val_loss: 0.1198 - learning_rate: 1.0000e-05\nEpoch 46/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 389ms/step - dense_72_accuracy: 0.9999 - dense_72_loss: 1.8942e-04 - dense_73_accuracy: 0.9992 - dense_73_loss: 9.4691e-04 - loss: 0.0011\nEpoch 46: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 397ms/step - dense_72_accuracy: 0.9999 - dense_72_loss: 1.9012e-04 - dense_73_accuracy: 0.9992 - dense_73_loss: 9.4650e-04 - loss: 0.0011 - val_dense_72_accuracy: 0.9660 - val_dense_72_loss: 0.1121 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0014 - val_loss: 0.1173 - learning_rate: 1.0000e-05\nEpoch 47/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 389ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 6.3093e-05 - dense_73_accuracy: 0.9992 - dense_73_loss: 7.7122e-04 - loss: 8.3110e-04\nEpoch 47: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 396ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 6.8499e-05 - dense_73_accuracy: 0.9992 - dense_73_loss: 7.7087e-04 - loss: 8.3297e-04 - val_dense_72_accuracy: 0.9660 - val_dense_72_loss: 0.1144 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0013 - val_loss: 0.1197 - learning_rate: 1.0000e-05\nEpoch 48/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 394ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 1.3639e-04 - dense_73_accuracy: 0.9995 - dense_73_loss: 7.3166e-04 - loss: 8.6016e-04\nEpoch 48: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 401ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 1.4890e-04 - dense_73_accuracy: 0.9995 - dense_73_loss: 7.3258e-04 - loss: 8.6576e-04 - val_dense_72_accuracy: 0.9630 - val_dense_72_loss: 0.1159 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0013 - val_loss: 0.1211 - learning_rate: 1.0000e-05\nEpoch 49/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 397ms/step - dense_72_accuracy: 0.9999 - dense_72_loss: 1.4841e-04 - dense_73_accuracy: 0.9999 - dense_73_loss: 3.6614e-04 - loss: 5.1456e-04\nEpoch 49: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 405ms/step - dense_72_accuracy: 0.9999 - dense_72_loss: 1.4844e-04 - dense_73_accuracy: 0.9999 - dense_73_loss: 3.6706e-04 - loss: 5.1551e-04 - val_dense_72_accuracy: 0.9630 - val_dense_72_loss: 0.1166 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0012 - val_loss: 0.1218 - learning_rate: 1.0000e-05\nEpoch 50/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 392ms/step - dense_72_accuracy: 0.9997 - dense_72_loss: 2.3079e-04 - dense_73_accuracy: 0.9991 - dense_73_loss: 0.0013 - loss: 0.0015\nEpoch 50: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 400ms/step - dense_72_accuracy: 0.9997 - dense_72_loss: 2.3109e-04 - dense_73_accuracy: 0.9991 - dense_73_loss: 0.0013 - loss: 0.0015 - val_dense_72_accuracy: 0.9645 - val_dense_72_loss: 0.1167 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0012 - val_loss: 0.1220 - learning_rate: 1.0000e-05\nEpoch 51/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 390ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 6.3542e-05 - dense_73_accuracy: 0.9991 - dense_73_loss: 0.0011 - loss: 0.0012\nEpoch 51: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 398ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 6.3678e-05 - dense_73_accuracy: 0.9991 - dense_73_loss: 0.0011 - loss: 0.0012 - val_dense_72_accuracy: 0.9645 - val_dense_72_loss: 0.1160 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0014 - val_loss: 0.1214 - learning_rate: 1.0000e-05\nEpoch 52/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 387ms/step - dense_72_accuracy: 0.9998 - dense_72_loss: 2.6546e-04 - dense_73_accuracy: 0.9999 - dense_73_loss: 4.8113e-04 - loss: 7.4659e-04\nEpoch 52: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 395ms/step - dense_72_accuracy: 0.9998 - dense_72_loss: 2.6584e-04 - dense_73_accuracy: 0.9999 - dense_73_loss: 4.8182e-04 - loss: 7.4767e-04 - val_dense_72_accuracy: 0.9645 - val_dense_72_loss: 0.1167 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0013 - val_loss: 0.1221 - learning_rate: 1.0000e-05\nEpoch 53/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 389ms/step - dense_72_accuracy: 0.9996 - dense_72_loss: 0.0014 - dense_73_accuracy: 0.9992 - dense_73_loss: 0.0012 - loss: 0.0025\nEpoch 53: val_loss did not improve from 0.10910\n\nEpoch 53: ReduceLROnPlateau reducing learning rate to 1e-05.\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 396ms/step - dense_72_accuracy: 0.9996 - dense_72_loss: 0.0014 - dense_73_accuracy: 0.9992 - dense_73_loss: 0.0012 - loss: 0.0025 - val_dense_72_accuracy: 0.9660 - val_dense_72_loss: 0.1170 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0012 - val_loss: 0.1223 - learning_rate: 1.0000e-05\nEpoch 54/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 389ms/step - dense_72_accuracy: 0.9999 - dense_72_loss: 2.7190e-04 - dense_73_accuracy: 0.9990 - dense_73_loss: 0.0011 - loss: 0.0014\nEpoch 54: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 396ms/step - dense_72_accuracy: 0.9999 - dense_72_loss: 2.7172e-04 - dense_73_accuracy: 0.9990 - dense_73_loss: 0.0011 - loss: 0.0014 - val_dense_72_accuracy: 0.9660 - val_dense_72_loss: 0.1168 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0012 - val_loss: 0.1220 - learning_rate: 1.0000e-05\nEpoch 55/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 399ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 7.4423e-05 - dense_73_accuracy: 0.9991 - dense_73_loss: 0.0015 - loss: 0.0016\nEpoch 55: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 406ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 7.4533e-05 - dense_73_accuracy: 0.9991 - dense_73_loss: 0.0015 - loss: 0.0016 - val_dense_72_accuracy: 0.9630 - val_dense_72_loss: 0.1194 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0012 - val_loss: 0.1248 - learning_rate: 1.0000e-05\nEpoch 56/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 389ms/step - dense_72_accuracy: 0.9998 - dense_72_loss: 5.4841e-04 - dense_73_accuracy: 0.9990 - dense_73_loss: 9.3354e-04 - loss: 0.0015\nEpoch 56: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 397ms/step - dense_72_accuracy: 0.9998 - dense_72_loss: 5.4928e-04 - dense_73_accuracy: 0.9990 - dense_73_loss: 9.3335e-04 - loss: 0.0015 - val_dense_72_accuracy: 0.9630 - val_dense_72_loss: 0.1201 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0012 - val_loss: 0.1255 - learning_rate: 1.0000e-05\nEpoch 57/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 392ms/step - dense_72_accuracy: 0.9999 - dense_72_loss: 3.2504e-04 - dense_73_accuracy: 0.9991 - dense_73_loss: 0.0020 - loss: 0.0023\nEpoch 57: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 400ms/step - dense_72_accuracy: 0.9999 - dense_72_loss: 3.2666e-04 - dense_73_accuracy: 0.9991 - dense_73_loss: 0.0020 - loss: 0.0023 - val_dense_72_accuracy: 0.9645 - val_dense_72_loss: 0.1174 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0012 - val_loss: 0.1227 - learning_rate: 1.0000e-05\nEpoch 58/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 396ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 8.2849e-05 - dense_73_accuracy: 0.9997 - dense_73_loss: 7.7652e-04 - loss: 8.5938e-04\nEpoch 58: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 404ms/step - dense_72_accuracy: 1.0000 - dense_72_loss: 8.2870e-05 - dense_73_accuracy: 0.9997 - dense_73_loss: 7.7786e-04 - loss: 8.6074e-04 - val_dense_72_accuracy: 0.9660 - val_dense_72_loss: 0.1189 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0011 - val_loss: 0.1242 - learning_rate: 1.0000e-05\nEpoch 59/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 395ms/step - dense_72_accuracy: 0.9994 - dense_72_loss: 0.0012 - dense_73_accuracy: 0.9995 - dense_73_loss: 0.0012 - loss: 0.0024\nEpoch 59: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 403ms/step - dense_72_accuracy: 0.9994 - dense_72_loss: 0.0012 - dense_73_accuracy: 0.9995 - dense_73_loss: 0.0012 - loss: 0.0024 - val_dense_72_accuracy: 0.9645 - val_dense_72_loss: 0.1169 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0011 - val_loss: 0.1220 - learning_rate: 1.0000e-05\nEpoch 60/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 404ms/step - dense_72_accuracy: 0.9999 - dense_72_loss: 9.5674e-05 - dense_73_accuracy: 0.9998 - dense_73_loss: 8.5302e-04 - loss: 9.4871e-04\nEpoch 60: val_loss did not improve from 0.10910\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 412ms/step - dense_72_accuracy: 0.9999 - dense_72_loss: 9.5689e-05 - dense_73_accuracy: 0.9998 - dense_73_loss: 8.5306e-04 - loss: 9.4877e-04 - val_dense_72_accuracy: 0.9645 - val_dense_72_loss: 0.1196 - val_dense_73_accuracy: 1.0000 - val_dense_73_loss: 0.0011 - val_loss: 0.1247 - learning_rate: 1.0000e-05\nEpoch 60: early stopping\nRestoring model weights from the end of the best epoch: 1.\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"model.evaluate([X_test_s, X_test_h1], [y_test_s, y_test_h1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T13:55:42.907778Z","iopub.execute_input":"2025-03-09T13:55:42.908079Z","iopub.status.idle":"2025-03-09T13:55:46.097062Z","shell.execute_reply.started":"2025-03-09T13:55:42.908059Z","shell.execute_reply":"2025-03-09T13:55:46.096226Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 100ms/step - dense_72_accuracy: 0.9636 - dense_72_loss: 0.1223 - dense_73_accuracy: 0.9879 - dense_73_loss: 0.0115 - loss: 0.1341\n","output_type":"stream"},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"[0.1364331990480423,\n 0.11849045753479004,\n 0.014336681924760342,\n 0.960493803024292,\n 0.9876543283462524]"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#def build_resnet18(input_shape=(128, 128, 3), num_classes=2):\ninput_shape=(128, 128, 3)\ninputs1 = Input(shape=input_shape)\ninputs2 = Input(shape=input_shape)\n\nimport tensorflow.keras.layers as L\n\n#input_data = Input(shape=input_shape, name='input_data')\n# Initial convolutional layer\n\nx1, x2 = residual_GLC_branch1(inputs1, inputs2)\n#print('x:',x.shape)\n\ncon = tf.keras.layers.Concatenate(axis=-1)([x1, x2])\n\ncon = tf.keras.layers.Dropout(0.25)(con, training = True)  ## MCD ####\n\nx = GlobalAveragePooling2D()(con)\n#print('GlobalAveragePooling2D x:',x.shape)\n\noutputs1 = Dense(5, activation='softmax')(x)\noutputs2 = Dense(7, activation='softmax')(x)\n\n# Create the model\nmodel = Model([inputs1, inputs2], [outputs1, outputs2])\n#return model\nprint(model.summary())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T00:24:52.523661Z","iopub.execute_input":"2025-03-08T00:24:52.523997Z","iopub.status.idle":"2025-03-08T00:25:05.288431Z","shell.execute_reply.started":"2025-03-08T00:24:52.523967Z","shell.execute_reply":"2025-03-08T00:25:05.287570Z"}},"outputs":[{"name":"stdout","text":"64 x_1, x_2 shape: (None, 32, 32, 64) (None, 32, 32, 64)\nx_11, x_22 shape: (None, 32, 32, 64) (None, 32, 32, 64)\n128 x11, x21 shape: (None, 16, 16, 128) (None, 16, 16, 128)\n128 x11, x21 shape: (None, 8, 8, 256) (None, 8, 8, 256)\n128 x11, x21 shape: (None, 4, 4, 512) (None, 4, 4, 512)\nx_111, x_222 shape: (None, 16, 16, 128) (None, 16, 16, 128)\nx_1111, x_2222 shape: (None, 16, 16, 128) (None, 16, 16, 128)\n128 x11, x21 shape: (None, 8, 8, 256) (None, 8, 8, 256)\n128 x11, x21 shape: (None, 4, 4, 512) (None, 4, 4, 512)\nx_11111, x_22222 shape: (None, 8, 8, 256) (None, 8, 8, 256)\nx_12, x_21 shape: (None, 8, 8, 256) (None, 8, 8, 256)\n128 x11, x21 shape: (None, 4, 4, 512) (None, 4, 4, 512)\nx_112, x_211 shape: (None, 4, 4, 512) (None, 4, 4, 512)\nx_1112, x_2111 shape: (None, 4, 4, 512) (None, 4, 4, 512)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\n\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m\n\n input_layer (\u001b[38;5;33mInputLayer\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  -                      \n\n input_layer_1              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  -                      \n (\u001b[38;5;33mInputLayer\u001b[0m)                                                                              \n\n conv2d (\u001b[38;5;33mConv2D\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m9,472\u001b[0m  input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n\n conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m9,472\u001b[0m  input_layer_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n batch_normalization        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  conv2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_1      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  conv2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n activation (\u001b[38;5;33mActivation\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization[\u001b[38;5;34m0\u001b[0m \n\n activation_1 (\u001b[38;5;33mActivation\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n\n max_pooling2d              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  activation[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n\n max_pooling2d_1            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  activation_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n (\u001b[38;5;33mMaxPooling2D\u001b[0m)                                                                            \n\n conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  max_pooling2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n depthwise_conv2d           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m1,600\u001b[0m  max_pooling2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_1         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m3,136\u001b[0m  max_pooling2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n conv2d_8 (\u001b[38;5;33mConv2D\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  max_pooling2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n\n depthwise_conv2d_5         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m1,600\u001b[0m  max_pooling2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_6         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m3,136\u001b[0m  max_pooling2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n batch_normalization_2      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  conv2d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_3      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  depthwise_conv2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_4      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  depthwise_conv2d_1[\u001b[38;5;34m0\u001b[0m] \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_9      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  conv2d_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_10     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  depthwise_conv2d_5[\u001b[38;5;34m0\u001b[0m] \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_11     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  depthwise_conv2d_6[\u001b[38;5;34m0\u001b[0m] \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n concatenate (\u001b[38;5;33mConcatenate\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m192\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_2 \n                                                                    batch_normalization_3 \n                                                                    batch_normalization_4 \n\n concatenate_2              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m192\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_9 \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      batch_normalization_1 \n                                                                    batch_normalization_1 \n\n conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m768\u001b[0m  concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n\n conv2d_9 (\u001b[38;5;33mConv2D\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m768\u001b[0m  concatenate_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n batch_normalization_5      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  conv2d_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_12     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  conv2d_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n global_average_pooling2d   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  batch_normalization_5 \n (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n\n global_max_pooling2d       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  batch_normalization_5 \n (\u001b[38;5;33mGlobalMaxPooling2D\u001b[0m)                                                                      \n\n global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n\n global_max_pooling2d_1     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mGlobalMaxPooling2D\u001b[0m)                                                                      \n\n add (\u001b[38;5;33mAdd\u001b[0m)                  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  global_average_poolin \n                                                                    global_max_pooling2d[\u001b[38;5;34m\u001b[0m \n\n add_4 (\u001b[38;5;33mAdd\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n dense (\u001b[38;5;33mDense\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)                          \u001b[38;5;34m256\u001b[0m  add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              \n\n dense_2 (\u001b[38;5;33mDense\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)                          \u001b[38;5;34m256\u001b[0m  add_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            \n\n dense_1 (\u001b[38;5;33mDense\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                         \u001b[38;5;34m256\u001b[0m  dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            \n\n dense_3 (\u001b[38;5;33mDense\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                         \u001b[38;5;34m256\u001b[0m  dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n\n conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m4,096\u001b[0m  max_pooling2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n reshape (\u001b[38;5;33mReshape\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)                     \u001b[38;5;34m0\u001b[0m  dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n\n conv2d_10 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m4,096\u001b[0m  max_pooling2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n\n reshape_1 (\u001b[38;5;33mReshape\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)                     \u001b[38;5;34m0\u001b[0m  dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n\n batch_normalization_6      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  conv2d_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n multiply (\u001b[38;5;33mMultiply\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_5 \n                                                                    reshape[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n\n batch_normalization_13     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  conv2d_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n multiply_1 (\u001b[38;5;33mMultiply\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n                                                                    reshape_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n\n add_1 (\u001b[38;5;33mAdd\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_6 \n                                                                    multiply[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n add_5 (\u001b[38;5;33mAdd\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n                                                                    multiply_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n activation_2 (\u001b[38;5;33mActivation\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            \n\n activation_6 (\u001b[38;5;33mActivation\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  add_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            \n\n batch_normalization_7      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  activation_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_14     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  activation_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n activation_3 (\u001b[38;5;33mActivation\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_7 \n\n activation_7 (\u001b[38;5;33mActivation\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n\n depthwise_conv2d_2         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m128\u001b[0m  activation_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_3         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m1,664\u001b[0m  activation_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_4         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m3,200\u001b[0m  activation_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_7         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m128\u001b[0m  activation_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_8         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m1,664\u001b[0m  activation_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_9         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m3,200\u001b[0m  activation_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n concatenate_1              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m192\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  depthwise_conv2d_2[\u001b[38;5;34m0\u001b[0m] \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      depthwise_conv2d_3[\u001b[38;5;34m0\u001b[0m] \n                                                                    depthwise_conv2d_4[\u001b[38;5;34m0\u001b[0m] \n\n concatenate_3              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m192\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  depthwise_conv2d_7[\u001b[38;5;34m0\u001b[0m] \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      depthwise_conv2d_8[\u001b[38;5;34m0\u001b[0m] \n                                                                    depthwise_conv2d_9[\u001b[38;5;34m0\u001b[0m] \n\n conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m4,160\u001b[0m  activation_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n\n conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m832\u001b[0m  concatenate_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n conv2d_12 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m4,160\u001b[0m  activation_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n\n conv2d_11 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m832\u001b[0m  concatenate_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n add_2 (\u001b[38;5;33mAdd\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  conv2d_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        \n                                                                    conv2d_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n add_6 (\u001b[38;5;33mAdd\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  conv2d_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       \n                                                                    conv2d_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n\n activation_4 (\u001b[38;5;33mActivation\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  add_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            \n\n activation_8 (\u001b[38;5;33mActivation\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  add_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            \n\n conv2d_7 (\u001b[38;5;33mConv2D\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m4,160\u001b[0m  activation_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n\n conv2d_13 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m4,160\u001b[0m  activation_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n\n batch_normalization_8      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  conv2d_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_15     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  conv2d_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n add_3 (\u001b[38;5;33mAdd\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_8 \n                                                                    max_pooling2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n add_7 (\u001b[38;5;33mAdd\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n                                                                    max_pooling2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n\n activation_5 (\u001b[38;5;33mActivation\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  add_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            \n\n activation_9 (\u001b[38;5;33mActivation\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  add_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            \n\n dropout (\u001b[38;5;33mDropout\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  activation_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n\n dropout_1 (\u001b[38;5;33mDropout\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  activation_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n\n attention_block            [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m),            \u001b[38;5;34m11,972\u001b[0m  dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],         \n (\u001b[38;5;33mAttentionBlock\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)]                     dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n\n conv2d_16 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  attention_block[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n\n depthwise_conv2d_18        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m1,600\u001b[0m  attention_block[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_19        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m3,136\u001b[0m  attention_block[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n conv2d_22 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  attention_block[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m]  \n\n depthwise_conv2d_23        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m1,600\u001b[0m  attention_block[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m]  \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_24        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m3,136\u001b[0m  attention_block[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m]  \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n batch_normalization_17     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  conv2d_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_18     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  depthwise_conv2d_18[\u001b[38;5;34m0\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_19     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  depthwise_conv2d_19[\u001b[38;5;34m0\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_24     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  conv2d_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_25     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  depthwise_conv2d_23[\u001b[38;5;34m0\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_26     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  depthwise_conv2d_24[\u001b[38;5;34m0\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n concatenate_5              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m192\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      batch_normalization_1 \n                                                                    batch_normalization_1 \n\n concatenate_7              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m192\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_2 \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      batch_normalization_2 \n                                                                    batch_normalization_2 \n\n conv2d_17 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m768\u001b[0m  concatenate_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n conv2d_23 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m768\u001b[0m  concatenate_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n batch_normalization_20     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  conv2d_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_27     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  conv2d_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  batch_normalization_2 \n (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n\n global_max_pooling2d_4     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  batch_normalization_2 \n (\u001b[38;5;33mGlobalMaxPooling2D\u001b[0m)                                                                      \n\n global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  batch_normalization_2 \n (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n\n global_max_pooling2d_5     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  batch_normalization_2 \n (\u001b[38;5;33mGlobalMaxPooling2D\u001b[0m)                                                                      \n\n add_15 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n add_19 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n dense_5 (\u001b[38;5;33mDense\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)                          \u001b[38;5;34m256\u001b[0m  add_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n\n dense_7 (\u001b[38;5;33mDense\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)                          \u001b[38;5;34m256\u001b[0m  add_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n\n dense_6 (\u001b[38;5;33mDense\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                         \u001b[38;5;34m256\u001b[0m  dense_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n\n dense_8 (\u001b[38;5;33mDense\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                         \u001b[38;5;34m256\u001b[0m  dense_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n\n conv2d_18 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m4,096\u001b[0m  attention_block[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n\n reshape_2 (\u001b[38;5;33mReshape\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)                     \u001b[38;5;34m0\u001b[0m  dense_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n\n conv2d_24 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m4,096\u001b[0m  attention_block[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m]  \n\n reshape_3 (\u001b[38;5;33mReshape\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)                     \u001b[38;5;34m0\u001b[0m  dense_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n\n batch_normalization_21     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  conv2d_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n multiply_2 (\u001b[38;5;33mMultiply\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_2 \n                                                                    reshape_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n\n batch_normalization_28     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  conv2d_24[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n multiply_3 (\u001b[38;5;33mMultiply\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_2 \n                                                                    reshape_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n\n add_16 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_2 \n                                                                    multiply_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n add_20 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_2 \n                                                                    multiply_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n activation_10              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  add_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_14              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  add_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n batch_normalization_22     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  activation_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_29     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  activation_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n activation_11              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_2 \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_15              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_2 \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n depthwise_conv2d_20        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m128\u001b[0m  activation_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_21        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m1,664\u001b[0m  activation_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_22        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m3,200\u001b[0m  activation_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_25        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m128\u001b[0m  activation_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_26        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m1,664\u001b[0m  activation_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_27        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m3,200\u001b[0m  activation_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n concatenate_6              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m192\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  depthwise_conv2d_20[\u001b[38;5;34m0\u001b[0m \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      depthwise_conv2d_21[\u001b[38;5;34m0\u001b[0m \n                                                                    depthwise_conv2d_22[\u001b[38;5;34m0\u001b[0m \n\n concatenate_8              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m192\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  depthwise_conv2d_25[\u001b[38;5;34m0\u001b[0m \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      depthwise_conv2d_26[\u001b[38;5;34m0\u001b[0m \n                                                                    depthwise_conv2d_27[\u001b[38;5;34m0\u001b[0m \n\n conv2d_20 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m4,160\u001b[0m  activation_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n conv2d_19 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m832\u001b[0m  concatenate_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n conv2d_26 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m4,160\u001b[0m  activation_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n conv2d_25 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m832\u001b[0m  concatenate_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n add_17 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  conv2d_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       \n                                                                    conv2d_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n\n add_21 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  conv2d_26[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       \n                                                                    conv2d_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n\n activation_12              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  add_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_16              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  add_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n conv2d_21 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m4,160\u001b[0m  activation_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n conv2d_27 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m4,160\u001b[0m  activation_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n batch_normalization_23     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  conv2d_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_30     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  conv2d_27[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n add_18 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_2 \n                                                                    attention_block[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n\n add_22 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_3 \n                                                                    attention_block[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m]  \n\n activation_13              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  add_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_17              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  add_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n dropout_3 (\u001b[38;5;33mDropout\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  activation_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n dropout_4 (\u001b[38;5;33mDropout\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  activation_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n attention_block_1          [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m),            \u001b[38;5;34m11,972\u001b[0m  dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       \n (\u001b[38;5;33mAttentionBlock\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)]                     dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n\n conv2d_48 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  attention_block_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n depthwise_conv2d_72        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m1,600\u001b[0m  attention_block_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_73        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m3,136\u001b[0m  attention_block_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n conv2d_55 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  attention_block_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n depthwise_conv2d_77        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m1,600\u001b[0m  attention_block_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_78        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m3,136\u001b[0m  attention_block_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n batch_normalization_32     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_48[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_33     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  depthwise_conv2d_72[\u001b[38;5;34m0\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_34     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  depthwise_conv2d_73[\u001b[38;5;34m0\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_40     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_55[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_41     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  depthwise_conv2d_77[\u001b[38;5;34m0\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_42     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  depthwise_conv2d_78[\u001b[38;5;34m0\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n concatenate_16             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m256\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_3 \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      batch_normalization_3 \n                                                                    batch_normalization_3 \n\n concatenate_18             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m256\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_4 \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      batch_normalization_4 \n                                                                    batch_normalization_4 \n\n conv2d_49 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m2,048\u001b[0m  concatenate_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n conv2d_56 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m2,048\u001b[0m  concatenate_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n batch_normalization_35     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_49[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_43     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_56[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_3 \n (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n\n global_max_pooling2d_8     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_3 \n (\u001b[38;5;33mGlobalMaxPooling2D\u001b[0m)                                                                      \n\n global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_4 \n (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n\n global_max_pooling2d_9     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_4 \n (\u001b[38;5;33mGlobalMaxPooling2D\u001b[0m)                                                                      \n\n add_36 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n add_40 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n dense_10 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)                        \u001b[38;5;34m1,024\u001b[0m  add_36[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n\n dense_12 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)                        \u001b[38;5;34m1,024\u001b[0m  add_40[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n\n dense_11 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                      \u001b[38;5;34m1,024\u001b[0m  dense_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n dense_13 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                      \u001b[38;5;34m1,024\u001b[0m  dense_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n conv2d_50 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m8,192\u001b[0m  attention_block_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n reshape_4 (\u001b[38;5;33mReshape\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  dense_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n conv2d_57 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m8,192\u001b[0m  attention_block_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n reshape_5 (\u001b[38;5;33mReshape\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  dense_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n batch_normalization_36     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_50[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n multiply_4 (\u001b[38;5;33mMultiply\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_3 \n                                                                    reshape_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n\n batch_normalization_44     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_57[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n multiply_5 (\u001b[38;5;33mMultiply\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_4 \n                                                                    reshape_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n\n add_37 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_3 \n                                                                    multiply_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n add_41 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_4 \n                                                                    multiply_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n activation_24              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  add_37[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_28              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  add_41[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n batch_normalization_37     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  activation_24[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_45     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  activation_28[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n activation_25              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_3 \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_29              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_4 \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n depthwise_conv2d_74        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m256\u001b[0m  activation_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_75        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m3,328\u001b[0m  activation_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_76        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m6,400\u001b[0m  activation_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_79        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m256\u001b[0m  activation_29[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_80        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m3,328\u001b[0m  activation_29[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_81        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m6,400\u001b[0m  activation_29[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n concatenate_17             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m384\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  depthwise_conv2d_74[\u001b[38;5;34m0\u001b[0m \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      depthwise_conv2d_75[\u001b[38;5;34m0\u001b[0m \n                                                                    depthwise_conv2d_76[\u001b[38;5;34m0\u001b[0m \n\n concatenate_19             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m384\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  depthwise_conv2d_79[\u001b[38;5;34m0\u001b[0m \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      depthwise_conv2d_80[\u001b[38;5;34m0\u001b[0m \n                                                                    depthwise_conv2d_81[\u001b[38;5;34m0\u001b[0m \n\n conv2d_52 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)             \u001b[38;5;34m16,512\u001b[0m  activation_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n conv2d_51 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m3,200\u001b[0m  concatenate_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n conv2d_59 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)             \u001b[38;5;34m16,512\u001b[0m  activation_29[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n conv2d_58 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m3,200\u001b[0m  concatenate_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n add_38 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  conv2d_52[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       \n                                                                    conv2d_51[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n\n add_42 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  conv2d_59[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       \n                                                                    conv2d_58[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n\n activation_26              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  add_38[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_30              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  add_42[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n conv2d_53 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)             \u001b[38;5;34m16,512\u001b[0m  activation_26[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n conv2d_54 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m8,320\u001b[0m  attention_block_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n conv2d_60 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)             \u001b[38;5;34m16,512\u001b[0m  activation_30[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n conv2d_61 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m8,320\u001b[0m  attention_block_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n batch_normalization_38     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_53[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_39     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_54[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_46     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_60[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_47     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_61[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n add_39 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_3 \n                                                                    batch_normalization_3 \n\n add_43 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_4 \n                                                                    batch_normalization_4 \n\n activation_27              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  add_39[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_31              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  add_43[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n dropout_6 (\u001b[38;5;33mDropout\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  activation_27[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n dropout_7 (\u001b[38;5;33mDropout\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  activation_31[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n attention_block_2          [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m),           \u001b[38;5;34m40,324\u001b[0m  dropout_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       \n (\u001b[38;5;33mAttentionBlock\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)]                    dropout_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n\n conv2d_64 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m1,024\u001b[0m  attention_block_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n depthwise_conv2d_90        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m3,200\u001b[0m  attention_block_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_91        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m6,272\u001b[0m  attention_block_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n conv2d_70 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m1,024\u001b[0m  attention_block_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n depthwise_conv2d_95        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m3,200\u001b[0m  attention_block_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_96        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m6,272\u001b[0m  attention_block_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n batch_normalization_49     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_64[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_50     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  depthwise_conv2d_90[\u001b[38;5;34m0\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_51     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  depthwise_conv2d_91[\u001b[38;5;34m0\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_56     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_70[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_57     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  depthwise_conv2d_95[\u001b[38;5;34m0\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_58     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  depthwise_conv2d_96[\u001b[38;5;34m0\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n concatenate_21             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m384\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_4 \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      batch_normalization_5 \n                                                                    batch_normalization_5 \n\n concatenate_23             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m384\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_5 \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      batch_normalization_5 \n                                                                    batch_normalization_5 \n\n conv2d_65 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m3,072\u001b[0m  concatenate_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n conv2d_71 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m3,072\u001b[0m  concatenate_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n batch_normalization_52     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_65[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_59     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_71[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_5 \n (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n\n global_max_pooling2d_12    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_5 \n (\u001b[38;5;33mGlobalMaxPooling2D\u001b[0m)                                                                      \n\n global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_5 \n (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n\n global_max_pooling2d_13    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_5 \n (\u001b[38;5;33mGlobalMaxPooling2D\u001b[0m)                                                                      \n\n add_51 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n add_55 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n dense_15 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)                        \u001b[38;5;34m1,024\u001b[0m  add_51[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n\n dense_17 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)                        \u001b[38;5;34m1,024\u001b[0m  add_55[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n\n dense_16 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                      \u001b[38;5;34m1,024\u001b[0m  dense_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n dense_18 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                      \u001b[38;5;34m1,024\u001b[0m  dense_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n conv2d_66 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)             \u001b[38;5;34m16,384\u001b[0m  attention_block_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n reshape_6 (\u001b[38;5;33mReshape\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  dense_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n conv2d_72 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)             \u001b[38;5;34m16,384\u001b[0m  attention_block_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n reshape_7 (\u001b[38;5;33mReshape\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  dense_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n batch_normalization_53     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_66[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n multiply_6 (\u001b[38;5;33mMultiply\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_5 \n                                                                    reshape_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n\n batch_normalization_60     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_72[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n multiply_7 (\u001b[38;5;33mMultiply\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_5 \n                                                                    reshape_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n\n add_52 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_5 \n                                                                    multiply_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n add_56 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_6 \n                                                                    multiply_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n activation_32              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  add_52[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_36              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  add_56[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n batch_normalization_54     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  activation_32[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_61     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  activation_36[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n activation_33              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_5 \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_37              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_6 \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n depthwise_conv2d_92        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m256\u001b[0m  activation_33[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_93        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m3,328\u001b[0m  activation_33[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_94        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m6,400\u001b[0m  activation_33[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_97        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m256\u001b[0m  activation_37[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_98        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m3,328\u001b[0m  activation_37[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_99        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m6,400\u001b[0m  activation_37[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n concatenate_22             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m384\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  depthwise_conv2d_92[\u001b[38;5;34m0\u001b[0m \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      depthwise_conv2d_93[\u001b[38;5;34m0\u001b[0m \n                                                                    depthwise_conv2d_94[\u001b[38;5;34m0\u001b[0m \n\n concatenate_24             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m384\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  depthwise_conv2d_97[\u001b[38;5;34m0\u001b[0m \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      depthwise_conv2d_98[\u001b[38;5;34m0\u001b[0m \n                                                                    depthwise_conv2d_99[\u001b[38;5;34m0\u001b[0m \n\n conv2d_68 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)             \u001b[38;5;34m16,512\u001b[0m  activation_33[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n conv2d_67 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m3,200\u001b[0m  concatenate_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n conv2d_74 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)             \u001b[38;5;34m16,512\u001b[0m  activation_37[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n conv2d_73 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m3,200\u001b[0m  concatenate_24[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n add_53 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  conv2d_68[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       \n                                                                    conv2d_67[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n\n add_57 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  conv2d_74[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       \n                                                                    conv2d_73[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n\n activation_34              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  add_53[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_38              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  add_57[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n conv2d_69 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)             \u001b[38;5;34m16,512\u001b[0m  activation_34[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n conv2d_75 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)             \u001b[38;5;34m16,512\u001b[0m  activation_38[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n batch_normalization_55     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_69[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_62     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv2d_75[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n add_54 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_5 \n                                                                    attention_block_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n add_58 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  batch_normalization_6 \n                                                                    attention_block_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n activation_35              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  add_54[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_39              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  add_58[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n dropout_9 (\u001b[38;5;33mDropout\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  activation_35[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n dropout_10 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  activation_39[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n attention_block_3          [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m),           \u001b[38;5;34m40,324\u001b[0m  dropout_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       \n (\u001b[38;5;33mAttentionBlock\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)]                    dropout_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n conv2d_90 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  attention_block_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n depthwise_conv2d_132       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m3,200\u001b[0m  attention_block_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_133       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m6,272\u001b[0m  attention_block_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n conv2d_97 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  attention_block_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n depthwise_conv2d_137       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m3,200\u001b[0m  attention_block_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_138       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m6,272\u001b[0m  attention_block_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n batch_normalization_64     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  conv2d_90[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_65     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m512\u001b[0m  depthwise_conv2d_132[\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_66     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m512\u001b[0m  depthwise_conv2d_133[\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_72     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  conv2d_97[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_73     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m512\u001b[0m  depthwise_conv2d_137[\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_74     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m512\u001b[0m  depthwise_conv2d_138[\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n concatenate_30             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_6 \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      batch_normalization_6 \n                                                                    batch_normalization_6 \n\n concatenate_32             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_7 \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      batch_normalization_7 \n                                                                    batch_normalization_7 \n\n conv2d_91 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m8,192\u001b[0m  concatenate_30[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n conv2d_98 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m8,192\u001b[0m  concatenate_32[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n batch_normalization_67     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  conv2d_91[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_75     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  conv2d_98[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_6 \n (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n\n global_max_pooling2d_16    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_6 \n (\u001b[38;5;33mGlobalMaxPooling2D\u001b[0m)                                                                      \n\n global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_7 \n (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n\n global_max_pooling2d_17    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_7 \n (\u001b[38;5;33mGlobalMaxPooling2D\u001b[0m)                                                                      \n\n add_70 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n add_74 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n dense_20 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)                       \u001b[38;5;34m4,096\u001b[0m  add_70[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n\n dense_22 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)                       \u001b[38;5;34m4,096\u001b[0m  add_74[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n\n dense_21 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                      \u001b[38;5;34m4,096\u001b[0m  dense_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n dense_23 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                      \u001b[38;5;34m4,096\u001b[0m  dense_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n conv2d_92 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)               \u001b[38;5;34m32,768\u001b[0m  attention_block_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n reshape_8 (\u001b[38;5;33mReshape\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  dense_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n conv2d_99 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)               \u001b[38;5;34m32,768\u001b[0m  attention_block_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n reshape_9 (\u001b[38;5;33mReshape\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  dense_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n batch_normalization_68     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  conv2d_92[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n multiply_8 (\u001b[38;5;33mMultiply\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_6 \n                                                                    reshape_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n\n batch_normalization_76     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  conv2d_99[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n multiply_9 (\u001b[38;5;33mMultiply\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_7 \n                                                                    reshape_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n\n add_71 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_6 \n                                                                    multiply_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n add_75 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_7 \n                                                                    multiply_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n activation_44              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_71[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_48              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_75[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n batch_normalization_69     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  activation_44[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_77     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  activation_48[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n activation_45              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_6 \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_49              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_7 \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n depthwise_conv2d_134       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                  \u001b[38;5;34m512\u001b[0m  activation_45[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_135       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m6,656\u001b[0m  activation_45[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_136       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)               \u001b[38;5;34m12,800\u001b[0m  activation_45[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_139       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                  \u001b[38;5;34m512\u001b[0m  activation_49[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_140       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m6,656\u001b[0m  activation_49[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_141       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)               \u001b[38;5;34m12,800\u001b[0m  activation_49[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n concatenate_31             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m768\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  depthwise_conv2d_134[\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      depthwise_conv2d_135[\u001b[38;5;34m\u001b[0m \n                                                                    depthwise_conv2d_136[\u001b[38;5;34m\u001b[0m \n\n concatenate_33             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m768\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  depthwise_conv2d_139[\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      depthwise_conv2d_140[\u001b[38;5;34m\u001b[0m \n                                                                    depthwise_conv2d_141[\u001b[38;5;34m\u001b[0m \n\n conv2d_94 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)               \u001b[38;5;34m65,792\u001b[0m  activation_45[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n conv2d_93 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)               \u001b[38;5;34m12,544\u001b[0m  concatenate_31[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n conv2d_101 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)               \u001b[38;5;34m65,792\u001b[0m  activation_49[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n conv2d_100 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)               \u001b[38;5;34m12,544\u001b[0m  concatenate_33[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n add_72 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  conv2d_94[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       \n                                                                    conv2d_93[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n\n add_76 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  conv2d_101[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n                                                                    conv2d_100[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n activation_46              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_72[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_50              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_76[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n conv2d_95 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)               \u001b[38;5;34m65,792\u001b[0m  activation_46[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n conv2d_96 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)               \u001b[38;5;34m33,024\u001b[0m  attention_block_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n conv2d_102 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)               \u001b[38;5;34m65,792\u001b[0m  activation_50[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n conv2d_103 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)               \u001b[38;5;34m33,024\u001b[0m  attention_block_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n batch_normalization_70     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  conv2d_95[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_71     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  conv2d_96[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_78     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  conv2d_102[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_79     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  conv2d_103[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n add_73 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_7 \n                                                                    batch_normalization_7 \n\n add_77 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_7 \n                                                                    batch_normalization_7 \n\n activation_47              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_73[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_51              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_77[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n dropout_12 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  activation_47[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n dropout_13 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  activation_51[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n attention_block_4          [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m),            \u001b[38;5;34m146,180\u001b[0m  dropout_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n (\u001b[38;5;33mAttentionBlock\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)]                      dropout_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n conv2d_106 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m4,096\u001b[0m  attention_block_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n depthwise_conv2d_150       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m6,400\u001b[0m  attention_block_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_151       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)               \u001b[38;5;34m12,544\u001b[0m  attention_block_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n conv2d_112 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m4,096\u001b[0m  attention_block_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n depthwise_conv2d_155       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m6,400\u001b[0m  attention_block_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_156       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)               \u001b[38;5;34m12,544\u001b[0m  attention_block_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n batch_normalization_81     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  conv2d_106[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_82     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  depthwise_conv2d_150[\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_83     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  depthwise_conv2d_151[\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_88     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  conv2d_112[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_89     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  depthwise_conv2d_155[\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_90     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  depthwise_conv2d_156[\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n concatenate_35             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m768\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_8 \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      batch_normalization_8 \n                                                                    batch_normalization_8 \n\n concatenate_37             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m768\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_8 \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      batch_normalization_8 \n                                                                    batch_normalization_9 \n\n conv2d_107 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)               \u001b[38;5;34m12,288\u001b[0m  concatenate_35[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n conv2d_113 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)               \u001b[38;5;34m12,288\u001b[0m  concatenate_37[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n batch_normalization_84     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  conv2d_107[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_91     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  conv2d_113[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_8 \n (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n\n global_max_pooling2d_20    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_8 \n (\u001b[38;5;33mGlobalMaxPooling2D\u001b[0m)                                                                      \n\n global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_9 \n (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n\n global_max_pooling2d_21    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_9 \n (\u001b[38;5;33mGlobalMaxPooling2D\u001b[0m)                                                                      \n\n add_85 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n add_89 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n dense_25 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)                       \u001b[38;5;34m4,096\u001b[0m  add_85[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n\n dense_27 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)                       \u001b[38;5;34m4,096\u001b[0m  add_89[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n\n dense_26 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                      \u001b[38;5;34m4,096\u001b[0m  dense_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n dense_28 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                      \u001b[38;5;34m4,096\u001b[0m  dense_27[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n conv2d_108 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)               \u001b[38;5;34m65,536\u001b[0m  attention_block_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n reshape_10 (\u001b[38;5;33mReshape\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  dense_26[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n conv2d_114 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)               \u001b[38;5;34m65,536\u001b[0m  attention_block_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n reshape_11 (\u001b[38;5;33mReshape\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  dense_28[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n batch_normalization_85     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  conv2d_108[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n multiply_10 (\u001b[38;5;33mMultiply\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_8 \n                                                                    reshape_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n batch_normalization_92     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  conv2d_114[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n multiply_11 (\u001b[38;5;33mMultiply\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_9 \n                                                                    reshape_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n add_86 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_8 \n                                                                    multiply_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n\n add_90 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_9 \n                                                                    multiply_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n\n activation_52              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_86[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_56              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_90[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n batch_normalization_86     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  activation_52[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_93     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  activation_56[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n activation_53              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_8 \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_57              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_9 \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n depthwise_conv2d_152       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                  \u001b[38;5;34m512\u001b[0m  activation_53[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_153       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m6,656\u001b[0m  activation_53[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_154       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)               \u001b[38;5;34m12,800\u001b[0m  activation_53[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_157       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                  \u001b[38;5;34m512\u001b[0m  activation_57[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_158       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m6,656\u001b[0m  activation_57[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_159       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)               \u001b[38;5;34m12,800\u001b[0m  activation_57[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n concatenate_36             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m768\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  depthwise_conv2d_152[\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      depthwise_conv2d_153[\u001b[38;5;34m\u001b[0m \n                                                                    depthwise_conv2d_154[\u001b[38;5;34m\u001b[0m \n\n concatenate_38             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m768\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  depthwise_conv2d_157[\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      depthwise_conv2d_158[\u001b[38;5;34m\u001b[0m \n                                                                    depthwise_conv2d_159[\u001b[38;5;34m\u001b[0m \n\n conv2d_110 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)               \u001b[38;5;34m65,792\u001b[0m  activation_53[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n conv2d_109 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)               \u001b[38;5;34m12,544\u001b[0m  concatenate_36[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n conv2d_116 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)               \u001b[38;5;34m65,792\u001b[0m  activation_57[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n conv2d_115 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)               \u001b[38;5;34m12,544\u001b[0m  concatenate_38[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n add_87 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  conv2d_110[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n                                                                    conv2d_109[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n add_91 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  conv2d_116[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n                                                                    conv2d_115[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n activation_54              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_87[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_58              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_91[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n conv2d_111 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)               \u001b[38;5;34m65,792\u001b[0m  activation_54[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n conv2d_117 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)               \u001b[38;5;34m65,792\u001b[0m  activation_58[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n batch_normalization_87     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  conv2d_111[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_94     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  conv2d_117[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n add_88 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_8 \n                                                                    attention_block_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n add_92 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_9 \n                                                                    attention_block_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n activation_55              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_88[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_59              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_92[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n dropout_15 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  activation_55[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n dropout_16 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  activation_59[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n attention_block_5          [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m),            \u001b[38;5;34m146,180\u001b[0m  dropout_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n (\u001b[38;5;33mAttentionBlock\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)]                      dropout_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n conv2d_126 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m8,192\u001b[0m  attention_block_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n depthwise_conv2d_180       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m6,400\u001b[0m  attention_block_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_181       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m)               \u001b[38;5;34m12,544\u001b[0m  attention_block_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n conv2d_133 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m8,192\u001b[0m  attention_block_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n depthwise_conv2d_185       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m6,400\u001b[0m  attention_block_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_186       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m)               \u001b[38;5;34m12,544\u001b[0m  attention_block_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n batch_normalization_96     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  conv2d_126[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_97     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  depthwise_conv2d_180[\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_98     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  depthwise_conv2d_181[\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_104    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  conv2d_133[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_105    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  depthwise_conv2d_185[\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_106    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  depthwise_conv2d_186[\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n concatenate_42             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_9 \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      batch_normalization_9 \n                                                                    batch_normalization_9 \n\n concatenate_44             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      batch_normalization_1 \n                                                                    batch_normalization_1 \n\n conv2d_127 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)               \u001b[38;5;34m32,768\u001b[0m  concatenate_42[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n conv2d_134 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)               \u001b[38;5;34m32,768\u001b[0m  concatenate_44[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n batch_normalization_99     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  conv2d_127[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_107    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  conv2d_134[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_9 \n (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n\n global_max_pooling2d_24    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_9 \n (\u001b[38;5;33mGlobalMaxPooling2D\u001b[0m)                                                                      \n\n global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n\n global_max_pooling2d_25    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mGlobalMaxPooling2D\u001b[0m)                                                                      \n\n add_102 (\u001b[38;5;33mAdd\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n add_106 (\u001b[38;5;33mAdd\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n dense_30 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                      \u001b[38;5;34m16,384\u001b[0m  add_102[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n\n dense_32 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                      \u001b[38;5;34m16,384\u001b[0m  add_106[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n\n dense_31 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                     \u001b[38;5;34m16,384\u001b[0m  dense_30[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n dense_33 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                     \u001b[38;5;34m16,384\u001b[0m  dense_32[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n conv2d_128 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)              \u001b[38;5;34m131,072\u001b[0m  attention_block_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n reshape_12 (\u001b[38;5;33mReshape\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  dense_31[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n conv2d_135 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)              \u001b[38;5;34m131,072\u001b[0m  attention_block_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n reshape_13 (\u001b[38;5;33mReshape\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  dense_33[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n batch_normalization_100    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  conv2d_128[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n multiply_12 (\u001b[38;5;33mMultiply\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_9 \n                                                                    reshape_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n batch_normalization_108    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  conv2d_135[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n multiply_13 (\u001b[38;5;33mMultiply\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n                                                                    reshape_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n add_103 (\u001b[38;5;33mAdd\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n                                                                    multiply_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n\n add_107 (\u001b[38;5;33mAdd\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n                                                                    multiply_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n\n activation_62              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_103[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_66              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_107[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n batch_normalization_101    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  activation_62[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_109    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  activation_66[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n activation_63              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_67              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n depthwise_conv2d_182       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  activation_63[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_183       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)               \u001b[38;5;34m13,312\u001b[0m  activation_63[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_184       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)               \u001b[38;5;34m25,600\u001b[0m  activation_63[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_187       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  activation_67[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_188       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)               \u001b[38;5;34m13,312\u001b[0m  activation_67[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_189       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)               \u001b[38;5;34m25,600\u001b[0m  activation_67[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n concatenate_43             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m1536\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  depthwise_conv2d_182[\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      depthwise_conv2d_183[\u001b[38;5;34m\u001b[0m \n                                                                    depthwise_conv2d_184[\u001b[38;5;34m\u001b[0m \n\n concatenate_45             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m1536\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  depthwise_conv2d_187[\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      depthwise_conv2d_188[\u001b[38;5;34m\u001b[0m \n                                                                    depthwise_conv2d_189[\u001b[38;5;34m\u001b[0m \n\n conv2d_130 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)              \u001b[38;5;34m262,656\u001b[0m  activation_63[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n conv2d_129 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)               \u001b[38;5;34m49,664\u001b[0m  concatenate_43[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n conv2d_137 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)              \u001b[38;5;34m262,656\u001b[0m  activation_67[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n conv2d_136 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)               \u001b[38;5;34m49,664\u001b[0m  concatenate_45[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n add_104 (\u001b[38;5;33mAdd\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  conv2d_130[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n                                                                    conv2d_129[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n add_108 (\u001b[38;5;33mAdd\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  conv2d_137[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n                                                                    conv2d_136[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n activation_64              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_104[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_68              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_108[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n conv2d_131 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)              \u001b[38;5;34m262,656\u001b[0m  activation_64[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n conv2d_132 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)              \u001b[38;5;34m131,584\u001b[0m  attention_block_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n conv2d_138 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)              \u001b[38;5;34m262,656\u001b[0m  activation_68[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n conv2d_139 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)              \u001b[38;5;34m131,584\u001b[0m  attention_block_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n batch_normalization_102    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  conv2d_131[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_103    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  conv2d_132[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_110    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  conv2d_138[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_111    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  conv2d_139[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n add_105 (\u001b[38;5;33mAdd\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n                                                                    batch_normalization_1 \n\n add_109 (\u001b[38;5;33mAdd\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n                                                                    batch_normalization_1 \n\n activation_65              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_105[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_69              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_109[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n attention_block_6          [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m),            \u001b[38;5;34m554,500\u001b[0m  activation_65[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   \n (\u001b[38;5;33mAttentionBlock\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)]                      activation_69[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n conv2d_142 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)               \u001b[38;5;34m16,384\u001b[0m  attention_block_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n depthwise_conv2d_198       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)               \u001b[38;5;34m12,800\u001b[0m  attention_block_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_199       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)               \u001b[38;5;34m25,088\u001b[0m  attention_block_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n conv2d_148 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)               \u001b[38;5;34m16,384\u001b[0m  attention_block_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n depthwise_conv2d_203       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)               \u001b[38;5;34m12,800\u001b[0m  attention_block_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_204       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)               \u001b[38;5;34m25,088\u001b[0m  attention_block_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n conv2d_30 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)               \u001b[38;5;34m2,080\u001b[0m  attention_block_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n depthwise_conv2d_36        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m640\u001b[0m  attention_block_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_37        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m1,664\u001b[0m  attention_block_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_38        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m640\u001b[0m  attention_block_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n batch_normalization_113    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  conv2d_142[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_114    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  depthwise_conv2d_198[\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_115    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  depthwise_conv2d_199[\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_120    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  conv2d_148[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_121    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  depthwise_conv2d_203[\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_122    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  depthwise_conv2d_204[\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n conv2d_33 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)               \u001b[38;5;34m2,080\u001b[0m  attention_block_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n depthwise_conv2d_42        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m640\u001b[0m  attention_block_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_43        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m1,664\u001b[0m  attention_block_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_44        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m640\u001b[0m  attention_block_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n concatenate_10             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m224\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  conv2d_30[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      depthwise_conv2d_36[\u001b[38;5;34m0\u001b[0m \n                                                                    depthwise_conv2d_37[\u001b[38;5;34m0\u001b[0m \n                                                                    depthwise_conv2d_38[\u001b[38;5;34m0\u001b[0m \n\n concatenate_47             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m1536\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      batch_normalization_1 \n                                                                    batch_normalization_1 \n\n concatenate_49             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m1536\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      batch_normalization_1 \n                                                                    batch_normalization_1 \n\n concatenate_11             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m224\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  conv2d_33[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      depthwise_conv2d_42[\u001b[38;5;34m0\u001b[0m \n                                                                    depthwise_conv2d_43[\u001b[38;5;34m0\u001b[0m \n                                                                    depthwise_conv2d_44[\u001b[38;5;34m0\u001b[0m \n\n lambda (\u001b[38;5;33mLambda\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m224\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  concatenate_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n conv2d_143 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)               \u001b[38;5;34m49,152\u001b[0m  concatenate_47[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n conv2d_149 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)               \u001b[38;5;34m49,152\u001b[0m  concatenate_49[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n lambda_1 (\u001b[38;5;33mLambda\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m224\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  concatenate_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n depthwise_conv2d_39        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m224\u001b[0m)              \u001b[38;5;34m2,240\u001b[0m  lambda[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n batch_normalization_116    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  conv2d_143[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_123    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  conv2d_149[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n depthwise_conv2d_45        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m224\u001b[0m)              \u001b[38;5;34m2,240\u001b[0m  lambda_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n conv2d_32 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m8,320\u001b[0m  attention_block_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n conv2d_31 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)             \u001b[38;5;34m28,800\u001b[0m  depthwise_conv2d_39[\u001b[38;5;34m0\u001b[0m \n\n global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n\n global_max_pooling2d_28    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mGlobalMaxPooling2D\u001b[0m)                                                                      \n\n global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n\n global_max_pooling2d_29    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mGlobalMaxPooling2D\u001b[0m)                                                                      \n\n conv2d_35 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m8,320\u001b[0m  attention_block_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n conv2d_34 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)             \u001b[38;5;34m28,800\u001b[0m  depthwise_conv2d_45[\u001b[38;5;34m0\u001b[0m \n\n depthwise_conv2d_41        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m1,280\u001b[0m  conv2d_32[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_40        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m1,280\u001b[0m  conv2d_31[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n add_117 (\u001b[38;5;33mAdd\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n add_121 (\u001b[38;5;33mAdd\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n depthwise_conv2d_47        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m1,280\u001b[0m  conv2d_35[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_46        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m1,280\u001b[0m  conv2d_34[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n add_30 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  depthwise_conv2d_41[\u001b[38;5;34m0\u001b[0m \n                                                                    depthwise_conv2d_40[\u001b[38;5;34m0\u001b[0m \n\n dense_35 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                      \u001b[38;5;34m16,384\u001b[0m  add_117[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n\n dense_37 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                      \u001b[38;5;34m16,384\u001b[0m  add_121[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n\n add_31 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  depthwise_conv2d_47[\u001b[38;5;34m0\u001b[0m \n                                                                    depthwise_conv2d_46[\u001b[38;5;34m0\u001b[0m \n\n activation_18              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  add_30[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n dense_36 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                     \u001b[38;5;34m16,384\u001b[0m  dense_35[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n dense_38 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                     \u001b[38;5;34m16,384\u001b[0m  dense_37[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n activation_19              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  add_31[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n conv2d_36 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m8,256\u001b[0m  activation_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n depthwise_conv2d_48        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m1,280\u001b[0m  activation_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_49        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m3,328\u001b[0m  activation_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_50        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m1,280\u001b[0m  activation_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n conv2d_78 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m8,256\u001b[0m  attention_block_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n depthwise_conv2d_108       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m1,280\u001b[0m  attention_block_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_109       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m3,328\u001b[0m  attention_block_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_110       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m1,280\u001b[0m  attention_block_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n conv2d_144 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)              \u001b[38;5;34m262,144\u001b[0m  attention_block_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n reshape_14 (\u001b[38;5;33mReshape\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  dense_36[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n conv2d_150 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)              \u001b[38;5;34m262,144\u001b[0m  attention_block_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n reshape_15 (\u001b[38;5;33mReshape\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  dense_38[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n\n conv2d_39 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m8,256\u001b[0m  activation_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n depthwise_conv2d_54        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m1,280\u001b[0m  activation_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_55        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m3,328\u001b[0m  activation_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_56        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m1,280\u001b[0m  activation_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n conv2d_81 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m8,256\u001b[0m  attention_block_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n depthwise_conv2d_114       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m1,280\u001b[0m  attention_block_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_115       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m3,328\u001b[0m  attention_block_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_116       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m1,280\u001b[0m  attention_block_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n concatenate_12             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m448\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  conv2d_36[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      depthwise_conv2d_48[\u001b[38;5;34m0\u001b[0m \n                                                                    depthwise_conv2d_49[\u001b[38;5;34m0\u001b[0m \n                                                                    depthwise_conv2d_50[\u001b[38;5;34m0\u001b[0m \n\n concatenate_26             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m448\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  conv2d_78[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      depthwise_conv2d_108[\u001b[38;5;34m\u001b[0m \n                                                                    depthwise_conv2d_109[\u001b[38;5;34m\u001b[0m \n                                                                    depthwise_conv2d_110[\u001b[38;5;34m\u001b[0m \n\n batch_normalization_117    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  conv2d_144[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n multiply_14 (\u001b[38;5;33mMultiply\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n                                                                    reshape_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n batch_normalization_124    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  conv2d_150[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n multiply_15 (\u001b[38;5;33mMultiply\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n                                                                    reshape_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n concatenate_13             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m448\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  conv2d_39[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      depthwise_conv2d_54[\u001b[38;5;34m0\u001b[0m \n                                                                    depthwise_conv2d_55[\u001b[38;5;34m0\u001b[0m \n                                                                    depthwise_conv2d_56[\u001b[38;5;34m0\u001b[0m \n\n concatenate_27             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m448\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  conv2d_81[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      depthwise_conv2d_114[\u001b[38;5;34m\u001b[0m \n                                                                    depthwise_conv2d_115[\u001b[38;5;34m\u001b[0m \n                                                                    depthwise_conv2d_116[\u001b[38;5;34m\u001b[0m \n\n lambda_2 (\u001b[38;5;33mLambda\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m448\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  concatenate_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n lambda_6 (\u001b[38;5;33mLambda\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m448\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  concatenate_26[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n add_118 (\u001b[38;5;33mAdd\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n                                                                    multiply_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n\n add_122 (\u001b[38;5;33mAdd\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n                                                                    multiply_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n\n lambda_3 (\u001b[38;5;33mLambda\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m448\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  concatenate_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n lambda_7 (\u001b[38;5;33mLambda\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m448\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  concatenate_27[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n depthwise_conv2d_51        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m448\u001b[0m)              \u001b[38;5;34m4,480\u001b[0m  lambda_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_111       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m448\u001b[0m)              \u001b[38;5;34m4,480\u001b[0m  lambda_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n activation_70              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_118[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_74              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_122[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n depthwise_conv2d_57        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m448\u001b[0m)              \u001b[38;5;34m4,480\u001b[0m  lambda_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_117       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m448\u001b[0m)              \u001b[38;5;34m4,480\u001b[0m  lambda_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n conv2d_38 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m256\u001b[0m)             \u001b[38;5;34m33,024\u001b[0m  activation_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n conv2d_37 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m256\u001b[0m)            \u001b[38;5;34m114,944\u001b[0m  depthwise_conv2d_51[\u001b[38;5;34m0\u001b[0m \n\n conv2d_80 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m256\u001b[0m)             \u001b[38;5;34m33,024\u001b[0m  attention_block_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n conv2d_79 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m256\u001b[0m)            \u001b[38;5;34m114,944\u001b[0m  depthwise_conv2d_111[\u001b[38;5;34m\u001b[0m \n\n batch_normalization_118    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  activation_70[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_125    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  activation_74[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n conv2d_41 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m256\u001b[0m)             \u001b[38;5;34m33,024\u001b[0m  activation_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n conv2d_40 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m256\u001b[0m)            \u001b[38;5;34m114,944\u001b[0m  depthwise_conv2d_57[\u001b[38;5;34m0\u001b[0m \n\n conv2d_83 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m256\u001b[0m)             \u001b[38;5;34m33,024\u001b[0m  attention_block_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n conv2d_82 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m256\u001b[0m)            \u001b[38;5;34m114,944\u001b[0m  depthwise_conv2d_117[\u001b[38;5;34m\u001b[0m \n\n depthwise_conv2d_53        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m2,560\u001b[0m  conv2d_38[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_52        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m2,560\u001b[0m  conv2d_37[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_113       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m2,560\u001b[0m  conv2d_80[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_112       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m2,560\u001b[0m  conv2d_79[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n activation_71              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_75              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n depthwise_conv2d_59        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m2,560\u001b[0m  conv2d_41[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_58        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m2,560\u001b[0m  conv2d_40[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_119       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m2,560\u001b[0m  conv2d_83[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_118       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m2,560\u001b[0m  conv2d_82[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n add_32 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  depthwise_conv2d_53[\u001b[38;5;34m0\u001b[0m \n                                                                    depthwise_conv2d_52[\u001b[38;5;34m0\u001b[0m \n\n add_66 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  depthwise_conv2d_113[\u001b[38;5;34m\u001b[0m \n                                                                    depthwise_conv2d_112[\u001b[38;5;34m\u001b[0m \n\n depthwise_conv2d_200       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  activation_71[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_201       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)               \u001b[38;5;34m13,312\u001b[0m  activation_71[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_202       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)               \u001b[38;5;34m25,600\u001b[0m  activation_71[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_205       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  activation_75[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_206       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)               \u001b[38;5;34m13,312\u001b[0m  activation_75[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_207       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)               \u001b[38;5;34m25,600\u001b[0m  activation_75[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n add_33 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  depthwise_conv2d_59[\u001b[38;5;34m0\u001b[0m \n                                                                    depthwise_conv2d_58[\u001b[38;5;34m0\u001b[0m \n\n add_67 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  depthwise_conv2d_119[\u001b[38;5;34m\u001b[0m \n                                                                    depthwise_conv2d_118[\u001b[38;5;34m\u001b[0m \n\n activation_20              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_32[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_40              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_66[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n concatenate_48             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m1536\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  depthwise_conv2d_200[\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      depthwise_conv2d_201[\u001b[38;5;34m\u001b[0m \n                                                                    depthwise_conv2d_202[\u001b[38;5;34m\u001b[0m \n\n concatenate_50             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m1536\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  depthwise_conv2d_205[\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      depthwise_conv2d_206[\u001b[38;5;34m\u001b[0m \n                                                                    depthwise_conv2d_207[\u001b[38;5;34m\u001b[0m \n\n activation_21              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_33[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_41              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_67[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n conv2d_42 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)               \u001b[38;5;34m32,896\u001b[0m  activation_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n depthwise_conv2d_60        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m2,560\u001b[0m  activation_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_61        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m6,656\u001b[0m  activation_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_62        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m2,560\u001b[0m  activation_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n conv2d_84 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)               \u001b[38;5;34m32,896\u001b[0m  activation_40[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n depthwise_conv2d_120       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m2,560\u001b[0m  activation_40[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_121       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m6,656\u001b[0m  activation_40[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_122       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m2,560\u001b[0m  activation_40[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n conv2d_120 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)               \u001b[38;5;34m32,896\u001b[0m  attention_block_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n depthwise_conv2d_168       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m2,560\u001b[0m  attention_block_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_169       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m6,656\u001b[0m  attention_block_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_170       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m2,560\u001b[0m  attention_block_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n conv2d_146 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)              \u001b[38;5;34m262,656\u001b[0m  activation_71[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n conv2d_145 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)               \u001b[38;5;34m49,664\u001b[0m  concatenate_48[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n conv2d_152 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)              \u001b[38;5;34m262,656\u001b[0m  activation_75[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n conv2d_151 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)               \u001b[38;5;34m49,664\u001b[0m  concatenate_50[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n conv2d_45 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)               \u001b[38;5;34m32,896\u001b[0m  activation_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n depthwise_conv2d_66        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m2,560\u001b[0m  activation_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_67        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m6,656\u001b[0m  activation_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_68        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m2,560\u001b[0m  activation_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n conv2d_87 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)               \u001b[38;5;34m32,896\u001b[0m  activation_41[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n depthwise_conv2d_126       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m2,560\u001b[0m  activation_41[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_127       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m6,656\u001b[0m  activation_41[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_128       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m2,560\u001b[0m  activation_41[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n conv2d_123 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)               \u001b[38;5;34m32,896\u001b[0m  attention_block_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n depthwise_conv2d_174       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m2,560\u001b[0m  attention_block_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_175       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m6,656\u001b[0m  attention_block_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_176       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m2,560\u001b[0m  attention_block_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n concatenate_14             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m896\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  conv2d_42[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      depthwise_conv2d_60[\u001b[38;5;34m0\u001b[0m \n                                                                    depthwise_conv2d_61[\u001b[38;5;34m0\u001b[0m \n                                                                    depthwise_conv2d_62[\u001b[38;5;34m0\u001b[0m \n\n concatenate_28             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m896\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  conv2d_84[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      depthwise_conv2d_120[\u001b[38;5;34m\u001b[0m \n                                                                    depthwise_conv2d_121[\u001b[38;5;34m\u001b[0m \n                                                                    depthwise_conv2d_122[\u001b[38;5;34m\u001b[0m \n\n concatenate_40             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m896\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  conv2d_120[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      depthwise_conv2d_168[\u001b[38;5;34m\u001b[0m \n                                                                    depthwise_conv2d_169[\u001b[38;5;34m\u001b[0m \n                                                                    depthwise_conv2d_170[\u001b[38;5;34m\u001b[0m \n\n add_119 (\u001b[38;5;33mAdd\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  conv2d_146[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n                                                                    conv2d_145[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n add_123 (\u001b[38;5;33mAdd\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  conv2d_152[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n                                                                    conv2d_151[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n concatenate_15             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m896\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  conv2d_45[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      depthwise_conv2d_66[\u001b[38;5;34m0\u001b[0m \n                                                                    depthwise_conv2d_67[\u001b[38;5;34m0\u001b[0m \n                                                                    depthwise_conv2d_68[\u001b[38;5;34m0\u001b[0m \n\n concatenate_29             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m896\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  conv2d_87[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      depthwise_conv2d_126[\u001b[38;5;34m\u001b[0m \n                                                                    depthwise_conv2d_127[\u001b[38;5;34m\u001b[0m \n                                                                    depthwise_conv2d_128[\u001b[38;5;34m\u001b[0m \n\n concatenate_41             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m896\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  conv2d_123[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      depthwise_conv2d_174[\u001b[38;5;34m\u001b[0m \n                                                                    depthwise_conv2d_175[\u001b[38;5;34m\u001b[0m \n                                                                    depthwise_conv2d_176[\u001b[38;5;34m\u001b[0m \n\n lambda_4 (\u001b[38;5;33mLambda\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m896\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  concatenate_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n lambda_8 (\u001b[38;5;33mLambda\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m896\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  concatenate_28[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n lambda_10 (\u001b[38;5;33mLambda\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m896\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  concatenate_40[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n activation_72              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_119[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_76              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_123[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n lambda_5 (\u001b[38;5;33mLambda\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m896\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  concatenate_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n lambda_9 (\u001b[38;5;33mLambda\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m896\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  concatenate_29[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n lambda_11 (\u001b[38;5;33mLambda\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m896\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  concatenate_41[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n depthwise_conv2d_63        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m896\u001b[0m)                \u001b[38;5;34m8,960\u001b[0m  lambda_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_123       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m896\u001b[0m)                \u001b[38;5;34m8,960\u001b[0m  lambda_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_171       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m896\u001b[0m)                \u001b[38;5;34m8,960\u001b[0m  lambda_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n conv2d_147 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)              \u001b[38;5;34m262,656\u001b[0m  activation_72[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n conv2d_153 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)              \u001b[38;5;34m262,656\u001b[0m  activation_76[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n depthwise_conv2d_69        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m896\u001b[0m)                \u001b[38;5;34m8,960\u001b[0m  lambda_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_129       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m896\u001b[0m)                \u001b[38;5;34m8,960\u001b[0m  lambda_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_177       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m896\u001b[0m)                \u001b[38;5;34m8,960\u001b[0m  lambda_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n conv2d_44 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m512\u001b[0m)              \u001b[38;5;34m131,584\u001b[0m  activation_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n conv2d_43 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m512\u001b[0m)              \u001b[38;5;34m459,264\u001b[0m  depthwise_conv2d_63[\u001b[38;5;34m0\u001b[0m \n\n conv2d_86 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m512\u001b[0m)              \u001b[38;5;34m131,584\u001b[0m  activation_40[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n conv2d_85 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m512\u001b[0m)              \u001b[38;5;34m459,264\u001b[0m  depthwise_conv2d_123[\u001b[38;5;34m\u001b[0m \n\n conv2d_122 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m512\u001b[0m)              \u001b[38;5;34m131,584\u001b[0m  attention_block_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n conv2d_121 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m512\u001b[0m)              \u001b[38;5;34m459,264\u001b[0m  depthwise_conv2d_171[\u001b[38;5;34m\u001b[0m \n\n batch_normalization_119    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  conv2d_147[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n batch_normalization_126    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m2,048\u001b[0m  conv2d_153[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n\n conv2d_47 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m512\u001b[0m)              \u001b[38;5;34m131,584\u001b[0m  activation_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n conv2d_46 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m512\u001b[0m)              \u001b[38;5;34m459,264\u001b[0m  depthwise_conv2d_69[\u001b[38;5;34m0\u001b[0m \n\n conv2d_89 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m512\u001b[0m)              \u001b[38;5;34m131,584\u001b[0m  activation_41[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n conv2d_88 (\u001b[38;5;33mConv2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m512\u001b[0m)              \u001b[38;5;34m459,264\u001b[0m  depthwise_conv2d_129[\u001b[38;5;34m\u001b[0m \n\n conv2d_125 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m512\u001b[0m)              \u001b[38;5;34m131,584\u001b[0m  attention_block_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n conv2d_124 (\u001b[38;5;33mConv2D\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m512\u001b[0m)              \u001b[38;5;34m459,264\u001b[0m  depthwise_conv2d_177[\u001b[38;5;34m\u001b[0m \n\n depthwise_conv2d_65        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m5,120\u001b[0m  conv2d_44[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_64        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m5,120\u001b[0m  conv2d_43[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_125       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m5,120\u001b[0m  conv2d_86[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_124       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m5,120\u001b[0m  conv2d_85[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_173       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m5,120\u001b[0m  conv2d_122[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_172       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m5,120\u001b[0m  conv2d_121[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n add_120 (\u001b[38;5;33mAdd\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n                                                                    attention_block_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n add_124 (\u001b[38;5;33mAdd\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n                                                                    attention_block_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n depthwise_conv2d_71        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m5,120\u001b[0m  conv2d_47[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_70        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m5,120\u001b[0m  conv2d_46[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_131       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m5,120\u001b[0m  conv2d_89[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_130       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m5,120\u001b[0m  conv2d_88[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_179       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m5,120\u001b[0m  conv2d_125[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n depthwise_conv2d_178       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                \u001b[38;5;34m5,120\u001b[0m  conv2d_124[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)                                                                         \n\n add_34 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  depthwise_conv2d_65[\u001b[38;5;34m0\u001b[0m \n                                                                    depthwise_conv2d_64[\u001b[38;5;34m0\u001b[0m \n\n add_68 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  depthwise_conv2d_125[\u001b[38;5;34m\u001b[0m \n                                                                    depthwise_conv2d_124[\u001b[38;5;34m\u001b[0m \n\n add_100 (\u001b[38;5;33mAdd\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  depthwise_conv2d_173[\u001b[38;5;34m\u001b[0m \n                                                                    depthwise_conv2d_172[\u001b[38;5;34m\u001b[0m \n\n activation_73              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_120[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_77              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_124[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n add_35 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  depthwise_conv2d_71[\u001b[38;5;34m0\u001b[0m \n                                                                    depthwise_conv2d_70[\u001b[38;5;34m0\u001b[0m \n\n add_69 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  depthwise_conv2d_131[\u001b[38;5;34m\u001b[0m \n                                                                    depthwise_conv2d_130[\u001b[38;5;34m\u001b[0m \n\n add_101 (\u001b[38;5;33mAdd\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  depthwise_conv2d_179[\u001b[38;5;34m\u001b[0m \n                                                                    depthwise_conv2d_178[\u001b[38;5;34m\u001b[0m \n\n activation_22              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_34[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_42              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_68[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_60              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_100[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n attention_block_7          [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m),            \u001b[38;5;34m554,500\u001b[0m  activation_73[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   \n (\u001b[38;5;33mAttentionBlock\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)]                      activation_77[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n activation_23              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_35[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_43              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_69[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n activation_61              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  add_101[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n (\u001b[38;5;33mActivation\u001b[0m)                                                                              \n\n concatenate_52             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m2048\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  activation_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      activation_42[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   \n                                                                    activation_60[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   \n                                                                    attention_block_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n concatenate_53             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m2048\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  activation_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      activation_43[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   \n                                                                    activation_61[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   \n                                                                    attention_block_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n\n dropout_20 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m2048\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  concatenate_52[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n dropout_21 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m2048\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  concatenate_53[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n concatenate_54             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4096\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  dropout_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n (\u001b[38;5;33mConcatenate\u001b[0m)                                                      dropout_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n dropout_22 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4096\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  concatenate_54[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n global_average_pooling2d  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m)                         \u001b[38;5;34m0\u001b[0m  dropout_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n\n dense_40 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)                       \u001b[38;5;34m20,485\u001b[0m  global_average_poolin \n\n dense_41 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)                       \u001b[38;5;34m28,679\u001b[0m  global_average_poolin \n\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n<span style=\"font-weight: bold\"> Layer (type)              </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">        Param # </span><span style=\"font-weight: bold\"> Connected to           </span>\n\n input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                      \n\n input_layer_1              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                      \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                                              \n\n conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">9,472</span>  input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n\n conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">9,472</span>  input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n batch_normalization        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_1      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n activation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n activation_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n\n max_pooling2d              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  activation[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n\n max_pooling2d_1            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  activation_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                                                                            \n\n conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  max_pooling2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n depthwise_conv2d           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">1,600</span>  max_pooling2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_1         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">3,136</span>  max_pooling2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n conv2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  max_pooling2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n\n depthwise_conv2d_5         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">1,600</span>  max_pooling2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_6         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">3,136</span>  max_pooling2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n batch_normalization_2      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_3      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  depthwise_conv2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_4      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  depthwise_conv2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_9      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_10     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  depthwise_conv2d_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_11     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  depthwise_conv2d_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n concatenate (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_2 \n                                                                    batch_normalization_3 \n                                                                    batch_normalization_4 \n\n concatenate_2              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_9 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      batch_normalization_1 \n                                                                    batch_normalization_1 \n\n conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>  concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n\n conv2d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>  concatenate_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n batch_normalization_5      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_12     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n global_average_pooling2d   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_5 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n\n global_max_pooling2d       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_5 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling2D</span>)                                                                      \n\n global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n\n global_max_pooling2d_1     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling2D</span>)                                                                      \n\n add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n                                                                    global_max_pooling2d[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n add_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              \n\n dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  add_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            \n\n dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            \n\n dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n\n conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span>  max_pooling2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n\n conv2d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span>  max_pooling2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n\n reshape_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n\n batch_normalization_6      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n multiply (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_5 \n                                                                    reshape[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n\n batch_normalization_13     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n multiply_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n                                                                    reshape_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n\n add_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_6 \n                                                                    multiply[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n add_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n                                                                    multiply_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n activation_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            \n\n activation_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            \n\n batch_normalization_7      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  activation_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_14     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  activation_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n activation_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_7 \n\n activation_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n\n depthwise_conv2d_2         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>  activation_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_3         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">1,664</span>  activation_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_4         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">3,200</span>  activation_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_7         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>  activation_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_8         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">1,664</span>  activation_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_9         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">3,200</span>  activation_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n concatenate_1              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  depthwise_conv2d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      depthwise_conv2d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n                                                                    depthwise_conv2d_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n\n concatenate_3              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  depthwise_conv2d_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      depthwise_conv2d_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n                                                                    depthwise_conv2d_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n\n conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span>  activation_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n\n conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">832</span>  concatenate_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n conv2d_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span>  activation_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n\n conv2d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">832</span>  concatenate_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n add_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv2d_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        \n                                                                    conv2d_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n add_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv2d_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       \n                                                                    conv2d_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n\n activation_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            \n\n activation_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            \n\n conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span>  activation_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n\n conv2d_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span>  activation_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n\n batch_normalization_8      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_15     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n add_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_8 \n                                                                    max_pooling2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n add_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n                                                                    max_pooling2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n\n activation_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            \n\n activation_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            \n\n dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  activation_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n\n dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  activation_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n\n attention_block            [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>),            <span style=\"color: #00af00; text-decoration-color: #00af00\">11,972</span>  dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],         \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AttentionBlock</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)]                     dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n\n conv2d_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  attention_block[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n\n depthwise_conv2d_18        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">1,600</span>  attention_block[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_19        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">3,136</span>  attention_block[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n conv2d_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  attention_block[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>]  \n\n depthwise_conv2d_23        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">1,600</span>  attention_block[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>]  \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_24        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">3,136</span>  attention_block[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>]  \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n batch_normalization_17     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_18     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  depthwise_conv2d_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_19     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  depthwise_conv2d_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_24     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_25     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  depthwise_conv2d_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_26     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  depthwise_conv2d_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n concatenate_5              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      batch_normalization_1 \n                                                                    batch_normalization_1 \n\n concatenate_7              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_2 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      batch_normalization_2 \n                                                                    batch_normalization_2 \n\n conv2d_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>  concatenate_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n conv2d_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>  concatenate_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n batch_normalization_20     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_27     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_2 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n\n global_max_pooling2d_4     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_2 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling2D</span>)                                                                      \n\n global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_2 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n\n global_max_pooling2d_5     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_2 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling2D</span>)                                                                      \n\n add_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n add_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  add_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n\n dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  add_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n\n dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  dense_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n\n dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  dense_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n\n conv2d_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span>  attention_block[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n\n reshape_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n\n conv2d_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span>  attention_block[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>]  \n\n reshape_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n\n batch_normalization_21     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n multiply_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_2 \n                                                                    reshape_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n\n batch_normalization_28     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n multiply_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_2 \n                                                                    reshape_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n\n add_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_2 \n                                                                    multiply_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n add_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_2 \n                                                                    multiply_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n activation_10              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_14              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n batch_normalization_22     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  activation_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_29     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  activation_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n activation_11              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_2 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_15              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_2 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n depthwise_conv2d_20        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>  activation_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_21        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">1,664</span>  activation_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_22        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">3,200</span>  activation_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_25        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>  activation_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_26        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">1,664</span>  activation_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_27        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">3,200</span>  activation_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n concatenate_6              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  depthwise_conv2d_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      depthwise_conv2d_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n                                                                    depthwise_conv2d_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n concatenate_8              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  depthwise_conv2d_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      depthwise_conv2d_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n                                                                    depthwise_conv2d_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n conv2d_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span>  activation_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n conv2d_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">832</span>  concatenate_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n conv2d_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span>  activation_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n conv2d_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">832</span>  concatenate_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n add_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv2d_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       \n                                                                    conv2d_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n\n add_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv2d_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       \n                                                                    conv2d_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n\n activation_12              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_16              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n conv2d_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span>  activation_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n conv2d_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span>  activation_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n batch_normalization_23     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_30     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  conv2d_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n add_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_2 \n                                                                    attention_block[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n\n add_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_3 \n                                                                    attention_block[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>]  \n\n activation_13              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_17              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  activation_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  activation_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n attention_block_1          [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>),            <span style=\"color: #00af00; text-decoration-color: #00af00\">11,972</span>  dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AttentionBlock</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)]                     dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n\n conv2d_48 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  attention_block_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n depthwise_conv2d_72        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">1,600</span>  attention_block_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_73        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">3,136</span>  attention_block_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n conv2d_55 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  attention_block_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n depthwise_conv2d_77        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">1,600</span>  attention_block_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_78        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">3,136</span>  attention_block_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n batch_normalization_32     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_48[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_33     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  depthwise_conv2d_72[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_34     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  depthwise_conv2d_73[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_40     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_55[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_41     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  depthwise_conv2d_77[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_42     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  depthwise_conv2d_78[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n concatenate_16             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_3 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      batch_normalization_3 \n                                                                    batch_normalization_3 \n\n concatenate_18             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_4 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      batch_normalization_4 \n                                                                    batch_normalization_4 \n\n conv2d_49 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  concatenate_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n conv2d_56 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  concatenate_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n batch_normalization_35     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_49[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_43     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_56[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_3 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n\n global_max_pooling2d_8     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_3 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling2D</span>)                                                                      \n\n global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_4 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n\n global_max_pooling2d_9     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_4 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling2D</span>)                                                                      \n\n add_36 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n add_40 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                        <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  add_36[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n\n dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                        <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  add_40[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n\n dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  dense_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  dense_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n conv2d_50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">8,192</span>  attention_block_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n reshape_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n conv2d_57 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">8,192</span>  attention_block_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n reshape_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n batch_normalization_36     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_50[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n multiply_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_3 \n                                                                    reshape_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n\n batch_normalization_44     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_57[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n multiply_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_4 \n                                                                    reshape_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n\n add_37 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_3 \n                                                                    multiply_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n add_41 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_4 \n                                                                    multiply_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n activation_24              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_37[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_28              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_41[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n batch_normalization_37     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  activation_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_45     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  activation_28[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n activation_25              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_3 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_29              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_4 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n depthwise_conv2d_74        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  activation_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_75        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">3,328</span>  activation_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_76        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">6,400</span>  activation_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_79        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  activation_29[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_80        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">3,328</span>  activation_29[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_81        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">6,400</span>  activation_29[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n concatenate_17             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  depthwise_conv2d_74[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      depthwise_conv2d_75[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n                                                                    depthwise_conv2d_76[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n concatenate_19             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  depthwise_conv2d_79[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      depthwise_conv2d_80[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n                                                                    depthwise_conv2d_81[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n conv2d_52 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span>  activation_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n conv2d_51 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">3,200</span>  concatenate_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n conv2d_59 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span>  activation_29[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n conv2d_58 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">3,200</span>  concatenate_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n add_38 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv2d_52[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       \n                                                                    conv2d_51[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n\n add_42 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv2d_59[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       \n                                                                    conv2d_58[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n\n activation_26              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_38[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_30              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_42[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n conv2d_53 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span>  activation_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n conv2d_54 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span>  attention_block_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n conv2d_60 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span>  activation_30[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n conv2d_61 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span>  attention_block_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n batch_normalization_38     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_53[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_39     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_54[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_46     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_60[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_47     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_61[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n add_39 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_3 \n                                                                    batch_normalization_3 \n\n add_43 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_4 \n                                                                    batch_normalization_4 \n\n activation_27              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_39[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_31              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_43[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  activation_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  activation_31[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n attention_block_2          [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>),           <span style=\"color: #00af00; text-decoration-color: #00af00\">40,324</span>  dropout_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AttentionBlock</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)]                    dropout_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n\n conv2d_64 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  attention_block_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n depthwise_conv2d_90        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">3,200</span>  attention_block_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_91        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">6,272</span>  attention_block_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n conv2d_70 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  attention_block_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n depthwise_conv2d_95        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">3,200</span>  attention_block_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_96        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">6,272</span>  attention_block_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n batch_normalization_49     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_64[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_50     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  depthwise_conv2d_90[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_51     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  depthwise_conv2d_91[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_56     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_70[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_57     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  depthwise_conv2d_95[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_58     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  depthwise_conv2d_96[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n concatenate_21             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_4 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      batch_normalization_5 \n                                                                    batch_normalization_5 \n\n concatenate_23             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_5 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      batch_normalization_5 \n                                                                    batch_normalization_5 \n\n conv2d_65 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">3,072</span>  concatenate_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n conv2d_71 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">3,072</span>  concatenate_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n batch_normalization_52     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_65[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_59     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_71[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_5 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n\n global_max_pooling2d_12    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_5 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling2D</span>)                                                                      \n\n global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_5 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n\n global_max_pooling2d_13    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_5 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling2D</span>)                                                                      \n\n add_51 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n add_55 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                        <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  add_51[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n\n dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                        <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  add_55[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n\n dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  dense_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n dense_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  dense_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n conv2d_66 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">16,384</span>  attention_block_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n reshape_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n conv2d_72 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">16,384</span>  attention_block_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n reshape_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n batch_normalization_53     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_66[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n multiply_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_5 \n                                                                    reshape_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n\n batch_normalization_60     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_72[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n multiply_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_5 \n                                                                    reshape_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n\n add_52 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_5 \n                                                                    multiply_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n add_56 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_6 \n                                                                    multiply_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n activation_32              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_52[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_36              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_56[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n batch_normalization_54     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  activation_32[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_61     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  activation_36[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n activation_33              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_5 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_37              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_6 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n depthwise_conv2d_92        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  activation_33[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_93        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">3,328</span>  activation_33[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_94        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">6,400</span>  activation_33[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_97        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  activation_37[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_98        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">3,328</span>  activation_37[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_99        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">6,400</span>  activation_37[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n concatenate_22             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  depthwise_conv2d_92[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      depthwise_conv2d_93[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n                                                                    depthwise_conv2d_94[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n concatenate_24             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  depthwise_conv2d_97[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      depthwise_conv2d_98[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n                                                                    depthwise_conv2d_99[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n conv2d_68 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span>  activation_33[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n conv2d_67 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">3,200</span>  concatenate_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n conv2d_74 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span>  activation_37[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n conv2d_73 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">3,200</span>  concatenate_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n add_53 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv2d_68[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       \n                                                                    conv2d_67[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n\n add_57 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv2d_74[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       \n                                                                    conv2d_73[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n\n activation_34              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_53[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_38              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_57[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n conv2d_69 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span>  activation_34[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n conv2d_75 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span>  activation_38[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n batch_normalization_55     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_69[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_62     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv2d_75[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n add_54 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_5 \n                                                                    attention_block_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n add_58 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_6 \n                                                                    attention_block_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n activation_35              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_54[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_39              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_58[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  activation_35[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n dropout_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  activation_39[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n attention_block_3          [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>),           <span style=\"color: #00af00; text-decoration-color: #00af00\">40,324</span>  dropout_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AttentionBlock</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)]                    dropout_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n conv2d_90 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  attention_block_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n depthwise_conv2d_132       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">3,200</span>  attention_block_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_133       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">6,272</span>  attention_block_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n conv2d_97 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  attention_block_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n depthwise_conv2d_137       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">3,200</span>  attention_block_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_138       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">6,272</span>  attention_block_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n batch_normalization_64     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_90[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_65     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  depthwise_conv2d_132[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_66     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  depthwise_conv2d_133[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_72     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_97[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_73     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  depthwise_conv2d_137[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_74     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  depthwise_conv2d_138[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n concatenate_30             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_6 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      batch_normalization_6 \n                                                                    batch_normalization_6 \n\n concatenate_32             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_7 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      batch_normalization_7 \n                                                                    batch_normalization_7 \n\n conv2d_91 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">8,192</span>  concatenate_30[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n conv2d_98 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">8,192</span>  concatenate_32[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n batch_normalization_67     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_91[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_75     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_98[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_6 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n\n global_max_pooling2d_16    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_6 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling2D</span>)                                                                      \n\n global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_7 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n\n global_max_pooling2d_17    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_7 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling2D</span>)                                                                      \n\n add_70 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n add_74 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n dense_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                       <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span>  add_70[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n\n dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                       <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span>  add_74[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n\n dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span>  dense_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n dense_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span>  dense_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n conv2d_92 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">32,768</span>  attention_block_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n reshape_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n conv2d_99 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">32,768</span>  attention_block_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n reshape_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n batch_normalization_68     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_92[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n multiply_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_6 \n                                                                    reshape_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n\n batch_normalization_76     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_99[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n multiply_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_7 \n                                                                    reshape_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n\n add_71 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_6 \n                                                                    multiply_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n add_75 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_7 \n                                                                    multiply_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n activation_44              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_71[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_48              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_75[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n batch_normalization_69     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  activation_44[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_77     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  activation_48[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n activation_45              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_6 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_49              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_7 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n depthwise_conv2d_134       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  activation_45[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_135       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">6,656</span>  activation_45[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_136       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">12,800</span>  activation_45[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_139       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  activation_49[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_140       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">6,656</span>  activation_49[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_141       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">12,800</span>  activation_49[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n concatenate_31             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  depthwise_conv2d_134[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      depthwise_conv2d_135[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n                                                                    depthwise_conv2d_136[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n concatenate_33             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  depthwise_conv2d_139[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      depthwise_conv2d_140[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n                                                                    depthwise_conv2d_141[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n conv2d_94 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span>  activation_45[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n conv2d_93 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">12,544</span>  concatenate_31[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n conv2d_101 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span>  activation_49[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n conv2d_100 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">12,544</span>  concatenate_33[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n add_72 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv2d_94[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       \n                                                                    conv2d_93[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n\n add_76 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv2d_101[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n                                                                    conv2d_100[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n activation_46              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_72[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_50              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_76[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n conv2d_95 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span>  activation_46[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n conv2d_96 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span>  attention_block_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n conv2d_102 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span>  activation_50[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n conv2d_103 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span>  attention_block_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n batch_normalization_70     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_95[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_71     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_96[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_78     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_102[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_79     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_103[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n add_73 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_7 \n                                                                    batch_normalization_7 \n\n add_77 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_7 \n                                                                    batch_normalization_7 \n\n activation_47              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_73[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_51              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_77[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n dropout_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  activation_47[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n dropout_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  activation_51[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n attention_block_4          [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),            <span style=\"color: #00af00; text-decoration-color: #00af00\">146,180</span>  dropout_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AttentionBlock</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]                      dropout_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n conv2d_106 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span>  attention_block_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n depthwise_conv2d_150       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">6,400</span>  attention_block_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_151       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">12,544</span>  attention_block_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n conv2d_112 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span>  attention_block_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n depthwise_conv2d_155       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">6,400</span>  attention_block_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_156       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">12,544</span>  attention_block_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n batch_normalization_81     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_106[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_82     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  depthwise_conv2d_150[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_83     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  depthwise_conv2d_151[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_88     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_112[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_89     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  depthwise_conv2d_155[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_90     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  depthwise_conv2d_156[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n concatenate_35             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_8 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      batch_normalization_8 \n                                                                    batch_normalization_8 \n\n concatenate_37             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_8 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      batch_normalization_8 \n                                                                    batch_normalization_9 \n\n conv2d_107 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">12,288</span>  concatenate_35[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n conv2d_113 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">12,288</span>  concatenate_37[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n batch_normalization_84     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_107[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_91     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_113[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_8 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n\n global_max_pooling2d_20    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_8 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling2D</span>)                                                                      \n\n global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_9 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n\n global_max_pooling2d_21    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_9 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling2D</span>)                                                                      \n\n add_85 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n add_89 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n dense_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                       <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span>  add_85[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n\n dense_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                       <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span>  add_89[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n\n dense_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span>  dense_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n dense_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span>  dense_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n conv2d_108 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">65,536</span>  attention_block_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n reshape_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n conv2d_114 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">65,536</span>  attention_block_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n reshape_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_28[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n batch_normalization_85     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_108[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n multiply_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_8 \n                                                                    reshape_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n batch_normalization_92     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_114[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n multiply_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_9 \n                                                                    reshape_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n add_86 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_8 \n                                                                    multiply_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n\n add_90 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_9 \n                                                                    multiply_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n\n activation_52              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_86[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_56              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_90[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n batch_normalization_86     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  activation_52[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_93     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  activation_56[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n activation_53              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_8 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_57              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_9 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n depthwise_conv2d_152       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  activation_53[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_153       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">6,656</span>  activation_53[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_154       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">12,800</span>  activation_53[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_157       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  activation_57[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_158       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">6,656</span>  activation_57[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_159       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">12,800</span>  activation_57[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n concatenate_36             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  depthwise_conv2d_152[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      depthwise_conv2d_153[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n                                                                    depthwise_conv2d_154[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n concatenate_38             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  depthwise_conv2d_157[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      depthwise_conv2d_158[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n                                                                    depthwise_conv2d_159[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n conv2d_110 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span>  activation_53[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n conv2d_109 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">12,544</span>  concatenate_36[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n conv2d_116 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span>  activation_57[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n conv2d_115 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">12,544</span>  concatenate_38[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n add_87 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv2d_110[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n                                                                    conv2d_109[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n add_91 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv2d_116[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n                                                                    conv2d_115[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n activation_54              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_87[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_58              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_91[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n conv2d_111 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span>  activation_54[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n conv2d_117 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span>  activation_58[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n batch_normalization_87     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_111[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_94     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  conv2d_117[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n add_88 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_8 \n                                                                    attention_block_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n add_92 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_9 \n                                                                    attention_block_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n activation_55              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_88[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_59              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_92[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n dropout_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  activation_55[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n dropout_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  activation_59[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n attention_block_5          [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),            <span style=\"color: #00af00; text-decoration-color: #00af00\">146,180</span>  dropout_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AttentionBlock</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]                      dropout_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n conv2d_126 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">8,192</span>  attention_block_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n depthwise_conv2d_180       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">6,400</span>  attention_block_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_181       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">12,544</span>  attention_block_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n conv2d_133 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">8,192</span>  attention_block_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n depthwise_conv2d_185       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">6,400</span>  attention_block_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_186       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">12,544</span>  attention_block_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n batch_normalization_96     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  conv2d_126[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_97     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  depthwise_conv2d_180[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_98     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  depthwise_conv2d_181[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_104    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  conv2d_133[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_105    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  depthwise_conv2d_185[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_106    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  depthwise_conv2d_186[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n concatenate_42             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_9 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      batch_normalization_9 \n                                                                    batch_normalization_9 \n\n concatenate_44             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      batch_normalization_1 \n                                                                    batch_normalization_1 \n\n conv2d_127 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">32,768</span>  concatenate_42[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n conv2d_134 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">32,768</span>  concatenate_44[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n batch_normalization_99     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  conv2d_127[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_107    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  conv2d_134[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_9 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n\n global_max_pooling2d_24    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_9 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling2D</span>)                                                                      \n\n global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n\n global_max_pooling2d_25    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling2D</span>)                                                                      \n\n add_102 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n add_106 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n dense_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">16,384</span>  add_102[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n\n dense_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">16,384</span>  add_106[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n\n dense_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,384</span>  dense_30[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n dense_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,384</span>  dense_32[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n conv2d_128 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">131,072</span>  attention_block_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n reshape_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_31[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n conv2d_135 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">131,072</span>  attention_block_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n reshape_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_33[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n batch_normalization_100    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  conv2d_128[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n multiply_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_9 \n                                                                    reshape_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n batch_normalization_108    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  conv2d_135[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n multiply_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n                                                                    reshape_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n add_103 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n                                                                    multiply_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n\n add_107 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n                                                                    multiply_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n\n activation_62              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_103[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_66              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_107[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n batch_normalization_101    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  activation_62[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_109    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  activation_66[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n activation_63              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_67              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n depthwise_conv2d_182       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  activation_63[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_183       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">13,312</span>  activation_63[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_184       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">25,600</span>  activation_63[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_187       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  activation_67[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_188       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">13,312</span>  activation_67[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_189       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">25,600</span>  activation_67[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n concatenate_43             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1536</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  depthwise_conv2d_182[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      depthwise_conv2d_183[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n                                                                    depthwise_conv2d_184[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n concatenate_45             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1536</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  depthwise_conv2d_187[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      depthwise_conv2d_188[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n                                                                    depthwise_conv2d_189[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n conv2d_130 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span>  activation_63[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n conv2d_129 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">49,664</span>  concatenate_43[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n conv2d_137 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span>  activation_67[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n conv2d_136 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">49,664</span>  concatenate_45[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n add_104 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv2d_130[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n                                                                    conv2d_129[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n add_108 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv2d_137[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n                                                                    conv2d_136[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n activation_64              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_104[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_68              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_108[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n conv2d_131 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span>  activation_64[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n conv2d_132 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span>  attention_block_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n conv2d_138 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span>  activation_68[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n conv2d_139 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span>  attention_block_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n batch_normalization_102    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  conv2d_131[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_103    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  conv2d_132[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_110    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  conv2d_138[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_111    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  conv2d_139[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n add_105 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n                                                                    batch_normalization_1 \n\n add_109 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n                                                                    batch_normalization_1 \n\n activation_65              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_105[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_69              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_109[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n attention_block_6          [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>),            <span style=\"color: #00af00; text-decoration-color: #00af00\">554,500</span>  activation_65[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AttentionBlock</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)]                      activation_69[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n conv2d_142 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">16,384</span>  attention_block_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n depthwise_conv2d_198       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">12,800</span>  attention_block_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_199       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">25,088</span>  attention_block_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n conv2d_148 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">16,384</span>  attention_block_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n depthwise_conv2d_203       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">12,800</span>  attention_block_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_204       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">25,088</span>  attention_block_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n conv2d_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span>  attention_block_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n depthwise_conv2d_36        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span>  attention_block_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_37        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">1,664</span>  attention_block_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_38        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span>  attention_block_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n batch_normalization_113    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  conv2d_142[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_114    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  depthwise_conv2d_198[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_115    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  depthwise_conv2d_199[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_120    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  conv2d_148[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_121    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  depthwise_conv2d_203[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_122    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  depthwise_conv2d_204[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n conv2d_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span>  attention_block_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n depthwise_conv2d_42        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span>  attention_block_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_43        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">1,664</span>  attention_block_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_44        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span>  attention_block_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n concatenate_10             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv2d_30[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      depthwise_conv2d_36[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n                                                                    depthwise_conv2d_37[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n                                                                    depthwise_conv2d_38[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n concatenate_47             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1536</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      batch_normalization_1 \n                                                                    batch_normalization_1 \n\n concatenate_49             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1536</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      batch_normalization_1 \n                                                                    batch_normalization_1 \n\n concatenate_11             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv2d_33[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      depthwise_conv2d_42[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n                                                                    depthwise_conv2d_43[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n                                                                    depthwise_conv2d_44[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n lambda (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  concatenate_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n conv2d_143 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">49,152</span>  concatenate_47[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n conv2d_149 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">49,152</span>  concatenate_49[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n lambda_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  concatenate_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n depthwise_conv2d_39        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">2,240</span>  lambda[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n batch_normalization_116    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  conv2d_143[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_123    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  conv2d_149[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n depthwise_conv2d_45        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">2,240</span>  lambda_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n conv2d_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span>  attention_block_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n conv2d_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">28,800</span>  depthwise_conv2d_39[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n\n global_max_pooling2d_28    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling2D</span>)                                                                      \n\n global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n\n global_max_pooling2d_29    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling2D</span>)                                                                      \n\n conv2d_35 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span>  attention_block_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n conv2d_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">28,800</span>  depthwise_conv2d_45[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n depthwise_conv2d_41        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280</span>  conv2d_32[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_40        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280</span>  conv2d_31[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n add_117 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n add_121 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_poolin \n                                                                    global_max_pooling2d_ \n\n depthwise_conv2d_47        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280</span>  conv2d_35[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_46        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280</span>  conv2d_34[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n add_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  depthwise_conv2d_41[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n                                                                    depthwise_conv2d_40[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n dense_35 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">16,384</span>  add_117[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n\n dense_37 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">16,384</span>  add_121[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n\n add_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  depthwise_conv2d_47[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n                                                                    depthwise_conv2d_46[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n activation_18              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_30[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n dense_36 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,384</span>  dense_35[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n dense_38 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,384</span>  dense_37[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n activation_19              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_31[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n conv2d_36 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span>  activation_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n depthwise_conv2d_48        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280</span>  activation_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_49        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">3,328</span>  activation_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_50        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280</span>  activation_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n conv2d_78 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span>  attention_block_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n depthwise_conv2d_108       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280</span>  attention_block_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_109       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">3,328</span>  attention_block_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_110       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280</span>  attention_block_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n conv2d_144 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">262,144</span>  attention_block_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n reshape_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_36[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n conv2d_150 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">262,144</span>  attention_block_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n reshape_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_38[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n\n conv2d_39 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span>  activation_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n depthwise_conv2d_54        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280</span>  activation_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_55        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">3,328</span>  activation_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_56        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280</span>  activation_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n conv2d_81 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span>  attention_block_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n depthwise_conv2d_114       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280</span>  attention_block_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_115       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">3,328</span>  attention_block_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_116       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280</span>  attention_block_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n concatenate_12             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv2d_36[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      depthwise_conv2d_48[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n                                                                    depthwise_conv2d_49[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n                                                                    depthwise_conv2d_50[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n concatenate_26             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv2d_78[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      depthwise_conv2d_108[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n                                                                    depthwise_conv2d_109[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n                                                                    depthwise_conv2d_110[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n batch_normalization_117    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  conv2d_144[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n multiply_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n                                                                    reshape_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n batch_normalization_124    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  conv2d_150[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n multiply_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n                                                                    reshape_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n concatenate_13             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv2d_39[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      depthwise_conv2d_54[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n                                                                    depthwise_conv2d_55[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n                                                                    depthwise_conv2d_56[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n concatenate_27             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv2d_81[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      depthwise_conv2d_114[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n                                                                    depthwise_conv2d_115[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n                                                                    depthwise_conv2d_116[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n lambda_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  concatenate_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n lambda_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  concatenate_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n add_118 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n                                                                    multiply_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n\n add_122 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n                                                                    multiply_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n\n lambda_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  concatenate_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n lambda_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  concatenate_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n depthwise_conv2d_51        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">4,480</span>  lambda_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_111       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">4,480</span>  lambda_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n activation_70              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_118[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_74              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_122[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n depthwise_conv2d_57        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">4,480</span>  lambda_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_117       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">4,480</span>  lambda_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n conv2d_38 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span>  activation_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n conv2d_37 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">114,944</span>  depthwise_conv2d_51[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n conv2d_80 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span>  attention_block_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n conv2d_79 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">114,944</span>  depthwise_conv2d_111[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n batch_normalization_118    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  activation_70[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_125    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  activation_74[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n conv2d_41 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span>  activation_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n conv2d_40 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">114,944</span>  depthwise_conv2d_57[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n conv2d_83 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span>  attention_block_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n conv2d_82 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">114,944</span>  depthwise_conv2d_117[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n depthwise_conv2d_53        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560</span>  conv2d_38[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_52        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560</span>  conv2d_37[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_113       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560</span>  conv2d_80[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_112       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560</span>  conv2d_79[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n activation_71              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_75              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n depthwise_conv2d_59        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560</span>  conv2d_41[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_58        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560</span>  conv2d_40[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_119       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560</span>  conv2d_83[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_118       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560</span>  conv2d_82[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n add_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  depthwise_conv2d_53[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n                                                                    depthwise_conv2d_52[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n add_66 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  depthwise_conv2d_113[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n                                                                    depthwise_conv2d_112[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n depthwise_conv2d_200       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  activation_71[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_201       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">13,312</span>  activation_71[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_202       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">25,600</span>  activation_71[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_205       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  activation_75[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_206       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">13,312</span>  activation_75[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_207       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">25,600</span>  activation_75[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n add_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  depthwise_conv2d_59[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n                                                                    depthwise_conv2d_58[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n add_67 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  depthwise_conv2d_119[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n                                                                    depthwise_conv2d_118[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n activation_20              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_32[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_40              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_66[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n concatenate_48             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1536</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  depthwise_conv2d_200[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      depthwise_conv2d_201[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n                                                                    depthwise_conv2d_202[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n concatenate_50             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1536</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  depthwise_conv2d_205[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      depthwise_conv2d_206[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n                                                                    depthwise_conv2d_207[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n activation_21              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_33[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_41              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_67[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n conv2d_42 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span>  activation_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n depthwise_conv2d_60        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560</span>  activation_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_61        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">6,656</span>  activation_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_62        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560</span>  activation_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n conv2d_84 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span>  activation_40[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n depthwise_conv2d_120       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560</span>  activation_40[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_121       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">6,656</span>  activation_40[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_122       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560</span>  activation_40[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n conv2d_120 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span>  attention_block_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n depthwise_conv2d_168       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560</span>  attention_block_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_169       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">6,656</span>  attention_block_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_170       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560</span>  attention_block_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n conv2d_146 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span>  activation_71[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n conv2d_145 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">49,664</span>  concatenate_48[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n conv2d_152 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span>  activation_75[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n conv2d_151 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">49,664</span>  concatenate_50[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n conv2d_45 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span>  activation_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n depthwise_conv2d_66        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560</span>  activation_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_67        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">6,656</span>  activation_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_68        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560</span>  activation_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n conv2d_87 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span>  activation_41[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n depthwise_conv2d_126       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560</span>  activation_41[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_127       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">6,656</span>  activation_41[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_128       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560</span>  activation_41[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n conv2d_123 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span>  attention_block_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n depthwise_conv2d_174       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560</span>  attention_block_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_175       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">6,656</span>  attention_block_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_176       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560</span>  attention_block_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n concatenate_14             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv2d_42[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      depthwise_conv2d_60[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n                                                                    depthwise_conv2d_61[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n                                                                    depthwise_conv2d_62[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n concatenate_28             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv2d_84[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      depthwise_conv2d_120[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n                                                                    depthwise_conv2d_121[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n                                                                    depthwise_conv2d_122[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n concatenate_40             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv2d_120[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      depthwise_conv2d_168[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n                                                                    depthwise_conv2d_169[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n                                                                    depthwise_conv2d_170[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n add_119 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv2d_146[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n                                                                    conv2d_145[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n add_123 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv2d_152[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n                                                                    conv2d_151[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n concatenate_15             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv2d_45[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      depthwise_conv2d_66[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n                                                                    depthwise_conv2d_67[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n                                                                    depthwise_conv2d_68[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n concatenate_29             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv2d_87[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      depthwise_conv2d_126[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n                                                                    depthwise_conv2d_127[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n                                                                    depthwise_conv2d_128[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n concatenate_41             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv2d_123[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      depthwise_conv2d_174[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n                                                                    depthwise_conv2d_175[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n                                                                    depthwise_conv2d_176[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n lambda_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  concatenate_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n lambda_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  concatenate_28[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n lambda_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  concatenate_40[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n activation_72              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_119[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_76              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_123[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n lambda_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  concatenate_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n lambda_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  concatenate_29[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n lambda_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  concatenate_41[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n depthwise_conv2d_63        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">8,960</span>  lambda_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_123       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">8,960</span>  lambda_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_171       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">8,960</span>  lambda_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n conv2d_147 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span>  activation_72[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n conv2d_153 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span>  activation_76[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n depthwise_conv2d_69        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">8,960</span>  lambda_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_129       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">8,960</span>  lambda_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_177       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">8,960</span>  lambda_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n conv2d_44 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span>  activation_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n conv2d_43 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">459,264</span>  depthwise_conv2d_63[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n conv2d_86 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span>  activation_40[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n conv2d_85 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">459,264</span>  depthwise_conv2d_123[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n conv2d_122 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span>  attention_block_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n conv2d_121 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">459,264</span>  depthwise_conv2d_171[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n batch_normalization_119    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  conv2d_147[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n batch_normalization_126    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  conv2d_153[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n\n conv2d_47 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span>  activation_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n conv2d_46 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">459,264</span>  depthwise_conv2d_69[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n conv2d_89 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span>  activation_41[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n conv2d_88 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">459,264</span>  depthwise_conv2d_129[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n conv2d_125 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span>  attention_block_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n conv2d_124 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">459,264</span>  depthwise_conv2d_177[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n depthwise_conv2d_65        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">5,120</span>  conv2d_44[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_64        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">5,120</span>  conv2d_43[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_125       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">5,120</span>  conv2d_86[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_124       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">5,120</span>  conv2d_85[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_173       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">5,120</span>  conv2d_122[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_172       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">5,120</span>  conv2d_121[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n add_120 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n                                                                    attention_block_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n add_124 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n                                                                    attention_block_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n depthwise_conv2d_71        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">5,120</span>  conv2d_47[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_70        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">5,120</span>  conv2d_46[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_131       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">5,120</span>  conv2d_89[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_130       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">5,120</span>  conv2d_88[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_179       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">5,120</span>  conv2d_125[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n depthwise_conv2d_178       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">5,120</span>  conv2d_124[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)                                                                         \n\n add_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  depthwise_conv2d_65[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n                                                                    depthwise_conv2d_64[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n add_68 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  depthwise_conv2d_125[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n                                                                    depthwise_conv2d_124[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n add_100 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  depthwise_conv2d_173[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n                                                                    depthwise_conv2d_172[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n activation_73              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_120[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_77              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_124[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n add_35 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  depthwise_conv2d_71[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n                                                                    depthwise_conv2d_70[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n add_69 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  depthwise_conv2d_131[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n                                                                    depthwise_conv2d_130[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n add_101 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  depthwise_conv2d_179[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n                                                                    depthwise_conv2d_178[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n activation_22              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_34[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_42              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_68[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_60              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_100[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n attention_block_7          [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>),            <span style=\"color: #00af00; text-decoration-color: #00af00\">554,500</span>  activation_73[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AttentionBlock</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)]                      activation_77[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n activation_23              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_35[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_43              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_69[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n activation_61              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_101[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                                                                              \n\n concatenate_52             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  activation_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      activation_42[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   \n                                                                    activation_60[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   \n                                                                    attention_block_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n concatenate_53             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  activation_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      activation_43[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   \n                                                                    activation_61[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   \n                                                                    attention_block_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n\n dropout_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  concatenate_52[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n dropout_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  concatenate_53[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n concatenate_54             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dropout_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      dropout_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n dropout_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  concatenate_54[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n global_average_pooling2d  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dropout_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n\n dense_40 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)                       <span style=\"color: #00af00; text-decoration-color: #00af00\">20,485</span>  global_average_poolin \n\n dense_41 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)                       <span style=\"color: #00af00; text-decoration-color: #00af00\">28,679</span>  global_average_poolin \n\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m11,818,604\u001b[0m (45.08 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,818,604</span> (45.08 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m11,760,748\u001b[0m (44.86 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,760,748</span> (44.86 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m57,856\u001b[0m (226.00 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">57,856</span> (226.00 KB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"None\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\n\ninitial_gamma = 0.5\nlearning_rate = 1e-2\noptimizer = Adam(learning_rate=0.001)\n#opt = Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.9, epsilon=None, amsgrad=False)\n# Compile the model with the custom optimizer\nmodel.compile(optimizer=optimizer,\n              loss=['categorical_crossentropy', 'categorical_crossentropy'],\n              loss_weights=[initial_gamma, (1 -  initial_gamma)],\n              metrics=['accuracy', 'accuracy'])\n\n\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\ndef checkpoint_callback():\n\n    checkpoint_filepath = 'best1_model_cer_skin_lung.keras'\n\n    model_checkpoint_callback= ModelCheckpoint(filepath=checkpoint_filepath,\n                           save_weights_only=False,\n                           #frequency='epoch',\n                           monitor='val_loss',\n                           save_best_only=True,\n                            mode='min',\n                           verbose=1)\n\n    return model_checkpoint_callback\n\ndef early_stopping(patience):\n    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', restore_best_weights=True, patience=60, verbose=1)\n    return es_callback\n\n\n\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n                              patience=50, verbose = 1, min_lr=0.00001)\n\ncheckpoint_callback = checkpoint_callback()\n\nearly_stopping = early_stopping(patience=100)\ncallbacks = [checkpoint_callback, early_stopping, reduce_lr]\n            \n\n# Fit the model with callbacks\nhistory = model.fit([X_train_s, X_train_h], [y_train_s, y_train_h],\n                    epochs=200,\n                    validation_data=([X_val_s, X_val_h1], [y_val_s, y_val_h1]), verbose=1,\n                    shuffle=True,\n                    callbacks=callbacks) # UpdateGammaCallback","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T00:25:15.885382Z","iopub.execute_input":"2025-03-08T00:25:15.885706Z","iopub.status.idle":"2025-03-08T05:11:14.282487Z","shell.execute_reply.started":"2025-03-08T00:25:15.885673Z","shell.execute_reply":"2025-03-08T05:11:14.281694Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - dense_40_accuracy: 0.4957 - dense_40_loss: 0.6073 - dense_41_accuracy: 0.6478 - dense_41_loss: 0.5539 - loss: 1.1612   \nEpoch 1: val_loss improved from inf to 1.54840, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m658s\u001b[0m 1s/step - dense_40_accuracy: 0.4962 - dense_40_loss: 0.6068 - dense_41_accuracy: 0.6478 - dense_41_loss: 0.5537 - loss: 1.1604 - val_dense_40_accuracy: 0.5679 - val_dense_40_loss: 0.5090 - val_dense_41_accuracy: 0.0802 - val_dense_41_loss: 1.0405 - val_loss: 1.5484 - learning_rate: 0.0010\nEpoch 2/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 325ms/step - dense_40_accuracy: 0.7673 - dense_40_loss: 0.3229 - dense_41_accuracy: 0.6716 - dense_41_loss: 0.4516 - loss: 0.7745\nEpoch 2: val_loss improved from 1.54840 to 1.34494, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 345ms/step - dense_40_accuracy: 0.7674 - dense_40_loss: 0.3228 - dense_41_accuracy: 0.6716 - dense_41_loss: 0.4516 - loss: 0.7744 - val_dense_40_accuracy: 0.7253 - val_dense_40_loss: 0.3808 - val_dense_41_accuracy: 0.3827 - val_dense_41_loss: 0.9653 - val_loss: 1.3449 - learning_rate: 0.0010\nEpoch 3/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 328ms/step - dense_40_accuracy: 0.8225 - dense_40_loss: 0.2493 - dense_41_accuracy: 0.6921 - dense_41_loss: 0.4280 - loss: 0.6772\nEpoch 3: val_loss did not improve from 1.34494\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 334ms/step - dense_40_accuracy: 0.8225 - dense_40_loss: 0.2493 - dense_41_accuracy: 0.6921 - dense_41_loss: 0.4279 - loss: 0.6772 - val_dense_40_accuracy: 0.7855 - val_dense_40_loss: 0.3116 - val_dense_41_accuracy: 0.2670 - val_dense_41_loss: 1.1453 - val_loss: 1.4623 - learning_rate: 0.0010\nEpoch 4/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 335ms/step - dense_40_accuracy: 0.8490 - dense_40_loss: 0.2096 - dense_41_accuracy: 0.6994 - dense_41_loss: 0.4056 - loss: 0.6152\nEpoch 4: val_loss improved from 1.34494 to 1.04644, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 355ms/step - dense_40_accuracy: 0.8490 - dense_40_loss: 0.2096 - dense_41_accuracy: 0.6994 - dense_41_loss: 0.4056 - loss: 0.6152 - val_dense_40_accuracy: 0.8302 - val_dense_40_loss: 0.2794 - val_dense_41_accuracy: 0.5000 - val_dense_41_loss: 0.7711 - val_loss: 1.0464 - learning_rate: 0.0010\nEpoch 5/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 317ms/step - dense_40_accuracy: 0.8797 - dense_40_loss: 0.1752 - dense_41_accuracy: 0.7047 - dense_41_loss: 0.3971 - loss: 0.5723\nEpoch 5: val_loss improved from 1.04644 to 0.63407, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 337ms/step - dense_40_accuracy: 0.8798 - dense_40_loss: 0.1752 - dense_41_accuracy: 0.7048 - dense_41_loss: 0.3970 - loss: 0.5722 - val_dense_40_accuracy: 0.8488 - val_dense_40_loss: 0.1917 - val_dense_41_accuracy: 0.6852 - val_dense_41_loss: 0.4364 - val_loss: 0.6341 - learning_rate: 0.0010\nEpoch 6/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 319ms/step - dense_40_accuracy: 0.9078 - dense_40_loss: 0.1385 - dense_41_accuracy: 0.7232 - dense_41_loss: 0.3825 - loss: 0.5210\nEpoch 6: val_loss did not improve from 0.63407\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 326ms/step - dense_40_accuracy: 0.9078 - dense_40_loss: 0.1385 - dense_41_accuracy: 0.7232 - dense_41_loss: 0.3825 - loss: 0.5209 - val_dense_40_accuracy: 0.6312 - val_dense_40_loss: 0.7605 - val_dense_41_accuracy: 0.5077 - val_dense_41_loss: 0.6987 - val_loss: 1.4338 - learning_rate: 0.0010\nEpoch 7/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - dense_40_accuracy: 0.9195 - dense_40_loss: 0.1186 - dense_41_accuracy: 0.7243 - dense_41_loss: 0.3730 - loss: 0.4915\nEpoch 7: val_loss did not improve from 0.63407\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 324ms/step - dense_40_accuracy: 0.9195 - dense_40_loss: 0.1185 - dense_41_accuracy: 0.7243 - dense_41_loss: 0.3730 - loss: 0.4915 - val_dense_40_accuracy: 0.7485 - val_dense_40_loss: 0.6118 - val_dense_41_accuracy: 0.6451 - val_dense_41_loss: 0.5058 - val_loss: 1.0868 - learning_rate: 0.0010\nEpoch 8/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 327ms/step - dense_40_accuracy: 0.9224 - dense_40_loss: 0.1080 - dense_41_accuracy: 0.7454 - dense_41_loss: 0.3432 - loss: 0.4513\nEpoch 8: val_loss did not improve from 0.63407\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 334ms/step - dense_40_accuracy: 0.9225 - dense_40_loss: 0.1080 - dense_41_accuracy: 0.7454 - dense_41_loss: 0.3433 - loss: 0.4513 - val_dense_40_accuracy: 0.8827 - val_dense_40_loss: 0.2226 - val_dense_41_accuracy: 0.4630 - val_dense_41_loss: 0.8425 - val_loss: 1.0790 - learning_rate: 0.0010\nEpoch 9/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 327ms/step - dense_40_accuracy: 0.9299 - dense_40_loss: 0.0989 - dense_41_accuracy: 0.7474 - dense_41_loss: 0.3490 - loss: 0.4478\nEpoch 9: val_loss did not improve from 0.63407\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 333ms/step - dense_40_accuracy: 0.9300 - dense_40_loss: 0.0988 - dense_41_accuracy: 0.7474 - dense_41_loss: 0.3490 - loss: 0.4478 - val_dense_40_accuracy: 0.7361 - val_dense_40_loss: 0.5624 - val_dense_41_accuracy: 0.6867 - val_dense_41_loss: 0.4264 - val_loss: 0.9719 - learning_rate: 0.0010\nEpoch 10/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 329ms/step - dense_40_accuracy: 0.9433 - dense_40_loss: 0.0838 - dense_41_accuracy: 0.7557 - dense_41_loss: 0.3375 - loss: 0.4213\nEpoch 10: val_loss improved from 0.63407 to 0.47327, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 349ms/step - dense_40_accuracy: 0.9433 - dense_40_loss: 0.0838 - dense_41_accuracy: 0.7557 - dense_41_loss: 0.3375 - loss: 0.4213 - val_dense_40_accuracy: 0.9198 - val_dense_40_loss: 0.1189 - val_dense_41_accuracy: 0.7361 - val_dense_41_loss: 0.3545 - val_loss: 0.4733 - learning_rate: 0.0010\nEpoch 11/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 319ms/step - dense_40_accuracy: 0.9489 - dense_40_loss: 0.0697 - dense_41_accuracy: 0.7510 - dense_41_loss: 0.3302 - loss: 0.3999\nEpoch 11: val_loss did not improve from 0.47327\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 326ms/step - dense_40_accuracy: 0.9490 - dense_40_loss: 0.0697 - dense_41_accuracy: 0.7510 - dense_41_loss: 0.3302 - loss: 0.3999 - val_dense_40_accuracy: 0.6528 - val_dense_40_loss: 0.8417 - val_dense_41_accuracy: 0.5602 - val_dense_41_loss: 0.6007 - val_loss: 1.4509 - learning_rate: 0.0010\nEpoch 12/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 327ms/step - dense_40_accuracy: 0.9501 - dense_40_loss: 0.0690 - dense_41_accuracy: 0.7715 - dense_41_loss: 0.3098 - loss: 0.3788\nEpoch 12: val_loss did not improve from 0.47327\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 334ms/step - dense_40_accuracy: 0.9501 - dense_40_loss: 0.0690 - dense_41_accuracy: 0.7714 - dense_41_loss: 0.3099 - loss: 0.3789 - val_dense_40_accuracy: 0.8981 - val_dense_40_loss: 0.1829 - val_dense_41_accuracy: 0.6790 - val_dense_41_loss: 0.4800 - val_loss: 0.6716 - learning_rate: 0.0010\nEpoch 13/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 331ms/step - dense_40_accuracy: 0.9647 - dense_40_loss: 0.0521 - dense_41_accuracy: 0.7683 - dense_41_loss: 0.3081 - loss: 0.3602\nEpoch 13: val_loss did not improve from 0.47327\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 338ms/step - dense_40_accuracy: 0.9647 - dense_40_loss: 0.0521 - dense_41_accuracy: 0.7683 - dense_41_loss: 0.3081 - loss: 0.3602 - val_dense_40_accuracy: 0.7593 - val_dense_40_loss: 0.5274 - val_dense_41_accuracy: 0.7068 - val_dense_41_loss: 0.4305 - val_loss: 0.9465 - learning_rate: 0.0010\nEpoch 14/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 324ms/step - dense_40_accuracy: 0.9610 - dense_40_loss: 0.0555 - dense_41_accuracy: 0.7852 - dense_41_loss: 0.2890 - loss: 0.3445\nEpoch 14: val_loss did not improve from 0.47327\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 330ms/step - dense_40_accuracy: 0.9610 - dense_40_loss: 0.0555 - dense_41_accuracy: 0.7851 - dense_41_loss: 0.2891 - loss: 0.3445 - val_dense_40_accuracy: 0.7299 - val_dense_40_loss: 0.5551 - val_dense_41_accuracy: 0.4892 - val_dense_41_loss: 0.7702 - val_loss: 1.3305 - learning_rate: 0.0010\nEpoch 15/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - dense_40_accuracy: 0.9570 - dense_40_loss: 0.0612 - dense_41_accuracy: 0.7775 - dense_41_loss: 0.2991 - loss: 0.3603\nEpoch 15: val_loss did not improve from 0.47327\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 324ms/step - dense_40_accuracy: 0.9570 - dense_40_loss: 0.0612 - dense_41_accuracy: 0.7775 - dense_41_loss: 0.2991 - loss: 0.3603 - val_dense_40_accuracy: 0.9244 - val_dense_40_loss: 0.1330 - val_dense_41_accuracy: 0.6898 - val_dense_41_loss: 0.4784 - val_loss: 0.6130 - learning_rate: 0.0010\nEpoch 16/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 319ms/step - dense_40_accuracy: 0.9704 - dense_40_loss: 0.0453 - dense_41_accuracy: 0.7842 - dense_41_loss: 0.2921 - loss: 0.3375\nEpoch 16: val_loss improved from 0.47327 to 0.46665, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 339ms/step - dense_40_accuracy: 0.9704 - dense_40_loss: 0.0453 - dense_41_accuracy: 0.7842 - dense_41_loss: 0.2922 - loss: 0.3375 - val_dense_40_accuracy: 0.9182 - val_dense_40_loss: 0.1252 - val_dense_41_accuracy: 0.7531 - val_dense_41_loss: 0.3385 - val_loss: 0.4667 - learning_rate: 0.0010\nEpoch 17/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 320ms/step - dense_40_accuracy: 0.9642 - dense_40_loss: 0.0533 - dense_41_accuracy: 0.7810 - dense_41_loss: 0.2916 - loss: 0.3449\nEpoch 17: val_loss did not improve from 0.46665\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 326ms/step - dense_40_accuracy: 0.9642 - dense_40_loss: 0.0533 - dense_41_accuracy: 0.7810 - dense_41_loss: 0.2916 - loss: 0.3449 - val_dense_40_accuracy: 0.8627 - val_dense_40_loss: 0.2520 - val_dense_41_accuracy: 0.7623 - val_dense_41_loss: 0.3083 - val_loss: 0.5669 - learning_rate: 0.0010\nEpoch 18/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 326ms/step - dense_40_accuracy: 0.9634 - dense_40_loss: 0.0560 - dense_41_accuracy: 0.7942 - dense_41_loss: 0.2689 - loss: 0.3249\nEpoch 18: val_loss did not improve from 0.46665\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 333ms/step - dense_40_accuracy: 0.9634 - dense_40_loss: 0.0560 - dense_41_accuracy: 0.7942 - dense_41_loss: 0.2689 - loss: 0.3249 - val_dense_40_accuracy: 0.8704 - val_dense_40_loss: 0.1986 - val_dense_41_accuracy: 0.5309 - val_dense_41_loss: 0.6565 - val_loss: 0.8526 - learning_rate: 0.0010\nEpoch 19/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 319ms/step - dense_40_accuracy: 0.9726 - dense_40_loss: 0.0406 - dense_41_accuracy: 0.8050 - dense_41_loss: 0.2666 - loss: 0.3072\nEpoch 19: val_loss did not improve from 0.46665\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 325ms/step - dense_40_accuracy: 0.9726 - dense_40_loss: 0.0406 - dense_41_accuracy: 0.8050 - dense_41_loss: 0.2666 - loss: 0.3072 - val_dense_40_accuracy: 0.9414 - val_dense_40_loss: 0.0882 - val_dense_41_accuracy: 0.6620 - val_dense_41_loss: 0.4499 - val_loss: 0.5337 - learning_rate: 0.0010\nEpoch 20/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 322ms/step - dense_40_accuracy: 0.9737 - dense_40_loss: 0.0410 - dense_41_accuracy: 0.8022 - dense_41_loss: 0.2657 - loss: 0.3066\nEpoch 20: val_loss improved from 0.46665 to 0.44768, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 342ms/step - dense_40_accuracy: 0.9737 - dense_40_loss: 0.0410 - dense_41_accuracy: 0.8022 - dense_41_loss: 0.2657 - loss: 0.3067 - val_dense_40_accuracy: 0.9167 - val_dense_40_loss: 0.1475 - val_dense_41_accuracy: 0.7870 - val_dense_41_loss: 0.3030 - val_loss: 0.4477 - learning_rate: 0.0010\nEpoch 21/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - dense_40_accuracy: 0.9723 - dense_40_loss: 0.0393 - dense_41_accuracy: 0.8077 - dense_41_loss: 0.2593 - loss: 0.2987\nEpoch 21: val_loss did not improve from 0.44768\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 324ms/step - dense_40_accuracy: 0.9723 - dense_40_loss: 0.0393 - dense_41_accuracy: 0.8077 - dense_41_loss: 0.2593 - loss: 0.2986 - val_dense_40_accuracy: 0.9275 - val_dense_40_loss: 0.1205 - val_dense_41_accuracy: 0.5864 - val_dense_41_loss: 0.5919 - val_loss: 0.7048 - learning_rate: 0.0010\nEpoch 22/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 319ms/step - dense_40_accuracy: 0.9780 - dense_40_loss: 0.0331 - dense_41_accuracy: 0.8081 - dense_41_loss: 0.2539 - loss: 0.2870\nEpoch 22: val_loss did not improve from 0.44768\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 325ms/step - dense_40_accuracy: 0.9780 - dense_40_loss: 0.0331 - dense_41_accuracy: 0.8081 - dense_41_loss: 0.2539 - loss: 0.2870 - val_dense_40_accuracy: 0.8951 - val_dense_40_loss: 0.1760 - val_dense_41_accuracy: 0.7299 - val_dense_41_loss: 0.3459 - val_loss: 0.5180 - learning_rate: 0.0010\nEpoch 23/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 333ms/step - dense_40_accuracy: 0.9765 - dense_40_loss: 0.0343 - dense_41_accuracy: 0.8090 - dense_41_loss: 0.2461 - loss: 0.2804\nEpoch 23: val_loss improved from 0.44768 to 0.41859, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 353ms/step - dense_40_accuracy: 0.9765 - dense_40_loss: 0.0344 - dense_41_accuracy: 0.8091 - dense_41_loss: 0.2461 - loss: 0.2804 - val_dense_40_accuracy: 0.9568 - val_dense_40_loss: 0.0709 - val_dense_41_accuracy: 0.7654 - val_dense_41_loss: 0.3525 - val_loss: 0.4186 - learning_rate: 0.0010\nEpoch 24/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - dense_40_accuracy: 0.9787 - dense_40_loss: 0.0327 - dense_41_accuracy: 0.8233 - dense_41_loss: 0.2313 - loss: 0.2640\nEpoch 24: val_loss did not improve from 0.41859\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 325ms/step - dense_40_accuracy: 0.9787 - dense_40_loss: 0.0327 - dense_41_accuracy: 0.8233 - dense_41_loss: 0.2314 - loss: 0.2640 - val_dense_40_accuracy: 0.9367 - val_dense_40_loss: 0.1348 - val_dense_41_accuracy: 0.7978 - val_dense_41_loss: 0.2884 - val_loss: 0.4240 - learning_rate: 0.0010\nEpoch 25/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321ms/step - dense_40_accuracy: 0.9780 - dense_40_loss: 0.0333 - dense_41_accuracy: 0.8238 - dense_41_loss: 0.2362 - loss: 0.2695\nEpoch 25: val_loss did not improve from 0.41859\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 328ms/step - dense_40_accuracy: 0.9780 - dense_40_loss: 0.0333 - dense_41_accuracy: 0.8238 - dense_41_loss: 0.2362 - loss: 0.2695 - val_dense_40_accuracy: 0.9182 - val_dense_40_loss: 0.1404 - val_dense_41_accuracy: 0.7238 - val_dense_41_loss: 0.3677 - val_loss: 0.5090 - learning_rate: 0.0010\nEpoch 26/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 323ms/step - dense_40_accuracy: 0.9843 - dense_40_loss: 0.0238 - dense_41_accuracy: 0.8365 - dense_41_loss: 0.2183 - loss: 0.2421\nEpoch 26: val_loss did not improve from 0.41859\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 329ms/step - dense_40_accuracy: 0.9843 - dense_40_loss: 0.0239 - dense_41_accuracy: 0.8365 - dense_41_loss: 0.2183 - loss: 0.2422 - val_dense_40_accuracy: 0.9028 - val_dense_40_loss: 0.1902 - val_dense_41_accuracy: 0.7207 - val_dense_41_loss: 0.3670 - val_loss: 0.5590 - learning_rate: 0.0010\nEpoch 27/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 319ms/step - dense_40_accuracy: 0.9883 - dense_40_loss: 0.0194 - dense_41_accuracy: 0.8369 - dense_41_loss: 0.2202 - loss: 0.2397\nEpoch 27: val_loss did not improve from 0.41859\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 325ms/step - dense_40_accuracy: 0.9882 - dense_40_loss: 0.0195 - dense_41_accuracy: 0.8369 - dense_41_loss: 0.2202 - loss: 0.2397 - val_dense_40_accuracy: 0.9367 - val_dense_40_loss: 0.1062 - val_dense_41_accuracy: 0.7654 - val_dense_41_loss: 0.3217 - val_loss: 0.4221 - learning_rate: 0.0010\nEpoch 28/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 323ms/step - dense_40_accuracy: 0.9846 - dense_40_loss: 0.0216 - dense_41_accuracy: 0.8405 - dense_41_loss: 0.2093 - loss: 0.2309\nEpoch 28: val_loss did not improve from 0.41859\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 329ms/step - dense_40_accuracy: 0.9846 - dense_40_loss: 0.0216 - dense_41_accuracy: 0.8405 - dense_41_loss: 0.2093 - loss: 0.2309 - val_dense_40_accuracy: 0.8997 - val_dense_40_loss: 0.1895 - val_dense_41_accuracy: 0.7762 - val_dense_41_loss: 0.3534 - val_loss: 0.5552 - learning_rate: 0.0010\nEpoch 29/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 320ms/step - dense_40_accuracy: 0.9817 - dense_40_loss: 0.0268 - dense_41_accuracy: 0.8483 - dense_41_loss: 0.2021 - loss: 0.2288\nEpoch 29: val_loss did not improve from 0.41859\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 326ms/step - dense_40_accuracy: 0.9817 - dense_40_loss: 0.0267 - dense_41_accuracy: 0.8482 - dense_41_loss: 0.2021 - loss: 0.2288 - val_dense_40_accuracy: 0.9444 - val_dense_40_loss: 0.1055 - val_dense_41_accuracy: 0.7407 - val_dense_41_loss: 0.4113 - val_loss: 0.5279 - learning_rate: 0.0010\nEpoch 30/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 327ms/step - dense_40_accuracy: 0.9888 - dense_40_loss: 0.0182 - dense_41_accuracy: 0.8578 - dense_41_loss: 0.1897 - loss: 0.2078\nEpoch 30: val_loss improved from 0.41859 to 0.32968, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 347ms/step - dense_40_accuracy: 0.9888 - dense_40_loss: 0.0182 - dense_41_accuracy: 0.8578 - dense_41_loss: 0.1897 - loss: 0.2079 - val_dense_40_accuracy: 0.9475 - val_dense_40_loss: 0.0837 - val_dense_41_accuracy: 0.8302 - val_dense_41_loss: 0.2485 - val_loss: 0.3297 - learning_rate: 0.0010\nEpoch 31/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321ms/step - dense_40_accuracy: 0.9808 - dense_40_loss: 0.0268 - dense_41_accuracy: 0.8635 - dense_41_loss: 0.1837 - loss: 0.2105\nEpoch 31: val_loss did not improve from 0.32968\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 327ms/step - dense_40_accuracy: 0.9808 - dense_40_loss: 0.0268 - dense_41_accuracy: 0.8635 - dense_41_loss: 0.1838 - loss: 0.2106 - val_dense_40_accuracy: 0.9522 - val_dense_40_loss: 0.1141 - val_dense_41_accuracy: 0.7855 - val_dense_41_loss: 0.3015 - val_loss: 0.4243 - learning_rate: 0.0010\nEpoch 32/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 319ms/step - dense_40_accuracy: 0.9856 - dense_40_loss: 0.0193 - dense_41_accuracy: 0.8663 - dense_41_loss: 0.1792 - loss: 0.1984\nEpoch 32: val_loss did not improve from 0.32968\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 326ms/step - dense_40_accuracy: 0.9856 - dense_40_loss: 0.0193 - dense_41_accuracy: 0.8663 - dense_41_loss: 0.1792 - loss: 0.1985 - val_dense_40_accuracy: 0.9228 - val_dense_40_loss: 0.1919 - val_dense_41_accuracy: 0.8287 - val_dense_41_loss: 0.2692 - val_loss: 0.4584 - learning_rate: 0.0010\nEpoch 33/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 326ms/step - dense_40_accuracy: 0.9854 - dense_40_loss: 0.0221 - dense_41_accuracy: 0.8748 - dense_41_loss: 0.1697 - loss: 0.1918\nEpoch 33: val_loss did not improve from 0.32968\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 332ms/step - dense_40_accuracy: 0.9854 - dense_40_loss: 0.0221 - dense_41_accuracy: 0.8748 - dense_41_loss: 0.1698 - loss: 0.1918 - val_dense_40_accuracy: 0.8688 - val_dense_40_loss: 0.2712 - val_dense_41_accuracy: 0.7824 - val_dense_41_loss: 0.3708 - val_loss: 0.6324 - learning_rate: 0.0010\nEpoch 34/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - dense_40_accuracy: 0.9911 - dense_40_loss: 0.0154 - dense_41_accuracy: 0.8837 - dense_41_loss: 0.1588 - loss: 0.1742\nEpoch 34: val_loss did not improve from 0.32968\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 325ms/step - dense_40_accuracy: 0.9911 - dense_40_loss: 0.0155 - dense_41_accuracy: 0.8837 - dense_41_loss: 0.1588 - loss: 0.1742 - val_dense_40_accuracy: 0.8827 - val_dense_40_loss: 0.1898 - val_dense_41_accuracy: 0.7994 - val_dense_41_loss: 0.2601 - val_loss: 0.4586 - learning_rate: 0.0010\nEpoch 35/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 317ms/step - dense_40_accuracy: 0.9790 - dense_40_loss: 0.0331 - dense_41_accuracy: 0.8744 - dense_41_loss: 0.1700 - loss: 0.2031\nEpoch 35: val_loss improved from 0.32968 to 0.31735, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 337ms/step - dense_40_accuracy: 0.9790 - dense_40_loss: 0.0330 - dense_41_accuracy: 0.8744 - dense_41_loss: 0.1700 - loss: 0.2030 - val_dense_40_accuracy: 0.9506 - val_dense_40_loss: 0.0969 - val_dense_41_accuracy: 0.8349 - val_dense_41_loss: 0.2208 - val_loss: 0.3173 - learning_rate: 0.0010\nEpoch 36/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 317ms/step - dense_40_accuracy: 0.9866 - dense_40_loss: 0.0196 - dense_41_accuracy: 0.8919 - dense_41_loss: 0.1431 - loss: 0.1628\nEpoch 36: val_loss did not improve from 0.31735\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 324ms/step - dense_40_accuracy: 0.9866 - dense_40_loss: 0.0196 - dense_41_accuracy: 0.8919 - dense_41_loss: 0.1431 - loss: 0.1628 - val_dense_40_accuracy: 0.9290 - val_dense_40_loss: 0.1489 - val_dense_41_accuracy: 0.7963 - val_dense_41_loss: 0.3218 - val_loss: 0.4634 - learning_rate: 0.0010\nEpoch 37/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 333ms/step - dense_40_accuracy: 0.9805 - dense_40_loss: 0.0296 - dense_41_accuracy: 0.8981 - dense_41_loss: 0.1394 - loss: 0.1691\nEpoch 37: val_loss did not improve from 0.31735\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 339ms/step - dense_40_accuracy: 0.9805 - dense_40_loss: 0.0296 - dense_41_accuracy: 0.8980 - dense_41_loss: 0.1395 - loss: 0.1691 - val_dense_40_accuracy: 0.8426 - val_dense_40_loss: 0.3343 - val_dense_41_accuracy: 0.8488 - val_dense_41_loss: 0.2152 - val_loss: 0.5387 - learning_rate: 0.0010\nEpoch 38/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 319ms/step - dense_40_accuracy: 0.9866 - dense_40_loss: 0.0192 - dense_41_accuracy: 0.9056 - dense_41_loss: 0.1336 - loss: 0.1528\nEpoch 38: val_loss improved from 0.31735 to 0.31082, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 338ms/step - dense_40_accuracy: 0.9866 - dense_40_loss: 0.0192 - dense_41_accuracy: 0.9055 - dense_41_loss: 0.1337 - loss: 0.1529 - val_dense_40_accuracy: 0.9367 - val_dense_40_loss: 0.1273 - val_dense_41_accuracy: 0.8642 - val_dense_41_loss: 0.1876 - val_loss: 0.3108 - learning_rate: 0.0010\nEpoch 39/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 317ms/step - dense_40_accuracy: 0.9870 - dense_40_loss: 0.0216 - dense_41_accuracy: 0.9163 - dense_41_loss: 0.1149 - loss: 0.1364\nEpoch 39: val_loss did not improve from 0.31082\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 324ms/step - dense_40_accuracy: 0.9870 - dense_40_loss: 0.0216 - dense_41_accuracy: 0.9163 - dense_41_loss: 0.1149 - loss: 0.1365 - val_dense_40_accuracy: 0.8843 - val_dense_40_loss: 0.2324 - val_dense_41_accuracy: 0.7546 - val_dense_41_loss: 0.3558 - val_loss: 0.5999 - learning_rate: 0.0010\nEpoch 40/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 316ms/step - dense_40_accuracy: 0.9869 - dense_40_loss: 0.0186 - dense_41_accuracy: 0.9041 - dense_41_loss: 0.1236 - loss: 0.1422\nEpoch 40: val_loss did not improve from 0.31082\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 322ms/step - dense_40_accuracy: 0.9869 - dense_40_loss: 0.0186 - dense_41_accuracy: 0.9041 - dense_41_loss: 0.1236 - loss: 0.1422 - val_dense_40_accuracy: 0.9429 - val_dense_40_loss: 0.1124 - val_dense_41_accuracy: 0.8488 - val_dense_41_loss: 0.2476 - val_loss: 0.3661 - learning_rate: 0.0010\nEpoch 41/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 328ms/step - dense_40_accuracy: 0.9902 - dense_40_loss: 0.0146 - dense_41_accuracy: 0.9172 - dense_41_loss: 0.1149 - loss: 0.1295\nEpoch 41: val_loss did not improve from 0.31082\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 335ms/step - dense_40_accuracy: 0.9902 - dense_40_loss: 0.0146 - dense_41_accuracy: 0.9171 - dense_41_loss: 0.1149 - loss: 0.1295 - val_dense_40_accuracy: 0.9645 - val_dense_40_loss: 0.0806 - val_dense_41_accuracy: 0.7747 - val_dense_41_loss: 0.4284 - val_loss: 0.5207 - learning_rate: 0.0010\nEpoch 42/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 326ms/step - dense_40_accuracy: 0.9908 - dense_40_loss: 0.0135 - dense_41_accuracy: 0.9250 - dense_41_loss: 0.1070 - loss: 0.1204\nEpoch 42: val_loss did not improve from 0.31082\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 333ms/step - dense_40_accuracy: 0.9908 - dense_40_loss: 0.0135 - dense_41_accuracy: 0.9250 - dense_41_loss: 0.1070 - loss: 0.1205 - val_dense_40_accuracy: 0.9059 - val_dense_40_loss: 0.1975 - val_dense_41_accuracy: 0.8225 - val_dense_41_loss: 0.2781 - val_loss: 0.4831 - learning_rate: 0.0010\nEpoch 43/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 327ms/step - dense_40_accuracy: 0.9872 - dense_40_loss: 0.0217 - dense_41_accuracy: 0.9210 - dense_41_loss: 0.1009 - loss: 0.1227\nEpoch 43: val_loss did not improve from 0.31082\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 334ms/step - dense_40_accuracy: 0.9873 - dense_40_loss: 0.0217 - dense_41_accuracy: 0.9210 - dense_41_loss: 0.1010 - loss: 0.1227 - val_dense_40_accuracy: 0.9275 - val_dense_40_loss: 0.1378 - val_dense_41_accuracy: 0.8503 - val_dense_41_loss: 0.2359 - val_loss: 0.3775 - learning_rate: 0.0010\nEpoch 44/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - dense_40_accuracy: 0.9950 - dense_40_loss: 0.0078 - dense_41_accuracy: 0.9297 - dense_41_loss: 0.0975 - loss: 0.1053\nEpoch 44: val_loss did not improve from 0.31082\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 325ms/step - dense_40_accuracy: 0.9950 - dense_40_loss: 0.0078 - dense_41_accuracy: 0.9297 - dense_41_loss: 0.0975 - loss: 0.1053 - val_dense_40_accuracy: 0.9568 - val_dense_40_loss: 0.1020 - val_dense_41_accuracy: 0.7454 - val_dense_41_loss: 0.4155 - val_loss: 0.5247 - learning_rate: 0.0010\nEpoch 45/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 327ms/step - dense_40_accuracy: 0.9886 - dense_40_loss: 0.0182 - dense_41_accuracy: 0.9200 - dense_41_loss: 0.1108 - loss: 0.1289\nEpoch 45: val_loss did not improve from 0.31082\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 334ms/step - dense_40_accuracy: 0.9886 - dense_40_loss: 0.0182 - dense_41_accuracy: 0.9200 - dense_41_loss: 0.1107 - loss: 0.1289 - val_dense_40_accuracy: 0.9352 - val_dense_40_loss: 0.1107 - val_dense_41_accuracy: 0.8395 - val_dense_41_loss: 0.2773 - val_loss: 0.3984 - learning_rate: 0.0010\nEpoch 46/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 319ms/step - dense_40_accuracy: 0.9920 - dense_40_loss: 0.0115 - dense_41_accuracy: 0.9374 - dense_41_loss: 0.0826 - loss: 0.0941\nEpoch 46: val_loss did not improve from 0.31082\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 325ms/step - dense_40_accuracy: 0.9920 - dense_40_loss: 0.0115 - dense_41_accuracy: 0.9374 - dense_41_loss: 0.0827 - loss: 0.0941 - val_dense_40_accuracy: 0.9429 - val_dense_40_loss: 0.1385 - val_dense_41_accuracy: 0.8750 - val_dense_41_loss: 0.1922 - val_loss: 0.3349 - learning_rate: 0.0010\nEpoch 47/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 327ms/step - dense_40_accuracy: 0.9900 - dense_40_loss: 0.0181 - dense_41_accuracy: 0.9414 - dense_41_loss: 0.0797 - loss: 0.0979\nEpoch 47: val_loss did not improve from 0.31082\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 333ms/step - dense_40_accuracy: 0.9900 - dense_40_loss: 0.0181 - dense_41_accuracy: 0.9413 - dense_41_loss: 0.0798 - loss: 0.0979 - val_dense_40_accuracy: 0.9506 - val_dense_40_loss: 0.0986 - val_dense_41_accuracy: 0.8688 - val_dense_41_loss: 0.2324 - val_loss: 0.3410 - learning_rate: 0.0010\nEpoch 48/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 323ms/step - dense_40_accuracy: 0.9928 - dense_40_loss: 0.0119 - dense_41_accuracy: 0.9370 - dense_41_loss: 0.0846 - loss: 0.0965\nEpoch 48: val_loss did not improve from 0.31082\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 329ms/step - dense_40_accuracy: 0.9928 - dense_40_loss: 0.0119 - dense_41_accuracy: 0.9370 - dense_41_loss: 0.0846 - loss: 0.0965 - val_dense_40_accuracy: 0.9552 - val_dense_40_loss: 0.1027 - val_dense_41_accuracy: 0.8179 - val_dense_41_loss: 0.3176 - val_loss: 0.4313 - learning_rate: 0.0010\nEpoch 49/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 325ms/step - dense_40_accuracy: 0.9908 - dense_40_loss: 0.0117 - dense_41_accuracy: 0.9396 - dense_41_loss: 0.0803 - loss: 0.0920\nEpoch 49: val_loss improved from 0.31082 to 0.30894, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 345ms/step - dense_40_accuracy: 0.9908 - dense_40_loss: 0.0117 - dense_41_accuracy: 0.9396 - dense_41_loss: 0.0804 - loss: 0.0920 - val_dense_40_accuracy: 0.9460 - val_dense_40_loss: 0.1333 - val_dense_41_accuracy: 0.8796 - val_dense_41_loss: 0.1675 - val_loss: 0.3089 - learning_rate: 0.0010\nEpoch 50/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 317ms/step - dense_40_accuracy: 0.9870 - dense_40_loss: 0.0211 - dense_41_accuracy: 0.9376 - dense_41_loss: 0.0833 - loss: 0.1044\nEpoch 50: val_loss did not improve from 0.30894\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 323ms/step - dense_40_accuracy: 0.9870 - dense_40_loss: 0.0211 - dense_41_accuracy: 0.9376 - dense_41_loss: 0.0833 - loss: 0.1044 - val_dense_40_accuracy: 0.9414 - val_dense_40_loss: 0.1170 - val_dense_41_accuracy: 0.8673 - val_dense_41_loss: 0.2315 - val_loss: 0.3537 - learning_rate: 0.0010\nEpoch 51/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 323ms/step - dense_40_accuracy: 0.9878 - dense_40_loss: 0.0191 - dense_41_accuracy: 0.9433 - dense_41_loss: 0.0784 - loss: 0.0974\nEpoch 51: val_loss improved from 0.30894 to 0.30184, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 342ms/step - dense_40_accuracy: 0.9878 - dense_40_loss: 0.0191 - dense_41_accuracy: 0.9433 - dense_41_loss: 0.0784 - loss: 0.0974 - val_dense_40_accuracy: 0.9491 - val_dense_40_loss: 0.1158 - val_dense_41_accuracy: 0.8719 - val_dense_41_loss: 0.1794 - val_loss: 0.3018 - learning_rate: 0.0010\nEpoch 52/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 319ms/step - dense_40_accuracy: 0.9892 - dense_40_loss: 0.0145 - dense_41_accuracy: 0.9412 - dense_41_loss: 0.0861 - loss: 0.1006\nEpoch 52: val_loss did not improve from 0.30184\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 325ms/step - dense_40_accuracy: 0.9892 - dense_40_loss: 0.0145 - dense_41_accuracy: 0.9412 - dense_41_loss: 0.0861 - loss: 0.1006 - val_dense_40_accuracy: 0.9506 - val_dense_40_loss: 0.1142 - val_dense_41_accuracy: 0.8117 - val_dense_41_loss: 0.3603 - val_loss: 0.4882 - learning_rate: 0.0010\nEpoch 53/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321ms/step - dense_40_accuracy: 0.9943 - dense_40_loss: 0.0089 - dense_41_accuracy: 0.9471 - dense_41_loss: 0.0719 - loss: 0.0809\nEpoch 53: val_loss did not improve from 0.30184\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 327ms/step - dense_40_accuracy: 0.9943 - dense_40_loss: 0.0090 - dense_41_accuracy: 0.9471 - dense_41_loss: 0.0719 - loss: 0.0809 - val_dense_40_accuracy: 0.9167 - val_dense_40_loss: 0.2217 - val_dense_41_accuracy: 0.8889 - val_dense_41_loss: 0.1603 - val_loss: 0.3929 - learning_rate: 0.0010\nEpoch 54/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - dense_40_accuracy: 0.9941 - dense_40_loss: 0.0090 - dense_41_accuracy: 0.9560 - dense_41_loss: 0.0578 - loss: 0.0668\nEpoch 54: val_loss improved from 0.30184 to 0.29152, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 338ms/step - dense_40_accuracy: 0.9941 - dense_40_loss: 0.0090 - dense_41_accuracy: 0.9560 - dense_41_loss: 0.0578 - loss: 0.0668 - val_dense_40_accuracy: 0.9444 - val_dense_40_loss: 0.1373 - val_dense_41_accuracy: 0.8981 - val_dense_41_loss: 0.1445 - val_loss: 0.2915 - learning_rate: 0.0010\nEpoch 55/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 323ms/step - dense_40_accuracy: 0.9951 - dense_40_loss: 0.0085 - dense_41_accuracy: 0.9511 - dense_41_loss: 0.0671 - loss: 0.0756\nEpoch 55: val_loss did not improve from 0.29152\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 329ms/step - dense_40_accuracy: 0.9951 - dense_40_loss: 0.0085 - dense_41_accuracy: 0.9511 - dense_41_loss: 0.0672 - loss: 0.0756 - val_dense_40_accuracy: 0.9522 - val_dense_40_loss: 0.1078 - val_dense_41_accuracy: 0.8596 - val_dense_41_loss: 0.2359 - val_loss: 0.3503 - learning_rate: 0.0010\nEpoch 56/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 327ms/step - dense_40_accuracy: 0.9899 - dense_40_loss: 0.0161 - dense_41_accuracy: 0.9524 - dense_41_loss: 0.0689 - loss: 0.0850\nEpoch 56: val_loss did not improve from 0.29152\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 333ms/step - dense_40_accuracy: 0.9899 - dense_40_loss: 0.0161 - dense_41_accuracy: 0.9524 - dense_41_loss: 0.0689 - loss: 0.0850 - val_dense_40_accuracy: 0.8904 - val_dense_40_loss: 0.3076 - val_dense_41_accuracy: 0.8241 - val_dense_41_loss: 0.3711 - val_loss: 0.6903 - learning_rate: 0.0010\nEpoch 57/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 328ms/step - dense_40_accuracy: 0.9886 - dense_40_loss: 0.0157 - dense_41_accuracy: 0.9670 - dense_41_loss: 0.0475 - loss: 0.0632\nEpoch 57: val_loss did not improve from 0.29152\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 334ms/step - dense_40_accuracy: 0.9886 - dense_40_loss: 0.0157 - dense_41_accuracy: 0.9669 - dense_41_loss: 0.0475 - loss: 0.0632 - val_dense_40_accuracy: 0.9398 - val_dense_40_loss: 0.1400 - val_dense_41_accuracy: 0.8673 - val_dense_41_loss: 0.2083 - val_loss: 0.3505 - learning_rate: 0.0010\nEpoch 58/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 327ms/step - dense_40_accuracy: 0.9938 - dense_40_loss: 0.0100 - dense_41_accuracy: 0.9613 - dense_41_loss: 0.0536 - loss: 0.0636\nEpoch 58: val_loss did not improve from 0.29152\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 334ms/step - dense_40_accuracy: 0.9938 - dense_40_loss: 0.0100 - dense_41_accuracy: 0.9613 - dense_41_loss: 0.0537 - loss: 0.0637 - val_dense_40_accuracy: 0.9475 - val_dense_40_loss: 0.0948 - val_dense_41_accuracy: 0.8580 - val_dense_41_loss: 0.2451 - val_loss: 0.3438 - learning_rate: 0.0010\nEpoch 59/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321ms/step - dense_40_accuracy: 0.9927 - dense_40_loss: 0.0106 - dense_41_accuracy: 0.9642 - dense_41_loss: 0.0556 - loss: 0.0662\nEpoch 59: val_loss did not improve from 0.29152\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 327ms/step - dense_40_accuracy: 0.9927 - dense_40_loss: 0.0106 - dense_41_accuracy: 0.9642 - dense_41_loss: 0.0556 - loss: 0.0663 - val_dense_40_accuracy: 0.9444 - val_dense_40_loss: 0.1220 - val_dense_41_accuracy: 0.8426 - val_dense_41_loss: 0.3060 - val_loss: 0.4265 - learning_rate: 0.0010\nEpoch 60/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 327ms/step - dense_40_accuracy: 0.9921 - dense_40_loss: 0.0111 - dense_41_accuracy: 0.9563 - dense_41_loss: 0.0577 - loss: 0.0688\nEpoch 60: val_loss did not improve from 0.29152\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 333ms/step - dense_40_accuracy: 0.9921 - dense_40_loss: 0.0111 - dense_41_accuracy: 0.9563 - dense_41_loss: 0.0577 - loss: 0.0688 - val_dense_40_accuracy: 0.9491 - val_dense_40_loss: 0.1228 - val_dense_41_accuracy: 0.8410 - val_dense_41_loss: 0.2664 - val_loss: 0.3976 - learning_rate: 0.0010\nEpoch 61/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 324ms/step - dense_40_accuracy: 0.9929 - dense_40_loss: 0.0097 - dense_41_accuracy: 0.9607 - dense_41_loss: 0.0605 - loss: 0.0701\nEpoch 61: val_loss did not improve from 0.29152\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 331ms/step - dense_40_accuracy: 0.9929 - dense_40_loss: 0.0096 - dense_41_accuracy: 0.9607 - dense_41_loss: 0.0605 - loss: 0.0701 - val_dense_40_accuracy: 0.9429 - val_dense_40_loss: 0.1470 - val_dense_41_accuracy: 0.8781 - val_dense_41_loss: 0.1924 - val_loss: 0.3501 - learning_rate: 0.0010\nEpoch 62/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 327ms/step - dense_40_accuracy: 0.9936 - dense_40_loss: 0.0083 - dense_41_accuracy: 0.9612 - dense_41_loss: 0.0533 - loss: 0.0615\nEpoch 62: val_loss did not improve from 0.29152\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 333ms/step - dense_40_accuracy: 0.9936 - dense_40_loss: 0.0083 - dense_41_accuracy: 0.9612 - dense_41_loss: 0.0533 - loss: 0.0615 - val_dense_40_accuracy: 0.9290 - val_dense_40_loss: 0.1742 - val_dense_41_accuracy: 0.8796 - val_dense_41_loss: 0.1777 - val_loss: 0.3628 - learning_rate: 0.0010\nEpoch 63/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - dense_40_accuracy: 0.9934 - dense_40_loss: 0.0089 - dense_41_accuracy: 0.9579 - dense_41_loss: 0.0559 - loss: 0.0648\nEpoch 63: val_loss did not improve from 0.29152\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 324ms/step - dense_40_accuracy: 0.9934 - dense_40_loss: 0.0090 - dense_41_accuracy: 0.9579 - dense_41_loss: 0.0559 - loss: 0.0648 - val_dense_40_accuracy: 0.9259 - val_dense_40_loss: 0.1893 - val_dense_41_accuracy: 0.8966 - val_dense_41_loss: 0.1800 - val_loss: 0.3758 - learning_rate: 0.0010\nEpoch 64/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 319ms/step - dense_40_accuracy: 0.9867 - dense_40_loss: 0.0216 - dense_41_accuracy: 0.9662 - dense_41_loss: 0.0505 - loss: 0.0722\nEpoch 64: val_loss improved from 0.29152 to 0.27897, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 338ms/step - dense_40_accuracy: 0.9867 - dense_40_loss: 0.0216 - dense_41_accuracy: 0.9662 - dense_41_loss: 0.0505 - loss: 0.0721 - val_dense_40_accuracy: 0.9568 - val_dense_40_loss: 0.0960 - val_dense_41_accuracy: 0.8966 - val_dense_41_loss: 0.1867 - val_loss: 0.2790 - learning_rate: 0.0010\nEpoch 65/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 327ms/step - dense_40_accuracy: 0.9960 - dense_40_loss: 0.0058 - dense_41_accuracy: 0.9697 - dense_41_loss: 0.0438 - loss: 0.0496\nEpoch 65: val_loss did not improve from 0.27897\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 333ms/step - dense_40_accuracy: 0.9960 - dense_40_loss: 0.0058 - dense_41_accuracy: 0.9697 - dense_41_loss: 0.0438 - loss: 0.0496 - val_dense_40_accuracy: 0.9383 - val_dense_40_loss: 0.1307 - val_dense_41_accuracy: 0.7438 - val_dense_41_loss: 0.5621 - val_loss: 0.6890 - learning_rate: 0.0010\nEpoch 66/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 327ms/step - dense_40_accuracy: 0.9911 - dense_40_loss: 0.0141 - dense_41_accuracy: 0.9635 - dense_41_loss: 0.0526 - loss: 0.0667\nEpoch 66: val_loss did not improve from 0.27897\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 334ms/step - dense_40_accuracy: 0.9911 - dense_40_loss: 0.0141 - dense_41_accuracy: 0.9635 - dense_41_loss: 0.0526 - loss: 0.0667 - val_dense_40_accuracy: 0.9552 - val_dense_40_loss: 0.0962 - val_dense_41_accuracy: 0.7994 - val_dense_41_loss: 0.3773 - val_loss: 0.4833 - learning_rate: 0.0010\nEpoch 67/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - dense_40_accuracy: 0.9941 - dense_40_loss: 0.0102 - dense_41_accuracy: 0.9730 - dense_41_loss: 0.0394 - loss: 0.0495\nEpoch 67: val_loss improved from 0.27897 to 0.27446, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 338ms/step - dense_40_accuracy: 0.9941 - dense_40_loss: 0.0102 - dense_41_accuracy: 0.9730 - dense_41_loss: 0.0394 - loss: 0.0496 - val_dense_40_accuracy: 0.9336 - val_dense_40_loss: 0.1505 - val_dense_41_accuracy: 0.9244 - val_dense_41_loss: 0.1237 - val_loss: 0.2745 - learning_rate: 0.0010\nEpoch 68/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 323ms/step - dense_40_accuracy: 0.9959 - dense_40_loss: 0.0056 - dense_41_accuracy: 0.9662 - dense_41_loss: 0.0455 - loss: 0.0511\nEpoch 68: val_loss did not improve from 0.27446\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 330ms/step - dense_40_accuracy: 0.9959 - dense_40_loss: 0.0056 - dense_41_accuracy: 0.9662 - dense_41_loss: 0.0455 - loss: 0.0511 - val_dense_40_accuracy: 0.9151 - val_dense_40_loss: 0.1876 - val_dense_41_accuracy: 0.8904 - val_dense_41_loss: 0.1680 - val_loss: 0.3672 - learning_rate: 0.0010\nEpoch 69/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 320ms/step - dense_40_accuracy: 0.9956 - dense_40_loss: 0.0084 - dense_41_accuracy: 0.9692 - dense_41_loss: 0.0434 - loss: 0.0518\nEpoch 69: val_loss did not improve from 0.27446\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 326ms/step - dense_40_accuracy: 0.9956 - dense_40_loss: 0.0084 - dense_41_accuracy: 0.9692 - dense_41_loss: 0.0434 - loss: 0.0519 - val_dense_40_accuracy: 0.9383 - val_dense_40_loss: 0.1306 - val_dense_41_accuracy: 0.8827 - val_dense_41_loss: 0.2486 - val_loss: 0.3927 - learning_rate: 0.0010\nEpoch 70/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 326ms/step - dense_40_accuracy: 0.9930 - dense_40_loss: 0.0114 - dense_41_accuracy: 0.9636 - dense_41_loss: 0.0525 - loss: 0.0639\nEpoch 70: val_loss did not improve from 0.27446\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 332ms/step - dense_40_accuracy: 0.9931 - dense_40_loss: 0.0114 - dense_41_accuracy: 0.9636 - dense_41_loss: 0.0525 - loss: 0.0639 - val_dense_40_accuracy: 0.9599 - val_dense_40_loss: 0.0900 - val_dense_41_accuracy: 0.8719 - val_dense_41_loss: 0.2161 - val_loss: 0.3036 - learning_rate: 0.0010\nEpoch 71/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - dense_40_accuracy: 0.9963 - dense_40_loss: 0.0069 - dense_41_accuracy: 0.9703 - dense_41_loss: 0.0428 - loss: 0.0497\nEpoch 71: val_loss did not improve from 0.27446\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 324ms/step - dense_40_accuracy: 0.9963 - dense_40_loss: 0.0069 - dense_41_accuracy: 0.9703 - dense_41_loss: 0.0428 - loss: 0.0497 - val_dense_40_accuracy: 0.9444 - val_dense_40_loss: 0.1093 - val_dense_41_accuracy: 0.8488 - val_dense_41_loss: 0.3122 - val_loss: 0.4264 - learning_rate: 0.0010\nEpoch 72/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 327ms/step - dense_40_accuracy: 0.9975 - dense_40_loss: 0.0040 - dense_41_accuracy: 0.9743 - dense_41_loss: 0.0328 - loss: 0.0368\nEpoch 72: val_loss did not improve from 0.27446\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 333ms/step - dense_40_accuracy: 0.9975 - dense_40_loss: 0.0040 - dense_41_accuracy: 0.9743 - dense_41_loss: 0.0328 - loss: 0.0368 - val_dense_40_accuracy: 0.9228 - val_dense_40_loss: 0.1642 - val_dense_41_accuracy: 0.8966 - val_dense_41_loss: 0.1662 - val_loss: 0.3397 - learning_rate: 0.0010\nEpoch 73/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 319ms/step - dense_40_accuracy: 0.9947 - dense_40_loss: 0.0080 - dense_41_accuracy: 0.9699 - dense_41_loss: 0.0435 - loss: 0.0515\nEpoch 73: val_loss did not improve from 0.27446\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 325ms/step - dense_40_accuracy: 0.9947 - dense_40_loss: 0.0080 - dense_41_accuracy: 0.9699 - dense_41_loss: 0.0435 - loss: 0.0515 - val_dense_40_accuracy: 0.9460 - val_dense_40_loss: 0.1154 - val_dense_41_accuracy: 0.8565 - val_dense_41_loss: 0.2568 - val_loss: 0.3783 - learning_rate: 0.0010\nEpoch 74/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - dense_40_accuracy: 0.9954 - dense_40_loss: 0.0071 - dense_41_accuracy: 0.9718 - dense_41_loss: 0.0414 - loss: 0.0486\nEpoch 74: val_loss did not improve from 0.27446\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 324ms/step - dense_40_accuracy: 0.9954 - dense_40_loss: 0.0071 - dense_41_accuracy: 0.9718 - dense_41_loss: 0.0414 - loss: 0.0486 - val_dense_40_accuracy: 0.9599 - val_dense_40_loss: 0.1113 - val_dense_41_accuracy: 0.9059 - val_dense_41_loss: 0.1582 - val_loss: 0.2748 - learning_rate: 0.0010\nEpoch 75/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 331ms/step - dense_40_accuracy: 0.9909 - dense_40_loss: 0.0139 - dense_41_accuracy: 0.9751 - dense_41_loss: 0.0370 - loss: 0.0508\nEpoch 75: val_loss did not improve from 0.27446\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 337ms/step - dense_40_accuracy: 0.9909 - dense_40_loss: 0.0139 - dense_41_accuracy: 0.9751 - dense_41_loss: 0.0370 - loss: 0.0508 - val_dense_40_accuracy: 0.9306 - val_dense_40_loss: 0.1589 - val_dense_41_accuracy: 0.8349 - val_dense_41_loss: 0.2522 - val_loss: 0.4056 - learning_rate: 0.0010\nEpoch 76/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 326ms/step - dense_40_accuracy: 0.9956 - dense_40_loss: 0.0069 - dense_41_accuracy: 0.9708 - dense_41_loss: 0.0418 - loss: 0.0487\nEpoch 76: val_loss did not improve from 0.27446\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 332ms/step - dense_40_accuracy: 0.9956 - dense_40_loss: 0.0069 - dense_41_accuracy: 0.9708 - dense_41_loss: 0.0418 - loss: 0.0487 - val_dense_40_accuracy: 0.9244 - val_dense_40_loss: 0.1562 - val_dense_41_accuracy: 0.8241 - val_dense_41_loss: 0.3305 - val_loss: 0.4828 - learning_rate: 0.0010\nEpoch 77/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - dense_40_accuracy: 0.9942 - dense_40_loss: 0.0094 - dense_41_accuracy: 0.9715 - dense_41_loss: 0.0378 - loss: 0.0472\nEpoch 77: val_loss did not improve from 0.27446\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 325ms/step - dense_40_accuracy: 0.9942 - dense_40_loss: 0.0094 - dense_41_accuracy: 0.9715 - dense_41_loss: 0.0378 - loss: 0.0472 - val_dense_40_accuracy: 0.9537 - val_dense_40_loss: 0.1157 - val_dense_41_accuracy: 0.9059 - val_dense_41_loss: 0.1523 - val_loss: 0.2751 - learning_rate: 0.0010\nEpoch 78/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - dense_40_accuracy: 0.9962 - dense_40_loss: 0.0050 - dense_41_accuracy: 0.9716 - dense_41_loss: 0.0373 - loss: 0.0422\nEpoch 78: val_loss did not improve from 0.27446\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 324ms/step - dense_40_accuracy: 0.9962 - dense_40_loss: 0.0050 - dense_41_accuracy: 0.9715 - dense_41_loss: 0.0373 - loss: 0.0422 - val_dense_40_accuracy: 0.9414 - val_dense_40_loss: 0.1618 - val_dense_41_accuracy: 0.7485 - val_dense_41_loss: 0.5393 - val_loss: 0.6926 - learning_rate: 0.0010\nEpoch 79/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 325ms/step - dense_40_accuracy: 0.9964 - dense_40_loss: 0.0053 - dense_41_accuracy: 0.9698 - dense_41_loss: 0.0407 - loss: 0.0461\nEpoch 79: val_loss did not improve from 0.27446\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 332ms/step - dense_40_accuracy: 0.9964 - dense_40_loss: 0.0053 - dense_41_accuracy: 0.9699 - dense_41_loss: 0.0407 - loss: 0.0461 - val_dense_40_accuracy: 0.9244 - val_dense_40_loss: 0.2121 - val_dense_41_accuracy: 0.8580 - val_dense_41_loss: 0.2684 - val_loss: 0.4919 - learning_rate: 0.0010\nEpoch 80/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 335ms/step - dense_40_accuracy: 0.9895 - dense_40_loss: 0.0145 - dense_41_accuracy: 0.9797 - dense_41_loss: 0.0322 - loss: 0.0467\nEpoch 80: val_loss did not improve from 0.27446\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 341ms/step - dense_40_accuracy: 0.9895 - dense_40_loss: 0.0145 - dense_41_accuracy: 0.9797 - dense_41_loss: 0.0322 - loss: 0.0467 - val_dense_40_accuracy: 0.9398 - val_dense_40_loss: 0.1425 - val_dense_41_accuracy: 0.8565 - val_dense_41_loss: 0.2792 - val_loss: 0.4343 - learning_rate: 0.0010\nEpoch 81/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - dense_40_accuracy: 0.9933 - dense_40_loss: 0.0104 - dense_41_accuracy: 0.9783 - dense_41_loss: 0.0315 - loss: 0.0419\nEpoch 81: val_loss did not improve from 0.27446\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 324ms/step - dense_40_accuracy: 0.9933 - dense_40_loss: 0.0104 - dense_41_accuracy: 0.9783 - dense_41_loss: 0.0315 - loss: 0.0419 - val_dense_40_accuracy: 0.9120 - val_dense_40_loss: 0.1767 - val_dense_41_accuracy: 0.9120 - val_dense_41_loss: 0.1138 - val_loss: 0.2882 - learning_rate: 0.0010\nEpoch 82/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 317ms/step - dense_40_accuracy: 0.9944 - dense_40_loss: 0.0093 - dense_41_accuracy: 0.9701 - dense_41_loss: 0.0429 - loss: 0.0522\nEpoch 82: val_loss did not improve from 0.27446\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 323ms/step - dense_40_accuracy: 0.9944 - dense_40_loss: 0.0093 - dense_41_accuracy: 0.9701 - dense_41_loss: 0.0429 - loss: 0.0522 - val_dense_40_accuracy: 0.9583 - val_dense_40_loss: 0.0846 - val_dense_41_accuracy: 0.8735 - val_dense_41_loss: 0.1811 - val_loss: 0.2745 - learning_rate: 0.0010\nEpoch 83/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 319ms/step - dense_40_accuracy: 0.9952 - dense_40_loss: 0.0074 - dense_41_accuracy: 0.9747 - dense_41_loss: 0.0362 - loss: 0.0436\nEpoch 83: val_loss did not improve from 0.27446\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 325ms/step - dense_40_accuracy: 0.9952 - dense_40_loss: 0.0074 - dense_41_accuracy: 0.9747 - dense_41_loss: 0.0362 - loss: 0.0436 - val_dense_40_accuracy: 0.9506 - val_dense_40_loss: 0.1236 - val_dense_41_accuracy: 0.9074 - val_dense_41_loss: 0.1464 - val_loss: 0.2795 - learning_rate: 0.0010\nEpoch 84/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 322ms/step - dense_40_accuracy: 0.9983 - dense_40_loss: 0.0033 - dense_41_accuracy: 0.9635 - dense_41_loss: 0.0496 - loss: 0.0528\nEpoch 84: val_loss did not improve from 0.27446\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 329ms/step - dense_40_accuracy: 0.9983 - dense_40_loss: 0.0033 - dense_41_accuracy: 0.9636 - dense_41_loss: 0.0496 - loss: 0.0528 - val_dense_40_accuracy: 0.9460 - val_dense_40_loss: 0.1443 - val_dense_41_accuracy: 0.8580 - val_dense_41_loss: 0.2894 - val_loss: 0.4333 - learning_rate: 0.0010\nEpoch 85/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 323ms/step - dense_40_accuracy: 0.9929 - dense_40_loss: 0.0131 - dense_41_accuracy: 0.9706 - dense_41_loss: 0.0393 - loss: 0.0524\nEpoch 85: val_loss did not improve from 0.27446\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 329ms/step - dense_40_accuracy: 0.9929 - dense_40_loss: 0.0131 - dense_41_accuracy: 0.9706 - dense_41_loss: 0.0393 - loss: 0.0524 - val_dense_40_accuracy: 0.9228 - val_dense_40_loss: 0.1540 - val_dense_41_accuracy: 0.8750 - val_dense_41_loss: 0.2110 - val_loss: 0.3778 - learning_rate: 0.0010\nEpoch 86/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 324ms/step - dense_40_accuracy: 0.9971 - dense_40_loss: 0.0052 - dense_41_accuracy: 0.9764 - dense_41_loss: 0.0343 - loss: 0.0395\nEpoch 86: val_loss improved from 0.27446 to 0.25926, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 344ms/step - dense_40_accuracy: 0.9971 - dense_40_loss: 0.0052 - dense_41_accuracy: 0.9764 - dense_41_loss: 0.0343 - loss: 0.0395 - val_dense_40_accuracy: 0.9444 - val_dense_40_loss: 0.1493 - val_dense_41_accuracy: 0.9306 - val_dense_41_loss: 0.1010 - val_loss: 0.2593 - learning_rate: 0.0010\nEpoch 87/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 317ms/step - dense_40_accuracy: 0.9953 - dense_40_loss: 0.0054 - dense_41_accuracy: 0.9813 - dense_41_loss: 0.0301 - loss: 0.0355\nEpoch 87: val_loss did not improve from 0.25926\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 324ms/step - dense_40_accuracy: 0.9953 - dense_40_loss: 0.0054 - dense_41_accuracy: 0.9813 - dense_41_loss: 0.0301 - loss: 0.0355 - val_dense_40_accuracy: 0.9506 - val_dense_40_loss: 0.1101 - val_dense_41_accuracy: 0.8534 - val_dense_41_loss: 0.2637 - val_loss: 0.3718 - learning_rate: 0.0010\nEpoch 88/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - dense_40_accuracy: 0.9974 - dense_40_loss: 0.0053 - dense_41_accuracy: 0.9777 - dense_41_loss: 0.0306 - loss: 0.0359\nEpoch 88: val_loss did not improve from 0.25926\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 324ms/step - dense_40_accuracy: 0.9974 - dense_40_loss: 0.0053 - dense_41_accuracy: 0.9778 - dense_41_loss: 0.0306 - loss: 0.0359 - val_dense_40_accuracy: 0.9583 - val_dense_40_loss: 0.1051 - val_dense_41_accuracy: 0.9028 - val_dense_41_loss: 0.1617 - val_loss: 0.2752 - learning_rate: 0.0010\nEpoch 89/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 335ms/step - dense_40_accuracy: 0.9943 - dense_40_loss: 0.0082 - dense_41_accuracy: 0.9778 - dense_41_loss: 0.0327 - loss: 0.0408\nEpoch 89: val_loss did not improve from 0.25926\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 341ms/step - dense_40_accuracy: 0.9943 - dense_40_loss: 0.0081 - dense_41_accuracy: 0.9778 - dense_41_loss: 0.0327 - loss: 0.0408 - val_dense_40_accuracy: 0.9460 - val_dense_40_loss: 0.1326 - val_dense_41_accuracy: 0.8719 - val_dense_41_loss: 0.2525 - val_loss: 0.3938 - learning_rate: 0.0010\nEpoch 90/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321ms/step - dense_40_accuracy: 0.9958 - dense_40_loss: 0.0069 - dense_41_accuracy: 0.9778 - dense_41_loss: 0.0330 - loss: 0.0399\nEpoch 90: val_loss improved from 0.25926 to 0.24567, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 341ms/step - dense_40_accuracy: 0.9958 - dense_40_loss: 0.0069 - dense_41_accuracy: 0.9778 - dense_41_loss: 0.0330 - loss: 0.0399 - val_dense_40_accuracy: 0.9522 - val_dense_40_loss: 0.1178 - val_dense_41_accuracy: 0.9383 - val_dense_41_loss: 0.1195 - val_loss: 0.2457 - learning_rate: 0.0010\nEpoch 91/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 328ms/step - dense_40_accuracy: 0.9960 - dense_40_loss: 0.0052 - dense_41_accuracy: 0.9767 - dense_41_loss: 0.0302 - loss: 0.0354\nEpoch 91: val_loss did not improve from 0.24567\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 335ms/step - dense_40_accuracy: 0.9960 - dense_40_loss: 0.0052 - dense_41_accuracy: 0.9767 - dense_41_loss: 0.0302 - loss: 0.0354 - val_dense_40_accuracy: 0.9475 - val_dense_40_loss: 0.1289 - val_dense_41_accuracy: 0.8318 - val_dense_41_loss: 0.3911 - val_loss: 0.5265 - learning_rate: 0.0010\nEpoch 92/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 328ms/step - dense_40_accuracy: 0.9975 - dense_40_loss: 0.0034 - dense_41_accuracy: 0.9816 - dense_41_loss: 0.0284 - loss: 0.0318\nEpoch 92: val_loss improved from 0.24567 to 0.21365, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 348ms/step - dense_40_accuracy: 0.9975 - dense_40_loss: 0.0034 - dense_41_accuracy: 0.9816 - dense_41_loss: 0.0284 - loss: 0.0318 - val_dense_40_accuracy: 0.9321 - val_dense_40_loss: 0.1299 - val_dense_41_accuracy: 0.9583 - val_dense_41_loss: 0.0786 - val_loss: 0.2137 - learning_rate: 0.0010\nEpoch 93/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - dense_40_accuracy: 0.9946 - dense_40_loss: 0.0096 - dense_41_accuracy: 0.9801 - dense_41_loss: 0.0332 - loss: 0.0428\nEpoch 93: val_loss did not improve from 0.21365\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 324ms/step - dense_40_accuracy: 0.9946 - dense_40_loss: 0.0096 - dense_41_accuracy: 0.9801 - dense_41_loss: 0.0332 - loss: 0.0428 - val_dense_40_accuracy: 0.9367 - val_dense_40_loss: 0.1790 - val_dense_41_accuracy: 0.8395 - val_dense_41_loss: 0.3385 - val_loss: 0.5255 - learning_rate: 0.0010\nEpoch 94/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 335ms/step - dense_40_accuracy: 0.9940 - dense_40_loss: 0.0085 - dense_41_accuracy: 0.9743 - dense_41_loss: 0.0366 - loss: 0.0451\nEpoch 94: val_loss did not improve from 0.21365\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 341ms/step - dense_40_accuracy: 0.9940 - dense_40_loss: 0.0085 - dense_41_accuracy: 0.9743 - dense_41_loss: 0.0366 - loss: 0.0451 - val_dense_40_accuracy: 0.9444 - val_dense_40_loss: 0.1548 - val_dense_41_accuracy: 0.9244 - val_dense_41_loss: 0.1142 - val_loss: 0.2772 - learning_rate: 0.0010\nEpoch 95/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 316ms/step - dense_40_accuracy: 0.9914 - dense_40_loss: 0.0122 - dense_41_accuracy: 0.9783 - dense_41_loss: 0.0310 - loss: 0.0432\nEpoch 95: val_loss did not improve from 0.21365\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 323ms/step - dense_40_accuracy: 0.9914 - dense_40_loss: 0.0121 - dense_41_accuracy: 0.9783 - dense_41_loss: 0.0310 - loss: 0.0432 - val_dense_40_accuracy: 0.9660 - val_dense_40_loss: 0.1137 - val_dense_41_accuracy: 0.9321 - val_dense_41_loss: 0.1328 - val_loss: 0.2546 - learning_rate: 0.0010\nEpoch 96/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 320ms/step - dense_40_accuracy: 0.9975 - dense_40_loss: 0.0040 - dense_41_accuracy: 0.9804 - dense_41_loss: 0.0259 - loss: 0.0298\nEpoch 96: val_loss did not improve from 0.21365\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 326ms/step - dense_40_accuracy: 0.9975 - dense_40_loss: 0.0040 - dense_41_accuracy: 0.9804 - dense_41_loss: 0.0259 - loss: 0.0298 - val_dense_40_accuracy: 0.9645 - val_dense_40_loss: 0.1033 - val_dense_41_accuracy: 0.8873 - val_dense_41_loss: 0.2193 - val_loss: 0.3335 - learning_rate: 0.0010\nEpoch 97/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 319ms/step - dense_40_accuracy: 0.9977 - dense_40_loss: 0.0024 - dense_41_accuracy: 0.9829 - dense_41_loss: 0.0220 - loss: 0.0244\nEpoch 97: val_loss did not improve from 0.21365\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 325ms/step - dense_40_accuracy: 0.9977 - dense_40_loss: 0.0024 - dense_41_accuracy: 0.9829 - dense_41_loss: 0.0220 - loss: 0.0244 - val_dense_40_accuracy: 0.9583 - val_dense_40_loss: 0.1133 - val_dense_41_accuracy: 0.9182 - val_dense_41_loss: 0.1433 - val_loss: 0.2657 - learning_rate: 0.0010\nEpoch 98/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 329ms/step - dense_40_accuracy: 0.9943 - dense_40_loss: 0.0092 - dense_41_accuracy: 0.9738 - dense_41_loss: 0.0363 - loss: 0.0455\nEpoch 98: val_loss did not improve from 0.21365\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 335ms/step - dense_40_accuracy: 0.9943 - dense_40_loss: 0.0092 - dense_41_accuracy: 0.9738 - dense_41_loss: 0.0363 - loss: 0.0455 - val_dense_40_accuracy: 0.9444 - val_dense_40_loss: 0.1228 - val_dense_41_accuracy: 0.9383 - val_dense_41_loss: 0.0969 - val_loss: 0.2277 - learning_rate: 0.0010\nEpoch 99/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 324ms/step - dense_40_accuracy: 0.9960 - dense_40_loss: 0.0063 - dense_41_accuracy: 0.9837 - dense_41_loss: 0.0229 - loss: 0.0292\nEpoch 99: val_loss did not improve from 0.21365\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 331ms/step - dense_40_accuracy: 0.9960 - dense_40_loss: 0.0063 - dense_41_accuracy: 0.9837 - dense_41_loss: 0.0230 - loss: 0.0293 - val_dense_40_accuracy: 0.9491 - val_dense_40_loss: 0.1237 - val_dense_41_accuracy: 0.8812 - val_dense_41_loss: 0.2022 - val_loss: 0.3354 - learning_rate: 0.0010\nEpoch 100/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 320ms/step - dense_40_accuracy: 0.9930 - dense_40_loss: 0.0107 - dense_41_accuracy: 0.9760 - dense_41_loss: 0.0326 - loss: 0.0433\nEpoch 100: val_loss did not improve from 0.21365\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 327ms/step - dense_40_accuracy: 0.9930 - dense_40_loss: 0.0107 - dense_41_accuracy: 0.9760 - dense_41_loss: 0.0326 - loss: 0.0433 - val_dense_40_accuracy: 0.9460 - val_dense_40_loss: 0.1556 - val_dense_41_accuracy: 0.9136 - val_dense_41_loss: 0.1342 - val_loss: 0.2794 - learning_rate: 0.0010\nEpoch 101/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 328ms/step - dense_40_accuracy: 0.9936 - dense_40_loss: 0.0085 - dense_41_accuracy: 0.9781 - dense_41_loss: 0.0311 - loss: 0.0397\nEpoch 101: val_loss did not improve from 0.21365\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 335ms/step - dense_40_accuracy: 0.9936 - dense_40_loss: 0.0085 - dense_41_accuracy: 0.9781 - dense_41_loss: 0.0312 - loss: 0.0397 - val_dense_40_accuracy: 0.9537 - val_dense_40_loss: 0.1560 - val_dense_41_accuracy: 0.9167 - val_dense_41_loss: 0.1405 - val_loss: 0.3010 - learning_rate: 0.0010\nEpoch 102/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 319ms/step - dense_40_accuracy: 0.9968 - dense_40_loss: 0.0055 - dense_41_accuracy: 0.9871 - dense_41_loss: 0.0213 - loss: 0.0267\nEpoch 102: val_loss did not improve from 0.21365\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 326ms/step - dense_40_accuracy: 0.9968 - dense_40_loss: 0.0054 - dense_41_accuracy: 0.9871 - dense_41_loss: 0.0213 - loss: 0.0268 - val_dense_40_accuracy: 0.9398 - val_dense_40_loss: 0.1476 - val_dense_41_accuracy: 0.8765 - val_dense_41_loss: 0.2299 - val_loss: 0.3896 - learning_rate: 0.0010\nEpoch 103/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 325ms/step - dense_40_accuracy: 0.9964 - dense_40_loss: 0.0051 - dense_41_accuracy: 0.9819 - dense_41_loss: 0.0261 - loss: 0.0312\nEpoch 103: val_loss improved from 0.21365 to 0.19787, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 344ms/step - dense_40_accuracy: 0.9964 - dense_40_loss: 0.0051 - dense_41_accuracy: 0.9819 - dense_41_loss: 0.0261 - loss: 0.0312 - val_dense_40_accuracy: 0.9630 - val_dense_40_loss: 0.0849 - val_dense_41_accuracy: 0.9352 - val_dense_41_loss: 0.1069 - val_loss: 0.1979 - learning_rate: 0.0010\nEpoch 104/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 323ms/step - dense_40_accuracy: 0.9976 - dense_40_loss: 0.0045 - dense_41_accuracy: 0.9809 - dense_41_loss: 0.0275 - loss: 0.0321\nEpoch 104: val_loss did not improve from 0.19787\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 329ms/step - dense_40_accuracy: 0.9976 - dense_40_loss: 0.0045 - dense_41_accuracy: 0.9809 - dense_41_loss: 0.0275 - loss: 0.0321 - val_dense_40_accuracy: 0.9290 - val_dense_40_loss: 0.1820 - val_dense_41_accuracy: 0.9182 - val_dense_41_loss: 0.1592 - val_loss: 0.3533 - learning_rate: 0.0010\nEpoch 105/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 317ms/step - dense_40_accuracy: 0.9967 - dense_40_loss: 0.0054 - dense_41_accuracy: 0.9756 - dense_41_loss: 0.0328 - loss: 0.0382\nEpoch 105: val_loss did not improve from 0.19787\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 324ms/step - dense_40_accuracy: 0.9967 - dense_40_loss: 0.0054 - dense_41_accuracy: 0.9756 - dense_41_loss: 0.0328 - loss: 0.0382 - val_dense_40_accuracy: 0.9383 - val_dense_40_loss: 0.1518 - val_dense_41_accuracy: 0.8302 - val_dense_41_loss: 0.3716 - val_loss: 0.5365 - learning_rate: 0.0010\nEpoch 106/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 324ms/step - dense_40_accuracy: 0.9954 - dense_40_loss: 0.0105 - dense_41_accuracy: 0.9800 - dense_41_loss: 0.0299 - loss: 0.0404\nEpoch 106: val_loss did not improve from 0.19787\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 331ms/step - dense_40_accuracy: 0.9954 - dense_40_loss: 0.0105 - dense_41_accuracy: 0.9800 - dense_41_loss: 0.0299 - loss: 0.0404 - val_dense_40_accuracy: 0.9614 - val_dense_40_loss: 0.0925 - val_dense_41_accuracy: 0.8796 - val_dense_41_loss: 0.2139 - val_loss: 0.3101 - learning_rate: 0.0010\nEpoch 107/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 317ms/step - dense_40_accuracy: 0.9974 - dense_40_loss: 0.0053 - dense_41_accuracy: 0.9791 - dense_41_loss: 0.0296 - loss: 0.0349\nEpoch 107: val_loss did not improve from 0.19787\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 324ms/step - dense_40_accuracy: 0.9974 - dense_40_loss: 0.0053 - dense_41_accuracy: 0.9791 - dense_41_loss: 0.0296 - loss: 0.0349 - val_dense_40_accuracy: 0.9352 - val_dense_40_loss: 0.1273 - val_dense_41_accuracy: 0.8997 - val_dense_41_loss: 0.1929 - val_loss: 0.3122 - learning_rate: 0.0010\nEpoch 108/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 333ms/step - dense_40_accuracy: 0.9976 - dense_40_loss: 0.0047 - dense_41_accuracy: 0.9854 - dense_41_loss: 0.0254 - loss: 0.0301\nEpoch 108: val_loss did not improve from 0.19787\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 339ms/step - dense_40_accuracy: 0.9976 - dense_40_loss: 0.0047 - dense_41_accuracy: 0.9854 - dense_41_loss: 0.0254 - loss: 0.0301 - val_dense_40_accuracy: 0.9491 - val_dense_40_loss: 0.1145 - val_dense_41_accuracy: 0.9244 - val_dense_41_loss: 0.1440 - val_loss: 0.2675 - learning_rate: 0.0010\nEpoch 109/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - dense_40_accuracy: 0.9955 - dense_40_loss: 0.0088 - dense_41_accuracy: 0.9823 - dense_41_loss: 0.0265 - loss: 0.0353\nEpoch 109: val_loss did not improve from 0.19787\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 324ms/step - dense_40_accuracy: 0.9955 - dense_40_loss: 0.0088 - dense_41_accuracy: 0.9823 - dense_41_loss: 0.0265 - loss: 0.0353 - val_dense_40_accuracy: 0.9398 - val_dense_40_loss: 0.1796 - val_dense_41_accuracy: 0.9167 - val_dense_41_loss: 0.1741 - val_loss: 0.3552 - learning_rate: 0.0010\nEpoch 110/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 320ms/step - dense_40_accuracy: 0.9950 - dense_40_loss: 0.0075 - dense_41_accuracy: 0.9810 - dense_41_loss: 0.0273 - loss: 0.0348\nEpoch 110: val_loss did not improve from 0.19787\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 326ms/step - dense_40_accuracy: 0.9950 - dense_40_loss: 0.0075 - dense_41_accuracy: 0.9810 - dense_41_loss: 0.0273 - loss: 0.0348 - val_dense_40_accuracy: 0.9599 - val_dense_40_loss: 0.0920 - val_dense_41_accuracy: 0.9290 - val_dense_41_loss: 0.1051 - val_loss: 0.2045 - learning_rate: 0.0010\nEpoch 111/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 323ms/step - dense_40_accuracy: 0.9965 - dense_40_loss: 0.0063 - dense_41_accuracy: 0.9838 - dense_41_loss: 0.0232 - loss: 0.0295\nEpoch 111: val_loss did not improve from 0.19787\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 329ms/step - dense_40_accuracy: 0.9965 - dense_40_loss: 0.0063 - dense_41_accuracy: 0.9838 - dense_41_loss: 0.0232 - loss: 0.0295 - val_dense_40_accuracy: 0.9552 - val_dense_40_loss: 0.1115 - val_dense_41_accuracy: 0.9028 - val_dense_41_loss: 0.1864 - val_loss: 0.3068 - learning_rate: 0.0010\nEpoch 112/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 316ms/step - dense_40_accuracy: 0.9967 - dense_40_loss: 0.0048 - dense_41_accuracy: 0.9859 - dense_41_loss: 0.0189 - loss: 0.0237\nEpoch 112: val_loss did not improve from 0.19787\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 322ms/step - dense_40_accuracy: 0.9967 - dense_40_loss: 0.0048 - dense_41_accuracy: 0.9859 - dense_41_loss: 0.0189 - loss: 0.0237 - val_dense_40_accuracy: 0.9414 - val_dense_40_loss: 0.1556 - val_dense_41_accuracy: 0.9059 - val_dense_41_loss: 0.2319 - val_loss: 0.3726 - learning_rate: 0.0010\nEpoch 113/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 325ms/step - dense_40_accuracy: 0.9963 - dense_40_loss: 0.0041 - dense_41_accuracy: 0.9867 - dense_41_loss: 0.0215 - loss: 0.0256\nEpoch 113: val_loss did not improve from 0.19787\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 331ms/step - dense_40_accuracy: 0.9963 - dense_40_loss: 0.0041 - dense_41_accuracy: 0.9867 - dense_41_loss: 0.0215 - loss: 0.0256 - val_dense_40_accuracy: 0.9321 - val_dense_40_loss: 0.1588 - val_dense_41_accuracy: 0.9059 - val_dense_41_loss: 0.1577 - val_loss: 0.3276 - learning_rate: 0.0010\nEpoch 114/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 332ms/step - dense_40_accuracy: 0.9943 - dense_40_loss: 0.0087 - dense_41_accuracy: 0.9832 - dense_41_loss: 0.0232 - loss: 0.0318\nEpoch 114: val_loss did not improve from 0.19787\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 338ms/step - dense_40_accuracy: 0.9943 - dense_40_loss: 0.0087 - dense_41_accuracy: 0.9832 - dense_41_loss: 0.0232 - loss: 0.0319 - val_dense_40_accuracy: 0.9290 - val_dense_40_loss: 0.1667 - val_dense_41_accuracy: 0.9167 - val_dense_41_loss: 0.1745 - val_loss: 0.3243 - learning_rate: 0.0010\nEpoch 115/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 316ms/step - dense_40_accuracy: 0.9926 - dense_40_loss: 0.0125 - dense_41_accuracy: 0.9853 - dense_41_loss: 0.0205 - loss: 0.0330\nEpoch 115: val_loss did not improve from 0.19787\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 322ms/step - dense_40_accuracy: 0.9926 - dense_40_loss: 0.0124 - dense_41_accuracy: 0.9853 - dense_41_loss: 0.0205 - loss: 0.0330 - val_dense_40_accuracy: 0.9522 - val_dense_40_loss: 0.1159 - val_dense_41_accuracy: 0.9151 - val_dense_41_loss: 0.1417 - val_loss: 0.2611 - learning_rate: 0.0010\nEpoch 116/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 317ms/step - dense_40_accuracy: 0.9985 - dense_40_loss: 0.0029 - dense_41_accuracy: 0.9860 - dense_41_loss: 0.0210 - loss: 0.0239\nEpoch 116: val_loss did not improve from 0.19787\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 323ms/step - dense_40_accuracy: 0.9985 - dense_40_loss: 0.0029 - dense_41_accuracy: 0.9860 - dense_41_loss: 0.0210 - loss: 0.0239 - val_dense_40_accuracy: 0.9522 - val_dense_40_loss: 0.1290 - val_dense_41_accuracy: 0.8904 - val_dense_41_loss: 0.1734 - val_loss: 0.3080 - learning_rate: 0.0010\nEpoch 117/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 322ms/step - dense_40_accuracy: 0.9967 - dense_40_loss: 0.0053 - dense_41_accuracy: 0.9735 - dense_41_loss: 0.0373 - loss: 0.0427\nEpoch 117: val_loss did not improve from 0.19787\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 329ms/step - dense_40_accuracy: 0.9967 - dense_40_loss: 0.0053 - dense_41_accuracy: 0.9735 - dense_41_loss: 0.0373 - loss: 0.0427 - val_dense_40_accuracy: 0.9398 - val_dense_40_loss: 0.1640 - val_dense_41_accuracy: 0.9275 - val_dense_41_loss: 0.1129 - val_loss: 0.2824 - learning_rate: 0.0010\nEpoch 118/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 325ms/step - dense_40_accuracy: 0.9963 - dense_40_loss: 0.0066 - dense_41_accuracy: 0.9839 - dense_41_loss: 0.0219 - loss: 0.0285\nEpoch 118: val_loss did not improve from 0.19787\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 331ms/step - dense_40_accuracy: 0.9963 - dense_40_loss: 0.0066 - dense_41_accuracy: 0.9839 - dense_41_loss: 0.0219 - loss: 0.0285 - val_dense_40_accuracy: 0.9506 - val_dense_40_loss: 0.1470 - val_dense_41_accuracy: 0.9012 - val_dense_41_loss: 0.1752 - val_loss: 0.3136 - learning_rate: 0.0010\nEpoch 119/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - dense_40_accuracy: 0.9975 - dense_40_loss: 0.0041 - dense_41_accuracy: 0.9839 - dense_41_loss: 0.0233 - loss: 0.0274\nEpoch 119: val_loss did not improve from 0.19787\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 324ms/step - dense_40_accuracy: 0.9975 - dense_40_loss: 0.0041 - dense_41_accuracy: 0.9839 - dense_41_loss: 0.0233 - loss: 0.0274 - val_dense_40_accuracy: 0.9444 - val_dense_40_loss: 0.1258 - val_dense_41_accuracy: 0.8966 - val_dense_41_loss: 0.1494 - val_loss: 0.2835 - learning_rate: 0.0010\nEpoch 120/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321ms/step - dense_40_accuracy: 0.9997 - dense_40_loss: 8.4475e-04 - dense_41_accuracy: 0.9843 - dense_41_loss: 0.0243 - loss: 0.0252\nEpoch 120: val_loss did not improve from 0.19787\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 328ms/step - dense_40_accuracy: 0.9997 - dense_40_loss: 8.4950e-04 - dense_41_accuracy: 0.9843 - dense_41_loss: 0.0243 - loss: 0.0252 - val_dense_40_accuracy: 0.9383 - val_dense_40_loss: 0.1493 - val_dense_41_accuracy: 0.9105 - val_dense_41_loss: 0.1650 - val_loss: 0.3209 - learning_rate: 0.0010\nEpoch 121/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 319ms/step - dense_40_accuracy: 0.9947 - dense_40_loss: 0.0060 - dense_41_accuracy: 0.9843 - dense_41_loss: 0.0226 - loss: 0.0287\nEpoch 121: val_loss did not improve from 0.19787\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 325ms/step - dense_40_accuracy: 0.9947 - dense_40_loss: 0.0060 - dense_41_accuracy: 0.9843 - dense_41_loss: 0.0226 - loss: 0.0287 - val_dense_40_accuracy: 0.9460 - val_dense_40_loss: 0.1598 - val_dense_41_accuracy: 0.8688 - val_dense_41_loss: 0.2665 - val_loss: 0.4420 - learning_rate: 0.0010\nEpoch 122/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - dense_40_accuracy: 0.9970 - dense_40_loss: 0.0037 - dense_41_accuracy: 0.9867 - dense_41_loss: 0.0207 - loss: 0.0244\nEpoch 122: val_loss did not improve from 0.19787\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 325ms/step - dense_40_accuracy: 0.9970 - dense_40_loss: 0.0037 - dense_41_accuracy: 0.9867 - dense_41_loss: 0.0207 - loss: 0.0244 - val_dense_40_accuracy: 0.9614 - val_dense_40_loss: 0.1081 - val_dense_41_accuracy: 0.9321 - val_dense_41_loss: 0.1109 - val_loss: 0.2270 - learning_rate: 0.0010\nEpoch 123/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 317ms/step - dense_40_accuracy: 0.9964 - dense_40_loss: 0.0054 - dense_41_accuracy: 0.9849 - dense_41_loss: 0.0213 - loss: 0.0267\nEpoch 123: val_loss did not improve from 0.19787\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 323ms/step - dense_40_accuracy: 0.9964 - dense_40_loss: 0.0054 - dense_41_accuracy: 0.9849 - dense_41_loss: 0.0213 - loss: 0.0267 - val_dense_40_accuracy: 0.9522 - val_dense_40_loss: 0.1243 - val_dense_41_accuracy: 0.9306 - val_dense_41_loss: 0.1373 - val_loss: 0.2713 - learning_rate: 0.0010\nEpoch 124/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 328ms/step - dense_40_accuracy: 0.9976 - dense_40_loss: 0.0034 - dense_41_accuracy: 0.9859 - dense_41_loss: 0.0193 - loss: 0.0227\nEpoch 124: val_loss did not improve from 0.19787\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 335ms/step - dense_40_accuracy: 0.9976 - dense_40_loss: 0.0034 - dense_41_accuracy: 0.9859 - dense_41_loss: 0.0193 - loss: 0.0227 - val_dense_40_accuracy: 0.9398 - val_dense_40_loss: 0.1605 - val_dense_41_accuracy: 0.9074 - val_dense_41_loss: 0.1720 - val_loss: 0.3301 - learning_rate: 0.0010\nEpoch 125/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 317ms/step - dense_40_accuracy: 0.9959 - dense_40_loss: 0.0068 - dense_41_accuracy: 0.9845 - dense_41_loss: 0.0261 - loss: 0.0329\nEpoch 125: val_loss did not improve from 0.19787\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 323ms/step - dense_40_accuracy: 0.9959 - dense_40_loss: 0.0068 - dense_41_accuracy: 0.9845 - dense_41_loss: 0.0261 - loss: 0.0329 - val_dense_40_accuracy: 0.9552 - val_dense_40_loss: 0.1248 - val_dense_41_accuracy: 0.9228 - val_dense_41_loss: 0.1377 - val_loss: 0.2701 - learning_rate: 0.0010\nEpoch 126/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 317ms/step - dense_40_accuracy: 0.9967 - dense_40_loss: 0.0056 - dense_41_accuracy: 0.9775 - dense_41_loss: 0.0337 - loss: 0.0393\nEpoch 126: val_loss did not improve from 0.19787\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 324ms/step - dense_40_accuracy: 0.9967 - dense_40_loss: 0.0056 - dense_41_accuracy: 0.9775 - dense_41_loss: 0.0337 - loss: 0.0393 - val_dense_40_accuracy: 0.9583 - val_dense_40_loss: 0.1195 - val_dense_41_accuracy: 0.8827 - val_dense_41_loss: 0.2175 - val_loss: 0.3463 - learning_rate: 0.0010\nEpoch 127/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321ms/step - dense_40_accuracy: 0.9977 - dense_40_loss: 0.0035 - dense_41_accuracy: 0.9829 - dense_41_loss: 0.0232 - loss: 0.0267\nEpoch 127: val_loss did not improve from 0.19787\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 327ms/step - dense_40_accuracy: 0.9977 - dense_40_loss: 0.0035 - dense_41_accuracy: 0.9829 - dense_41_loss: 0.0232 - loss: 0.0267 - val_dense_40_accuracy: 0.9460 - val_dense_40_loss: 0.1660 - val_dense_41_accuracy: 0.9228 - val_dense_41_loss: 0.1523 - val_loss: 0.3297 - learning_rate: 0.0010\nEpoch 128/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 326ms/step - dense_40_accuracy: 0.9950 - dense_40_loss: 0.0093 - dense_41_accuracy: 0.9856 - dense_41_loss: 0.0216 - loss: 0.0308\nEpoch 128: val_loss did not improve from 0.19787\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 332ms/step - dense_40_accuracy: 0.9950 - dense_40_loss: 0.0093 - dense_41_accuracy: 0.9856 - dense_41_loss: 0.0215 - loss: 0.0308 - val_dense_40_accuracy: 0.9491 - val_dense_40_loss: 0.1344 - val_dense_41_accuracy: 0.9367 - val_dense_41_loss: 0.1246 - val_loss: 0.2670 - learning_rate: 0.0010\nEpoch 129/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - dense_40_accuracy: 0.9951 - dense_40_loss: 0.0090 - dense_41_accuracy: 0.9839 - dense_41_loss: 0.0225 - loss: 0.0315\nEpoch 129: val_loss did not improve from 0.19787\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 325ms/step - dense_40_accuracy: 0.9951 - dense_40_loss: 0.0090 - dense_41_accuracy: 0.9839 - dense_41_loss: 0.0225 - loss: 0.0315 - val_dense_40_accuracy: 0.9444 - val_dense_40_loss: 0.1751 - val_dense_41_accuracy: 0.8904 - val_dense_41_loss: 0.2042 - val_loss: 0.3729 - learning_rate: 0.0010\nEpoch 130/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 316ms/step - dense_40_accuracy: 0.9976 - dense_40_loss: 0.0036 - dense_41_accuracy: 0.9857 - dense_41_loss: 0.0204 - loss: 0.0239\nEpoch 130: val_loss did not improve from 0.19787\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 322ms/step - dense_40_accuracy: 0.9976 - dense_40_loss: 0.0036 - dense_41_accuracy: 0.9857 - dense_41_loss: 0.0204 - loss: 0.0240 - val_dense_40_accuracy: 0.9460 - val_dense_40_loss: 0.1227 - val_dense_41_accuracy: 0.8966 - val_dense_41_loss: 0.2129 - val_loss: 0.3429 - learning_rate: 0.0010\nEpoch 131/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 324ms/step - dense_40_accuracy: 0.9963 - dense_40_loss: 0.0057 - dense_41_accuracy: 0.9884 - dense_41_loss: 0.0161 - loss: 0.0218\nEpoch 131: val_loss did not improve from 0.19787\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 331ms/step - dense_40_accuracy: 0.9963 - dense_40_loss: 0.0057 - dense_41_accuracy: 0.9884 - dense_41_loss: 0.0161 - loss: 0.0218 - val_dense_40_accuracy: 0.9583 - val_dense_40_loss: 0.1206 - val_dense_41_accuracy: 0.9167 - val_dense_41_loss: 0.1523 - val_loss: 0.2766 - learning_rate: 0.0010\nEpoch 132/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 315ms/step - dense_40_accuracy: 0.9950 - dense_40_loss: 0.0069 - dense_41_accuracy: 0.9813 - dense_41_loss: 0.0294 - loss: 0.0363\nEpoch 132: val_loss did not improve from 0.19787\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 322ms/step - dense_40_accuracy: 0.9950 - dense_40_loss: 0.0069 - dense_41_accuracy: 0.9813 - dense_41_loss: 0.0294 - loss: 0.0363 - val_dense_40_accuracy: 0.9321 - val_dense_40_loss: 0.1585 - val_dense_41_accuracy: 0.9275 - val_dense_41_loss: 0.1213 - val_loss: 0.2859 - learning_rate: 0.0010\nEpoch 133/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - dense_40_accuracy: 0.9945 - dense_40_loss: 0.0090 - dense_41_accuracy: 0.9868 - dense_41_loss: 0.0212 - loss: 0.0302\nEpoch 133: val_loss did not improve from 0.19787\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 324ms/step - dense_40_accuracy: 0.9945 - dense_40_loss: 0.0090 - dense_41_accuracy: 0.9868 - dense_41_loss: 0.0212 - loss: 0.0302 - val_dense_40_accuracy: 0.9444 - val_dense_40_loss: 0.1360 - val_dense_41_accuracy: 0.9120 - val_dense_41_loss: 0.1473 - val_loss: 0.2934 - learning_rate: 0.0010\nEpoch 134/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 331ms/step - dense_40_accuracy: 0.9989 - dense_40_loss: 0.0026 - dense_41_accuracy: 0.9897 - dense_41_loss: 0.0154 - loss: 0.0180\nEpoch 134: val_loss did not improve from 0.19787\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 338ms/step - dense_40_accuracy: 0.9989 - dense_40_loss: 0.0026 - dense_41_accuracy: 0.9897 - dense_41_loss: 0.0154 - loss: 0.0180 - val_dense_40_accuracy: 0.9198 - val_dense_40_loss: 0.1768 - val_dense_41_accuracy: 0.9228 - val_dense_41_loss: 0.1756 - val_loss: 0.3537 - learning_rate: 0.0010\nEpoch 135/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 315ms/step - dense_40_accuracy: 0.9958 - dense_40_loss: 0.0072 - dense_41_accuracy: 0.9896 - dense_41_loss: 0.0167 - loss: 0.0239\nEpoch 135: val_loss did not improve from 0.19787\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 322ms/step - dense_40_accuracy: 0.9958 - dense_40_loss: 0.0072 - dense_41_accuracy: 0.9896 - dense_41_loss: 0.0168 - loss: 0.0239 - val_dense_40_accuracy: 0.9506 - val_dense_40_loss: 0.1305 - val_dense_41_accuracy: 0.9074 - val_dense_41_loss: 0.1689 - val_loss: 0.3081 - learning_rate: 0.0010\nEpoch 136/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 317ms/step - dense_40_accuracy: 0.9974 - dense_40_loss: 0.0050 - dense_41_accuracy: 0.9785 - dense_41_loss: 0.0310 - loss: 0.0360\nEpoch 136: val_loss did not improve from 0.19787\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 323ms/step - dense_40_accuracy: 0.9974 - dense_40_loss: 0.0050 - dense_41_accuracy: 0.9785 - dense_41_loss: 0.0310 - loss: 0.0360 - val_dense_40_accuracy: 0.9552 - val_dense_40_loss: 0.1021 - val_dense_41_accuracy: 0.9460 - val_dense_41_loss: 0.0913 - val_loss: 0.2004 - learning_rate: 0.0010\nEpoch 137/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321ms/step - dense_40_accuracy: 0.9976 - dense_40_loss: 0.0038 - dense_41_accuracy: 0.9846 - dense_41_loss: 0.0201 - loss: 0.0238\nEpoch 137: val_loss did not improve from 0.19787\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 328ms/step - dense_40_accuracy: 0.9976 - dense_40_loss: 0.0038 - dense_41_accuracy: 0.9846 - dense_41_loss: 0.0201 - loss: 0.0238 - val_dense_40_accuracy: 0.9398 - val_dense_40_loss: 0.1490 - val_dense_41_accuracy: 0.8951 - val_dense_41_loss: 0.2792 - val_loss: 0.4283 - learning_rate: 0.0010\nEpoch 138/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 331ms/step - dense_40_accuracy: 0.9966 - dense_40_loss: 0.0048 - dense_41_accuracy: 0.9877 - dense_41_loss: 0.0194 - loss: 0.0242\nEpoch 138: val_loss did not improve from 0.19787\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 337ms/step - dense_40_accuracy: 0.9966 - dense_40_loss: 0.0048 - dense_41_accuracy: 0.9877 - dense_41_loss: 0.0194 - loss: 0.0242 - val_dense_40_accuracy: 0.9491 - val_dense_40_loss: 0.1251 - val_dense_41_accuracy: 0.8843 - val_dense_41_loss: 0.2593 - val_loss: 0.3908 - learning_rate: 0.0010\nEpoch 139/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - dense_40_accuracy: 0.9966 - dense_40_loss: 0.0054 - dense_41_accuracy: 0.9880 - dense_41_loss: 0.0160 - loss: 0.0214\nEpoch 139: val_loss did not improve from 0.19787\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 325ms/step - dense_40_accuracy: 0.9966 - dense_40_loss: 0.0054 - dense_41_accuracy: 0.9880 - dense_41_loss: 0.0160 - loss: 0.0214 - val_dense_40_accuracy: 0.9614 - val_dense_40_loss: 0.1317 - val_dense_41_accuracy: 0.9182 - val_dense_41_loss: 0.1565 - val_loss: 0.2962 - learning_rate: 0.0010\nEpoch 140/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 316ms/step - dense_40_accuracy: 0.9959 - dense_40_loss: 0.0082 - dense_41_accuracy: 0.9861 - dense_41_loss: 0.0229 - loss: 0.0311\nEpoch 140: val_loss did not improve from 0.19787\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 322ms/step - dense_40_accuracy: 0.9959 - dense_40_loss: 0.0082 - dense_41_accuracy: 0.9860 - dense_41_loss: 0.0229 - loss: 0.0311 - val_dense_40_accuracy: 0.9429 - val_dense_40_loss: 0.1488 - val_dense_41_accuracy: 0.8966 - val_dense_41_loss: 0.2134 - val_loss: 0.3748 - learning_rate: 0.0010\nEpoch 141/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 316ms/step - dense_40_accuracy: 0.9993 - dense_40_loss: 0.0016 - dense_41_accuracy: 0.9841 - dense_41_loss: 0.0221 - loss: 0.0237\nEpoch 141: val_loss did not improve from 0.19787\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 323ms/step - dense_40_accuracy: 0.9993 - dense_40_loss: 0.0016 - dense_41_accuracy: 0.9841 - dense_41_loss: 0.0221 - loss: 0.0237 - val_dense_40_accuracy: 0.9475 - val_dense_40_loss: 0.1503 - val_dense_41_accuracy: 0.8472 - val_dense_41_loss: 0.3238 - val_loss: 0.4781 - learning_rate: 0.0010\nEpoch 142/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 317ms/step - dense_40_accuracy: 0.9952 - dense_40_loss: 0.0049 - dense_41_accuracy: 0.9842 - dense_41_loss: 0.0235 - loss: 0.0284\nEpoch 142: val_loss did not improve from 0.19787\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 323ms/step - dense_40_accuracy: 0.9952 - dense_40_loss: 0.0049 - dense_41_accuracy: 0.9842 - dense_41_loss: 0.0235 - loss: 0.0284 - val_dense_40_accuracy: 0.9568 - val_dense_40_loss: 0.1258 - val_dense_41_accuracy: 0.9151 - val_dense_41_loss: 0.1621 - val_loss: 0.2888 - learning_rate: 0.0010\nEpoch 143/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 316ms/step - dense_40_accuracy: 0.9968 - dense_40_loss: 0.0057 - dense_41_accuracy: 0.9877 - dense_41_loss: 0.0199 - loss: 0.0256\nEpoch 143: val_loss did not improve from 0.19787\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 323ms/step - dense_40_accuracy: 0.9968 - dense_40_loss: 0.0057 - dense_41_accuracy: 0.9877 - dense_41_loss: 0.0199 - loss: 0.0256 - val_dense_40_accuracy: 0.9506 - val_dense_40_loss: 0.1366 - val_dense_41_accuracy: 0.9043 - val_dense_41_loss: 0.2246 - val_loss: 0.3329 - learning_rate: 0.0010\nEpoch 144/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 324ms/step - dense_40_accuracy: 0.9995 - dense_40_loss: 0.0014 - dense_41_accuracy: 0.9865 - dense_41_loss: 0.0226 - loss: 0.0241\nEpoch 144: val_loss improved from 0.19787 to 0.19778, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 343ms/step - dense_40_accuracy: 0.9995 - dense_40_loss: 0.0014 - dense_41_accuracy: 0.9865 - dense_41_loss: 0.0226 - loss: 0.0241 - val_dense_40_accuracy: 0.9568 - val_dense_40_loss: 0.1006 - val_dense_41_accuracy: 0.9522 - val_dense_41_loss: 0.0921 - val_loss: 0.1978 - learning_rate: 0.0010\nEpoch 145/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - dense_40_accuracy: 0.9989 - dense_40_loss: 0.0024 - dense_41_accuracy: 0.9930 - dense_41_loss: 0.0109 - loss: 0.0133\nEpoch 145: val_loss did not improve from 0.19778\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 324ms/step - dense_40_accuracy: 0.9989 - dense_40_loss: 0.0024 - dense_41_accuracy: 0.9930 - dense_41_loss: 0.0109 - loss: 0.0133 - val_dense_40_accuracy: 0.9522 - val_dense_40_loss: 0.1364 - val_dense_41_accuracy: 0.8765 - val_dense_41_loss: 0.2352 - val_loss: 0.3707 - learning_rate: 0.0010\nEpoch 146/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - dense_40_accuracy: 0.9962 - dense_40_loss: 0.0045 - dense_41_accuracy: 0.9878 - dense_41_loss: 0.0169 - loss: 0.0213\nEpoch 146: val_loss did not improve from 0.19778\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 324ms/step - dense_40_accuracy: 0.9962 - dense_40_loss: 0.0045 - dense_41_accuracy: 0.9878 - dense_41_loss: 0.0169 - loss: 0.0214 - val_dense_40_accuracy: 0.9321 - val_dense_40_loss: 0.1778 - val_dense_41_accuracy: 0.8904 - val_dense_41_loss: 0.1856 - val_loss: 0.3727 - learning_rate: 0.0010\nEpoch 147/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - dense_40_accuracy: 0.9958 - dense_40_loss: 0.0065 - dense_41_accuracy: 0.9860 - dense_41_loss: 0.0181 - loss: 0.0245\nEpoch 147: val_loss did not improve from 0.19778\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 324ms/step - dense_40_accuracy: 0.9958 - dense_40_loss: 0.0065 - dense_41_accuracy: 0.9860 - dense_41_loss: 0.0181 - loss: 0.0245 - val_dense_40_accuracy: 0.9568 - val_dense_40_loss: 0.1231 - val_dense_41_accuracy: 0.8997 - val_dense_41_loss: 0.2210 - val_loss: 0.3492 - learning_rate: 0.0010\nEpoch 148/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 325ms/step - dense_40_accuracy: 0.9965 - dense_40_loss: 0.0048 - dense_41_accuracy: 0.9872 - dense_41_loss: 0.0202 - loss: 0.0251\nEpoch 148: val_loss did not improve from 0.19778\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 332ms/step - dense_40_accuracy: 0.9965 - dense_40_loss: 0.0048 - dense_41_accuracy: 0.9872 - dense_41_loss: 0.0202 - loss: 0.0251 - val_dense_40_accuracy: 0.9367 - val_dense_40_loss: 0.1646 - val_dense_41_accuracy: 0.9290 - val_dense_41_loss: 0.1090 - val_loss: 0.2822 - learning_rate: 0.0010\nEpoch 149/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 323ms/step - dense_40_accuracy: 0.9963 - dense_40_loss: 0.0062 - dense_41_accuracy: 0.9870 - dense_41_loss: 0.0199 - loss: 0.0261\nEpoch 149: val_loss did not improve from 0.19778\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 330ms/step - dense_40_accuracy: 0.9963 - dense_40_loss: 0.0062 - dense_41_accuracy: 0.9870 - dense_41_loss: 0.0199 - loss: 0.0261 - val_dense_40_accuracy: 0.9367 - val_dense_40_loss: 0.1668 - val_dense_41_accuracy: 0.9275 - val_dense_41_loss: 0.1137 - val_loss: 0.2842 - learning_rate: 0.0010\nEpoch 150/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 319ms/step - dense_40_accuracy: 0.9969 - dense_40_loss: 0.0046 - dense_41_accuracy: 0.9852 - dense_41_loss: 0.0227 - loss: 0.0274\nEpoch 150: val_loss did not improve from 0.19778\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 325ms/step - dense_40_accuracy: 0.9969 - dense_40_loss: 0.0046 - dense_41_accuracy: 0.9852 - dense_41_loss: 0.0227 - loss: 0.0274 - val_dense_40_accuracy: 0.9429 - val_dense_40_loss: 0.1428 - val_dense_41_accuracy: 0.8765 - val_dense_41_loss: 0.2451 - val_loss: 0.4001 - learning_rate: 0.0010\nEpoch 151/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 327ms/step - dense_40_accuracy: 0.9938 - dense_40_loss: 0.0148 - dense_41_accuracy: 0.9885 - dense_41_loss: 0.0139 - loss: 0.0287\nEpoch 151: val_loss did not improve from 0.19778\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 333ms/step - dense_40_accuracy: 0.9938 - dense_40_loss: 0.0147 - dense_41_accuracy: 0.9885 - dense_41_loss: 0.0139 - loss: 0.0287 - val_dense_40_accuracy: 0.9537 - val_dense_40_loss: 0.1210 - val_dense_41_accuracy: 0.9460 - val_dense_41_loss: 0.0903 - val_loss: 0.2189 - learning_rate: 0.0010\nEpoch 152/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 319ms/step - dense_40_accuracy: 0.9977 - dense_40_loss: 0.0035 - dense_41_accuracy: 0.9895 - dense_41_loss: 0.0141 - loss: 0.0177\nEpoch 152: val_loss did not improve from 0.19778\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 325ms/step - dense_40_accuracy: 0.9977 - dense_40_loss: 0.0035 - dense_41_accuracy: 0.9895 - dense_41_loss: 0.0141 - loss: 0.0177 - val_dense_40_accuracy: 0.9568 - val_dense_40_loss: 0.0965 - val_dense_41_accuracy: 0.9090 - val_dense_41_loss: 0.1825 - val_loss: 0.2893 - learning_rate: 0.0010\nEpoch 153/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 326ms/step - dense_40_accuracy: 0.9982 - dense_40_loss: 0.0023 - dense_41_accuracy: 0.9890 - dense_41_loss: 0.0158 - loss: 0.0181\nEpoch 153: val_loss did not improve from 0.19778\n\nEpoch 153: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 333ms/step - dense_40_accuracy: 0.9982 - dense_40_loss: 0.0023 - dense_41_accuracy: 0.9890 - dense_41_loss: 0.0158 - loss: 0.0181 - val_dense_40_accuracy: 0.9491 - val_dense_40_loss: 0.1296 - val_dense_41_accuracy: 0.9367 - val_dense_41_loss: 0.1152 - val_loss: 0.2532 - learning_rate: 0.0010\nEpoch 154/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321ms/step - dense_40_accuracy: 0.9987 - dense_40_loss: 0.0027 - dense_41_accuracy: 0.9952 - dense_41_loss: 0.0075 - loss: 0.0102\nEpoch 154: val_loss improved from 0.19778 to 0.15949, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 341ms/step - dense_40_accuracy: 0.9987 - dense_40_loss: 0.0027 - dense_41_accuracy: 0.9952 - dense_41_loss: 0.0075 - loss: 0.0102 - val_dense_40_accuracy: 0.9583 - val_dense_40_loss: 0.1018 - val_dense_41_accuracy: 0.9722 - val_dense_41_loss: 0.0523 - val_loss: 0.1595 - learning_rate: 1.0000e-04\nEpoch 155/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 322ms/step - dense_40_accuracy: 0.9989 - dense_40_loss: 0.0015 - dense_41_accuracy: 0.9961 - dense_41_loss: 0.0068 - loss: 0.0083\nEpoch 155: val_loss improved from 0.15949 to 0.15212, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 342ms/step - dense_40_accuracy: 0.9989 - dense_40_loss: 0.0015 - dense_41_accuracy: 0.9961 - dense_41_loss: 0.0068 - loss: 0.0083 - val_dense_40_accuracy: 0.9614 - val_dense_40_loss: 0.0981 - val_dense_41_accuracy: 0.9707 - val_dense_41_loss: 0.0488 - val_loss: 0.1521 - learning_rate: 1.0000e-04\nEpoch 156/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - dense_40_accuracy: 0.9987 - dense_40_loss: 0.0012 - dense_41_accuracy: 0.9975 - dense_41_loss: 0.0040 - loss: 0.0052\nEpoch 156: val_loss improved from 0.15212 to 0.14970, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 337ms/step - dense_40_accuracy: 0.9987 - dense_40_loss: 0.0012 - dense_41_accuracy: 0.9975 - dense_41_loss: 0.0040 - loss: 0.0052 - val_dense_40_accuracy: 0.9630 - val_dense_40_loss: 0.0959 - val_dense_41_accuracy: 0.9722 - val_dense_41_loss: 0.0485 - val_loss: 0.1497 - learning_rate: 1.0000e-04\nEpoch 157/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 326ms/step - dense_40_accuracy: 0.9990 - dense_40_loss: 0.0011 - dense_41_accuracy: 0.9974 - dense_41_loss: 0.0044 - loss: 0.0054\nEpoch 157: val_loss improved from 0.14970 to 0.14514, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 346ms/step - dense_40_accuracy: 0.9990 - dense_40_loss: 0.0011 - dense_41_accuracy: 0.9975 - dense_41_loss: 0.0044 - loss: 0.0054 - val_dense_40_accuracy: 0.9614 - val_dense_40_loss: 0.0931 - val_dense_41_accuracy: 0.9722 - val_dense_41_loss: 0.0469 - val_loss: 0.1451 - learning_rate: 1.0000e-04\nEpoch 158/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 319ms/step - dense_40_accuracy: 0.9998 - dense_40_loss: 5.3641e-04 - dense_41_accuracy: 0.9982 - dense_41_loss: 0.0031 - loss: 0.0037\nEpoch 158: val_loss did not improve from 0.14514\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 325ms/step - dense_40_accuracy: 0.9998 - dense_40_loss: 5.3685e-04 - dense_41_accuracy: 0.9982 - dense_41_loss: 0.0031 - loss: 0.0037 - val_dense_40_accuracy: 0.9614 - val_dense_40_loss: 0.0993 - val_dense_41_accuracy: 0.9738 - val_dense_41_loss: 0.0485 - val_loss: 0.1531 - learning_rate: 1.0000e-04\nEpoch 159/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 317ms/step - dense_40_accuracy: 0.9993 - dense_40_loss: 7.3224e-04 - dense_41_accuracy: 0.9987 - dense_41_loss: 0.0028 - loss: 0.0035\nEpoch 159: val_loss did not improve from 0.14514\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 323ms/step - dense_40_accuracy: 0.9993 - dense_40_loss: 7.3269e-04 - dense_41_accuracy: 0.9986 - dense_41_loss: 0.0028 - loss: 0.0035 - val_dense_40_accuracy: 0.9676 - val_dense_40_loss: 0.0940 - val_dense_41_accuracy: 0.9707 - val_dense_41_loss: 0.0509 - val_loss: 0.1501 - learning_rate: 1.0000e-04\nEpoch 160/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 317ms/step - dense_40_accuracy: 0.9997 - dense_40_loss: 6.5345e-04 - dense_41_accuracy: 0.9989 - dense_41_loss: 0.0023 - loss: 0.0029\nEpoch 160: val_loss did not improve from 0.14514\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 323ms/step - dense_40_accuracy: 0.9997 - dense_40_loss: 6.5462e-04 - dense_41_accuracy: 0.9989 - dense_41_loss: 0.0023 - loss: 0.0029 - val_dense_40_accuracy: 0.9660 - val_dense_40_loss: 0.1044 - val_dense_41_accuracy: 0.9691 - val_dense_41_loss: 0.0452 - val_loss: 0.1550 - learning_rate: 1.0000e-04\nEpoch 161/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 323ms/step - dense_40_accuracy: 0.9999 - dense_40_loss: 3.2032e-04 - dense_41_accuracy: 0.9979 - dense_41_loss: 0.0032 - loss: 0.0035\nEpoch 161: val_loss did not improve from 0.14514\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 329ms/step - dense_40_accuracy: 0.9999 - dense_40_loss: 3.2023e-04 - dense_41_accuracy: 0.9979 - dense_41_loss: 0.0032 - loss: 0.0035 - val_dense_40_accuracy: 0.9630 - val_dense_40_loss: 0.1011 - val_dense_41_accuracy: 0.9722 - val_dense_41_loss: 0.0468 - val_loss: 0.1533 - learning_rate: 1.0000e-04\nEpoch 162/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 334ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 1.9937e-04 - dense_41_accuracy: 0.9989 - dense_41_loss: 0.0025 - loss: 0.0027\nEpoch 162: val_loss did not improve from 0.14514\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 341ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 1.9998e-04 - dense_41_accuracy: 0.9989 - dense_41_loss: 0.0025 - loss: 0.0027 - val_dense_40_accuracy: 0.9676 - val_dense_40_loss: 0.0975 - val_dense_41_accuracy: 0.9738 - val_dense_41_loss: 0.0490 - val_loss: 0.1518 - learning_rate: 1.0000e-04\nEpoch 163/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 317ms/step - dense_40_accuracy: 0.9996 - dense_40_loss: 3.7919e-04 - dense_41_accuracy: 0.9972 - dense_41_loss: 0.0040 - loss: 0.0044\nEpoch 163: val_loss did not improve from 0.14514\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 324ms/step - dense_40_accuracy: 0.9996 - dense_40_loss: 3.7901e-04 - dense_41_accuracy: 0.9972 - dense_41_loss: 0.0040 - loss: 0.0044 - val_dense_40_accuracy: 0.9660 - val_dense_40_loss: 0.0967 - val_dense_41_accuracy: 0.9691 - val_dense_41_loss: 0.0514 - val_loss: 0.1533 - learning_rate: 1.0000e-04\nEpoch 164/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321ms/step - dense_40_accuracy: 0.9999 - dense_40_loss: 1.7371e-04 - dense_41_accuracy: 0.9987 - dense_41_loss: 0.0023 - loss: 0.0024\nEpoch 164: val_loss improved from 0.14514 to 0.14327, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 340ms/step - dense_40_accuracy: 0.9999 - dense_40_loss: 1.7467e-04 - dense_41_accuracy: 0.9987 - dense_41_loss: 0.0023 - loss: 0.0024 - val_dense_40_accuracy: 0.9722 - val_dense_40_loss: 0.0913 - val_dense_41_accuracy: 0.9691 - val_dense_41_loss: 0.0470 - val_loss: 0.1433 - learning_rate: 1.0000e-04\nEpoch 165/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 316ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 1.9516e-04 - dense_41_accuracy: 0.9994 - dense_41_loss: 0.0018 - loss: 0.0020\nEpoch 165: val_loss did not improve from 0.14327\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 322ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 1.9513e-04 - dense_41_accuracy: 0.9994 - dense_41_loss: 0.0018 - loss: 0.0020 - val_dense_40_accuracy: 0.9691 - val_dense_40_loss: 0.0970 - val_dense_41_accuracy: 0.9691 - val_dense_41_loss: 0.0506 - val_loss: 0.1530 - learning_rate: 1.0000e-04\nEpoch 166/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 322ms/step - dense_40_accuracy: 0.9999 - dense_40_loss: 1.8550e-04 - dense_41_accuracy: 0.9977 - dense_41_loss: 0.0026 - loss: 0.0028\nEpoch 166: val_loss did not improve from 0.14327\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 329ms/step - dense_40_accuracy: 0.9999 - dense_40_loss: 1.8615e-04 - dense_41_accuracy: 0.9977 - dense_41_loss: 0.0026 - loss: 0.0028 - val_dense_40_accuracy: 0.9660 - val_dense_40_loss: 0.0953 - val_dense_41_accuracy: 0.9645 - val_dense_41_loss: 0.0484 - val_loss: 0.1490 - learning_rate: 1.0000e-04\nEpoch 167/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 325ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 2.1073e-04 - dense_41_accuracy: 0.9977 - dense_41_loss: 0.0029 - loss: 0.0031\nEpoch 167: val_loss did not improve from 0.14327\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 331ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 2.1036e-04 - dense_41_accuracy: 0.9977 - dense_41_loss: 0.0029 - loss: 0.0031 - val_dense_40_accuracy: 0.9676 - val_dense_40_loss: 0.0949 - val_dense_41_accuracy: 0.9722 - val_dense_41_loss: 0.0501 - val_loss: 0.1504 - learning_rate: 1.0000e-04\nEpoch 168/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 317ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 2.1237e-04 - dense_41_accuracy: 0.9991 - dense_41_loss: 0.0015 - loss: 0.0017\nEpoch 168: val_loss did not improve from 0.14327\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 323ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 2.1232e-04 - dense_41_accuracy: 0.9991 - dense_41_loss: 0.0015 - loss: 0.0017 - val_dense_40_accuracy: 0.9707 - val_dense_40_loss: 0.0940 - val_dense_41_accuracy: 0.9738 - val_dense_41_loss: 0.0501 - val_loss: 0.1494 - learning_rate: 1.0000e-04\nEpoch 169/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 319ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 1.4654e-04 - dense_41_accuracy: 0.9981 - dense_41_loss: 0.0021 - loss: 0.0023\nEpoch 169: val_loss did not improve from 0.14327\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 325ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 1.4658e-04 - dense_41_accuracy: 0.9981 - dense_41_loss: 0.0021 - loss: 0.0023 - val_dense_40_accuracy: 0.9676 - val_dense_40_loss: 0.0965 - val_dense_41_accuracy: 0.9676 - val_dense_41_loss: 0.0519 - val_loss: 0.1539 - learning_rate: 1.0000e-04\nEpoch 170/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - dense_40_accuracy: 0.9999 - dense_40_loss: 2.4525e-04 - dense_41_accuracy: 0.9985 - dense_41_loss: 0.0021 - loss: 0.0023\nEpoch 170: val_loss did not improve from 0.14327\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 324ms/step - dense_40_accuracy: 0.9999 - dense_40_loss: 2.4727e-04 - dense_41_accuracy: 0.9985 - dense_41_loss: 0.0021 - loss: 0.0023 - val_dense_40_accuracy: 0.9630 - val_dense_40_loss: 0.1018 - val_dense_41_accuracy: 0.9738 - val_dense_41_loss: 0.0456 - val_loss: 0.1528 - learning_rate: 1.0000e-04\nEpoch 171/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 315ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 1.7603e-04 - dense_41_accuracy: 0.9996 - dense_41_loss: 0.0012 - loss: 0.0014\nEpoch 171: val_loss did not improve from 0.14327\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 322ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 1.7596e-04 - dense_41_accuracy: 0.9996 - dense_41_loss: 0.0012 - loss: 0.0014 - val_dense_40_accuracy: 0.9676 - val_dense_40_loss: 0.0975 - val_dense_41_accuracy: 0.9707 - val_dense_41_loss: 0.0484 - val_loss: 0.1513 - learning_rate: 1.0000e-04\nEpoch 172/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 328ms/step - dense_40_accuracy: 0.9997 - dense_40_loss: 8.4654e-04 - dense_41_accuracy: 0.9992 - dense_41_loss: 0.0013 - loss: 0.0021\nEpoch 172: val_loss did not improve from 0.14327\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 334ms/step - dense_40_accuracy: 0.9997 - dense_40_loss: 8.4479e-04 - dense_41_accuracy: 0.9992 - dense_41_loss: 0.0013 - loss: 0.0021 - val_dense_40_accuracy: 0.9707 - val_dense_40_loss: 0.0936 - val_dense_41_accuracy: 0.9707 - val_dense_41_loss: 0.0476 - val_loss: 0.1464 - learning_rate: 1.0000e-04\nEpoch 173/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 324ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 9.9654e-05 - dense_41_accuracy: 0.9991 - dense_41_loss: 0.0021 - loss: 0.0022\nEpoch 173: val_loss did not improve from 0.14327\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 331ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 1.0006e-04 - dense_41_accuracy: 0.9991 - dense_41_loss: 0.0021 - loss: 0.0022 - val_dense_40_accuracy: 0.9707 - val_dense_40_loss: 0.0933 - val_dense_41_accuracy: 0.9738 - val_dense_41_loss: 0.0501 - val_loss: 0.1488 - learning_rate: 1.0000e-04\nEpoch 174/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - dense_40_accuracy: 0.9999 - dense_40_loss: 7.0495e-04 - dense_41_accuracy: 0.9996 - dense_41_loss: 0.0010 - loss: 0.0017\nEpoch 174: val_loss did not improve from 0.14327\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 325ms/step - dense_40_accuracy: 0.9999 - dense_40_loss: 7.0860e-04 - dense_41_accuracy: 0.9996 - dense_41_loss: 0.0010 - loss: 0.0017 - val_dense_40_accuracy: 0.9660 - val_dense_40_loss: 0.0914 - val_dense_41_accuracy: 0.9753 - val_dense_41_loss: 0.0517 - val_loss: 0.1484 - learning_rate: 1.0000e-04\nEpoch 175/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - dense_40_accuracy: 0.9998 - dense_40_loss: 4.6283e-04 - dense_41_accuracy: 0.9986 - dense_41_loss: 0.0024 - loss: 0.0029\nEpoch 175: val_loss did not improve from 0.14327\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 324ms/step - dense_40_accuracy: 0.9998 - dense_40_loss: 4.6286e-04 - dense_41_accuracy: 0.9986 - dense_41_loss: 0.0024 - loss: 0.0029 - val_dense_40_accuracy: 0.9676 - val_dense_40_loss: 0.0989 - val_dense_41_accuracy: 0.9738 - val_dense_41_loss: 0.0499 - val_loss: 0.1543 - learning_rate: 1.0000e-04\nEpoch 176/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - dense_40_accuracy: 0.9997 - dense_40_loss: 3.1275e-04 - dense_41_accuracy: 0.9992 - dense_41_loss: 9.9452e-04 - loss: 0.0013\nEpoch 176: val_loss did not improve from 0.14327\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 324ms/step - dense_40_accuracy: 0.9997 - dense_40_loss: 3.1299e-04 - dense_41_accuracy: 0.9992 - dense_41_loss: 9.9634e-04 - loss: 0.0013 - val_dense_40_accuracy: 0.9707 - val_dense_40_loss: 0.0938 - val_dense_41_accuracy: 0.9691 - val_dense_41_loss: 0.0583 - val_loss: 0.1577 - learning_rate: 1.0000e-04\nEpoch 177/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 326ms/step - dense_40_accuracy: 0.9998 - dense_40_loss: 3.3753e-04 - dense_41_accuracy: 0.9970 - dense_41_loss: 0.0042 - loss: 0.0046\nEpoch 177: val_loss did not improve from 0.14327\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 332ms/step - dense_40_accuracy: 0.9998 - dense_40_loss: 3.3747e-04 - dense_41_accuracy: 0.9970 - dense_41_loss: 0.0042 - loss: 0.0045 - val_dense_40_accuracy: 0.9707 - val_dense_40_loss: 0.1026 - val_dense_41_accuracy: 0.9753 - val_dense_41_loss: 0.0541 - val_loss: 0.1625 - learning_rate: 1.0000e-04\nEpoch 178/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 317ms/step - dense_40_accuracy: 0.9998 - dense_40_loss: 3.5820e-04 - dense_41_accuracy: 0.9987 - dense_41_loss: 0.0017 - loss: 0.0021\nEpoch 178: val_loss did not improve from 0.14327\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 324ms/step - dense_40_accuracy: 0.9998 - dense_40_loss: 3.5925e-04 - dense_41_accuracy: 0.9987 - dense_41_loss: 0.0017 - loss: 0.0021 - val_dense_40_accuracy: 0.9660 - val_dense_40_loss: 0.1073 - val_dense_41_accuracy: 0.9691 - val_dense_41_loss: 0.0510 - val_loss: 0.1641 - learning_rate: 1.0000e-04\nEpoch 179/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 319ms/step - dense_40_accuracy: 0.9999 - dense_40_loss: 1.7663e-04 - dense_41_accuracy: 0.9993 - dense_41_loss: 0.0017 - loss: 0.0019\nEpoch 179: val_loss did not improve from 0.14327\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 325ms/step - dense_40_accuracy: 0.9999 - dense_40_loss: 1.7676e-04 - dense_41_accuracy: 0.9993 - dense_41_loss: 0.0017 - loss: 0.0019 - val_dense_40_accuracy: 0.9707 - val_dense_40_loss: 0.1008 - val_dense_41_accuracy: 0.9722 - val_dense_41_loss: 0.0555 - val_loss: 0.1620 - learning_rate: 1.0000e-04\nEpoch 180/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 320ms/step - dense_40_accuracy: 0.9998 - dense_40_loss: 2.3363e-04 - dense_41_accuracy: 0.9987 - dense_41_loss: 0.0020 - loss: 0.0023\nEpoch 180: val_loss did not improve from 0.14327\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 326ms/step - dense_40_accuracy: 0.9998 - dense_40_loss: 2.3352e-04 - dense_41_accuracy: 0.9987 - dense_41_loss: 0.0020 - loss: 0.0023 - val_dense_40_accuracy: 0.9676 - val_dense_40_loss: 0.0992 - val_dense_41_accuracy: 0.9691 - val_dense_41_loss: 0.0546 - val_loss: 0.1596 - learning_rate: 1.0000e-04\nEpoch 181/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 319ms/step - dense_40_accuracy: 0.9997 - dense_40_loss: 2.9810e-04 - dense_41_accuracy: 0.9983 - dense_41_loss: 0.0024 - loss: 0.0027\nEpoch 181: val_loss did not improve from 0.14327\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 325ms/step - dense_40_accuracy: 0.9997 - dense_40_loss: 2.9801e-04 - dense_41_accuracy: 0.9983 - dense_41_loss: 0.0024 - loss: 0.0027 - val_dense_40_accuracy: 0.9676 - val_dense_40_loss: 0.0980 - val_dense_41_accuracy: 0.9722 - val_dense_41_loss: 0.0515 - val_loss: 0.1551 - learning_rate: 1.0000e-04\nEpoch 182/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 326ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 4.5580e-05 - dense_41_accuracy: 0.9990 - dense_41_loss: 0.0022 - loss: 0.0023\nEpoch 182: val_loss did not improve from 0.14327\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 332ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 4.5631e-05 - dense_41_accuracy: 0.9990 - dense_41_loss: 0.0022 - loss: 0.0023 - val_dense_40_accuracy: 0.9676 - val_dense_40_loss: 0.0993 - val_dense_41_accuracy: 0.9660 - val_dense_41_loss: 0.0573 - val_loss: 0.1623 - learning_rate: 1.0000e-04\nEpoch 183/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 317ms/step - dense_40_accuracy: 0.9996 - dense_40_loss: 6.0655e-04 - dense_41_accuracy: 0.9994 - dense_41_loss: 0.0010 - loss: 0.0016\nEpoch 183: val_loss did not improve from 0.14327\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 324ms/step - dense_40_accuracy: 0.9996 - dense_40_loss: 6.0577e-04 - dense_41_accuracy: 0.9994 - dense_41_loss: 0.0010 - loss: 0.0016 - val_dense_40_accuracy: 0.9722 - val_dense_40_loss: 0.1032 - val_dense_41_accuracy: 0.9738 - val_dense_41_loss: 0.0512 - val_loss: 0.1601 - learning_rate: 1.0000e-04\nEpoch 184/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 317ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 1.2375e-04 - dense_41_accuracy: 0.9997 - dense_41_loss: 7.7622e-04 - loss: 8.9991e-04\nEpoch 184: val_loss did not improve from 0.14327\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 324ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 1.2411e-04 - dense_41_accuracy: 0.9997 - dense_41_loss: 7.7685e-04 - loss: 9.0085e-04 - val_dense_40_accuracy: 0.9645 - val_dense_40_loss: 0.1066 - val_dense_41_accuracy: 0.9769 - val_dense_41_loss: 0.0463 - val_loss: 0.1585 - learning_rate: 1.0000e-04\nEpoch 185/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 325ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 3.2199e-04 - dense_41_accuracy: 0.9995 - dense_41_loss: 0.0012 - loss: 0.0015\nEpoch 185: val_loss did not improve from 0.14327\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 331ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 3.2137e-04 - dense_41_accuracy: 0.9995 - dense_41_loss: 0.0012 - loss: 0.0015 - val_dense_40_accuracy: 0.9676 - val_dense_40_loss: 0.0977 - val_dense_41_accuracy: 0.9722 - val_dense_41_loss: 0.0583 - val_loss: 0.1618 - learning_rate: 1.0000e-04\nEpoch 186/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - dense_40_accuracy: 0.9999 - dense_40_loss: 1.3071e-04 - dense_41_accuracy: 0.9993 - dense_41_loss: 0.0014 - loss: 0.0015\nEpoch 186: val_loss did not improve from 0.14327\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 324ms/step - dense_40_accuracy: 0.9999 - dense_40_loss: 1.3062e-04 - dense_41_accuracy: 0.9993 - dense_41_loss: 0.0014 - loss: 0.0015 - val_dense_40_accuracy: 0.9660 - val_dense_40_loss: 0.1025 - val_dense_41_accuracy: 0.9707 - val_dense_41_loss: 0.0551 - val_loss: 0.1635 - learning_rate: 1.0000e-04\nEpoch 187/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 327ms/step - dense_40_accuracy: 0.9998 - dense_40_loss: 2.0667e-04 - dense_41_accuracy: 0.9999 - dense_41_loss: 6.2648e-04 - loss: 8.3316e-04\nEpoch 187: val_loss did not improve from 0.14327\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 334ms/step - dense_40_accuracy: 0.9998 - dense_40_loss: 2.0742e-04 - dense_41_accuracy: 0.9999 - dense_41_loss: 6.2730e-04 - loss: 8.3475e-04 - val_dense_40_accuracy: 0.9691 - val_dense_40_loss: 0.0999 - val_dense_41_accuracy: 0.9753 - val_dense_41_loss: 0.0532 - val_loss: 0.1588 - learning_rate: 1.0000e-04\nEpoch 188/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 315ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 3.0522e-05 - dense_41_accuracy: 0.9993 - dense_41_loss: 9.7787e-04 - loss: 0.0010 \nEpoch 188: val_loss did not improve from 0.14327\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 322ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 3.0564e-05 - dense_41_accuracy: 0.9993 - dense_41_loss: 9.8017e-04 - loss: 0.0010 - val_dense_40_accuracy: 0.9691 - val_dense_40_loss: 0.0952 - val_dense_41_accuracy: 0.9722 - val_dense_41_loss: 0.0501 - val_loss: 0.1506 - learning_rate: 1.0000e-04\nEpoch 189/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 323ms/step - dense_40_accuracy: 0.9999 - dense_40_loss: 1.7258e-04 - dense_41_accuracy: 0.9995 - dense_41_loss: 8.4296e-04 - loss: 0.0010\nEpoch 189: val_loss did not improve from 0.14327\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 330ms/step - dense_40_accuracy: 0.9999 - dense_40_loss: 1.7351e-04 - dense_41_accuracy: 0.9995 - dense_41_loss: 8.4395e-04 - loss: 0.0010 - val_dense_40_accuracy: 0.9691 - val_dense_40_loss: 0.0978 - val_dense_41_accuracy: 0.9707 - val_dense_41_loss: 0.0478 - val_loss: 0.1511 - learning_rate: 1.0000e-04\nEpoch 190/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 324ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 4.3181e-05 - dense_41_accuracy: 0.9997 - dense_41_loss: 6.7178e-04 - loss: 7.1367e-04\nEpoch 190: val_loss did not improve from 0.14327\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 331ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 4.3207e-05 - dense_41_accuracy: 0.9997 - dense_41_loss: 6.7506e-04 - loss: 7.1571e-04 - val_dense_40_accuracy: 0.9660 - val_dense_40_loss: 0.0945 - val_dense_41_accuracy: 0.9722 - val_dense_41_loss: 0.0516 - val_loss: 0.1515 - learning_rate: 1.0000e-04\nEpoch 191/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 328ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 1.3123e-04 - dense_41_accuracy: 0.9990 - dense_41_loss: 0.0018 - loss: 0.0019\nEpoch 191: val_loss did not improve from 0.14327\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 334ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 1.3142e-04 - dense_41_accuracy: 0.9990 - dense_41_loss: 0.0018 - loss: 0.0019 - val_dense_40_accuracy: 0.9722 - val_dense_40_loss: 0.0966 - val_dense_41_accuracy: 0.9738 - val_dense_41_loss: 0.0490 - val_loss: 0.1510 - learning_rate: 1.0000e-04\nEpoch 192/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 320ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 3.4201e-04 - dense_41_accuracy: 0.9996 - dense_41_loss: 9.4030e-04 - loss: 0.0013\nEpoch 192: val_loss improved from 0.14327 to 0.13934, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 340ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 3.4130e-04 - dense_41_accuracy: 0.9996 - dense_41_loss: 9.4245e-04 - loss: 0.0013 - val_dense_40_accuracy: 0.9676 - val_dense_40_loss: 0.0922 - val_dense_41_accuracy: 0.9738 - val_dense_41_loss: 0.0422 - val_loss: 0.1393 - learning_rate: 1.0000e-04\nEpoch 193/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 316ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 8.4021e-05 - dense_41_accuracy: 0.9996 - dense_41_loss: 9.9277e-04 - loss: 0.0011\nEpoch 193: val_loss did not improve from 0.13934\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 323ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 8.3865e-05 - dense_41_accuracy: 0.9996 - dense_41_loss: 9.9307e-04 - loss: 0.0011 - val_dense_40_accuracy: 0.9676 - val_dense_40_loss: 0.0982 - val_dense_41_accuracy: 0.9707 - val_dense_41_loss: 0.0490 - val_loss: 0.1526 - learning_rate: 1.0000e-04\nEpoch 194/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 324ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 9.3097e-05 - dense_41_accuracy: 0.9991 - dense_41_loss: 0.0017 - loss: 0.0018\nEpoch 194: val_loss did not improve from 0.13934\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 331ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 9.2994e-05 - dense_41_accuracy: 0.9991 - dense_41_loss: 0.0017 - loss: 0.0018 - val_dense_40_accuracy: 0.9660 - val_dense_40_loss: 0.1001 - val_dense_41_accuracy: 0.9691 - val_dense_41_loss: 0.0510 - val_loss: 0.1568 - learning_rate: 1.0000e-04\nEpoch 195/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 326ms/step - dense_40_accuracy: 0.9999 - dense_40_loss: 1.4603e-04 - dense_41_accuracy: 0.9994 - dense_41_loss: 0.0011 - loss: 0.0013\nEpoch 195: val_loss did not improve from 0.13934\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 333ms/step - dense_40_accuracy: 0.9999 - dense_40_loss: 1.4622e-04 - dense_41_accuracy: 0.9994 - dense_41_loss: 0.0011 - loss: 0.0013 - val_dense_40_accuracy: 0.9676 - val_dense_40_loss: 0.1007 - val_dense_41_accuracy: 0.9537 - val_dense_41_loss: 0.0874 - val_loss: 0.1951 - learning_rate: 1.0000e-04\nEpoch 196/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 4.8135e-05 - dense_41_accuracy: 0.9993 - dense_41_loss: 0.0012 - loss: 0.0012\nEpoch 196: val_loss did not improve from 0.13934\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 325ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 4.8130e-05 - dense_41_accuracy: 0.9993 - dense_41_loss: 0.0012 - loss: 0.0012 - val_dense_40_accuracy: 0.9676 - val_dense_40_loss: 0.1003 - val_dense_41_accuracy: 0.9738 - val_dense_41_loss: 0.0479 - val_loss: 0.1537 - learning_rate: 1.0000e-04\nEpoch 197/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 334ms/step - dense_40_accuracy: 0.9998 - dense_40_loss: 1.7826e-04 - dense_41_accuracy: 1.0000 - dense_41_loss: 6.8302e-04 - loss: 8.6116e-04\nEpoch 197: val_loss did not improve from 0.13934\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 340ms/step - dense_40_accuracy: 0.9998 - dense_40_loss: 1.7803e-04 - dense_41_accuracy: 1.0000 - dense_41_loss: 6.8394e-04 - loss: 8.6173e-04 - val_dense_40_accuracy: 0.9645 - val_dense_40_loss: 0.1072 - val_dense_41_accuracy: 0.9707 - val_dense_41_loss: 0.0554 - val_loss: 0.1686 - learning_rate: 1.0000e-04\nEpoch 198/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 326ms/step - dense_40_accuracy: 0.9996 - dense_40_loss: 5.7882e-04 - dense_41_accuracy: 0.9990 - dense_41_loss: 0.0012 - loss: 0.0018\nEpoch 198: val_loss did not improve from 0.13934\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 333ms/step - dense_40_accuracy: 0.9996 - dense_40_loss: 5.7774e-04 - dense_41_accuracy: 0.9990 - dense_41_loss: 0.0012 - loss: 0.0018 - val_dense_40_accuracy: 0.9630 - val_dense_40_loss: 0.1086 - val_dense_41_accuracy: 0.9691 - val_dense_41_loss: 0.0550 - val_loss: 0.1697 - learning_rate: 1.0000e-04\nEpoch 199/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 325ms/step - dense_40_accuracy: 0.9996 - dense_40_loss: 5.8972e-04 - dense_41_accuracy: 0.9983 - dense_41_loss: 0.0016 - loss: 0.0022\nEpoch 199: val_loss did not improve from 0.13934\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 331ms/step - dense_40_accuracy: 0.9996 - dense_40_loss: 5.9009e-04 - dense_41_accuracy: 0.9983 - dense_41_loss: 0.0016 - loss: 0.0022 - val_dense_40_accuracy: 0.9630 - val_dense_40_loss: 0.1266 - val_dense_41_accuracy: 0.9691 - val_dense_41_loss: 0.0575 - val_loss: 0.1909 - learning_rate: 1.0000e-04\nEpoch 200/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - dense_40_accuracy: 0.9999 - dense_40_loss: 1.9317e-04 - dense_41_accuracy: 0.9990 - dense_41_loss: 0.0011 - loss: 0.0013\nEpoch 200: val_loss did not improve from 0.13934\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 325ms/step - dense_40_accuracy: 0.9999 - dense_40_loss: 1.9316e-04 - dense_41_accuracy: 0.9990 - dense_41_loss: 0.0011 - loss: 0.0013 - val_dense_40_accuracy: 0.9676 - val_dense_40_loss: 0.1142 - val_dense_41_accuracy: 0.9645 - val_dense_41_loss: 0.0629 - val_loss: 0.1836 - learning_rate: 1.0000e-04\nRestoring model weights from the end of the best epoch: 192.\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"model.evaluate([X_test_s, X_test_h1], [y_test_s, y_test_h1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T06:33:43.990134Z","iopub.execute_input":"2025-03-08T06:33:43.990443Z","iopub.status.idle":"2025-03-08T06:34:08.336800Z","shell.execute_reply.started":"2025-03-08T06:33:43.990419Z","shell.execute_reply":"2025-03-08T06:34:08.335909Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 947ms/step - dense_40_accuracy: 0.9627 - dense_40_loss: 0.1534 - dense_41_accuracy: 0.9861 - dense_41_loss: 0.0373 - loss: 0.1910\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"[0.15989463031291962,\n 0.12884709239006042,\n 0.027215830981731415,\n 0.9679012298583984,\n 0.9876543283462524]"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"history = model.fit([X_train_s, X_train_h], [y_train_s, y_train_h],\n                    epochs=200,\n                    validation_data=([X_val_s, X_val_h1], [y_val_s, y_val_h1]), verbose=1,\n                    shuffle=True,\n                    callbacks=callbacks) # UpdateGammaCallback","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T06:34:22.396545Z","iopub.execute_input":"2025-03-08T06:34:22.396826Z","iopub.status.idle":"2025-03-08T08:04:33.605768Z","shell.execute_reply.started":"2025-03-08T06:34:22.396804Z","shell.execute_reply":"2025-03-08T08:04:33.604798Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 328ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 1.0659e-04 - dense_41_accuracy: 1.0000 - dense_41_loss: 3.6410e-04 - loss: 4.7042e-04\nEpoch 1: val_loss did not improve from 0.13934\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 337ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 1.0805e-04 - dense_41_accuracy: 1.0000 - dense_41_loss: 3.6446e-04 - loss: 4.7196e-04 - val_dense_40_accuracy: 0.9676 - val_dense_40_loss: 0.1070 - val_dense_41_accuracy: 0.9753 - val_dense_41_loss: 0.0444 - val_loss: 0.1569 - learning_rate: 1.0000e-04\nEpoch 2/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321ms/step - dense_40_accuracy: 0.9998 - dense_40_loss: 2.4671e-04 - dense_41_accuracy: 0.9996 - dense_41_loss: 7.1636e-04 - loss: 9.6307e-04\nEpoch 2: val_loss did not improve from 0.13934\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 327ms/step - dense_40_accuracy: 0.9998 - dense_40_loss: 2.4710e-04 - dense_41_accuracy: 0.9996 - dense_41_loss: 7.1746e-04 - loss: 9.6458e-04 - val_dense_40_accuracy: 0.9645 - val_dense_40_loss: 0.0994 - val_dense_41_accuracy: 0.9769 - val_dense_41_loss: 0.0411 - val_loss: 0.1457 - learning_rate: 1.0000e-04\nEpoch 3/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 1.2030e-04 - dense_41_accuracy: 0.9991 - dense_41_loss: 0.0014 - loss: 0.0015\nEpoch 3: val_loss did not improve from 0.13934\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 325ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 1.2014e-04 - dense_41_accuracy: 0.9991 - dense_41_loss: 0.0014 - loss: 0.0015 - val_dense_40_accuracy: 0.9676 - val_dense_40_loss: 0.1006 - val_dense_41_accuracy: 0.9753 - val_dense_41_loss: 0.0465 - val_loss: 0.1525 - learning_rate: 1.0000e-04\nEpoch 4/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 7.7035e-05 - dense_41_accuracy: 0.9992 - dense_41_loss: 0.0012 - loss: 0.0013\nEpoch 4: val_loss did not improve from 0.13934\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 327ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 7.7305e-05 - dense_41_accuracy: 0.9992 - dense_41_loss: 0.0012 - loss: 0.0013 - val_dense_40_accuracy: 0.9691 - val_dense_40_loss: 0.1016 - val_dense_41_accuracy: 0.9722 - val_dense_41_loss: 0.0499 - val_loss: 0.1571 - learning_rate: 1.0000e-04\nEpoch 5/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 319ms/step - dense_40_accuracy: 0.9996 - dense_40_loss: 2.4646e-04 - dense_41_accuracy: 0.9994 - dense_41_loss: 9.9559e-04 - loss: 0.0012\nEpoch 5: val_loss did not improve from 0.13934\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 326ms/step - dense_40_accuracy: 0.9996 - dense_40_loss: 2.4687e-04 - dense_41_accuracy: 0.9994 - dense_41_loss: 9.9730e-04 - loss: 0.0012 - val_dense_40_accuracy: 0.9676 - val_dense_40_loss: 0.0997 - val_dense_41_accuracy: 0.9722 - val_dense_41_loss: 0.0591 - val_loss: 0.1647 - learning_rate: 1.0000e-04\nEpoch 6/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 328ms/step - dense_40_accuracy: 0.9997 - dense_40_loss: 3.5784e-04 - dense_41_accuracy: 0.9986 - dense_41_loss: 0.0019 - loss: 0.0022\nEpoch 6: val_loss improved from 0.13934 to 0.13667, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 348ms/step - dense_40_accuracy: 0.9997 - dense_40_loss: 3.5714e-04 - dense_41_accuracy: 0.9986 - dense_41_loss: 0.0019 - loss: 0.0022 - val_dense_40_accuracy: 0.9738 - val_dense_40_loss: 0.0924 - val_dense_41_accuracy: 0.9738 - val_dense_41_loss: 0.0394 - val_loss: 0.1367 - learning_rate: 1.0000e-04\nEpoch 7/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 316ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 1.1336e-04 - dense_41_accuracy: 0.9994 - dense_41_loss: 8.9117e-04 - loss: 0.0010\nEpoch 7: val_loss did not improve from 0.13667\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 322ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 1.1345e-04 - dense_41_accuracy: 0.9994 - dense_41_loss: 8.9174e-04 - loss: 0.0010 - val_dense_40_accuracy: 0.9753 - val_dense_40_loss: 0.0922 - val_dense_41_accuracy: 0.9753 - val_dense_41_loss: 0.0477 - val_loss: 0.1450 - learning_rate: 1.0000e-04\nEpoch 8/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 320ms/step - dense_40_accuracy: 0.9996 - dense_40_loss: 3.8410e-04 - dense_41_accuracy: 0.9989 - dense_41_loss: 0.0010 - loss: 0.0014\nEpoch 8: val_loss did not improve from 0.13667\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 327ms/step - dense_40_accuracy: 0.9996 - dense_40_loss: 3.8438e-04 - dense_41_accuracy: 0.9989 - dense_41_loss: 0.0010 - loss: 0.0014 - val_dense_40_accuracy: 0.9707 - val_dense_40_loss: 0.0998 - val_dense_41_accuracy: 0.9738 - val_dense_41_loss: 0.0510 - val_loss: 0.1563 - learning_rate: 1.0000e-04\nEpoch 9/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321ms/step - dense_40_accuracy: 0.9999 - dense_40_loss: 3.5832e-04 - dense_41_accuracy: 0.9993 - dense_41_loss: 9.7838e-04 - loss: 0.0013\nEpoch 9: val_loss did not improve from 0.13667\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 327ms/step - dense_40_accuracy: 0.9999 - dense_40_loss: 3.5893e-04 - dense_41_accuracy: 0.9993 - dense_41_loss: 9.7813e-04 - loss: 0.0013 - val_dense_40_accuracy: 0.9691 - val_dense_40_loss: 0.0977 - val_dense_41_accuracy: 0.9753 - val_dense_41_loss: 0.0541 - val_loss: 0.1574 - learning_rate: 1.0000e-04\nEpoch 10/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 322ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 6.8040e-05 - dense_41_accuracy: 0.9995 - dense_41_loss: 7.8721e-04 - loss: 8.5524e-04\nEpoch 10: val_loss did not improve from 0.13667\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 329ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 6.8210e-05 - dense_41_accuracy: 0.9995 - dense_41_loss: 7.8840e-04 - loss: 8.5659e-04 - val_dense_40_accuracy: 0.9722 - val_dense_40_loss: 0.0998 - val_dense_41_accuracy: 0.9660 - val_dense_41_loss: 0.0643 - val_loss: 0.1701 - learning_rate: 1.0000e-04\nEpoch 11/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 325ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 8.0892e-05 - dense_41_accuracy: 0.9998 - dense_41_loss: 4.1229e-04 - loss: 4.9319e-04\nEpoch 11: val_loss did not improve from 0.13667\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 332ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 8.0773e-05 - dense_41_accuracy: 0.9998 - dense_41_loss: 4.1224e-04 - loss: 4.9302e-04 - val_dense_40_accuracy: 0.9722 - val_dense_40_loss: 0.0973 - val_dense_41_accuracy: 0.9738 - val_dense_41_loss: 0.0493 - val_loss: 0.1519 - learning_rate: 1.0000e-04\nEpoch 12/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - dense_40_accuracy: 0.9998 - dense_40_loss: 2.4275e-04 - dense_41_accuracy: 0.9991 - dense_41_loss: 0.0010 - loss: 0.0013\nEpoch 12: val_loss did not improve from 0.13667\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 324ms/step - dense_40_accuracy: 0.9998 - dense_40_loss: 2.4255e-04 - dense_41_accuracy: 0.9991 - dense_41_loss: 0.0010 - loss: 0.0013 - val_dense_40_accuracy: 0.9691 - val_dense_40_loss: 0.1077 - val_dense_41_accuracy: 0.9722 - val_dense_41_loss: 0.0521 - val_loss: 0.1656 - learning_rate: 1.0000e-04\nEpoch 13/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 322ms/step - dense_40_accuracy: 0.9998 - dense_40_loss: 1.6407e-04 - dense_41_accuracy: 0.9998 - dense_41_loss: 5.7653e-04 - loss: 7.4060e-04\nEpoch 13: val_loss did not improve from 0.13667\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 328ms/step - dense_40_accuracy: 0.9998 - dense_40_loss: 1.6393e-04 - dense_41_accuracy: 0.9998 - dense_41_loss: 5.7768e-04 - loss: 7.4163e-04 - val_dense_40_accuracy: 0.9707 - val_dense_40_loss: 0.1028 - val_dense_41_accuracy: 0.9738 - val_dense_41_loss: 0.0478 - val_loss: 0.1562 - learning_rate: 1.0000e-04\nEpoch 14/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 322ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 1.7334e-04 - dense_41_accuracy: 0.9996 - dense_41_loss: 0.0010 - loss: 0.0012\nEpoch 14: val_loss did not improve from 0.13667\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 329ms/step - dense_40_accuracy: 0.9999 - dense_40_loss: 1.7357e-04 - dense_41_accuracy: 0.9996 - dense_41_loss: 0.0010 - loss: 0.0012 - val_dense_40_accuracy: 0.9707 - val_dense_40_loss: 0.1043 - val_dense_41_accuracy: 0.9738 - val_dense_41_loss: 0.0462 - val_loss: 0.1561 - learning_rate: 1.0000e-04\nEpoch 15/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 326ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 4.8875e-05 - dense_41_accuracy: 0.9996 - dense_41_loss: 9.1477e-04 - loss: 9.6353e-04\nEpoch 15: val_loss did not improve from 0.13667\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 332ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 4.8986e-05 - dense_41_accuracy: 0.9996 - dense_41_loss: 9.1468e-04 - loss: 9.6343e-04 - val_dense_40_accuracy: 0.9691 - val_dense_40_loss: 0.1045 - val_dense_41_accuracy: 0.9722 - val_dense_41_loss: 0.0528 - val_loss: 0.1631 - learning_rate: 1.0000e-04\nEpoch 16/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - dense_40_accuracy: 0.9999 - dense_40_loss: 8.2270e-05 - dense_41_accuracy: 0.9995 - dense_41_loss: 0.0011 - loss: 0.0012\nEpoch 16: val_loss did not improve from 0.13667\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 324ms/step - dense_40_accuracy: 0.9999 - dense_40_loss: 8.2364e-05 - dense_41_accuracy: 0.9995 - dense_41_loss: 0.0011 - loss: 0.0012 - val_dense_40_accuracy: 0.9691 - val_dense_40_loss: 0.1122 - val_dense_41_accuracy: 0.9769 - val_dense_41_loss: 0.0493 - val_loss: 0.1675 - learning_rate: 1.0000e-04\nEpoch 17/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 317ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 2.6779e-04 - dense_41_accuracy: 0.9993 - dense_41_loss: 7.8248e-04 - loss: 0.0011\nEpoch 17: val_loss did not improve from 0.13667\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 324ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 2.6720e-04 - dense_41_accuracy: 0.9993 - dense_41_loss: 7.8194e-04 - loss: 0.0010 - val_dense_40_accuracy: 0.9691 - val_dense_40_loss: 0.1146 - val_dense_41_accuracy: 0.9753 - val_dense_41_loss: 0.0493 - val_loss: 0.1699 - learning_rate: 1.0000e-04\nEpoch 18/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 322ms/step - dense_40_accuracy: 0.9996 - dense_40_loss: 1.9764e-04 - dense_41_accuracy: 0.9997 - dense_41_loss: 7.3490e-04 - loss: 9.3255e-04\nEpoch 18: val_loss did not improve from 0.13667\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 328ms/step - dense_40_accuracy: 0.9996 - dense_40_loss: 1.9778e-04 - dense_41_accuracy: 0.9997 - dense_41_loss: 7.3527e-04 - loss: 9.3308e-04 - val_dense_40_accuracy: 0.9691 - val_dense_40_loss: 0.1131 - val_dense_41_accuracy: 0.9707 - val_dense_41_loss: 0.0516 - val_loss: 0.1708 - learning_rate: 1.0000e-04\nEpoch 19/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 319ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 6.3355e-05 - dense_41_accuracy: 0.9998 - dense_41_loss: 5.8103e-04 - loss: 6.4439e-04\nEpoch 19: val_loss did not improve from 0.13667\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 325ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 6.3388e-05 - dense_41_accuracy: 0.9998 - dense_41_loss: 5.8331e-04 - loss: 6.4672e-04 - val_dense_40_accuracy: 0.9691 - val_dense_40_loss: 0.1178 - val_dense_41_accuracy: 0.9707 - val_dense_41_loss: 0.0515 - val_loss: 0.1755 - learning_rate: 1.0000e-04\nEpoch 20/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 317ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 2.0251e-04 - dense_41_accuracy: 0.9993 - dense_41_loss: 7.7154e-04 - loss: 9.7406e-04\nEpoch 20: val_loss did not improve from 0.13667\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 324ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 2.0423e-04 - dense_41_accuracy: 0.9993 - dense_41_loss: 7.7111e-04 - loss: 9.7536e-04 - val_dense_40_accuracy: 0.9691 - val_dense_40_loss: 0.1162 - val_dense_41_accuracy: 0.9738 - val_dense_41_loss: 0.0587 - val_loss: 0.1814 - learning_rate: 1.0000e-04\nEpoch 21/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 328ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 1.3063e-04 - dense_41_accuracy: 0.9997 - dense_41_loss: 5.7153e-04 - loss: 7.0217e-04\nEpoch 21: val_loss did not improve from 0.13667\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 334ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 1.3057e-04 - dense_41_accuracy: 0.9997 - dense_41_loss: 5.7267e-04 - loss: 7.0326e-04 - val_dense_40_accuracy: 0.9645 - val_dense_40_loss: 0.1227 - val_dense_41_accuracy: 0.9707 - val_dense_41_loss: 0.0586 - val_loss: 0.1880 - learning_rate: 1.0000e-04\nEpoch 22/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 317ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 3.4527e-05 - dense_41_accuracy: 0.9997 - dense_41_loss: 7.8763e-04 - loss: 8.2216e-04\nEpoch 22: val_loss did not improve from 0.13667\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 323ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 3.4595e-05 - dense_41_accuracy: 0.9997 - dense_41_loss: 7.8800e-04 - loss: 8.2260e-04 - val_dense_40_accuracy: 0.9676 - val_dense_40_loss: 0.1276 - val_dense_41_accuracy: 0.9738 - val_dense_41_loss: 0.0573 - val_loss: 0.1917 - learning_rate: 1.0000e-04\nEpoch 23/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 316ms/step - dense_40_accuracy: 0.9994 - dense_40_loss: 8.0554e-04 - dense_41_accuracy: 0.9995 - dense_41_loss: 6.6936e-04 - loss: 0.0015\nEpoch 23: val_loss did not improve from 0.13667\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 322ms/step - dense_40_accuracy: 0.9994 - dense_40_loss: 8.0512e-04 - dense_41_accuracy: 0.9995 - dense_41_loss: 6.7038e-04 - loss: 0.0015 - val_dense_40_accuracy: 0.9645 - val_dense_40_loss: 0.1231 - val_dense_41_accuracy: 0.9753 - val_dense_41_loss: 0.0472 - val_loss: 0.1766 - learning_rate: 1.0000e-04\nEpoch 24/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 316ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 3.4649e-05 - dense_41_accuracy: 0.9995 - dense_41_loss: 0.0019 - loss: 0.0019\nEpoch 24: val_loss did not improve from 0.13667\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 322ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 3.4807e-05 - dense_41_accuracy: 0.9995 - dense_41_loss: 0.0019 - loss: 0.0019 - val_dense_40_accuracy: 0.9660 - val_dense_40_loss: 0.1223 - val_dense_41_accuracy: 0.9753 - val_dense_41_loss: 0.0483 - val_loss: 0.1769 - learning_rate: 1.0000e-04\nEpoch 25/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 322ms/step - dense_40_accuracy: 0.9996 - dense_40_loss: 1.8439e-04 - dense_41_accuracy: 0.9998 - dense_41_loss: 5.7632e-04 - loss: 7.5990e-04\nEpoch 25: val_loss did not improve from 0.13667\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 329ms/step - dense_40_accuracy: 0.9996 - dense_40_loss: 1.8416e-04 - dense_41_accuracy: 0.9998 - dense_41_loss: 5.7814e-04 - loss: 7.6069e-04 - val_dense_40_accuracy: 0.9645 - val_dense_40_loss: 0.1210 - val_dense_41_accuracy: 0.9738 - val_dense_41_loss: 0.0456 - val_loss: 0.1728 - learning_rate: 1.0000e-04\nEpoch 26/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 317ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 5.0173e-05 - dense_41_accuracy: 0.9997 - dense_41_loss: 6.9321e-04 - loss: 7.4338e-04\nEpoch 26: val_loss did not improve from 0.13667\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 324ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 5.0222e-05 - dense_41_accuracy: 0.9997 - dense_41_loss: 6.9236e-04 - loss: 7.4259e-04 - val_dense_40_accuracy: 0.9691 - val_dense_40_loss: 0.1112 - val_dense_41_accuracy: 0.9738 - val_dense_41_loss: 0.0478 - val_loss: 0.1649 - learning_rate: 1.0000e-04\nEpoch 27/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - dense_40_accuracy: 0.9995 - dense_40_loss: 6.3078e-04 - dense_41_accuracy: 0.9992 - dense_41_loss: 9.1842e-04 - loss: 0.0015\nEpoch 27: val_loss did not improve from 0.13667\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 324ms/step - dense_40_accuracy: 0.9995 - dense_40_loss: 6.2907e-04 - dense_41_accuracy: 0.9992 - dense_41_loss: 9.1778e-04 - loss: 0.0015 - val_dense_40_accuracy: 0.9599 - val_dense_40_loss: 0.1260 - val_dense_41_accuracy: 0.9753 - val_dense_41_loss: 0.0519 - val_loss: 0.1844 - learning_rate: 1.0000e-04\nEpoch 28/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 314ms/step - dense_40_accuracy: 0.9999 - dense_40_loss: 8.8208e-05 - dense_41_accuracy: 0.9997 - dense_41_loss: 6.6249e-04 - loss: 7.5059e-04\nEpoch 28: val_loss did not improve from 0.13667\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 320ms/step - dense_40_accuracy: 0.9999 - dense_40_loss: 8.8427e-05 - dense_41_accuracy: 0.9997 - dense_41_loss: 6.6452e-04 - loss: 7.5273e-04 - val_dense_40_accuracy: 0.9676 - val_dense_40_loss: 0.1109 - val_dense_41_accuracy: 0.9769 - val_dense_41_loss: 0.0552 - val_loss: 0.1722 - learning_rate: 1.0000e-04\nEpoch 29/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 314ms/step - dense_40_accuracy: 0.9999 - dense_40_loss: 3.3572e-04 - dense_41_accuracy: 0.9990 - dense_41_loss: 9.1892e-04 - loss: 0.0013\nEpoch 29: val_loss did not improve from 0.13667\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 320ms/step - dense_40_accuracy: 0.9999 - dense_40_loss: 3.3505e-04 - dense_41_accuracy: 0.9990 - dense_41_loss: 9.1809e-04 - loss: 0.0013 - val_dense_40_accuracy: 0.9691 - val_dense_40_loss: 0.1052 - val_dense_41_accuracy: 0.9738 - val_dense_41_loss: 0.0540 - val_loss: 0.1651 - learning_rate: 1.0000e-04\nEpoch 30/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 320ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 6.0970e-05 - dense_41_accuracy: 0.9997 - dense_41_loss: 5.3285e-04 - loss: 5.9383e-04\nEpoch 30: val_loss did not improve from 0.13667\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 327ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 6.1264e-05 - dense_41_accuracy: 0.9997 - dense_41_loss: 5.3306e-04 - loss: 5.9434e-04 - val_dense_40_accuracy: 0.9630 - val_dense_40_loss: 0.1171 - val_dense_41_accuracy: 0.9769 - val_dense_41_loss: 0.0497 - val_loss: 0.1730 - learning_rate: 1.0000e-04\nEpoch 31/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 328ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 3.2121e-05 - dense_41_accuracy: 0.9987 - dense_41_loss: 9.9949e-04 - loss: 0.0010\nEpoch 31: val_loss did not improve from 0.13667\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 335ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 3.2130e-05 - dense_41_accuracy: 0.9987 - dense_41_loss: 9.9847e-04 - loss: 0.0010 - val_dense_40_accuracy: 0.9645 - val_dense_40_loss: 0.1107 - val_dense_41_accuracy: 0.9738 - val_dense_41_loss: 0.0538 - val_loss: 0.1705 - learning_rate: 1.0000e-04\nEpoch 32/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 319ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 1.4813e-05 - dense_41_accuracy: 0.9991 - dense_41_loss: 0.0014 - loss: 0.0015\nEpoch 32: val_loss did not improve from 0.13667\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 325ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 1.4859e-05 - dense_41_accuracy: 0.9991 - dense_41_loss: 0.0014 - loss: 0.0015 - val_dense_40_accuracy: 0.9645 - val_dense_40_loss: 0.1162 - val_dense_41_accuracy: 0.9753 - val_dense_41_loss: 0.0468 - val_loss: 0.1691 - learning_rate: 1.0000e-04\nEpoch 33/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 5.8094e-05 - dense_41_accuracy: 0.9998 - dense_41_loss: 5.2324e-04 - loss: 5.8134e-04\nEpoch 33: val_loss did not improve from 0.13667\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 324ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 5.8108e-05 - dense_41_accuracy: 0.9998 - dense_41_loss: 5.2328e-04 - loss: 5.8140e-04 - val_dense_40_accuracy: 0.9660 - val_dense_40_loss: 0.1223 - val_dense_41_accuracy: 0.9753 - val_dense_41_loss: 0.0516 - val_loss: 0.1804 - learning_rate: 1.0000e-04\nEpoch 34/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - dense_40_accuracy: 0.9998 - dense_40_loss: 3.2132e-04 - dense_41_accuracy: 0.9998 - dense_41_loss: 5.8617e-04 - loss: 9.0748e-04\nEpoch 34: val_loss did not improve from 0.13667\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 324ms/step - dense_40_accuracy: 0.9998 - dense_40_loss: 3.2153e-04 - dense_41_accuracy: 0.9998 - dense_41_loss: 5.8702e-04 - loss: 9.0852e-04 - val_dense_40_accuracy: 0.9645 - val_dense_40_loss: 0.1221 - val_dense_41_accuracy: 0.9676 - val_dense_41_loss: 0.0654 - val_loss: 0.1945 - learning_rate: 1.0000e-04\nEpoch 35/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 325ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 2.7965e-05 - dense_41_accuracy: 0.9999 - dense_41_loss: 5.9166e-04 - loss: 6.1963e-04\nEpoch 35: val_loss did not improve from 0.13667\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 331ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 2.8103e-05 - dense_41_accuracy: 0.9999 - dense_41_loss: 5.9176e-04 - loss: 6.1988e-04 - val_dense_40_accuracy: 0.9614 - val_dense_40_loss: 0.1258 - val_dense_41_accuracy: 0.9722 - val_dense_41_loss: 0.0553 - val_loss: 0.1878 - learning_rate: 1.0000e-04\nEpoch 36/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 316ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 2.7925e-05 - dense_41_accuracy: 0.9999 - dense_41_loss: 4.7817e-04 - loss: 5.0610e-04\nEpoch 36: val_loss did not improve from 0.13667\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 323ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 2.7934e-05 - dense_41_accuracy: 0.9999 - dense_41_loss: 4.7872e-04 - loss: 5.0666e-04 - val_dense_40_accuracy: 0.9599 - val_dense_40_loss: 0.1266 - val_dense_41_accuracy: 0.9691 - val_dense_41_loss: 0.0624 - val_loss: 0.1960 - learning_rate: 1.0000e-04\nEpoch 37/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - dense_40_accuracy: 0.9998 - dense_40_loss: 1.3710e-04 - dense_41_accuracy: 0.9988 - dense_41_loss: 0.0014 - loss: 0.0015\nEpoch 37: val_loss did not improve from 0.13667\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 324ms/step - dense_40_accuracy: 0.9998 - dense_40_loss: 1.3701e-04 - dense_41_accuracy: 0.9988 - dense_41_loss: 0.0014 - loss: 0.0015 - val_dense_40_accuracy: 0.9583 - val_dense_40_loss: 0.1290 - val_dense_41_accuracy: 0.9738 - val_dense_41_loss: 0.0631 - val_loss: 0.1992 - learning_rate: 1.0000e-04\nEpoch 38/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 319ms/step - dense_40_accuracy: 0.9999 - dense_40_loss: 1.6673e-04 - dense_41_accuracy: 0.9990 - dense_41_loss: 0.0015 - loss: 0.0016\nEpoch 38: val_loss did not improve from 0.13667\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 325ms/step - dense_40_accuracy: 0.9999 - dense_40_loss: 1.6687e-04 - dense_41_accuracy: 0.9990 - dense_41_loss: 0.0015 - loss: 0.0016 - val_dense_40_accuracy: 0.9660 - val_dense_40_loss: 0.1211 - val_dense_41_accuracy: 0.9722 - val_dense_41_loss: 0.0513 - val_loss: 0.1788 - learning_rate: 1.0000e-04\nEpoch 39/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 317ms/step - dense_40_accuracy: 0.9993 - dense_40_loss: 6.5973e-04 - dense_41_accuracy: 0.9987 - dense_41_loss: 0.0022 - loss: 0.0028\nEpoch 39: val_loss did not improve from 0.13667\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 323ms/step - dense_40_accuracy: 0.9993 - dense_40_loss: 6.5834e-04 - dense_41_accuracy: 0.9987 - dense_41_loss: 0.0021 - loss: 0.0028 - val_dense_40_accuracy: 0.9676 - val_dense_40_loss: 0.1077 - val_dense_41_accuracy: 0.9676 - val_dense_41_loss: 0.0538 - val_loss: 0.1674 - learning_rate: 1.0000e-04\nEpoch 40/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 320ms/step - dense_40_accuracy: 0.9997 - dense_40_loss: 1.7257e-04 - dense_41_accuracy: 0.9995 - dense_41_loss: 0.0011 - loss: 0.0012\nEpoch 40: val_loss did not improve from 0.13667\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 326ms/step - dense_40_accuracy: 0.9997 - dense_40_loss: 1.7227e-04 - dense_41_accuracy: 0.9995 - dense_41_loss: 0.0011 - loss: 0.0012 - val_dense_40_accuracy: 0.9630 - val_dense_40_loss: 0.1118 - val_dense_41_accuracy: 0.9691 - val_dense_41_loss: 0.0536 - val_loss: 0.1715 - learning_rate: 1.0000e-04\nEpoch 41/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 325ms/step - dense_40_accuracy: 0.9998 - dense_40_loss: 1.4105e-04 - dense_41_accuracy: 1.0000 - dense_41_loss: 1.8620e-04 - loss: 3.2708e-04\nEpoch 41: val_loss did not improve from 0.13667\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 331ms/step - dense_40_accuracy: 0.9998 - dense_40_loss: 1.4117e-04 - dense_41_accuracy: 1.0000 - dense_41_loss: 1.8674e-04 - loss: 3.2758e-04 - val_dense_40_accuracy: 0.9614 - val_dense_40_loss: 0.1196 - val_dense_41_accuracy: 0.9707 - val_dense_41_loss: 0.0523 - val_loss: 0.1782 - learning_rate: 1.0000e-04\nEpoch 42/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 315ms/step - dense_40_accuracy: 0.9998 - dense_40_loss: 2.9342e-04 - dense_41_accuracy: 0.9998 - dense_41_loss: 6.7058e-04 - loss: 9.6307e-04\nEpoch 42: val_loss did not improve from 0.13667\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 322ms/step - dense_40_accuracy: 0.9998 - dense_40_loss: 2.9589e-04 - dense_41_accuracy: 0.9998 - dense_41_loss: 6.7083e-04 - loss: 9.6485e-04 - val_dense_40_accuracy: 0.9707 - val_dense_40_loss: 0.1009 - val_dense_41_accuracy: 0.9753 - val_dense_41_loss: 0.0470 - val_loss: 0.1533 - learning_rate: 1.0000e-04\nEpoch 43/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 319ms/step - dense_40_accuracy: 0.9997 - dense_40_loss: 2.1438e-04 - dense_41_accuracy: 0.9990 - dense_41_loss: 0.0017 - loss: 0.0019\nEpoch 43: val_loss did not improve from 0.13667\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 326ms/step - dense_40_accuracy: 0.9997 - dense_40_loss: 2.1436e-04 - dense_41_accuracy: 0.9990 - dense_41_loss: 0.0017 - loss: 0.0019 - val_dense_40_accuracy: 0.9691 - val_dense_40_loss: 0.1151 - val_dense_41_accuracy: 0.9691 - val_dense_41_loss: 0.0573 - val_loss: 0.1788 - learning_rate: 1.0000e-04\nEpoch 44/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321ms/step - dense_40_accuracy: 0.9998 - dense_40_loss: 4.4656e-04 - dense_41_accuracy: 0.9996 - dense_41_loss: 7.6050e-04 - loss: 0.0012\nEpoch 44: val_loss did not improve from 0.13667\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 327ms/step - dense_40_accuracy: 0.9998 - dense_40_loss: 4.5137e-04 - dense_41_accuracy: 0.9996 - dense_41_loss: 7.6277e-04 - loss: 0.0012 - val_dense_40_accuracy: 0.9707 - val_dense_40_loss: 0.1054 - val_dense_41_accuracy: 0.9660 - val_dense_41_loss: 0.0587 - val_loss: 0.1702 - learning_rate: 1.0000e-04\nEpoch 45/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 323ms/step - dense_40_accuracy: 0.9996 - dense_40_loss: 4.3709e-04 - dense_41_accuracy: 1.0000 - dense_41_loss: 3.6901e-04 - loss: 8.0611e-04\nEpoch 45: val_loss did not improve from 0.13667\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 330ms/step - dense_40_accuracy: 0.9996 - dense_40_loss: 4.3641e-04 - dense_41_accuracy: 1.0000 - dense_41_loss: 3.6925e-04 - loss: 8.0567e-04 - val_dense_40_accuracy: 0.9645 - val_dense_40_loss: 0.1092 - val_dense_41_accuracy: 0.9753 - val_dense_41_loss: 0.0534 - val_loss: 0.1686 - learning_rate: 1.0000e-04\nEpoch 46/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 320ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 4.3955e-05 - dense_41_accuracy: 0.9993 - dense_41_loss: 9.5051e-04 - loss: 9.9447e-04\nEpoch 46: val_loss did not improve from 0.13667\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 326ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 4.4874e-05 - dense_41_accuracy: 0.9993 - dense_41_loss: 9.4906e-04 - loss: 9.9395e-04 - val_dense_40_accuracy: 0.9660 - val_dense_40_loss: 0.1162 - val_dense_41_accuracy: 0.9722 - val_dense_41_loss: 0.0528 - val_loss: 0.1752 - learning_rate: 1.0000e-04\nEpoch 47/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 314ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 1.2790e-04 - dense_41_accuracy: 0.9998 - dense_41_loss: 6.9271e-04 - loss: 8.2062e-04\nEpoch 47: val_loss did not improve from 0.13667\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 320ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 1.2783e-04 - dense_41_accuracy: 0.9998 - dense_41_loss: 6.9292e-04 - loss: 8.2076e-04 - val_dense_40_accuracy: 0.9722 - val_dense_40_loss: 0.1153 - val_dense_41_accuracy: 0.9738 - val_dense_41_loss: 0.0567 - val_loss: 0.1784 - learning_rate: 1.0000e-04\nEpoch 48/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - dense_40_accuracy: 0.9996 - dense_40_loss: 2.6591e-04 - dense_41_accuracy: 0.9995 - dense_41_loss: 0.0011 - loss: 0.0014\nEpoch 48: val_loss did not improve from 0.13667\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 325ms/step - dense_40_accuracy: 0.9996 - dense_40_loss: 2.6581e-04 - dense_41_accuracy: 0.9995 - dense_41_loss: 0.0011 - loss: 0.0014 - val_dense_40_accuracy: 0.9645 - val_dense_40_loss: 0.1316 - val_dense_41_accuracy: 0.9691 - val_dense_41_loss: 0.0666 - val_loss: 0.2055 - learning_rate: 1.0000e-04\nEpoch 49/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 320ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 2.4699e-04 - dense_41_accuracy: 0.9999 - dense_41_loss: 6.6725e-04 - loss: 9.1424e-04\nEpoch 49: val_loss did not improve from 0.13667\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 326ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 2.4665e-04 - dense_41_accuracy: 0.9999 - dense_41_loss: 6.6610e-04 - loss: 9.1274e-04 - val_dense_40_accuracy: 0.9691 - val_dense_40_loss: 0.1158 - val_dense_41_accuracy: 0.9753 - val_dense_41_loss: 0.0474 - val_loss: 0.1692 - learning_rate: 1.0000e-04\nEpoch 50/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - dense_40_accuracy: 0.9998 - dense_40_loss: 2.3391e-04 - dense_41_accuracy: 0.9996 - dense_41_loss: 0.0011 - loss: 0.0013\nEpoch 50: val_loss did not improve from 0.13667\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 324ms/step - dense_40_accuracy: 0.9998 - dense_40_loss: 2.3432e-04 - dense_41_accuracy: 0.9996 - dense_41_loss: 0.0011 - loss: 0.0013 - val_dense_40_accuracy: 0.9707 - val_dense_40_loss: 0.1282 - val_dense_41_accuracy: 0.9707 - val_dense_41_loss: 0.0549 - val_loss: 0.1898 - learning_rate: 1.0000e-04\nEpoch 51/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 327ms/step - dense_40_accuracy: 0.9998 - dense_40_loss: 4.0587e-04 - dense_41_accuracy: 0.9995 - dense_41_loss: 7.2487e-04 - loss: 0.0011\nEpoch 51: val_loss did not improve from 0.13667\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 334ms/step - dense_40_accuracy: 0.9998 - dense_40_loss: 4.0602e-04 - dense_41_accuracy: 0.9995 - dense_41_loss: 7.2541e-04 - loss: 0.0011 - val_dense_40_accuracy: 0.9676 - val_dense_40_loss: 0.1273 - val_dense_41_accuracy: 0.9691 - val_dense_41_loss: 0.0622 - val_loss: 0.1965 - learning_rate: 1.0000e-04\nEpoch 52/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 317ms/step - dense_40_accuracy: 0.9999 - dense_40_loss: 7.4253e-05 - dense_41_accuracy: 0.9997 - dense_41_loss: 7.9268e-04 - loss: 8.6694e-04\nEpoch 52: val_loss did not improve from 0.13667\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 323ms/step - dense_40_accuracy: 0.9999 - dense_40_loss: 7.4433e-05 - dense_41_accuracy: 0.9997 - dense_41_loss: 7.9280e-04 - loss: 8.6725e-04 - val_dense_40_accuracy: 0.9660 - val_dense_40_loss: 0.1338 - val_dense_41_accuracy: 0.9722 - val_dense_41_loss: 0.0465 - val_loss: 0.1869 - learning_rate: 1.0000e-04\nEpoch 53/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 317ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 1.1678e-04 - dense_41_accuracy: 0.9998 - dense_41_loss: 5.6154e-04 - loss: 6.7792e-04\nEpoch 53: val_loss did not improve from 0.13667\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 323ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 1.1676e-04 - dense_41_accuracy: 0.9998 - dense_41_loss: 5.6271e-04 - loss: 6.7868e-04 - val_dense_40_accuracy: 0.9660 - val_dense_40_loss: 0.1334 - val_dense_41_accuracy: 0.9753 - val_dense_41_loss: 0.0509 - val_loss: 0.1912 - learning_rate: 1.0000e-04\nEpoch 54/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 314ms/step - dense_40_accuracy: 0.9999 - dense_40_loss: 6.4583e-05 - dense_41_accuracy: 0.9992 - dense_41_loss: 0.0012 - loss: 0.0012\nEpoch 54: val_loss did not improve from 0.13667\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 321ms/step - dense_40_accuracy: 0.9999 - dense_40_loss: 6.4652e-05 - dense_41_accuracy: 0.9992 - dense_41_loss: 0.0012 - loss: 0.0012 - val_dense_40_accuracy: 0.9630 - val_dense_40_loss: 0.1311 - val_dense_41_accuracy: 0.9738 - val_dense_41_loss: 0.0480 - val_loss: 0.1858 - learning_rate: 1.0000e-04\nEpoch 55/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 323ms/step - dense_40_accuracy: 0.9994 - dense_40_loss: 4.4662e-04 - dense_41_accuracy: 0.9996 - dense_41_loss: 8.0246e-04 - loss: 0.0012\nEpoch 55: val_loss did not improve from 0.13667\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 329ms/step - dense_40_accuracy: 0.9994 - dense_40_loss: 4.4576e-04 - dense_41_accuracy: 0.9996 - dense_41_loss: 8.0119e-04 - loss: 0.0012 - val_dense_40_accuracy: 0.9691 - val_dense_40_loss: 0.1181 - val_dense_41_accuracy: 0.9722 - val_dense_41_loss: 0.0487 - val_loss: 0.1730 - learning_rate: 1.0000e-04\nEpoch 56/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 316ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 5.9893e-05 - dense_41_accuracy: 0.9999 - dense_41_loss: 4.1436e-04 - loss: 4.7426e-04\nEpoch 56: val_loss did not improve from 0.13667\n\nEpoch 56: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 322ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 6.0321e-05 - dense_41_accuracy: 0.9999 - dense_41_loss: 4.1489e-04 - loss: 4.7523e-04 - val_dense_40_accuracy: 0.9660 - val_dense_40_loss: 0.1326 - val_dense_41_accuracy: 0.9738 - val_dense_41_loss: 0.0443 - val_loss: 0.1835 - learning_rate: 1.0000e-04\nEpoch 57/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 1.3493e-04 - dense_41_accuracy: 0.9995 - dense_41_loss: 5.8986e-04 - loss: 7.2480e-04\nEpoch 57: val_loss did not improve from 0.13667\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 324ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 1.3581e-04 - dense_41_accuracy: 0.9995 - dense_41_loss: 5.9064e-04 - loss: 7.2647e-04 - val_dense_40_accuracy: 0.9660 - val_dense_40_loss: 0.1284 - val_dense_41_accuracy: 0.9691 - val_dense_41_loss: 0.0469 - val_loss: 0.1818 - learning_rate: 1.0000e-05\nEpoch 58/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 316ms/step - dense_40_accuracy: 0.9999 - dense_40_loss: 2.6296e-04 - dense_41_accuracy: 0.9993 - dense_41_loss: 0.0018 - loss: 0.0021\nEpoch 58: val_loss did not improve from 0.13667\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 323ms/step - dense_40_accuracy: 0.9999 - dense_40_loss: 2.6275e-04 - dense_41_accuracy: 0.9993 - dense_41_loss: 0.0018 - loss: 0.0021 - val_dense_40_accuracy: 0.9660 - val_dense_40_loss: 0.1261 - val_dense_41_accuracy: 0.9691 - val_dense_41_loss: 0.0463 - val_loss: 0.1788 - learning_rate: 1.0000e-05\nEpoch 59/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 319ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 1.4643e-04 - dense_41_accuracy: 0.9995 - dense_41_loss: 0.0011 - loss: 0.0012\nEpoch 59: val_loss did not improve from 0.13667\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 326ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 1.4610e-04 - dense_41_accuracy: 0.9995 - dense_41_loss: 0.0011 - loss: 0.0012 - val_dense_40_accuracy: 0.9676 - val_dense_40_loss: 0.1252 - val_dense_41_accuracy: 0.9707 - val_dense_41_loss: 0.0444 - val_loss: 0.1758 - learning_rate: 1.0000e-05\nEpoch 60/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 319ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 3.1335e-05 - dense_41_accuracy: 0.9998 - dense_41_loss: 3.7258e-04 - loss: 4.0392e-04\nEpoch 60: val_loss did not improve from 0.13667\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 325ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 3.1539e-05 - dense_41_accuracy: 0.9998 - dense_41_loss: 3.7237e-04 - loss: 4.0391e-04 - val_dense_40_accuracy: 0.9660 - val_dense_40_loss: 0.1260 - val_dense_41_accuracy: 0.9691 - val_dense_41_loss: 0.0450 - val_loss: 0.1774 - learning_rate: 1.0000e-05\nEpoch 61/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 324ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 2.9194e-05 - dense_41_accuracy: 0.9995 - dense_41_loss: 6.7225e-04 - loss: 7.0144e-04\nEpoch 61: val_loss did not improve from 0.13667\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 331ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 2.9327e-05 - dense_41_accuracy: 0.9995 - dense_41_loss: 6.7238e-04 - loss: 7.0172e-04 - val_dense_40_accuracy: 0.9660 - val_dense_40_loss: 0.1275 - val_dense_41_accuracy: 0.9691 - val_dense_41_loss: 0.0454 - val_loss: 0.1793 - learning_rate: 1.0000e-05\nEpoch 62/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 314ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 1.2678e-05 - dense_41_accuracy: 0.9998 - dense_41_loss: 4.4134e-04 - loss: 4.5402e-04\nEpoch 62: val_loss did not improve from 0.13667\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 320ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 1.2664e-05 - dense_41_accuracy: 0.9998 - dense_41_loss: 4.4128e-04 - loss: 4.5396e-04 - val_dense_40_accuracy: 0.9676 - val_dense_40_loss: 0.1258 - val_dense_41_accuracy: 0.9707 - val_dense_41_loss: 0.0438 - val_loss: 0.1758 - learning_rate: 1.0000e-05\nEpoch 63/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 316ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 3.7855e-05 - dense_41_accuracy: 0.9993 - dense_41_loss: 8.3098e-04 - loss: 8.6879e-04\nEpoch 63: val_loss did not improve from 0.13667\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 323ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 3.7960e-05 - dense_41_accuracy: 0.9993 - dense_41_loss: 8.2939e-04 - loss: 8.6727e-04 - val_dense_40_accuracy: 0.9676 - val_dense_40_loss: 0.1260 - val_dense_41_accuracy: 0.9722 - val_dense_41_loss: 0.0440 - val_loss: 0.1763 - learning_rate: 1.0000e-05\nEpoch 64/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 315ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 4.0365e-05 - dense_41_accuracy: 0.9996 - dense_41_loss: 6.6138e-04 - loss: 7.0114e-04\nEpoch 64: val_loss did not improve from 0.13667\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 321ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 4.0466e-05 - dense_41_accuracy: 0.9996 - dense_41_loss: 6.6254e-04 - loss: 7.0180e-04 - val_dense_40_accuracy: 0.9676 - val_dense_40_loss: 0.1258 - val_dense_41_accuracy: 0.9707 - val_dense_41_loss: 0.0429 - val_loss: 0.1750 - learning_rate: 1.0000e-05\nEpoch 65/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 324ms/step - dense_40_accuracy: 0.9993 - dense_40_loss: 0.0014 - dense_41_accuracy: 0.9994 - dense_41_loss: 0.0011 - loss: 0.0025\nEpoch 65: val_loss did not improve from 0.13667\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 330ms/step - dense_40_accuracy: 0.9993 - dense_40_loss: 0.0014 - dense_41_accuracy: 0.9994 - dense_41_loss: 0.0011 - loss: 0.0025 - val_dense_40_accuracy: 0.9660 - val_dense_40_loss: 0.1284 - val_dense_41_accuracy: 0.9722 - val_dense_41_loss: 0.0446 - val_loss: 0.1794 - learning_rate: 1.0000e-05\nEpoch 66/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 319ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 1.9188e-05 - dense_41_accuracy: 0.9996 - dense_41_loss: 4.7657e-04 - loss: 4.9573e-04\nEpoch 66: val_loss did not improve from 0.13667\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 326ms/step - dense_40_accuracy: 1.0000 - dense_40_loss: 1.9261e-05 - dense_41_accuracy: 0.9996 - dense_41_loss: 4.7591e-04 - loss: 4.9512e-04 - val_dense_40_accuracy: 0.9660 - val_dense_40_loss: 0.1291 - val_dense_41_accuracy: 0.9753 - val_dense_41_loss: 0.0435 - val_loss: 0.1790 - learning_rate: 1.0000e-05\nEpoch 66: early stopping\nRestoring model weights from the end of the best epoch: 6.\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"model.evaluate([X_test_s, X_test_h1], [y_test_s, y_test_h1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T08:04:33.607476Z","iopub.execute_input":"2025-03-08T08:04:33.607717Z","iopub.status.idle":"2025-03-08T08:04:36.271554Z","shell.execute_reply.started":"2025-03-08T08:04:33.607698Z","shell.execute_reply":"2025-03-08T08:04:36.270789Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 80ms/step - dense_40_accuracy: 0.9613 - dense_40_loss: 0.1654 - dense_41_accuracy: 0.9791 - dense_41_loss: 0.0428 - loss: 0.2085\n","output_type":"stream"},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"[0.17169909179210663,\n 0.1347561925649643,\n 0.03273071348667145,\n 0.9654321074485779,\n 0.9827160239219666]"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\n\ninitial_gamma = 0.5\nlearning_rate = 1e-2\noptimizer = Adam(learning_rate=0.001)\n#opt = Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.9, epsilon=None, amsgrad=False)\n# Compile the model with the custom optimizer\nmodel.compile(optimizer=optimizer,\n              loss=['categorical_crossentropy', 'categorical_crossentropy'],\n              loss_weights=[initial_gamma, (1 -  initial_gamma)],\n              metrics=['accuracy', 'accuracy'])\n\n\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\ndef checkpoint_callback():\n\n    checkpoint_filepath = 'best1_model_cer_skin_lung.keras'\n\n    model_checkpoint_callback= ModelCheckpoint(filepath=checkpoint_filepath,\n                           save_weights_only=False,\n                           #frequency='epoch',\n                           monitor='val_loss',\n                           save_best_only=True,\n                            mode='min',\n                           verbose=1)\n\n    return model_checkpoint_callback\n\ndef early_stopping(patience):\n    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', restore_best_weights=True, patience=60, verbose=1)\n    return es_callback\n\n\n\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n                              patience=50, verbose = 1, min_lr=0.00001)\n\ncheckpoint_callback = checkpoint_callback()\n\nearly_stopping = early_stopping(patience=100)\ncallbacks = [checkpoint_callback, early_stopping, reduce_lr]\n            \n\n# Fit the model with callbacks\nhistory = model.fit([X_train_s, X_train_h], [y_train_s, y_train_h],\n                    epochs=200,\n                    validation_data=([X_val_s, X_val_h1], [y_val_s, y_val_h1]), verbose=1,\n                    shuffle=True,\n                    callbacks=callbacks) # UpdateGammaCallback","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T11:56:52.854385Z","iopub.execute_input":"2025-03-05T11:56:52.854712Z","execution_failed":"2025-03-05T12:59:51.144Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 724ms/step - dense_40_accuracy: 0.5691 - dense_40_loss: 0.5800 - dense_41_accuracy: 0.6278 - dense_41_loss: 0.5805 - loss: 1.1605\nEpoch 1: val_loss improved from inf to 2.66162, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m491s\u001b[0m 910ms/step - dense_40_accuracy: 0.5695 - dense_40_loss: 0.5794 - dense_41_accuracy: 0.6280 - dense_41_loss: 0.5802 - loss: 1.1596 - val_dense_40_accuracy: 0.4213 - val_dense_40_loss: 0.8700 - val_dense_41_accuracy: 0.0432 - val_dense_41_loss: 1.7756 - val_loss: 2.6616 - learning_rate: 0.0010\nEpoch 2/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step - dense_40_accuracy: 0.7933 - dense_40_loss: 0.2996 - dense_41_accuracy: 0.6950 - dense_41_loss: 0.4319 - loss: 0.7315\nEpoch 2: val_loss improved from 2.66162 to 1.29585, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 200ms/step - dense_40_accuracy: 0.7933 - dense_40_loss: 0.2995 - dense_41_accuracy: 0.6950 - dense_41_loss: 0.4319 - loss: 0.7314 - val_dense_40_accuracy: 0.7145 - val_dense_40_loss: 0.3835 - val_dense_41_accuracy: 0.3071 - val_dense_41_loss: 0.9074 - val_loss: 1.2958 - learning_rate: 0.0010\nEpoch 3/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - dense_40_accuracy: 0.8291 - dense_40_loss: 0.2311 - dense_41_accuracy: 0.7079 - dense_41_loss: 0.3998 - loss: 0.6309\nEpoch 3: val_loss improved from 1.29585 to 1.00727, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 200ms/step - dense_40_accuracy: 0.8292 - dense_40_loss: 0.2311 - dense_41_accuracy: 0.7079 - dense_41_loss: 0.3998 - loss: 0.6308 - val_dense_40_accuracy: 0.7623 - val_dense_40_loss: 0.3209 - val_dense_41_accuracy: 0.5031 - val_dense_41_loss: 0.6858 - val_loss: 1.0073 - learning_rate: 0.0010\nEpoch 4/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - dense_40_accuracy: 0.8669 - dense_40_loss: 0.1913 - dense_41_accuracy: 0.7239 - dense_41_loss: 0.3771 - loss: 0.5685\nEpoch 4: val_loss did not improve from 1.00727\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 189ms/step - dense_40_accuracy: 0.8669 - dense_40_loss: 0.1913 - dense_41_accuracy: 0.7239 - dense_41_loss: 0.3771 - loss: 0.5684 - val_dense_40_accuracy: 0.7978 - val_dense_40_loss: 0.2821 - val_dense_41_accuracy: 0.3025 - val_dense_41_loss: 1.1433 - val_loss: 1.4340 - learning_rate: 0.0010\nEpoch 5/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - dense_40_accuracy: 0.9007 - dense_40_loss: 0.1473 - dense_41_accuracy: 0.7350 - dense_41_loss: 0.3606 - loss: 0.5079\nEpoch 5: val_loss did not improve from 1.00727\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 188ms/step - dense_40_accuracy: 0.9007 - dense_40_loss: 0.1473 - dense_41_accuracy: 0.7350 - dense_41_loss: 0.3606 - loss: 0.5079 - val_dense_40_accuracy: 0.8503 - val_dense_40_loss: 0.3010 - val_dense_41_accuracy: 0.5108 - val_dense_41_loss: 0.7562 - val_loss: 1.0635 - learning_rate: 0.0010\nEpoch 6/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - dense_40_accuracy: 0.9227 - dense_40_loss: 0.1144 - dense_41_accuracy: 0.7447 - dense_41_loss: 0.3476 - loss: 0.4619\nEpoch 6: val_loss did not improve from 1.00727\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 188ms/step - dense_40_accuracy: 0.9227 - dense_40_loss: 0.1144 - dense_41_accuracy: 0.7447 - dense_41_loss: 0.3476 - loss: 0.4620 - val_dense_40_accuracy: 0.8565 - val_dense_40_loss: 0.2069 - val_dense_41_accuracy: 0.3549 - val_dense_41_loss: 0.8286 - val_loss: 1.0526 - learning_rate: 0.0010\nEpoch 7/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step - dense_40_accuracy: 0.9201 - dense_40_loss: 0.1202 - dense_41_accuracy: 0.7617 - dense_41_loss: 0.3235 - loss: 0.4437\nEpoch 7: val_loss improved from 1.00727 to 0.92773, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 201ms/step - dense_40_accuracy: 0.9201 - dense_40_loss: 0.1201 - dense_41_accuracy: 0.7617 - dense_41_loss: 0.3236 - loss: 0.4437 - val_dense_40_accuracy: 0.8565 - val_dense_40_loss: 0.2451 - val_dense_41_accuracy: 0.5170 - val_dense_41_loss: 0.6768 - val_loss: 0.9277 - learning_rate: 0.0010\nEpoch 8/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - dense_40_accuracy: 0.9321 - dense_40_loss: 0.0974 - dense_41_accuracy: 0.7538 - dense_41_loss: 0.3298 - loss: 0.4272\nEpoch 8: val_loss improved from 0.92773 to 0.86274, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 200ms/step - dense_40_accuracy: 0.9321 - dense_40_loss: 0.0973 - dense_41_accuracy: 0.7539 - dense_41_loss: 0.3298 - loss: 0.4272 - val_dense_40_accuracy: 0.7701 - val_dense_40_loss: 0.4577 - val_dense_41_accuracy: 0.6651 - val_dense_41_loss: 0.4098 - val_loss: 0.8627 - learning_rate: 0.0010\nEpoch 9/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - dense_40_accuracy: 0.9373 - dense_40_loss: 0.0877 - dense_41_accuracy: 0.7674 - dense_41_loss: 0.3098 - loss: 0.3975\nEpoch 9: val_loss improved from 0.86274 to 0.62025, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 200ms/step - dense_40_accuracy: 0.9373 - dense_40_loss: 0.0877 - dense_41_accuracy: 0.7673 - dense_41_loss: 0.3099 - loss: 0.3976 - val_dense_40_accuracy: 0.8873 - val_dense_40_loss: 0.1982 - val_dense_41_accuracy: 0.6852 - val_dense_41_loss: 0.4191 - val_loss: 0.6203 - learning_rate: 0.0010\nEpoch 10/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - dense_40_accuracy: 0.9374 - dense_40_loss: 0.0932 - dense_41_accuracy: 0.7637 - dense_41_loss: 0.3150 - loss: 0.4081\nEpoch 10: val_loss did not improve from 0.62025\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 189ms/step - dense_40_accuracy: 0.9374 - dense_40_loss: 0.0931 - dense_41_accuracy: 0.7637 - dense_41_loss: 0.3149 - loss: 0.4081 - val_dense_40_accuracy: 0.8765 - val_dense_40_loss: 0.2313 - val_dense_41_accuracy: 0.6713 - val_dense_41_loss: 0.5666 - val_loss: 0.8124 - learning_rate: 0.0010\nEpoch 11/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - dense_40_accuracy: 0.9435 - dense_40_loss: 0.0794 - dense_41_accuracy: 0.7771 - dense_41_loss: 0.2994 - loss: 0.3787\nEpoch 11: val_loss did not improve from 0.62025\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 189ms/step - dense_40_accuracy: 0.9435 - dense_40_loss: 0.0793 - dense_41_accuracy: 0.7771 - dense_41_loss: 0.2994 - loss: 0.3788 - val_dense_40_accuracy: 0.9275 - val_dense_40_loss: 0.1365 - val_dense_41_accuracy: 0.6373 - val_dense_41_loss: 0.5429 - val_loss: 0.6889 - learning_rate: 0.0010\nEpoch 12/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step - dense_40_accuracy: 0.9562 - dense_40_loss: 0.0601 - dense_41_accuracy: 0.7774 - dense_41_loss: 0.3013 - loss: 0.3614\nEpoch 12: val_loss improved from 0.62025 to 0.56882, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 200ms/step - dense_40_accuracy: 0.9561 - dense_40_loss: 0.0601 - dense_41_accuracy: 0.7774 - dense_41_loss: 0.3013 - loss: 0.3615 - val_dense_40_accuracy: 0.9244 - val_dense_40_loss: 0.1062 - val_dense_41_accuracy: 0.6574 - val_dense_41_loss: 0.4561 - val_loss: 0.5688 - learning_rate: 0.0010\nEpoch 13/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step - dense_40_accuracy: 0.9635 - dense_40_loss: 0.0533 - dense_41_accuracy: 0.7864 - dense_41_loss: 0.2874 - loss: 0.3407\nEpoch 13: val_loss improved from 0.56882 to 0.53442, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 200ms/step - dense_40_accuracy: 0.9635 - dense_40_loss: 0.0534 - dense_41_accuracy: 0.7864 - dense_41_loss: 0.2874 - loss: 0.3408 - val_dense_40_accuracy: 0.9090 - val_dense_40_loss: 0.1290 - val_dense_41_accuracy: 0.7099 - val_dense_41_loss: 0.4020 - val_loss: 0.5344 - learning_rate: 0.0010\nEpoch 14/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - dense_40_accuracy: 0.9670 - dense_40_loss: 0.0492 - dense_41_accuracy: 0.7937 - dense_41_loss: 0.2793 - loss: 0.3285\nEpoch 14: val_loss did not improve from 0.53442\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 188ms/step - dense_40_accuracy: 0.9670 - dense_40_loss: 0.0493 - dense_41_accuracy: 0.7936 - dense_41_loss: 0.2793 - loss: 0.3286 - val_dense_40_accuracy: 0.8781 - val_dense_40_loss: 0.1760 - val_dense_41_accuracy: 0.6759 - val_dense_41_loss: 0.5368 - val_loss: 0.7255 - learning_rate: 0.0010\nEpoch 15/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - dense_40_accuracy: 0.9684 - dense_40_loss: 0.0516 - dense_41_accuracy: 0.7944 - dense_41_loss: 0.2718 - loss: 0.3234\nEpoch 15: val_loss did not improve from 0.53442\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 189ms/step - dense_40_accuracy: 0.9684 - dense_40_loss: 0.0517 - dense_41_accuracy: 0.7944 - dense_41_loss: 0.2718 - loss: 0.3234 - val_dense_40_accuracy: 0.9074 - val_dense_40_loss: 0.1901 - val_dense_41_accuracy: 0.6466 - val_dense_41_loss: 0.6018 - val_loss: 0.8086 - learning_rate: 0.0010\nEpoch 16/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - dense_40_accuracy: 0.9716 - dense_40_loss: 0.0438 - dense_41_accuracy: 0.8027 - dense_41_loss: 0.2635 - loss: 0.3074\nEpoch 16: val_loss did not improve from 0.53442\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 189ms/step - dense_40_accuracy: 0.9716 - dense_40_loss: 0.0439 - dense_41_accuracy: 0.8027 - dense_41_loss: 0.2636 - loss: 0.3074 - val_dense_40_accuracy: 0.8457 - val_dense_40_loss: 0.2384 - val_dense_41_accuracy: 0.7253 - val_dense_41_loss: 0.3678 - val_loss: 0.6092 - learning_rate: 0.0010\nEpoch 17/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - dense_40_accuracy: 0.9747 - dense_40_loss: 0.0390 - dense_41_accuracy: 0.8027 - dense_41_loss: 0.2625 - loss: 0.3015\nEpoch 17: val_loss improved from 0.53442 to 0.44114, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 199ms/step - dense_40_accuracy: 0.9747 - dense_40_loss: 0.0390 - dense_41_accuracy: 0.8027 - dense_41_loss: 0.2625 - loss: 0.3015 - val_dense_40_accuracy: 0.9306 - val_dense_40_loss: 0.1072 - val_dense_41_accuracy: 0.7531 - val_dense_41_loss: 0.3285 - val_loss: 0.4411 - learning_rate: 0.0010\nEpoch 18/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - dense_40_accuracy: 0.9734 - dense_40_loss: 0.0376 - dense_41_accuracy: 0.8088 - dense_41_loss: 0.2553 - loss: 0.2928\nEpoch 18: val_loss improved from 0.44114 to 0.39213, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 200ms/step - dense_40_accuracy: 0.9734 - dense_40_loss: 0.0376 - dense_41_accuracy: 0.8088 - dense_41_loss: 0.2553 - loss: 0.2928 - val_dense_40_accuracy: 0.9259 - val_dense_40_loss: 0.1154 - val_dense_41_accuracy: 0.7855 - val_dense_41_loss: 0.2781 - val_loss: 0.3921 - learning_rate: 0.0010\nEpoch 19/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - dense_40_accuracy: 0.9768 - dense_40_loss: 0.0355 - dense_41_accuracy: 0.8222 - dense_41_loss: 0.2344 - loss: 0.2699\nEpoch 19: val_loss did not improve from 0.39213\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 189ms/step - dense_40_accuracy: 0.9768 - dense_40_loss: 0.0355 - dense_41_accuracy: 0.8222 - dense_41_loss: 0.2345 - loss: 0.2700 - val_dense_40_accuracy: 0.9213 - val_dense_40_loss: 0.1163 - val_dense_41_accuracy: 0.7238 - val_dense_41_loss: 0.3572 - val_loss: 0.4769 - learning_rate: 0.0010\nEpoch 20/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - dense_40_accuracy: 0.9757 - dense_40_loss: 0.0346 - dense_41_accuracy: 0.8168 - dense_41_loss: 0.2382 - loss: 0.2729\nEpoch 20: val_loss did not improve from 0.39213\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 189ms/step - dense_40_accuracy: 0.9757 - dense_40_loss: 0.0346 - dense_41_accuracy: 0.8168 - dense_41_loss: 0.2382 - loss: 0.2729 - val_dense_40_accuracy: 0.9290 - val_dense_40_loss: 0.1254 - val_dense_41_accuracy: 0.7562 - val_dense_41_loss: 0.3336 - val_loss: 0.4635 - learning_rate: 0.0010\nEpoch 21/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - dense_40_accuracy: 0.9742 - dense_40_loss: 0.0373 - dense_41_accuracy: 0.8311 - dense_41_loss: 0.2226 - loss: 0.2599\nEpoch 21: val_loss did not improve from 0.39213\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 189ms/step - dense_40_accuracy: 0.9742 - dense_40_loss: 0.0373 - dense_41_accuracy: 0.8310 - dense_41_loss: 0.2227 - loss: 0.2600 - val_dense_40_accuracy: 0.9275 - val_dense_40_loss: 0.1195 - val_dense_41_accuracy: 0.7685 - val_dense_41_loss: 0.2996 - val_loss: 0.4258 - learning_rate: 0.0010\nEpoch 22/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - dense_40_accuracy: 0.9799 - dense_40_loss: 0.0299 - dense_41_accuracy: 0.8392 - dense_41_loss: 0.2128 - loss: 0.2427\nEpoch 22: val_loss did not improve from 0.39213\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 189ms/step - dense_40_accuracy: 0.9799 - dense_40_loss: 0.0299 - dense_41_accuracy: 0.8392 - dense_41_loss: 0.2128 - loss: 0.2427 - val_dense_40_accuracy: 0.7870 - val_dense_40_loss: 0.3963 - val_dense_41_accuracy: 0.4429 - val_dense_41_loss: 0.9019 - val_loss: 1.3069 - learning_rate: 0.0010\nEpoch 23/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - dense_40_accuracy: 0.9790 - dense_40_loss: 0.0318 - dense_41_accuracy: 0.8372 - dense_41_loss: 0.2207 - loss: 0.2524\nEpoch 23: val_loss did not improve from 0.39213\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 188ms/step - dense_40_accuracy: 0.9790 - dense_40_loss: 0.0318 - dense_41_accuracy: 0.8372 - dense_41_loss: 0.2207 - loss: 0.2524 - val_dense_40_accuracy: 0.9383 - val_dense_40_loss: 0.1147 - val_dense_41_accuracy: 0.7546 - val_dense_41_loss: 0.3960 - val_loss: 0.5058 - learning_rate: 0.0010\nEpoch 24/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - dense_40_accuracy: 0.9853 - dense_40_loss: 0.0228 - dense_41_accuracy: 0.8563 - dense_41_loss: 0.1886 - loss: 0.2114\nEpoch 24: val_loss did not improve from 0.39213\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 188ms/step - dense_40_accuracy: 0.9853 - dense_40_loss: 0.0228 - dense_41_accuracy: 0.8563 - dense_41_loss: 0.1887 - loss: 0.2114 - val_dense_40_accuracy: 0.9383 - val_dense_40_loss: 0.0912 - val_dense_41_accuracy: 0.7886 - val_dense_41_loss: 0.3070 - val_loss: 0.4055 - learning_rate: 0.0010\nEpoch 25/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - dense_40_accuracy: 0.9856 - dense_40_loss: 0.0241 - dense_41_accuracy: 0.8682 - dense_41_loss: 0.1805 - loss: 0.2046\nEpoch 25: val_loss improved from 0.39213 to 0.34391, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 199ms/step - dense_40_accuracy: 0.9856 - dense_40_loss: 0.0241 - dense_41_accuracy: 0.8682 - dense_41_loss: 0.1806 - loss: 0.2046 - val_dense_40_accuracy: 0.9414 - val_dense_40_loss: 0.1049 - val_dense_41_accuracy: 0.8287 - val_dense_41_loss: 0.2332 - val_loss: 0.3439 - learning_rate: 0.0010\nEpoch 26/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - dense_40_accuracy: 0.9811 - dense_40_loss: 0.0278 - dense_41_accuracy: 0.8764 - dense_41_loss: 0.1655 - loss: 0.1934\nEpoch 26: val_loss did not improve from 0.34391\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 188ms/step - dense_40_accuracy: 0.9811 - dense_40_loss: 0.0278 - dense_41_accuracy: 0.8763 - dense_41_loss: 0.1656 - loss: 0.1934 - val_dense_40_accuracy: 0.9367 - val_dense_40_loss: 0.1329 - val_dense_41_accuracy: 0.7377 - val_dense_41_loss: 0.5421 - val_loss: 0.6760 - learning_rate: 0.0010\nEpoch 27/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - dense_40_accuracy: 0.9872 - dense_40_loss: 0.0180 - dense_41_accuracy: 0.8866 - dense_41_loss: 0.1509 - loss: 0.1689\nEpoch 27: val_loss did not improve from 0.34391\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 189ms/step - dense_40_accuracy: 0.9872 - dense_40_loss: 0.0181 - dense_41_accuracy: 0.8866 - dense_41_loss: 0.1509 - loss: 0.1690 - val_dense_40_accuracy: 0.9429 - val_dense_40_loss: 0.1280 - val_dense_41_accuracy: 0.7948 - val_dense_41_loss: 0.3298 - val_loss: 0.4637 - learning_rate: 0.0010\nEpoch 28/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - dense_40_accuracy: 0.9820 - dense_40_loss: 0.0291 - dense_41_accuracy: 0.8846 - dense_41_loss: 0.1546 - loss: 0.1838\nEpoch 28: val_loss did not improve from 0.34391\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 189ms/step - dense_40_accuracy: 0.9820 - dense_40_loss: 0.0291 - dense_41_accuracy: 0.8846 - dense_41_loss: 0.1547 - loss: 0.1838 - val_dense_40_accuracy: 0.9213 - val_dense_40_loss: 0.1522 - val_dense_41_accuracy: 0.8210 - val_dense_41_loss: 0.2794 - val_loss: 0.4412 - learning_rate: 0.0010\nEpoch 29/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - dense_40_accuracy: 0.9777 - dense_40_loss: 0.0288 - dense_41_accuracy: 0.9063 - dense_41_loss: 0.1273 - loss: 0.1562\nEpoch 29: val_loss did not improve from 0.34391\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 188ms/step - dense_40_accuracy: 0.9777 - dense_40_loss: 0.0288 - dense_41_accuracy: 0.9062 - dense_41_loss: 0.1274 - loss: 0.1562 - val_dense_40_accuracy: 0.9398 - val_dense_40_loss: 0.1003 - val_dense_41_accuracy: 0.7623 - val_dense_41_loss: 0.3843 - val_loss: 0.4876 - learning_rate: 0.0010\nEpoch 30/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - dense_40_accuracy: 0.9800 - dense_40_loss: 0.0296 - dense_41_accuracy: 0.9072 - dense_41_loss: 0.1237 - loss: 0.1533\nEpoch 30: val_loss did not improve from 0.34391\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 188ms/step - dense_40_accuracy: 0.9800 - dense_40_loss: 0.0296 - dense_41_accuracy: 0.9072 - dense_41_loss: 0.1237 - loss: 0.1533 - val_dense_40_accuracy: 0.9367 - val_dense_40_loss: 0.1291 - val_dense_41_accuracy: 0.7577 - val_dense_41_loss: 0.3874 - val_loss: 0.5213 - learning_rate: 0.0010\nEpoch 31/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - dense_40_accuracy: 0.9902 - dense_40_loss: 0.0167 - dense_41_accuracy: 0.9068 - dense_41_loss: 0.1209 - loss: 0.1377\nEpoch 31: val_loss improved from 0.34391 to 0.26194, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 200ms/step - dense_40_accuracy: 0.9902 - dense_40_loss: 0.0167 - dense_41_accuracy: 0.9068 - dense_41_loss: 0.1210 - loss: 0.1377 - val_dense_40_accuracy: 0.9475 - val_dense_40_loss: 0.1135 - val_dense_41_accuracy: 0.8904 - val_dense_41_loss: 0.1433 - val_loss: 0.2619 - learning_rate: 0.0010\nEpoch 32/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - dense_40_accuracy: 0.9853 - dense_40_loss: 0.0220 - dense_41_accuracy: 0.9214 - dense_41_loss: 0.1066 - loss: 0.1285\nEpoch 32: val_loss did not improve from 0.26194\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 189ms/step - dense_40_accuracy: 0.9853 - dense_40_loss: 0.0219 - dense_41_accuracy: 0.9214 - dense_41_loss: 0.1066 - loss: 0.1285 - val_dense_40_accuracy: 0.9475 - val_dense_40_loss: 0.0973 - val_dense_41_accuracy: 0.7407 - val_dense_41_loss: 0.4766 - val_loss: 0.5897 - learning_rate: 0.0010\nEpoch 33/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - dense_40_accuracy: 0.9867 - dense_40_loss: 0.0188 - dense_41_accuracy: 0.9294 - dense_41_loss: 0.0955 - loss: 0.1142\nEpoch 33: val_loss did not improve from 0.26194\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 189ms/step - dense_40_accuracy: 0.9867 - dense_40_loss: 0.0188 - dense_41_accuracy: 0.9293 - dense_41_loss: 0.0955 - loss: 0.1143 - val_dense_40_accuracy: 0.9398 - val_dense_40_loss: 0.1102 - val_dense_41_accuracy: 0.8673 - val_dense_41_loss: 0.2080 - val_loss: 0.3144 - learning_rate: 0.0010\nEpoch 34/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - dense_40_accuracy: 0.9913 - dense_40_loss: 0.0124 - dense_41_accuracy: 0.9334 - dense_41_loss: 0.0919 - loss: 0.1043\nEpoch 34: val_loss improved from 0.26194 to 0.22459, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 200ms/step - dense_40_accuracy: 0.9913 - dense_40_loss: 0.0124 - dense_41_accuracy: 0.9334 - dense_41_loss: 0.0919 - loss: 0.1043 - val_dense_40_accuracy: 0.9383 - val_dense_40_loss: 0.1115 - val_dense_41_accuracy: 0.9182 - val_dense_41_loss: 0.1100 - val_loss: 0.2246 - learning_rate: 0.0010\nEpoch 35/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step - dense_40_accuracy: 0.9845 - dense_40_loss: 0.0226 - dense_41_accuracy: 0.9373 - dense_41_loss: 0.0923 - loss: 0.1149\nEpoch 35: val_loss did not improve from 0.22459\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 189ms/step - dense_40_accuracy: 0.9845 - dense_40_loss: 0.0226 - dense_41_accuracy: 0.9372 - dense_41_loss: 0.0923 - loss: 0.1149 - val_dense_40_accuracy: 0.9105 - val_dense_40_loss: 0.1982 - val_dense_41_accuracy: 0.7716 - val_dense_41_loss: 0.4507 - val_loss: 0.6354 - learning_rate: 0.0010\nEpoch 36/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - dense_40_accuracy: 0.9935 - dense_40_loss: 0.0121 - dense_41_accuracy: 0.9415 - dense_41_loss: 0.0791 - loss: 0.0912\nEpoch 36: val_loss did not improve from 0.22459\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 189ms/step - dense_40_accuracy: 0.9935 - dense_40_loss: 0.0121 - dense_41_accuracy: 0.9415 - dense_41_loss: 0.0791 - loss: 0.0912 - val_dense_40_accuracy: 0.9182 - val_dense_40_loss: 0.1551 - val_dense_41_accuracy: 0.8843 - val_dense_41_loss: 0.1603 - val_loss: 0.3247 - learning_rate: 0.0010\nEpoch 37/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - dense_40_accuracy: 0.9814 - dense_40_loss: 0.0261 - dense_41_accuracy: 0.9433 - dense_41_loss: 0.0768 - loss: 0.1029\nEpoch 37: val_loss did not improve from 0.22459\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 189ms/step - dense_40_accuracy: 0.9814 - dense_40_loss: 0.0260 - dense_41_accuracy: 0.9433 - dense_41_loss: 0.0768 - loss: 0.1029 - val_dense_40_accuracy: 0.9383 - val_dense_40_loss: 0.1060 - val_dense_41_accuracy: 0.8750 - val_dense_41_loss: 0.1813 - val_loss: 0.2914 - learning_rate: 0.0010\nEpoch 38/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - dense_40_accuracy: 0.9906 - dense_40_loss: 0.0118 - dense_41_accuracy: 0.9464 - dense_41_loss: 0.0704 - loss: 0.0822\nEpoch 38: val_loss did not improve from 0.22459\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 188ms/step - dense_40_accuracy: 0.9906 - dense_40_loss: 0.0118 - dense_41_accuracy: 0.9464 - dense_41_loss: 0.0704 - loss: 0.0822 - val_dense_40_accuracy: 0.9059 - val_dense_40_loss: 0.1866 - val_dense_41_accuracy: 0.6867 - val_dense_41_loss: 0.7644 - val_loss: 0.9621 - learning_rate: 0.0010\nEpoch 39/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - dense_40_accuracy: 0.9846 - dense_40_loss: 0.0209 - dense_41_accuracy: 0.9545 - dense_41_loss: 0.0582 - loss: 0.0792\nEpoch 39: val_loss did not improve from 0.22459\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 188ms/step - dense_40_accuracy: 0.9846 - dense_40_loss: 0.0209 - dense_41_accuracy: 0.9545 - dense_41_loss: 0.0583 - loss: 0.0792 - val_dense_40_accuracy: 0.9290 - val_dense_40_loss: 0.1568 - val_dense_41_accuracy: 0.8210 - val_dense_41_loss: 0.3392 - val_loss: 0.4865 - learning_rate: 0.0010\nEpoch 40/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - dense_40_accuracy: 0.9931 - dense_40_loss: 0.0101 - dense_41_accuracy: 0.9586 - dense_41_loss: 0.0587 - loss: 0.0688\nEpoch 40: val_loss did not improve from 0.22459\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 189ms/step - dense_40_accuracy: 0.9931 - dense_40_loss: 0.0101 - dense_41_accuracy: 0.9586 - dense_41_loss: 0.0587 - loss: 0.0689 - val_dense_40_accuracy: 0.9151 - val_dense_40_loss: 0.2091 - val_dense_41_accuracy: 0.9244 - val_dense_41_loss: 0.1056 - val_loss: 0.3149 - learning_rate: 0.0010\nEpoch 41/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - dense_40_accuracy: 0.9885 - dense_40_loss: 0.0166 - dense_41_accuracy: 0.9597 - dense_41_loss: 0.0582 - loss: 0.0748\nEpoch 41: val_loss did not improve from 0.22459\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 188ms/step - dense_40_accuracy: 0.9885 - dense_40_loss: 0.0166 - dense_41_accuracy: 0.9597 - dense_41_loss: 0.0582 - loss: 0.0748 - val_dense_40_accuracy: 0.9475 - val_dense_40_loss: 0.1128 - val_dense_41_accuracy: 0.8642 - val_dense_41_loss: 0.2313 - val_loss: 0.3506 - learning_rate: 0.0010\nEpoch 42/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - dense_40_accuracy: 0.9868 - dense_40_loss: 0.0214 - dense_41_accuracy: 0.9595 - dense_41_loss: 0.0622 - loss: 0.0836\nEpoch 42: val_loss did not improve from 0.22459\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 188ms/step - dense_40_accuracy: 0.9869 - dense_40_loss: 0.0214 - dense_41_accuracy: 0.9595 - dense_41_loss: 0.0622 - loss: 0.0836 - val_dense_40_accuracy: 0.9429 - val_dense_40_loss: 0.0931 - val_dense_41_accuracy: 0.7701 - val_dense_41_loss: 0.5182 - val_loss: 0.6028 - learning_rate: 0.0010\nEpoch 43/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - dense_40_accuracy: 0.9934 - dense_40_loss: 0.0096 - dense_41_accuracy: 0.9635 - dense_41_loss: 0.0506 - loss: 0.0602\nEpoch 43: val_loss did not improve from 0.22459\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 188ms/step - dense_40_accuracy: 0.9934 - dense_40_loss: 0.0096 - dense_41_accuracy: 0.9635 - dense_41_loss: 0.0506 - loss: 0.0602 - val_dense_40_accuracy: 0.9398 - val_dense_40_loss: 0.1306 - val_dense_41_accuracy: 0.8750 - val_dense_41_loss: 0.1874 - val_loss: 0.3118 - learning_rate: 0.0010\nEpoch 44/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - dense_40_accuracy: 0.9878 - dense_40_loss: 0.0180 - dense_41_accuracy: 0.9676 - dense_41_loss: 0.0479 - loss: 0.0659\nEpoch 44: val_loss did not improve from 0.22459\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 189ms/step - dense_40_accuracy: 0.9878 - dense_40_loss: 0.0179 - dense_41_accuracy: 0.9676 - dense_41_loss: 0.0479 - loss: 0.0659 - val_dense_40_accuracy: 0.9136 - val_dense_40_loss: 0.1732 - val_dense_41_accuracy: 0.9105 - val_dense_41_loss: 0.1359 - val_loss: 0.2985 - learning_rate: 0.0010\nEpoch 45/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - dense_40_accuracy: 0.9948 - dense_40_loss: 0.0078 - dense_41_accuracy: 0.9718 - dense_41_loss: 0.0455 - loss: 0.0533\nEpoch 45: val_loss did not improve from 0.22459\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 188ms/step - dense_40_accuracy: 0.9948 - dense_40_loss: 0.0078 - dense_41_accuracy: 0.9718 - dense_41_loss: 0.0455 - loss: 0.0534 - val_dense_40_accuracy: 0.9336 - val_dense_40_loss: 0.1315 - val_dense_41_accuracy: 0.8673 - val_dense_41_loss: 0.2468 - val_loss: 0.3742 - learning_rate: 0.0010\nEpoch 46/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - dense_40_accuracy: 0.9930 - dense_40_loss: 0.0112 - dense_41_accuracy: 0.9652 - dense_41_loss: 0.0500 - loss: 0.0612\nEpoch 46: val_loss did not improve from 0.22459\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 188ms/step - dense_40_accuracy: 0.9930 - dense_40_loss: 0.0112 - dense_41_accuracy: 0.9651 - dense_41_loss: 0.0500 - loss: 0.0612 - val_dense_40_accuracy: 0.9321 - val_dense_40_loss: 0.1309 - val_dense_41_accuracy: 0.8827 - val_dense_41_loss: 0.2463 - val_loss: 0.3602 - learning_rate: 0.0010\nEpoch 47/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - dense_40_accuracy: 0.9948 - dense_40_loss: 0.0088 - dense_41_accuracy: 0.9666 - dense_41_loss: 0.0490 - loss: 0.0578\nEpoch 47: val_loss did not improve from 0.22459\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 188ms/step - dense_40_accuracy: 0.9948 - dense_40_loss: 0.0088 - dense_41_accuracy: 0.9666 - dense_41_loss: 0.0490 - loss: 0.0578 - val_dense_40_accuracy: 0.9105 - val_dense_40_loss: 0.2061 - val_dense_41_accuracy: 0.9352 - val_dense_41_loss: 0.1037 - val_loss: 0.3107 - learning_rate: 0.0010\nEpoch 48/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - dense_40_accuracy: 0.9908 - dense_40_loss: 0.0170 - dense_41_accuracy: 0.9707 - dense_41_loss: 0.0399 - loss: 0.0569\nEpoch 48: val_loss did not improve from 0.22459\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 188ms/step - dense_40_accuracy: 0.9908 - dense_40_loss: 0.0170 - dense_41_accuracy: 0.9707 - dense_41_loss: 0.0399 - loss: 0.0569 - val_dense_40_accuracy: 0.9414 - val_dense_40_loss: 0.1369 - val_dense_41_accuracy: 0.9167 - val_dense_41_loss: 0.1587 - val_loss: 0.2901 - learning_rate: 0.0010\nEpoch 49/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - dense_40_accuracy: 0.9930 - dense_40_loss: 0.0106 - dense_41_accuracy: 0.9723 - dense_41_loss: 0.0385 - loss: 0.0491\nEpoch 49: val_loss did not improve from 0.22459\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 188ms/step - dense_40_accuracy: 0.9930 - dense_40_loss: 0.0106 - dense_41_accuracy: 0.9722 - dense_41_loss: 0.0385 - loss: 0.0491 - val_dense_40_accuracy: 0.9321 - val_dense_40_loss: 0.1710 - val_dense_41_accuracy: 0.9228 - val_dense_41_loss: 0.1211 - val_loss: 0.3023 - learning_rate: 0.0010\nEpoch 50/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - dense_40_accuracy: 0.9898 - dense_40_loss: 0.0168 - dense_41_accuracy: 0.9736 - dense_41_loss: 0.0366 - loss: 0.0535\nEpoch 50: val_loss did not improve from 0.22459\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 189ms/step - dense_40_accuracy: 0.9899 - dense_40_loss: 0.0168 - dense_41_accuracy: 0.9736 - dense_41_loss: 0.0366 - loss: 0.0534 - val_dense_40_accuracy: 0.9460 - val_dense_40_loss: 0.1079 - val_dense_41_accuracy: 0.8673 - val_dense_41_loss: 0.2552 - val_loss: 0.3419 - learning_rate: 0.0010\nEpoch 51/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - dense_40_accuracy: 0.9923 - dense_40_loss: 0.0115 - dense_41_accuracy: 0.9717 - dense_41_loss: 0.0406 - loss: 0.0521\nEpoch 51: val_loss improved from 0.22459 to 0.18852, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 199ms/step - dense_40_accuracy: 0.9923 - dense_40_loss: 0.0116 - dense_41_accuracy: 0.9717 - dense_41_loss: 0.0406 - loss: 0.0521 - val_dense_40_accuracy: 0.9583 - val_dense_40_loss: 0.1503 - val_dense_41_accuracy: 0.9552 - val_dense_41_loss: 0.0570 - val_loss: 0.1885 - learning_rate: 0.0010\nEpoch 52/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - dense_40_accuracy: 0.9922 - dense_40_loss: 0.0112 - dense_41_accuracy: 0.9763 - dense_41_loss: 0.0384 - loss: 0.0497\nEpoch 52: val_loss did not improve from 0.18852\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 188ms/step - dense_40_accuracy: 0.9922 - dense_40_loss: 0.0112 - dense_41_accuracy: 0.9762 - dense_41_loss: 0.0385 - loss: 0.0497 - val_dense_40_accuracy: 0.9321 - val_dense_40_loss: 0.1572 - val_dense_41_accuracy: 0.9336 - val_dense_41_loss: 0.1005 - val_loss: 0.2460 - learning_rate: 0.0010\nEpoch 53/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - dense_40_accuracy: 0.9916 - dense_40_loss: 0.0116 - dense_41_accuracy: 0.9734 - dense_41_loss: 0.0392 - loss: 0.0508\nEpoch 53: val_loss did not improve from 0.18852\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 189ms/step - dense_40_accuracy: 0.9916 - dense_40_loss: 0.0116 - dense_41_accuracy: 0.9734 - dense_41_loss: 0.0392 - loss: 0.0508 - val_dense_40_accuracy: 0.9537 - val_dense_40_loss: 0.0869 - val_dense_41_accuracy: 0.9167 - val_dense_41_loss: 0.1353 - val_loss: 0.2137 - learning_rate: 0.0010\nEpoch 54/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - dense_40_accuracy: 0.9951 - dense_40_loss: 0.0080 - dense_41_accuracy: 0.9749 - dense_41_loss: 0.0338 - loss: 0.0418\nEpoch 54: val_loss did not improve from 0.18852\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 189ms/step - dense_40_accuracy: 0.9951 - dense_40_loss: 0.0081 - dense_41_accuracy: 0.9749 - dense_41_loss: 0.0338 - loss: 0.0419 - val_dense_40_accuracy: 0.9306 - val_dense_40_loss: 0.1226 - val_dense_41_accuracy: 0.9367 - val_dense_41_loss: 0.1039 - val_loss: 0.2175 - learning_rate: 0.0010\nEpoch 55/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - dense_40_accuracy: 0.9947 - dense_40_loss: 0.0088 - dense_41_accuracy: 0.9691 - dense_41_loss: 0.0425 - loss: 0.0512\nEpoch 55: val_loss did not improve from 0.18852\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 189ms/step - dense_40_accuracy: 0.9947 - dense_40_loss: 0.0088 - dense_41_accuracy: 0.9691 - dense_41_loss: 0.0425 - loss: 0.0512 - val_dense_40_accuracy: 0.9228 - val_dense_40_loss: 0.1449 - val_dense_41_accuracy: 0.8904 - val_dense_41_loss: 0.1993 - val_loss: 0.3360 - learning_rate: 0.0010\nEpoch 56/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - dense_40_accuracy: 0.9934 - dense_40_loss: 0.0084 - dense_41_accuracy: 0.9778 - dense_41_loss: 0.0317 - loss: 0.0401\nEpoch 56: val_loss did not improve from 0.18852\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 187ms/step - dense_40_accuracy: 0.9934 - dense_40_loss: 0.0084 - dense_41_accuracy: 0.9778 - dense_41_loss: 0.0317 - loss: 0.0401 - val_dense_40_accuracy: 0.9429 - val_dense_40_loss: 0.1090 - val_dense_41_accuracy: 0.9228 - val_dense_41_loss: 0.1051 - val_loss: 0.2142 - learning_rate: 0.0010\nEpoch 57/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - dense_40_accuracy: 0.9942 - dense_40_loss: 0.0082 - dense_41_accuracy: 0.9788 - dense_41_loss: 0.0278 - loss: 0.0360\nEpoch 57: val_loss improved from 0.18852 to 0.18484, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 198ms/step - dense_40_accuracy: 0.9942 - dense_40_loss: 0.0082 - dense_41_accuracy: 0.9788 - dense_41_loss: 0.0279 - loss: 0.0360 - val_dense_40_accuracy: 0.9367 - val_dense_40_loss: 0.1246 - val_dense_41_accuracy: 0.9630 - val_dense_41_loss: 0.0604 - val_loss: 0.1848 - learning_rate: 0.0010\nEpoch 58/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - dense_40_accuracy: 0.9972 - dense_40_loss: 0.0051 - dense_41_accuracy: 0.9785 - dense_41_loss: 0.0315 - loss: 0.0366\nEpoch 58: val_loss did not improve from 0.18484\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 187ms/step - dense_40_accuracy: 0.9972 - dense_40_loss: 0.0051 - dense_41_accuracy: 0.9785 - dense_41_loss: 0.0315 - loss: 0.0366 - val_dense_40_accuracy: 0.9352 - val_dense_40_loss: 0.1330 - val_dense_41_accuracy: 0.9043 - val_dense_41_loss: 0.2391 - val_loss: 0.3557 - learning_rate: 0.0010\nEpoch 59/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - dense_40_accuracy: 0.9942 - dense_40_loss: 0.0098 - dense_41_accuracy: 0.9772 - dense_41_loss: 0.0352 - loss: 0.0449\nEpoch 59: val_loss did not improve from 0.18484\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 188ms/step - dense_40_accuracy: 0.9942 - dense_40_loss: 0.0098 - dense_41_accuracy: 0.9771 - dense_41_loss: 0.0352 - loss: 0.0450 - val_dense_40_accuracy: 0.9228 - val_dense_40_loss: 0.1630 - val_dense_41_accuracy: 0.9414 - val_dense_41_loss: 0.0864 - val_loss: 0.2493 - learning_rate: 0.0010\nEpoch 60/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - dense_40_accuracy: 0.9930 - dense_40_loss: 0.0101 - dense_41_accuracy: 0.9829 - dense_41_loss: 0.0246 - loss: 0.0347\nEpoch 60: val_loss did not improve from 0.18484\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 187ms/step - dense_40_accuracy: 0.9930 - dense_40_loss: 0.0101 - dense_41_accuracy: 0.9829 - dense_41_loss: 0.0246 - loss: 0.0347 - val_dense_40_accuracy: 0.9167 - val_dense_40_loss: 0.1875 - val_dense_41_accuracy: 0.8750 - val_dense_41_loss: 0.2459 - val_loss: 0.4114 - learning_rate: 0.0010\nEpoch 61/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - dense_40_accuracy: 0.9815 - dense_40_loss: 0.0263 - dense_41_accuracy: 0.9769 - dense_41_loss: 0.0344 - loss: 0.0607\nEpoch 61: val_loss did not improve from 0.18484\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 187ms/step - dense_40_accuracy: 0.9816 - dense_40_loss: 0.0263 - dense_41_accuracy: 0.9769 - dense_41_loss: 0.0344 - loss: 0.0606 - val_dense_40_accuracy: 0.9429 - val_dense_40_loss: 0.0948 - val_dense_41_accuracy: 0.9275 - val_dense_41_loss: 0.1216 - val_loss: 0.2183 - learning_rate: 0.0010\nEpoch 62/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - dense_40_accuracy: 0.9958 - dense_40_loss: 0.0058 - dense_41_accuracy: 0.9773 - dense_41_loss: 0.0346 - loss: 0.0404\nEpoch 62: val_loss improved from 0.18484 to 0.15538, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 198ms/step - dense_40_accuracy: 0.9957 - dense_40_loss: 0.0058 - dense_41_accuracy: 0.9773 - dense_41_loss: 0.0346 - loss: 0.0404 - val_dense_40_accuracy: 0.9491 - val_dense_40_loss: 0.1014 - val_dense_41_accuracy: 0.9614 - val_dense_41_loss: 0.0531 - val_loss: 0.1554 - learning_rate: 0.0010\nEpoch 63/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - dense_40_accuracy: 0.9947 - dense_40_loss: 0.0081 - dense_41_accuracy: 0.9844 - dense_41_loss: 0.0230 - loss: 0.0311\nEpoch 63: val_loss did not improve from 0.15538\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 187ms/step - dense_40_accuracy: 0.9947 - dense_40_loss: 0.0081 - dense_41_accuracy: 0.9844 - dense_41_loss: 0.0230 - loss: 0.0311 - val_dense_40_accuracy: 0.9383 - val_dense_40_loss: 0.1225 - val_dense_41_accuracy: 0.9552 - val_dense_41_loss: 0.0591 - val_loss: 0.1865 - learning_rate: 0.0010\nEpoch 64/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - dense_40_accuracy: 0.9914 - dense_40_loss: 0.0099 - dense_41_accuracy: 0.9825 - dense_41_loss: 0.0251 - loss: 0.0350\nEpoch 64: val_loss did not improve from 0.15538\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 187ms/step - dense_40_accuracy: 0.9914 - dense_40_loss: 0.0099 - dense_41_accuracy: 0.9825 - dense_41_loss: 0.0251 - loss: 0.0350 - val_dense_40_accuracy: 0.9537 - val_dense_40_loss: 0.0927 - val_dense_41_accuracy: 0.9167 - val_dense_41_loss: 0.1261 - val_loss: 0.2176 - learning_rate: 0.0010\nEpoch 65/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - dense_40_accuracy: 0.9971 - dense_40_loss: 0.0059 - dense_41_accuracy: 0.9806 - dense_41_loss: 0.0267 - loss: 0.0326\nEpoch 65: val_loss did not improve from 0.15538\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 188ms/step - dense_40_accuracy: 0.9971 - dense_40_loss: 0.0059 - dense_41_accuracy: 0.9806 - dense_41_loss: 0.0267 - loss: 0.0326 - val_dense_40_accuracy: 0.9290 - val_dense_40_loss: 0.1435 - val_dense_41_accuracy: 0.9429 - val_dense_41_loss: 0.0953 - val_loss: 0.2417 - learning_rate: 0.0010\nEpoch 66/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - dense_40_accuracy: 0.9926 - dense_40_loss: 0.0096 - dense_41_accuracy: 0.9774 - dense_41_loss: 0.0319 - loss: 0.0414\nEpoch 66: val_loss did not improve from 0.15538\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 187ms/step - dense_40_accuracy: 0.9926 - dense_40_loss: 0.0095 - dense_41_accuracy: 0.9774 - dense_41_loss: 0.0319 - loss: 0.0414 - val_dense_40_accuracy: 0.9522 - val_dense_40_loss: 0.0852 - val_dense_41_accuracy: 0.7901 - val_dense_41_loss: 0.4937 - val_loss: 0.5597 - learning_rate: 0.0010\nEpoch 67/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - dense_40_accuracy: 0.9987 - dense_40_loss: 0.0022 - dense_41_accuracy: 0.9835 - dense_41_loss: 0.0223 - loss: 0.0245\nEpoch 67: val_loss did not improve from 0.15538\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 188ms/step - dense_40_accuracy: 0.9987 - dense_40_loss: 0.0022 - dense_41_accuracy: 0.9835 - dense_41_loss: 0.0224 - loss: 0.0245 - val_dense_40_accuracy: 0.9552 - val_dense_40_loss: 0.0977 - val_dense_41_accuracy: 0.9321 - val_dense_41_loss: 0.1580 - val_loss: 0.2347 - learning_rate: 0.0010\nEpoch 68/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - dense_40_accuracy: 0.9972 - dense_40_loss: 0.0043 - dense_41_accuracy: 0.9811 - dense_41_loss: 0.0261 - loss: 0.0303\nEpoch 68: val_loss did not improve from 0.15538\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 187ms/step - dense_40_accuracy: 0.9971 - dense_40_loss: 0.0043 - dense_41_accuracy: 0.9811 - dense_41_loss: 0.0261 - loss: 0.0304 - val_dense_40_accuracy: 0.9336 - val_dense_40_loss: 0.1138 - val_dense_41_accuracy: 0.8889 - val_dense_41_loss: 0.2210 - val_loss: 0.3439 - learning_rate: 0.0010\nEpoch 69/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - dense_40_accuracy: 0.9957 - dense_40_loss: 0.0067 - dense_41_accuracy: 0.9828 - dense_41_loss: 0.0250 - loss: 0.0317\nEpoch 69: val_loss did not improve from 0.15538\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 187ms/step - dense_40_accuracy: 0.9957 - dense_40_loss: 0.0067 - dense_41_accuracy: 0.9828 - dense_41_loss: 0.0250 - loss: 0.0317 - val_dense_40_accuracy: 0.9105 - val_dense_40_loss: 0.1886 - val_dense_41_accuracy: 0.9429 - val_dense_41_loss: 0.0911 - val_loss: 0.2879 - learning_rate: 0.0010\nEpoch 70/200\n\u001b[1m134/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 184ms/step - dense_40_accuracy: 0.9930 - dense_40_loss: 0.0084 - dense_41_accuracy: 0.9859 - dense_41_loss: 0.0236 - loss: 0.0320","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\n\ninitial_gamma = 0.5\nlearning_rate = 1e-2\noptimizer = Adam(learning_rate=0.001)\n#opt = Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.9, epsilon=None, amsgrad=False)\n# Compile the model with the custom optimizer\nmodel.compile(optimizer=optimizer,\n              loss=['categorical_crossentropy', 'categorical_crossentropy'],\n              loss_weights=[initial_gamma, (1 -  initial_gamma)],\n              metrics=['accuracy', 'accuracy'])\n\n\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\ndef checkpoint_callback():\n\n    checkpoint_filepath = 'best1_model_cer_skin_lung.keras'\n\n    model_checkpoint_callback= ModelCheckpoint(filepath=checkpoint_filepath,\n                           save_weights_only=False,\n                           #frequency='epoch',\n                           monitor='val_loss',\n                           save_best_only=True,\n                            mode='min',\n                           verbose=1)\n\n    return model_checkpoint_callback\n\ndef early_stopping(patience):\n    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', restore_best_weights=True, patience=60, verbose=1)\n    return es_callback\n\n\n\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n                              patience=50, verbose = 1, min_lr=0.00001)\n\ncheckpoint_callback = checkpoint_callback()\n\nearly_stopping = early_stopping(patience=100)\ncallbacks = [checkpoint_callback, early_stopping, reduce_lr]\n            \n\n# Fit the model with callbacks\nhistory = model.fit([X_train_s, X_train_h], [y_train_s, y_train_h],\n                    epochs=200,\n                    validation_data=([X_val_s, X_val_h1], [y_val_s, y_val_h1]), verbose=1,\n                    shuffle=True,\n                    callbacks=callbacks) # UpdateGammaCallback","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T02:41:30.671565Z","iopub.execute_input":"2025-03-05T02:41:30.671864Z","execution_failed":"2025-03-05T07:56:47.230Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578ms/step - dense_65_accuracy: 0.5779 - dense_65_loss: 0.5675 - dense_66_accuracy: 0.6165 - dense_66_loss: 0.6055 - loss: 1.1731\nEpoch 1: val_loss improved from inf to 3.91863, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m387s\u001b[0m 737ms/step - dense_65_accuracy: 0.5783 - dense_65_loss: 0.5670 - dense_66_accuracy: 0.6166 - dense_66_loss: 0.6052 - loss: 1.1722 - val_dense_65_accuracy: 0.4444 - val_dense_65_loss: 2.4734 - val_dense_66_accuracy: 0.0787 - val_dense_66_loss: 1.4872 - val_loss: 3.9186 - learning_rate: 0.0010\nEpoch 2/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - dense_65_accuracy: 0.7514 - dense_65_loss: 0.3339 - dense_66_accuracy: 0.6893 - dense_66_loss: 0.4382 - loss: 0.7720\nEpoch 2: val_loss improved from 3.91863 to 2.29518, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 178ms/step - dense_65_accuracy: 0.7515 - dense_65_loss: 0.3338 - dense_66_accuracy: 0.6893 - dense_66_loss: 0.4382 - loss: 0.7720 - val_dense_65_accuracy: 0.4985 - val_dense_65_loss: 1.0673 - val_dense_66_accuracy: 0.3025 - val_dense_66_loss: 1.2376 - val_loss: 2.2952 - learning_rate: 0.0010\nEpoch 3/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - dense_65_accuracy: 0.7951 - dense_65_loss: 0.2702 - dense_66_accuracy: 0.7040 - dense_66_loss: 0.4112 - loss: 0.6814\nEpoch 3: val_loss improved from 2.29518 to 2.24065, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 179ms/step - dense_65_accuracy: 0.7952 - dense_65_loss: 0.2701 - dense_66_accuracy: 0.7040 - dense_66_loss: 0.4112 - loss: 0.6813 - val_dense_65_accuracy: 0.6096 - val_dense_65_loss: 0.5842 - val_dense_66_accuracy: 0.0988 - val_dense_66_loss: 1.6657 - val_loss: 2.2406 - learning_rate: 0.0010\nEpoch 4/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - dense_65_accuracy: 0.8299 - dense_65_loss: 0.2327 - dense_66_accuracy: 0.7060 - dense_66_loss: 0.3967 - loss: 0.6293\nEpoch 4: val_loss did not improve from 2.24065\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 170ms/step - dense_65_accuracy: 0.8300 - dense_65_loss: 0.2326 - dense_66_accuracy: 0.7061 - dense_66_loss: 0.3967 - loss: 0.6293 - val_dense_65_accuracy: 0.7500 - val_dense_65_loss: 0.3426 - val_dense_66_accuracy: 0.1559 - val_dense_66_loss: 2.0374 - val_loss: 2.3888 - learning_rate: 0.0010\nEpoch 5/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - dense_65_accuracy: 0.8650 - dense_65_loss: 0.1980 - dense_66_accuracy: 0.7204 - dense_66_loss: 0.3812 - loss: 0.5792\nEpoch 5: val_loss improved from 2.24065 to 0.81132, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 180ms/step - dense_65_accuracy: 0.8650 - dense_65_loss: 0.1979 - dense_66_accuracy: 0.7204 - dense_66_loss: 0.3812 - loss: 0.5791 - val_dense_65_accuracy: 0.8102 - val_dense_65_loss: 0.2786 - val_dense_66_accuracy: 0.6343 - val_dense_66_loss: 0.5291 - val_loss: 0.8113 - learning_rate: 0.0010\nEpoch 6/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - dense_65_accuracy: 0.8796 - dense_65_loss: 0.1700 - dense_66_accuracy: 0.7279 - dense_66_loss: 0.3682 - loss: 0.5381\nEpoch 6: val_loss did not improve from 0.81132\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 171ms/step - dense_65_accuracy: 0.8796 - dense_65_loss: 0.1700 - dense_66_accuracy: 0.7279 - dense_66_loss: 0.3682 - loss: 0.5381 - val_dense_65_accuracy: 0.7778 - val_dense_65_loss: 0.3405 - val_dense_66_accuracy: 0.6713 - val_dense_66_loss: 0.6810 - val_loss: 1.0284 - learning_rate: 0.0010\nEpoch 7/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - dense_65_accuracy: 0.8943 - dense_65_loss: 0.1482 - dense_66_accuracy: 0.7419 - dense_66_loss: 0.3468 - loss: 0.4950\nEpoch 7: val_loss did not improve from 0.81132\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 170ms/step - dense_65_accuracy: 0.8943 - dense_65_loss: 0.1482 - dense_66_accuracy: 0.7419 - dense_66_loss: 0.3468 - loss: 0.4950 - val_dense_65_accuracy: 0.6327 - val_dense_65_loss: 0.8085 - val_dense_66_accuracy: 0.6420 - val_dense_66_loss: 0.5267 - val_loss: 1.3278 - learning_rate: 0.0010\nEpoch 8/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - dense_65_accuracy: 0.9194 - dense_65_loss: 0.1223 - dense_66_accuracy: 0.7433 - dense_66_loss: 0.3520 - loss: 0.4743\nEpoch 8: val_loss did not improve from 0.81132\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 170ms/step - dense_65_accuracy: 0.9193 - dense_65_loss: 0.1223 - dense_66_accuracy: 0.7433 - dense_66_loss: 0.3520 - loss: 0.4743 - val_dense_65_accuracy: 0.4954 - val_dense_65_loss: 2.0497 - val_dense_66_accuracy: 0.4198 - val_dense_66_loss: 1.0630 - val_loss: 3.0753 - learning_rate: 0.0010\nEpoch 9/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - dense_65_accuracy: 0.9177 - dense_65_loss: 0.1166 - dense_66_accuracy: 0.7570 - dense_66_loss: 0.3289 - loss: 0.4455\nEpoch 9: val_loss improved from 0.81132 to 0.72413, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 179ms/step - dense_65_accuracy: 0.9177 - dense_65_loss: 0.1166 - dense_66_accuracy: 0.7570 - dense_66_loss: 0.3290 - loss: 0.4456 - val_dense_65_accuracy: 0.8380 - val_dense_65_loss: 0.2266 - val_dense_66_accuracy: 0.6620 - val_dense_66_loss: 0.4985 - val_loss: 0.7241 - learning_rate: 0.0010\nEpoch 10/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - dense_65_accuracy: 0.9261 - dense_65_loss: 0.1074 - dense_66_accuracy: 0.7531 - dense_66_loss: 0.3259 - loss: 0.4334\nEpoch 10: val_loss did not improve from 0.72413\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 170ms/step - dense_65_accuracy: 0.9261 - dense_65_loss: 0.1074 - dense_66_accuracy: 0.7531 - dense_66_loss: 0.3260 - loss: 0.4334 - val_dense_65_accuracy: 0.7562 - val_dense_65_loss: 0.4662 - val_dense_66_accuracy: 0.6265 - val_dense_66_loss: 0.5783 - val_loss: 1.0451 - learning_rate: 0.0010\nEpoch 11/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - dense_65_accuracy: 0.9313 - dense_65_loss: 0.0972 - dense_66_accuracy: 0.7552 - dense_66_loss: 0.3284 - loss: 0.4256\nEpoch 11: val_loss did not improve from 0.72413\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 170ms/step - dense_65_accuracy: 0.9313 - dense_65_loss: 0.0972 - dense_66_accuracy: 0.7552 - dense_66_loss: 0.3284 - loss: 0.4256 - val_dense_65_accuracy: 0.6898 - val_dense_65_loss: 0.7242 - val_dense_66_accuracy: 0.3102 - val_dense_66_loss: 0.8295 - val_loss: 1.5282 - learning_rate: 0.0010\nEpoch 12/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - dense_65_accuracy: 0.9438 - dense_65_loss: 0.0812 - dense_66_accuracy: 0.7645 - dense_66_loss: 0.3168 - loss: 0.3980\nEpoch 12: val_loss did not improve from 0.72413\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 170ms/step - dense_65_accuracy: 0.9438 - dense_65_loss: 0.0812 - dense_66_accuracy: 0.7645 - dense_66_loss: 0.3168 - loss: 0.3980 - val_dense_65_accuracy: 0.7377 - val_dense_65_loss: 0.5019 - val_dense_66_accuracy: 0.6620 - val_dense_66_loss: 0.4987 - val_loss: 1.0033 - learning_rate: 0.0010\nEpoch 13/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - dense_65_accuracy: 0.9414 - dense_65_loss: 0.0830 - dense_66_accuracy: 0.7775 - dense_66_loss: 0.3089 - loss: 0.3919\nEpoch 13: val_loss did not improve from 0.72413\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 169ms/step - dense_65_accuracy: 0.9414 - dense_65_loss: 0.0830 - dense_66_accuracy: 0.7775 - dense_66_loss: 0.3089 - loss: 0.3919 - val_dense_65_accuracy: 0.7639 - val_dense_65_loss: 0.3885 - val_dense_66_accuracy: 0.6481 - val_dense_66_loss: 0.6006 - val_loss: 0.9839 - learning_rate: 0.0010\nEpoch 14/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - dense_65_accuracy: 0.9453 - dense_65_loss: 0.0750 - dense_66_accuracy: 0.7659 - dense_66_loss: 0.3098 - loss: 0.3848\nEpoch 14: val_loss improved from 0.72413 to 0.66724, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 179ms/step - dense_65_accuracy: 0.9453 - dense_65_loss: 0.0750 - dense_66_accuracy: 0.7659 - dense_66_loss: 0.3098 - loss: 0.3848 - val_dense_65_accuracy: 0.9012 - val_dense_65_loss: 0.1535 - val_dense_66_accuracy: 0.5910 - val_dense_66_loss: 0.5108 - val_loss: 0.6672 - learning_rate: 0.0010\nEpoch 15/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - dense_65_accuracy: 0.9408 - dense_65_loss: 0.0830 - dense_66_accuracy: 0.7904 - dense_66_loss: 0.2848 - loss: 0.3678\nEpoch 15: val_loss improved from 0.66724 to 0.58971, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 179ms/step - dense_65_accuracy: 0.9408 - dense_65_loss: 0.0829 - dense_66_accuracy: 0.7904 - dense_66_loss: 0.2849 - loss: 0.3678 - val_dense_65_accuracy: 0.8966 - val_dense_65_loss: 0.1385 - val_dense_66_accuracy: 0.7052 - val_dense_66_loss: 0.4464 - val_loss: 0.5897 - learning_rate: 0.0010\nEpoch 16/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - dense_65_accuracy: 0.9568 - dense_65_loss: 0.0620 - dense_66_accuracy: 0.7847 - dense_66_loss: 0.2852 - loss: 0.3471\nEpoch 16: val_loss did not improve from 0.58971\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 170ms/step - dense_65_accuracy: 0.9567 - dense_65_loss: 0.0620 - dense_66_accuracy: 0.7847 - dense_66_loss: 0.2852 - loss: 0.3471 - val_dense_65_accuracy: 0.7284 - val_dense_65_loss: 0.5562 - val_dense_66_accuracy: 0.6929 - val_dense_66_loss: 0.4406 - val_loss: 0.9871 - learning_rate: 0.0010\nEpoch 17/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - dense_65_accuracy: 0.9609 - dense_65_loss: 0.0557 - dense_66_accuracy: 0.7851 - dense_66_loss: 0.2865 - loss: 0.3422\nEpoch 17: val_loss did not improve from 0.58971\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 169ms/step - dense_65_accuracy: 0.9608 - dense_65_loss: 0.0557 - dense_66_accuracy: 0.7851 - dense_66_loss: 0.2865 - loss: 0.3422 - val_dense_65_accuracy: 0.8781 - val_dense_65_loss: 0.2041 - val_dense_66_accuracy: 0.7269 - val_dense_66_loss: 0.3933 - val_loss: 0.6078 - learning_rate: 0.0010\nEpoch 18/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - dense_65_accuracy: 0.9571 - dense_65_loss: 0.0609 - dense_66_accuracy: 0.7899 - dense_66_loss: 0.2771 - loss: 0.3379\nEpoch 18: val_loss did not improve from 0.58971\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 170ms/step - dense_65_accuracy: 0.9571 - dense_65_loss: 0.0609 - dense_66_accuracy: 0.7900 - dense_66_loss: 0.2771 - loss: 0.3379 - val_dense_65_accuracy: 0.7052 - val_dense_65_loss: 1.0524 - val_dense_66_accuracy: 0.6852 - val_dense_66_loss: 0.5975 - val_loss: 1.6138 - learning_rate: 0.0010\nEpoch 19/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - dense_65_accuracy: 0.9678 - dense_65_loss: 0.0464 - dense_66_accuracy: 0.8050 - dense_66_loss: 0.2667 - loss: 0.3130\nEpoch 19: val_loss did not improve from 0.58971\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 170ms/step - dense_65_accuracy: 0.9678 - dense_65_loss: 0.0464 - dense_66_accuracy: 0.8049 - dense_66_loss: 0.2667 - loss: 0.3131 - val_dense_65_accuracy: 0.8997 - val_dense_65_loss: 0.1698 - val_dense_66_accuracy: 0.6528 - val_dense_66_loss: 0.4788 - val_loss: 0.6527 - learning_rate: 0.0010\nEpoch 20/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - dense_65_accuracy: 0.9694 - dense_65_loss: 0.0461 - dense_66_accuracy: 0.7993 - dense_66_loss: 0.2608 - loss: 0.3069\nEpoch 20: val_loss improved from 0.58971 to 0.45244, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 179ms/step - dense_65_accuracy: 0.9694 - dense_65_loss: 0.0461 - dense_66_accuracy: 0.7993 - dense_66_loss: 0.2608 - loss: 0.3069 - val_dense_65_accuracy: 0.9198 - val_dense_65_loss: 0.1276 - val_dense_66_accuracy: 0.7469 - val_dense_66_loss: 0.3193 - val_loss: 0.4524 - learning_rate: 0.0010\nEpoch 21/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - dense_65_accuracy: 0.9653 - dense_65_loss: 0.0524 - dense_66_accuracy: 0.8080 - dense_66_loss: 0.2468 - loss: 0.2992\nEpoch 21: val_loss did not improve from 0.45244\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 170ms/step - dense_65_accuracy: 0.9653 - dense_65_loss: 0.0524 - dense_66_accuracy: 0.8080 - dense_66_loss: 0.2469 - loss: 0.2992 - val_dense_65_accuracy: 0.9167 - val_dense_65_loss: 0.1548 - val_dense_66_accuracy: 0.6096 - val_dense_66_loss: 0.6325 - val_loss: 0.7866 - learning_rate: 0.0010\nEpoch 22/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - dense_65_accuracy: 0.9698 - dense_65_loss: 0.0380 - dense_66_accuracy: 0.8212 - dense_66_loss: 0.2391 - loss: 0.2771\nEpoch 22: val_loss did not improve from 0.45244\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 170ms/step - dense_65_accuracy: 0.9698 - dense_65_loss: 0.0380 - dense_66_accuracy: 0.8211 - dense_66_loss: 0.2391 - loss: 0.2771 - val_dense_65_accuracy: 0.7407 - val_dense_65_loss: 0.7854 - val_dense_66_accuracy: 0.6852 - val_dense_66_loss: 0.5316 - val_loss: 1.2773 - learning_rate: 0.0010\nEpoch 23/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - dense_65_accuracy: 0.9754 - dense_65_loss: 0.0349 - dense_66_accuracy: 0.8219 - dense_66_loss: 0.2328 - loss: 0.2677\nEpoch 23: val_loss did not improve from 0.45244\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 169ms/step - dense_65_accuracy: 0.9754 - dense_65_loss: 0.0349 - dense_66_accuracy: 0.8219 - dense_66_loss: 0.2328 - loss: 0.2677 - val_dense_65_accuracy: 0.9136 - val_dense_65_loss: 0.1304 - val_dense_66_accuracy: 0.7068 - val_dense_66_loss: 0.4167 - val_loss: 0.5563 - learning_rate: 0.0010\nEpoch 24/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - dense_65_accuracy: 0.9716 - dense_65_loss: 0.0451 - dense_66_accuracy: 0.8324 - dense_66_loss: 0.2261 - loss: 0.2712\nEpoch 24: val_loss did not improve from 0.45244\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 169ms/step - dense_65_accuracy: 0.9716 - dense_65_loss: 0.0451 - dense_66_accuracy: 0.8324 - dense_66_loss: 0.2262 - loss: 0.2712 - val_dense_65_accuracy: 0.8920 - val_dense_65_loss: 0.2245 - val_dense_66_accuracy: 0.7515 - val_dense_66_loss: 0.3381 - val_loss: 0.5770 - learning_rate: 0.0010\nEpoch 25/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - dense_65_accuracy: 0.9801 - dense_65_loss: 0.0292 - dense_66_accuracy: 0.8310 - dense_66_loss: 0.2239 - loss: 0.2531\nEpoch 25: val_loss did not improve from 0.45244\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 171ms/step - dense_65_accuracy: 0.9801 - dense_65_loss: 0.0292 - dense_66_accuracy: 0.8310 - dense_66_loss: 0.2239 - loss: 0.2531 - val_dense_65_accuracy: 0.8657 - val_dense_65_loss: 0.2408 - val_dense_66_accuracy: 0.5895 - val_dense_66_loss: 0.6125 - val_loss: 0.8673 - learning_rate: 0.0010\nEpoch 26/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - dense_65_accuracy: 0.9768 - dense_65_loss: 0.0318 - dense_66_accuracy: 0.8469 - dense_66_loss: 0.1988 - loss: 0.2306\nEpoch 26: val_loss did not improve from 0.45244\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 169ms/step - dense_65_accuracy: 0.9768 - dense_65_loss: 0.0318 - dense_66_accuracy: 0.8468 - dense_66_loss: 0.1989 - loss: 0.2306 - val_dense_65_accuracy: 0.7948 - val_dense_65_loss: 0.4023 - val_dense_66_accuracy: 0.5710 - val_dense_66_loss: 0.6244 - val_loss: 1.0474 - learning_rate: 0.0010\nEpoch 27/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - dense_65_accuracy: 0.9718 - dense_65_loss: 0.0472 - dense_66_accuracy: 0.8607 - dense_66_loss: 0.1902 - loss: 0.2374\nEpoch 27: val_loss did not improve from 0.45244\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 170ms/step - dense_65_accuracy: 0.9718 - dense_65_loss: 0.0472 - dense_66_accuracy: 0.8606 - dense_66_loss: 0.1903 - loss: 0.2374 - val_dense_65_accuracy: 0.7284 - val_dense_65_loss: 0.6241 - val_dense_66_accuracy: 0.7485 - val_dense_66_loss: 0.3556 - val_loss: 0.9768 - learning_rate: 0.0010\nEpoch 28/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - dense_65_accuracy: 0.9798 - dense_65_loss: 0.0310 - dense_66_accuracy: 0.8636 - dense_66_loss: 0.1780 - loss: 0.2090\nEpoch 28: val_loss improved from 0.45244 to 0.42008, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 179ms/step - dense_65_accuracy: 0.9798 - dense_65_loss: 0.0310 - dense_66_accuracy: 0.8636 - dense_66_loss: 0.1780 - loss: 0.2090 - val_dense_65_accuracy: 0.9120 - val_dense_65_loss: 0.1803 - val_dense_66_accuracy: 0.8164 - val_dense_66_loss: 0.2295 - val_loss: 0.4201 - learning_rate: 0.0010\nEpoch 29/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - dense_65_accuracy: 0.9829 - dense_65_loss: 0.0265 - dense_66_accuracy: 0.8734 - dense_66_loss: 0.1689 - loss: 0.1954\nEpoch 29: val_loss did not improve from 0.42008\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 170ms/step - dense_65_accuracy: 0.9829 - dense_65_loss: 0.0265 - dense_66_accuracy: 0.8734 - dense_66_loss: 0.1689 - loss: 0.1954 - val_dense_65_accuracy: 0.9306 - val_dense_65_loss: 0.1202 - val_dense_66_accuracy: 0.7052 - val_dense_66_loss: 0.4390 - val_loss: 0.5689 - learning_rate: 0.0010\nEpoch 30/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - dense_65_accuracy: 0.9748 - dense_65_loss: 0.0372 - dense_66_accuracy: 0.8822 - dense_66_loss: 0.1583 - loss: 0.1955\nEpoch 30: val_loss did not improve from 0.42008\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 170ms/step - dense_65_accuracy: 0.9748 - dense_65_loss: 0.0372 - dense_66_accuracy: 0.8822 - dense_66_loss: 0.1584 - loss: 0.1956 - val_dense_65_accuracy: 0.9028 - val_dense_65_loss: 0.1853 - val_dense_66_accuracy: 0.7485 - val_dense_66_loss: 0.3901 - val_loss: 0.5847 - learning_rate: 0.0010\nEpoch 31/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - dense_65_accuracy: 0.9744 - dense_65_loss: 0.0370 - dense_66_accuracy: 0.8878 - dense_66_loss: 0.1469 - loss: 0.1838\nEpoch 31: val_loss did not improve from 0.42008\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 170ms/step - dense_65_accuracy: 0.9744 - dense_65_loss: 0.0369 - dense_66_accuracy: 0.8877 - dense_66_loss: 0.1469 - loss: 0.1838 - val_dense_65_accuracy: 0.9105 - val_dense_65_loss: 0.1607 - val_dense_66_accuracy: 0.7670 - val_dense_66_loss: 0.3878 - val_loss: 0.5528 - learning_rate: 0.0010\nEpoch 32/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - dense_65_accuracy: 0.9870 - dense_65_loss: 0.0190 - dense_66_accuracy: 0.8894 - dense_66_loss: 0.1472 - loss: 0.1662\nEpoch 32: val_loss did not improve from 0.42008\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 170ms/step - dense_65_accuracy: 0.9870 - dense_65_loss: 0.0190 - dense_66_accuracy: 0.8894 - dense_66_loss: 0.1472 - loss: 0.1662 - val_dense_65_accuracy: 0.9090 - val_dense_65_loss: 0.1837 - val_dense_66_accuracy: 0.8056 - val_dense_66_loss: 0.2386 - val_loss: 0.4282 - learning_rate: 0.0010\nEpoch 33/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - dense_65_accuracy: 0.9843 - dense_65_loss: 0.0245 - dense_66_accuracy: 0.8987 - dense_66_loss: 0.1321 - loss: 0.1566\nEpoch 33: val_loss improved from 0.42008 to 0.41306, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 178ms/step - dense_65_accuracy: 0.9843 - dense_65_loss: 0.0245 - dense_66_accuracy: 0.8986 - dense_66_loss: 0.1321 - loss: 0.1566 - val_dense_65_accuracy: 0.9028 - val_dense_65_loss: 0.1661 - val_dense_66_accuracy: 0.8364 - val_dense_66_loss: 0.2366 - val_loss: 0.4131 - learning_rate: 0.0010\nEpoch 34/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - dense_65_accuracy: 0.9840 - dense_65_loss: 0.0215 - dense_66_accuracy: 0.9088 - dense_66_loss: 0.1264 - loss: 0.1479\nEpoch 34: val_loss did not improve from 0.41306\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 169ms/step - dense_65_accuracy: 0.9840 - dense_65_loss: 0.0215 - dense_66_accuracy: 0.9088 - dense_66_loss: 0.1264 - loss: 0.1480 - val_dense_65_accuracy: 0.8858 - val_dense_65_loss: 0.2429 - val_dense_66_accuracy: 0.8210 - val_dense_66_loss: 0.2470 - val_loss: 0.5021 - learning_rate: 0.0010\nEpoch 35/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - dense_65_accuracy: 0.9878 - dense_65_loss: 0.0186 - dense_66_accuracy: 0.9147 - dense_66_loss: 0.1189 - loss: 0.1375\nEpoch 35: val_loss did not improve from 0.41306\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 170ms/step - dense_65_accuracy: 0.9878 - dense_65_loss: 0.0186 - dense_66_accuracy: 0.9146 - dense_66_loss: 0.1189 - loss: 0.1375 - val_dense_65_accuracy: 0.7500 - val_dense_65_loss: 0.5583 - val_dense_66_accuracy: 0.8256 - val_dense_66_loss: 0.2288 - val_loss: 0.7980 - learning_rate: 0.0010\nEpoch 36/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - dense_65_accuracy: 0.9850 - dense_65_loss: 0.0207 - dense_66_accuracy: 0.9279 - dense_66_loss: 0.0989 - loss: 0.1196\nEpoch 36: val_loss did not improve from 0.41306\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 171ms/step - dense_65_accuracy: 0.9850 - dense_65_loss: 0.0207 - dense_66_accuracy: 0.9278 - dense_66_loss: 0.0990 - loss: 0.1196 - val_dense_65_accuracy: 0.8673 - val_dense_65_loss: 0.2929 - val_dense_66_accuracy: 0.8827 - val_dense_66_loss: 0.1610 - val_loss: 0.4583 - learning_rate: 0.0010\nEpoch 37/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - dense_65_accuracy: 0.9858 - dense_65_loss: 0.0189 - dense_66_accuracy: 0.9191 - dense_66_loss: 0.1070 - loss: 0.1258\nEpoch 37: val_loss did not improve from 0.41306\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 170ms/step - dense_65_accuracy: 0.9858 - dense_65_loss: 0.0189 - dense_66_accuracy: 0.9191 - dense_66_loss: 0.1070 - loss: 0.1259 - val_dense_65_accuracy: 0.9151 - val_dense_65_loss: 0.2098 - val_dense_66_accuracy: 0.7870 - val_dense_66_loss: 0.3896 - val_loss: 0.6168 - learning_rate: 0.0010\nEpoch 38/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - dense_65_accuracy: 0.9800 - dense_65_loss: 0.0291 - dense_66_accuracy: 0.9273 - dense_66_loss: 0.1000 - loss: 0.1291\nEpoch 38: val_loss improved from 0.41306 to 0.35426, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 178ms/step - dense_65_accuracy: 0.9801 - dense_65_loss: 0.0291 - dense_66_accuracy: 0.9273 - dense_66_loss: 0.1000 - loss: 0.1291 - val_dense_65_accuracy: 0.9290 - val_dense_65_loss: 0.1269 - val_dense_66_accuracy: 0.8472 - val_dense_66_loss: 0.2243 - val_loss: 0.3543 - learning_rate: 0.0010\nEpoch 39/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - dense_65_accuracy: 0.9917 - dense_65_loss: 0.0147 - dense_66_accuracy: 0.9378 - dense_66_loss: 0.0891 - loss: 0.1038\nEpoch 39: val_loss did not improve from 0.35426\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 170ms/step - dense_65_accuracy: 0.9916 - dense_65_loss: 0.0147 - dense_66_accuracy: 0.9378 - dense_66_loss: 0.0892 - loss: 0.1038 - val_dense_65_accuracy: 0.8735 - val_dense_65_loss: 0.2759 - val_dense_66_accuracy: 0.7377 - val_dense_66_loss: 0.4510 - val_loss: 0.7312 - learning_rate: 0.0010\nEpoch 40/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - dense_65_accuracy: 0.9861 - dense_65_loss: 0.0215 - dense_66_accuracy: 0.9344 - dense_66_loss: 0.0899 - loss: 0.1113\nEpoch 40: val_loss did not improve from 0.35426\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 170ms/step - dense_65_accuracy: 0.9861 - dense_65_loss: 0.0214 - dense_66_accuracy: 0.9344 - dense_66_loss: 0.0899 - loss: 0.1113 - val_dense_65_accuracy: 0.9120 - val_dense_65_loss: 0.1931 - val_dense_66_accuracy: 0.8009 - val_dense_66_loss: 0.2882 - val_loss: 0.4794 - learning_rate: 0.0010\nEpoch 41/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - dense_65_accuracy: 0.9858 - dense_65_loss: 0.0210 - dense_66_accuracy: 0.9360 - dense_66_loss: 0.0835 - loss: 0.1045\nEpoch 41: val_loss did not improve from 0.35426\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 170ms/step - dense_65_accuracy: 0.9858 - dense_65_loss: 0.0210 - dense_66_accuracy: 0.9360 - dense_66_loss: 0.0835 - loss: 0.1045 - val_dense_65_accuracy: 0.9151 - val_dense_65_loss: 0.1951 - val_dense_66_accuracy: 0.7824 - val_dense_66_loss: 0.4019 - val_loss: 0.6100 - learning_rate: 0.0010\nEpoch 42/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - dense_65_accuracy: 0.9830 - dense_65_loss: 0.0229 - dense_66_accuracy: 0.9445 - dense_66_loss: 0.0768 - loss: 0.0997\nEpoch 42: val_loss did not improve from 0.35426\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 170ms/step - dense_65_accuracy: 0.9830 - dense_65_loss: 0.0229 - dense_66_accuracy: 0.9446 - dense_66_loss: 0.0768 - loss: 0.0997 - val_dense_65_accuracy: 0.9228 - val_dense_65_loss: 0.1382 - val_dense_66_accuracy: 0.5031 - val_dense_66_loss: 0.9882 - val_loss: 1.1473 - learning_rate: 0.0010\nEpoch 43/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - dense_65_accuracy: 0.9894 - dense_65_loss: 0.0161 - dense_66_accuracy: 0.9501 - dense_66_loss: 0.0744 - loss: 0.0905\nEpoch 43: val_loss did not improve from 0.35426\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 170ms/step - dense_65_accuracy: 0.9894 - dense_65_loss: 0.0160 - dense_66_accuracy: 0.9500 - dense_66_loss: 0.0745 - loss: 0.0905 - val_dense_65_accuracy: 0.9259 - val_dense_65_loss: 0.1412 - val_dense_66_accuracy: 0.7886 - val_dense_66_loss: 0.4214 - val_loss: 0.5476 - learning_rate: 0.0010\nEpoch 44/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - dense_65_accuracy: 0.9898 - dense_65_loss: 0.0150 - dense_66_accuracy: 0.9546 - dense_66_loss: 0.0666 - loss: 0.0816\nEpoch 44: val_loss did not improve from 0.35426\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 170ms/step - dense_65_accuracy: 0.9898 - dense_65_loss: 0.0150 - dense_66_accuracy: 0.9546 - dense_66_loss: 0.0666 - loss: 0.0816 - val_dense_65_accuracy: 0.8827 - val_dense_65_loss: 0.2800 - val_dense_66_accuracy: 0.7546 - val_dense_66_loss: 0.5477 - val_loss: 0.8428 - learning_rate: 0.0010\nEpoch 45/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - dense_65_accuracy: 0.9924 - dense_65_loss: 0.0114 - dense_66_accuracy: 0.9485 - dense_66_loss: 0.0744 - loss: 0.0858\nEpoch 45: val_loss did not improve from 0.35426\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 170ms/step - dense_65_accuracy: 0.9924 - dense_65_loss: 0.0114 - dense_66_accuracy: 0.9485 - dense_66_loss: 0.0744 - loss: 0.0858 - val_dense_65_accuracy: 0.9012 - val_dense_65_loss: 0.2380 - val_dense_66_accuracy: 0.8503 - val_dense_66_loss: 0.2474 - val_loss: 0.4907 - learning_rate: 0.0010\nEpoch 46/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - dense_65_accuracy: 0.9860 - dense_65_loss: 0.0209 - dense_66_accuracy: 0.9564 - dense_66_loss: 0.0606 - loss: 0.0816\nEpoch 46: val_loss did not improve from 0.35426\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 169ms/step - dense_65_accuracy: 0.9860 - dense_65_loss: 0.0209 - dense_66_accuracy: 0.9564 - dense_66_loss: 0.0606 - loss: 0.0816 - val_dense_65_accuracy: 0.9198 - val_dense_65_loss: 0.1593 - val_dense_66_accuracy: 0.8565 - val_dense_66_loss: 0.2569 - val_loss: 0.4176 - learning_rate: 0.0010\nEpoch 47/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - dense_65_accuracy: 0.9829 - dense_65_loss: 0.0235 - dense_66_accuracy: 0.9524 - dense_66_loss: 0.0677 - loss: 0.0912\nEpoch 47: val_loss did not improve from 0.35426\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 170ms/step - dense_65_accuracy: 0.9829 - dense_65_loss: 0.0235 - dense_66_accuracy: 0.9524 - dense_66_loss: 0.0677 - loss: 0.0912 - val_dense_65_accuracy: 0.8642 - val_dense_65_loss: 0.3740 - val_dense_66_accuracy: 0.8827 - val_dense_66_loss: 0.1745 - val_loss: 0.5679 - learning_rate: 0.0010\nEpoch 48/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - dense_65_accuracy: 0.9920 - dense_65_loss: 0.0116 - dense_66_accuracy: 0.9603 - dense_66_loss: 0.0603 - loss: 0.0719\nEpoch 48: val_loss did not improve from 0.35426\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 169ms/step - dense_65_accuracy: 0.9920 - dense_65_loss: 0.0116 - dense_66_accuracy: 0.9603 - dense_66_loss: 0.0603 - loss: 0.0719 - val_dense_65_accuracy: 0.9198 - val_dense_65_loss: 0.2202 - val_dense_66_accuracy: 0.8333 - val_dense_66_loss: 0.3338 - val_loss: 0.5743 - learning_rate: 0.0010\nEpoch 49/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - dense_65_accuracy: 0.9934 - dense_65_loss: 0.0103 - dense_66_accuracy: 0.9640 - dense_66_loss: 0.0518 - loss: 0.0622\nEpoch 49: val_loss did not improve from 0.35426\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 170ms/step - dense_65_accuracy: 0.9934 - dense_65_loss: 0.0103 - dense_66_accuracy: 0.9640 - dense_66_loss: 0.0518 - loss: 0.0622 - val_dense_65_accuracy: 0.8812 - val_dense_65_loss: 0.3522 - val_dense_66_accuracy: 0.7701 - val_dense_66_loss: 0.6197 - val_loss: 0.9994 - learning_rate: 0.0010\nEpoch 50/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - dense_65_accuracy: 0.9916 - dense_65_loss: 0.0101 - dense_66_accuracy: 0.9607 - dense_66_loss: 0.0559 - loss: 0.0659\nEpoch 50: val_loss improved from 0.35426 to 0.35286, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 178ms/step - dense_65_accuracy: 0.9916 - dense_65_loss: 0.0101 - dense_66_accuracy: 0.9607 - dense_66_loss: 0.0559 - loss: 0.0659 - val_dense_65_accuracy: 0.9151 - val_dense_65_loss: 0.1654 - val_dense_66_accuracy: 0.8889 - val_dense_66_loss: 0.1771 - val_loss: 0.3529 - learning_rate: 0.0010\nEpoch 51/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - dense_65_accuracy: 0.9885 - dense_65_loss: 0.0165 - dense_66_accuracy: 0.9657 - dense_66_loss: 0.0516 - loss: 0.0681\nEpoch 51: val_loss did not improve from 0.35286\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 171ms/step - dense_65_accuracy: 0.9885 - dense_65_loss: 0.0165 - dense_66_accuracy: 0.9657 - dense_66_loss: 0.0516 - loss: 0.0681 - val_dense_65_accuracy: 0.9259 - val_dense_65_loss: 0.2089 - val_dense_66_accuracy: 0.8673 - val_dense_66_loss: 0.1965 - val_loss: 0.4161 - learning_rate: 0.0010\nEpoch 52/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - dense_65_accuracy: 0.9909 - dense_65_loss: 0.0118 - dense_66_accuracy: 0.9636 - dense_66_loss: 0.0482 - loss: 0.0600\nEpoch 52: val_loss did not improve from 0.35286\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 170ms/step - dense_65_accuracy: 0.9909 - dense_65_loss: 0.0119 - dense_66_accuracy: 0.9637 - dense_66_loss: 0.0482 - loss: 0.0600 - val_dense_65_accuracy: 0.9352 - val_dense_65_loss: 0.1630 - val_dense_66_accuracy: 0.8086 - val_dense_66_loss: 0.2974 - val_loss: 0.4750 - learning_rate: 0.0010\nEpoch 53/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - dense_65_accuracy: 0.9897 - dense_65_loss: 0.0164 - dense_66_accuracy: 0.9667 - dense_66_loss: 0.0477 - loss: 0.0641\nEpoch 53: val_loss did not improve from 0.35286\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 169ms/step - dense_65_accuracy: 0.9897 - dense_65_loss: 0.0164 - dense_66_accuracy: 0.9667 - dense_66_loss: 0.0478 - loss: 0.0642 - val_dense_65_accuracy: 0.9136 - val_dense_65_loss: 0.1644 - val_dense_66_accuracy: 0.7994 - val_dense_66_loss: 0.3482 - val_loss: 0.5314 - learning_rate: 0.0010\nEpoch 54/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - dense_65_accuracy: 0.9932 - dense_65_loss: 0.0109 - dense_66_accuracy: 0.9644 - dense_66_loss: 0.0519 - loss: 0.0628\nEpoch 54: val_loss improved from 0.35286 to 0.30550, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 178ms/step - dense_65_accuracy: 0.9932 - dense_65_loss: 0.0109 - dense_66_accuracy: 0.9644 - dense_66_loss: 0.0519 - loss: 0.0628 - val_dense_65_accuracy: 0.9336 - val_dense_65_loss: 0.1505 - val_dense_66_accuracy: 0.9028 - val_dense_66_loss: 0.1498 - val_loss: 0.3055 - learning_rate: 0.0010\nEpoch 55/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - dense_65_accuracy: 0.9915 - dense_65_loss: 0.0141 - dense_66_accuracy: 0.9678 - dense_66_loss: 0.0422 - loss: 0.0563\nEpoch 55: val_loss did not improve from 0.30550\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 168ms/step - dense_65_accuracy: 0.9915 - dense_65_loss: 0.0141 - dense_66_accuracy: 0.9677 - dense_66_loss: 0.0422 - loss: 0.0563 - val_dense_65_accuracy: 0.9321 - val_dense_65_loss: 0.1839 - val_dense_66_accuracy: 0.7917 - val_dense_66_loss: 0.4923 - val_loss: 0.6881 - learning_rate: 0.0010\nEpoch 56/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - dense_65_accuracy: 0.9871 - dense_65_loss: 0.0176 - dense_66_accuracy: 0.9668 - dense_66_loss: 0.0457 - loss: 0.0633\nEpoch 56: val_loss did not improve from 0.30550\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 169ms/step - dense_65_accuracy: 0.9871 - dense_65_loss: 0.0176 - dense_66_accuracy: 0.9668 - dense_66_loss: 0.0457 - loss: 0.0633 - val_dense_65_accuracy: 0.8519 - val_dense_65_loss: 0.3543 - val_dense_66_accuracy: 0.8410 - val_dense_66_loss: 0.3012 - val_loss: 0.6740 - learning_rate: 0.0010\nEpoch 57/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - dense_65_accuracy: 0.9914 - dense_65_loss: 0.0140 - dense_66_accuracy: 0.9630 - dense_66_loss: 0.0484 - loss: 0.0624\nEpoch 57: val_loss did not improve from 0.30550\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 168ms/step - dense_65_accuracy: 0.9914 - dense_65_loss: 0.0140 - dense_66_accuracy: 0.9630 - dense_66_loss: 0.0484 - loss: 0.0624 - val_dense_65_accuracy: 0.9336 - val_dense_65_loss: 0.1353 - val_dense_66_accuracy: 0.8426 - val_dense_66_loss: 0.2817 - val_loss: 0.4304 - learning_rate: 0.0010\nEpoch 58/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - dense_65_accuracy: 0.9951 - dense_65_loss: 0.0083 - dense_66_accuracy: 0.9695 - dense_66_loss: 0.0399 - loss: 0.0482\nEpoch 58: val_loss did not improve from 0.30550\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 169ms/step - dense_65_accuracy: 0.9951 - dense_65_loss: 0.0083 - dense_66_accuracy: 0.9695 - dense_66_loss: 0.0399 - loss: 0.0482 - val_dense_65_accuracy: 0.9090 - val_dense_65_loss: 0.1803 - val_dense_66_accuracy: 0.8519 - val_dense_66_loss: 0.2479 - val_loss: 0.4436 - learning_rate: 0.0010\nEpoch 59/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - dense_65_accuracy: 0.9899 - dense_65_loss: 0.0192 - dense_66_accuracy: 0.9785 - dense_66_loss: 0.0331 - loss: 0.0523\nEpoch 59: val_loss did not improve from 0.30550\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 169ms/step - dense_65_accuracy: 0.9899 - dense_65_loss: 0.0192 - dense_66_accuracy: 0.9785 - dense_66_loss: 0.0331 - loss: 0.0523 - val_dense_65_accuracy: 0.9059 - val_dense_65_loss: 0.2227 - val_dense_66_accuracy: 0.8441 - val_dense_66_loss: 0.3388 - val_loss: 0.5576 - learning_rate: 0.0010\nEpoch 60/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - dense_65_accuracy: 0.9854 - dense_65_loss: 0.0212 - dense_66_accuracy: 0.9669 - dense_66_loss: 0.0466 - loss: 0.0678\nEpoch 60: val_loss did not improve from 0.30550\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 169ms/step - dense_65_accuracy: 0.9854 - dense_65_loss: 0.0212 - dense_66_accuracy: 0.9669 - dense_66_loss: 0.0466 - loss: 0.0677 - val_dense_65_accuracy: 0.9182 - val_dense_65_loss: 0.1714 - val_dense_66_accuracy: 0.8256 - val_dense_66_loss: 0.3685 - val_loss: 0.5535 - learning_rate: 0.0010\nEpoch 61/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - dense_65_accuracy: 0.9949 - dense_65_loss: 0.0063 - dense_66_accuracy: 0.9696 - dense_66_loss: 0.0430 - loss: 0.0493\nEpoch 61: val_loss did not improve from 0.30550\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 169ms/step - dense_65_accuracy: 0.9949 - dense_65_loss: 0.0063 - dense_66_accuracy: 0.9696 - dense_66_loss: 0.0430 - loss: 0.0493 - val_dense_65_accuracy: 0.9012 - val_dense_65_loss: 0.1665 - val_dense_66_accuracy: 0.8349 - val_dense_66_loss: 0.2331 - val_loss: 0.4132 - learning_rate: 0.0010\nEpoch 62/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - dense_65_accuracy: 0.9951 - dense_65_loss: 0.0083 - dense_66_accuracy: 0.9713 - dense_66_loss: 0.0426 - loss: 0.0509\nEpoch 62: val_loss did not improve from 0.30550\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 169ms/step - dense_65_accuracy: 0.9950 - dense_65_loss: 0.0083 - dense_66_accuracy: 0.9713 - dense_66_loss: 0.0426 - loss: 0.0509 - val_dense_65_accuracy: 0.9043 - val_dense_65_loss: 0.2437 - val_dense_66_accuracy: 0.8827 - val_dense_66_loss: 0.1980 - val_loss: 0.4579 - learning_rate: 0.0010\nEpoch 63/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - dense_65_accuracy: 0.9921 - dense_65_loss: 0.0135 - dense_66_accuracy: 0.9749 - dense_66_loss: 0.0347 - loss: 0.0482\nEpoch 63: val_loss did not improve from 0.30550\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 170ms/step - dense_65_accuracy: 0.9921 - dense_65_loss: 0.0135 - dense_66_accuracy: 0.9749 - dense_66_loss: 0.0347 - loss: 0.0482 - val_dense_65_accuracy: 0.9259 - val_dense_65_loss: 0.1913 - val_dense_66_accuracy: 0.9028 - val_dense_66_loss: 0.1517 - val_loss: 0.3545 - learning_rate: 0.0010\nEpoch 64/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - dense_65_accuracy: 0.9898 - dense_65_loss: 0.0159 - dense_66_accuracy: 0.9724 - dense_66_loss: 0.0400 - loss: 0.0559\nEpoch 64: val_loss did not improve from 0.30550\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 168ms/step - dense_65_accuracy: 0.9898 - dense_65_loss: 0.0159 - dense_66_accuracy: 0.9724 - dense_66_loss: 0.0401 - loss: 0.0559 - val_dense_65_accuracy: 0.8827 - val_dense_65_loss: 0.2371 - val_dense_66_accuracy: 0.4336 - val_dense_66_loss: 1.6823 - val_loss: 1.9195 - learning_rate: 0.0010\nEpoch 65/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - dense_65_accuracy: 0.9931 - dense_65_loss: 0.0122 - dense_66_accuracy: 0.9738 - dense_66_loss: 0.0423 - loss: 0.0545\nEpoch 65: val_loss did not improve from 0.30550\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 169ms/step - dense_65_accuracy: 0.9931 - dense_65_loss: 0.0122 - dense_66_accuracy: 0.9738 - dense_66_loss: 0.0423 - loss: 0.0545 - val_dense_65_accuracy: 0.9321 - val_dense_65_loss: 0.1503 - val_dense_66_accuracy: 0.9028 - val_dense_66_loss: 0.1555 - val_loss: 0.3166 - learning_rate: 0.0010\nEpoch 66/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - dense_65_accuracy: 0.9947 - dense_65_loss: 0.0070 - dense_66_accuracy: 0.9773 - dense_66_loss: 0.0304 - loss: 0.0374\nEpoch 66: val_loss did not improve from 0.30550\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 169ms/step - dense_65_accuracy: 0.9947 - dense_65_loss: 0.0070 - dense_66_accuracy: 0.9773 - dense_66_loss: 0.0304 - loss: 0.0375 - val_dense_65_accuracy: 0.9136 - val_dense_65_loss: 0.1978 - val_dense_66_accuracy: 0.9105 - val_dense_66_loss: 0.1478 - val_loss: 0.3570 - learning_rate: 0.0010\nEpoch 67/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - dense_65_accuracy: 0.9920 - dense_65_loss: 0.0116 - dense_66_accuracy: 0.9767 - dense_66_loss: 0.0380 - loss: 0.0495\nEpoch 67: val_loss did not improve from 0.30550\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 169ms/step - dense_65_accuracy: 0.9920 - dense_65_loss: 0.0115 - dense_66_accuracy: 0.9767 - dense_66_loss: 0.0380 - loss: 0.0495 - val_dense_65_accuracy: 0.9244 - val_dense_65_loss: 0.1616 - val_dense_66_accuracy: 0.8642 - val_dense_66_loss: 0.2551 - val_loss: 0.4247 - learning_rate: 0.0010\nEpoch 68/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - dense_65_accuracy: 0.9954 - dense_65_loss: 0.0072 - dense_66_accuracy: 0.9738 - dense_66_loss: 0.0344 - loss: 0.0416\nEpoch 68: val_loss did not improve from 0.30550\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 168ms/step - dense_65_accuracy: 0.9954 - dense_65_loss: 0.0072 - dense_66_accuracy: 0.9739 - dense_66_loss: 0.0344 - loss: 0.0416 - val_dense_65_accuracy: 0.9182 - val_dense_65_loss: 0.2108 - val_dense_66_accuracy: 0.8735 - val_dense_66_loss: 0.2046 - val_loss: 0.4300 - learning_rate: 0.0010\nEpoch 69/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - dense_65_accuracy: 0.9970 - dense_65_loss: 0.0055 - dense_66_accuracy: 0.9791 - dense_66_loss: 0.0308 - loss: 0.0363\nEpoch 69: val_loss did not improve from 0.30550\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 169ms/step - dense_65_accuracy: 0.9970 - dense_65_loss: 0.0055 - dense_66_accuracy: 0.9791 - dense_66_loss: 0.0308 - loss: 0.0363 - val_dense_65_accuracy: 0.9043 - val_dense_65_loss: 0.1853 - val_dense_66_accuracy: 0.9028 - val_dense_66_loss: 0.1751 - val_loss: 0.3644 - learning_rate: 0.0010\nEpoch 70/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - dense_65_accuracy: 0.9937 - dense_65_loss: 0.0106 - dense_66_accuracy: 0.9751 - dense_66_loss: 0.0354 - loss: 0.0460\nEpoch 70: val_loss did not improve from 0.30550\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 169ms/step - dense_65_accuracy: 0.9937 - dense_65_loss: 0.0107 - dense_66_accuracy: 0.9751 - dense_66_loss: 0.0354 - loss: 0.0460 - val_dense_65_accuracy: 0.8673 - val_dense_65_loss: 0.3012 - val_dense_66_accuracy: 0.8565 - val_dense_66_loss: 0.3101 - val_loss: 0.6174 - learning_rate: 0.0010\nEpoch 71/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - dense_65_accuracy: 0.9894 - dense_65_loss: 0.0154 - dense_66_accuracy: 0.9785 - dense_66_loss: 0.0302 - loss: 0.0455\nEpoch 71: val_loss did not improve from 0.30550\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 169ms/step - dense_65_accuracy: 0.9894 - dense_65_loss: 0.0153 - dense_66_accuracy: 0.9785 - dense_66_loss: 0.0302 - loss: 0.0455 - val_dense_65_accuracy: 0.8904 - val_dense_65_loss: 0.2726 - val_dense_66_accuracy: 0.9105 - val_dense_66_loss: 0.1387 - val_loss: 0.4256 - learning_rate: 0.0010\nEpoch 72/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - dense_65_accuracy: 0.9914 - dense_65_loss: 0.0115 - dense_66_accuracy: 0.9794 - dense_66_loss: 0.0262 - loss: 0.0377\nEpoch 72: val_loss did not improve from 0.30550\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 169ms/step - dense_65_accuracy: 0.9914 - dense_65_loss: 0.0115 - dense_66_accuracy: 0.9794 - dense_66_loss: 0.0263 - loss: 0.0377 - val_dense_65_accuracy: 0.9136 - val_dense_65_loss: 0.2052 - val_dense_66_accuracy: 0.9290 - val_dense_66_loss: 0.1306 - val_loss: 0.3480 - learning_rate: 0.0010\nEpoch 73/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - dense_65_accuracy: 0.9909 - dense_65_loss: 0.0134 - dense_66_accuracy: 0.9752 - dense_66_loss: 0.0344 - loss: 0.0479\nEpoch 73: val_loss did not improve from 0.30550\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 169ms/step - dense_65_accuracy: 0.9909 - dense_65_loss: 0.0134 - dense_66_accuracy: 0.9752 - dense_66_loss: 0.0344 - loss: 0.0479 - val_dense_65_accuracy: 0.9275 - val_dense_65_loss: 0.1931 - val_dense_66_accuracy: 0.8395 - val_dense_66_loss: 0.4212 - val_loss: 0.6123 - learning_rate: 0.0010\nEpoch 74/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - dense_65_accuracy: 0.9953 - dense_65_loss: 0.0074 - dense_66_accuracy: 0.9725 - dense_66_loss: 0.0394 - loss: 0.0468\nEpoch 74: val_loss did not improve from 0.30550\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 169ms/step - dense_65_accuracy: 0.9953 - dense_65_loss: 0.0074 - dense_66_accuracy: 0.9725 - dense_66_loss: 0.0393 - loss: 0.0467 - val_dense_65_accuracy: 0.9105 - val_dense_65_loss: 0.2472 - val_dense_66_accuracy: 0.7654 - val_dense_66_loss: 0.5875 - val_loss: 0.8320 - learning_rate: 0.0010\nEpoch 75/200\n\u001b[1m173/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 166ms/step - dense_65_accuracy: 0.9930 - dense_65_loss: 0.0107 - dense_66_accuracy: 0.9759 - dense_66_loss: 0.0316 - loss: 0.0422","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"history = model.fit([X_train_s, X_train_h], [y_train_s, y_train_h],\n                    epochs=200,\n                    validation_data=([X_val_s, X_val_h1], [y_val_s, y_val_h1]), verbose=1,\n                    shuffle=True,\n                    callbacks=callbacks) # UpdateGammaCallback","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-05T07:56:47.230Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.evaluate([X_test_s, X_test_h1], [y_test_s, y_test_h1])","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-05T07:56:47.230Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit([X_train_s, X_train_h], [y_train_s, y_train_h],\n                    epochs=200,\n                    validation_data=([X_val_s, X_val_h1], [y_val_s, y_val_h1]), verbose=1,\n                    shuffle=True,\n                    callbacks=callbacks) # UpdateGammaCallback","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T06:34:32.194810Z","iopub.execute_input":"2025-03-05T06:34:32.195071Z","iopub.status.idle":"2025-03-05T07:17:32.239589Z","shell.execute_reply.started":"2025-03-05T06:34:32.195040Z","shell.execute_reply":"2025-03-05T07:17:32.238876Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - dense_65_accuracy: 1.0000 - dense_65_loss: 1.0120e-04 - dense_66_accuracy: 0.9992 - dense_66_loss: 0.0019 - loss: 0.0020\nEpoch 1: val_loss did not improve from 0.17103\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 170ms/step - dense_65_accuracy: 1.0000 - dense_65_loss: 1.0178e-04 - dense_66_accuracy: 0.9992 - dense_66_loss: 0.0019 - loss: 0.0020 - val_dense_65_accuracy: 0.9367 - val_dense_65_loss: 0.1678 - val_dense_66_accuracy: 0.9969 - val_dense_66_loss: 0.0046 - val_loss: 0.1779 - learning_rate: 1.0000e-05\nEpoch 2/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - dense_65_accuracy: 0.9996 - dense_65_loss: 4.2862e-04 - dense_66_accuracy: 0.9991 - dense_66_loss: 0.0020 - loss: 0.0024\nEpoch 2: val_loss did not improve from 0.17103\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 169ms/step - dense_65_accuracy: 0.9996 - dense_65_loss: 4.2853e-04 - dense_66_accuracy: 0.9991 - dense_66_loss: 0.0020 - loss: 0.0024 - val_dense_65_accuracy: 0.9383 - val_dense_65_loss: 0.1682 - val_dense_66_accuracy: 0.9969 - val_dense_66_loss: 0.0049 - val_loss: 0.1786 - learning_rate: 1.0000e-05\nEpoch 3/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - dense_65_accuracy: 0.9997 - dense_65_loss: 4.1136e-04 - dense_66_accuracy: 0.9985 - dense_66_loss: 0.0022 - loss: 0.0026\nEpoch 3: val_loss did not improve from 0.17103\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 169ms/step - dense_65_accuracy: 0.9997 - dense_65_loss: 4.1115e-04 - dense_66_accuracy: 0.9985 - dense_66_loss: 0.0022 - loss: 0.0026 - val_dense_65_accuracy: 0.9352 - val_dense_65_loss: 0.1682 - val_dense_66_accuracy: 0.9969 - val_dense_66_loss: 0.0043 - val_loss: 0.1781 - learning_rate: 1.0000e-05\nEpoch 4/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - dense_65_accuracy: 0.9998 - dense_65_loss: 3.3429e-04 - dense_66_accuracy: 0.9988 - dense_66_loss: 0.0016 - loss: 0.0019\nEpoch 4: val_loss did not improve from 0.17103\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 168ms/step - dense_65_accuracy: 0.9998 - dense_65_loss: 3.3472e-04 - dense_66_accuracy: 0.9988 - dense_66_loss: 0.0016 - loss: 0.0019 - val_dense_65_accuracy: 0.9352 - val_dense_65_loss: 0.1667 - val_dense_66_accuracy: 0.9969 - val_dense_66_loss: 0.0047 - val_loss: 0.1770 - learning_rate: 1.0000e-05\nEpoch 5/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - dense_65_accuracy: 1.0000 - dense_65_loss: 1.9206e-04 - dense_66_accuracy: 0.9996 - dense_66_loss: 8.4786e-04 - loss: 0.0010\nEpoch 5: val_loss did not improve from 0.17103\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 167ms/step - dense_65_accuracy: 1.0000 - dense_65_loss: 1.9237e-04 - dense_66_accuracy: 0.9996 - dense_66_loss: 8.4788e-04 - loss: 0.0010 - val_dense_65_accuracy: 0.9352 - val_dense_65_loss: 0.1677 - val_dense_66_accuracy: 0.9969 - val_dense_66_loss: 0.0047 - val_loss: 0.1780 - learning_rate: 1.0000e-05\nEpoch 6/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - dense_65_accuracy: 0.9999 - dense_65_loss: 4.2163e-04 - dense_66_accuracy: 0.9989 - dense_66_loss: 0.0022 - loss: 0.0026\nEpoch 6: val_loss did not improve from 0.17103\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 169ms/step - dense_65_accuracy: 0.9999 - dense_65_loss: 4.2188e-04 - dense_66_accuracy: 0.9989 - dense_66_loss: 0.0022 - loss: 0.0026 - val_dense_65_accuracy: 0.9383 - val_dense_65_loss: 0.1649 - val_dense_66_accuracy: 0.9969 - val_dense_66_loss: 0.0045 - val_loss: 0.1749 - learning_rate: 1.0000e-05\nEpoch 7/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - dense_65_accuracy: 0.9998 - dense_65_loss: 1.9739e-04 - dense_66_accuracy: 0.9993 - dense_66_loss: 0.0010 - loss: 0.0012\nEpoch 7: val_loss did not improve from 0.17103\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 168ms/step - dense_65_accuracy: 0.9998 - dense_65_loss: 1.9977e-04 - dense_66_accuracy: 0.9993 - dense_66_loss: 0.0010 - loss: 0.0012 - val_dense_65_accuracy: 0.9383 - val_dense_65_loss: 0.1662 - val_dense_66_accuracy: 0.9969 - val_dense_66_loss: 0.0046 - val_loss: 0.1763 - learning_rate: 1.0000e-05\nEpoch 8/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - dense_65_accuracy: 0.9993 - dense_65_loss: 9.6191e-04 - dense_66_accuracy: 0.9995 - dense_66_loss: 8.9668e-04 - loss: 0.0019\nEpoch 8: val_loss did not improve from 0.17103\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 169ms/step - dense_65_accuracy: 0.9993 - dense_65_loss: 9.6036e-04 - dense_66_accuracy: 0.9995 - dense_66_loss: 8.9735e-04 - loss: 0.0019 - val_dense_65_accuracy: 0.9383 - val_dense_65_loss: 0.1670 - val_dense_66_accuracy: 0.9969 - val_dense_66_loss: 0.0049 - val_loss: 0.1775 - learning_rate: 1.0000e-05\nEpoch 9/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - dense_65_accuracy: 0.9999 - dense_65_loss: 1.9907e-04 - dense_66_accuracy: 0.9992 - dense_66_loss: 0.0016 - loss: 0.0018\nEpoch 9: val_loss did not improve from 0.17103\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 169ms/step - dense_65_accuracy: 0.9999 - dense_65_loss: 2.0071e-04 - dense_66_accuracy: 0.9992 - dense_66_loss: 0.0016 - loss: 0.0018 - val_dense_65_accuracy: 0.9383 - val_dense_65_loss: 0.1662 - val_dense_66_accuracy: 0.9969 - val_dense_66_loss: 0.0049 - val_loss: 0.1768 - learning_rate: 1.0000e-05\nEpoch 10/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - dense_65_accuracy: 0.9997 - dense_65_loss: 3.5069e-04 - dense_66_accuracy: 0.9990 - dense_66_loss: 0.0015 - loss: 0.0019\nEpoch 10: val_loss did not improve from 0.17103\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 168ms/step - dense_65_accuracy: 0.9997 - dense_65_loss: 3.5026e-04 - dense_66_accuracy: 0.9990 - dense_66_loss: 0.0015 - loss: 0.0019 - val_dense_65_accuracy: 0.9367 - val_dense_65_loss: 0.1649 - val_dense_66_accuracy: 0.9969 - val_dense_66_loss: 0.0049 - val_loss: 0.1753 - learning_rate: 1.0000e-05\nEpoch 11/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - dense_65_accuracy: 1.0000 - dense_65_loss: 1.3189e-04 - dense_66_accuracy: 0.9997 - dense_66_loss: 6.1545e-04 - loss: 7.4723e-04\nEpoch 11: val_loss did not improve from 0.17103\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 169ms/step - dense_65_accuracy: 1.0000 - dense_65_loss: 1.3227e-04 - dense_66_accuracy: 0.9997 - dense_66_loss: 6.1614e-04 - loss: 7.4819e-04 - val_dense_65_accuracy: 0.9383 - val_dense_65_loss: 0.1663 - val_dense_66_accuracy: 0.9969 - val_dense_66_loss: 0.0054 - val_loss: 0.1773 - learning_rate: 1.0000e-05\nEpoch 12/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - dense_65_accuracy: 1.0000 - dense_65_loss: 1.2036e-04 - dense_66_accuracy: 0.9996 - dense_66_loss: 8.7722e-04 - loss: 9.9744e-04\nEpoch 12: val_loss did not improve from 0.17103\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 169ms/step - dense_65_accuracy: 1.0000 - dense_65_loss: 1.2071e-04 - dense_66_accuracy: 0.9996 - dense_66_loss: 8.7920e-04 - loss: 9.9961e-04 - val_dense_65_accuracy: 0.9367 - val_dense_65_loss: 0.1660 - val_dense_66_accuracy: 0.9969 - val_dense_66_loss: 0.0049 - val_loss: 0.1764 - learning_rate: 1.0000e-05\nEpoch 13/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - dense_65_accuracy: 1.0000 - dense_65_loss: 1.4269e-04 - dense_66_accuracy: 0.9991 - dense_66_loss: 0.0016 - loss: 0.0017\nEpoch 13: val_loss did not improve from 0.17103\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 167ms/step - dense_65_accuracy: 1.0000 - dense_65_loss: 1.4343e-04 - dense_66_accuracy: 0.9991 - dense_66_loss: 0.0016 - loss: 0.0017 - val_dense_65_accuracy: 0.9367 - val_dense_65_loss: 0.1646 - val_dense_66_accuracy: 0.9969 - val_dense_66_loss: 0.0055 - val_loss: 0.1755 - learning_rate: 1.0000e-05\nEpoch 14/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - dense_65_accuracy: 1.0000 - dense_65_loss: 1.2855e-04 - dense_66_accuracy: 0.9994 - dense_66_loss: 8.3095e-04 - loss: 9.5951e-04\nEpoch 14: val_loss did not improve from 0.17103\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 168ms/step - dense_65_accuracy: 1.0000 - dense_65_loss: 1.2867e-04 - dense_66_accuracy: 0.9994 - dense_66_loss: 8.3240e-04 - loss: 9.6109e-04 - val_dense_65_accuracy: 0.9383 - val_dense_65_loss: 0.1636 - val_dense_66_accuracy: 0.9969 - val_dense_66_loss: 0.0046 - val_loss: 0.1739 - learning_rate: 1.0000e-05\nEpoch 15/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - dense_65_accuracy: 0.9999 - dense_65_loss: 2.3751e-04 - dense_66_accuracy: 0.9993 - dense_66_loss: 0.0015 - loss: 0.0018\nEpoch 15: val_loss did not improve from 0.17103\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 169ms/step - dense_65_accuracy: 0.9999 - dense_65_loss: 2.3755e-04 - dense_66_accuracy: 0.9993 - dense_66_loss: 0.0015 - loss: 0.0018 - val_dense_65_accuracy: 0.9383 - val_dense_65_loss: 0.1646 - val_dense_66_accuracy: 0.9954 - val_dense_66_loss: 0.0048 - val_loss: 0.1748 - learning_rate: 1.0000e-05\nEpoch 16/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - dense_65_accuracy: 0.9999 - dense_65_loss: 2.8141e-04 - dense_66_accuracy: 0.9982 - dense_66_loss: 0.0021 - loss: 0.0024\nEpoch 16: val_loss did not improve from 0.17103\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 169ms/step - dense_65_accuracy: 0.9999 - dense_65_loss: 2.8171e-04 - dense_66_accuracy: 0.9982 - dense_66_loss: 0.0021 - loss: 0.0023 - val_dense_65_accuracy: 0.9367 - val_dense_65_loss: 0.1650 - val_dense_66_accuracy: 0.9969 - val_dense_66_loss: 0.0048 - val_loss: 0.1754 - learning_rate: 1.0000e-05\nEpoch 17/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - dense_65_accuracy: 1.0000 - dense_65_loss: 1.4988e-04 - dense_66_accuracy: 0.9991 - dense_66_loss: 0.0020 - loss: 0.0021\nEpoch 17: val_loss did not improve from 0.17103\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 168ms/step - dense_65_accuracy: 1.0000 - dense_65_loss: 1.5100e-04 - dense_66_accuracy: 0.9991 - dense_66_loss: 0.0020 - loss: 0.0021 - val_dense_65_accuracy: 0.9367 - val_dense_65_loss: 0.1653 - val_dense_66_accuracy: 0.9969 - val_dense_66_loss: 0.0052 - val_loss: 0.1761 - learning_rate: 1.0000e-05\nEpoch 18/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - dense_65_accuracy: 0.9999 - dense_65_loss: 2.9606e-04 - dense_66_accuracy: 0.9990 - dense_66_loss: 0.0016 - loss: 0.0019\nEpoch 18: val_loss did not improve from 0.17103\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 169ms/step - dense_65_accuracy: 0.9999 - dense_65_loss: 2.9580e-04 - dense_66_accuracy: 0.9990 - dense_66_loss: 0.0016 - loss: 0.0019 - val_dense_65_accuracy: 0.9367 - val_dense_65_loss: 0.1673 - val_dense_66_accuracy: 0.9969 - val_dense_66_loss: 0.0054 - val_loss: 0.1781 - learning_rate: 1.0000e-05\nEpoch 19/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - dense_65_accuracy: 1.0000 - dense_65_loss: 9.4869e-05 - dense_66_accuracy: 0.9983 - dense_66_loss: 0.0014 - loss: 0.0015\nEpoch 19: val_loss did not improve from 0.17103\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 168ms/step - dense_65_accuracy: 1.0000 - dense_65_loss: 9.4994e-05 - dense_66_accuracy: 0.9983 - dense_66_loss: 0.0014 - loss: 0.0015 - val_dense_65_accuracy: 0.9367 - val_dense_65_loss: 0.1683 - val_dense_66_accuracy: 0.9969 - val_dense_66_loss: 0.0049 - val_loss: 0.1787 - learning_rate: 1.0000e-05\nEpoch 20/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - dense_65_accuracy: 0.9997 - dense_65_loss: 7.3244e-04 - dense_66_accuracy: 0.9998 - dense_66_loss: 6.8033e-04 - loss: 0.0014\nEpoch 20: val_loss did not improve from 0.17103\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 169ms/step - dense_65_accuracy: 0.9997 - dense_65_loss: 7.3652e-04 - dense_66_accuracy: 0.9998 - dense_66_loss: 6.8074e-04 - loss: 0.0014 - val_dense_65_accuracy: 0.9383 - val_dense_65_loss: 0.1693 - val_dense_66_accuracy: 0.9969 - val_dense_66_loss: 0.0043 - val_loss: 0.1793 - learning_rate: 1.0000e-05\nEpoch 21/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - dense_65_accuracy: 0.9999 - dense_65_loss: 2.4485e-04 - dense_66_accuracy: 0.9992 - dense_66_loss: 0.0012 - loss: 0.0015\nEpoch 21: val_loss did not improve from 0.17103\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 169ms/step - dense_65_accuracy: 0.9999 - dense_65_loss: 2.4564e-04 - dense_66_accuracy: 0.9992 - dense_66_loss: 0.0012 - loss: 0.0015 - val_dense_65_accuracy: 0.9367 - val_dense_65_loss: 0.1688 - val_dense_66_accuracy: 0.9969 - val_dense_66_loss: 0.0056 - val_loss: 0.1800 - learning_rate: 1.0000e-05\nEpoch 22/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - dense_65_accuracy: 0.9999 - dense_65_loss: 1.6741e-04 - dense_66_accuracy: 0.9991 - dense_66_loss: 0.0010 - loss: 0.0012\nEpoch 22: val_loss did not improve from 0.17103\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 169ms/step - dense_65_accuracy: 0.9999 - dense_65_loss: 1.6785e-04 - dense_66_accuracy: 0.9991 - dense_66_loss: 0.0010 - loss: 0.0012 - val_dense_65_accuracy: 0.9383 - val_dense_65_loss: 0.1691 - val_dense_66_accuracy: 0.9969 - val_dense_66_loss: 0.0047 - val_loss: 0.1795 - learning_rate: 1.0000e-05\nEpoch 23/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - dense_65_accuracy: 0.9996 - dense_65_loss: 4.6304e-04 - dense_66_accuracy: 0.9995 - dense_66_loss: 0.0010 - loss: 0.0015\nEpoch 23: val_loss did not improve from 0.17103\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 169ms/step - dense_65_accuracy: 0.9996 - dense_65_loss: 4.6267e-04 - dense_66_accuracy: 0.9995 - dense_66_loss: 0.0010 - loss: 0.0015 - val_dense_65_accuracy: 0.9367 - val_dense_65_loss: 0.1695 - val_dense_66_accuracy: 0.9969 - val_dense_66_loss: 0.0051 - val_loss: 0.1804 - learning_rate: 1.0000e-05\nEpoch 24/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - dense_65_accuracy: 0.9999 - dense_65_loss: 2.5223e-04 - dense_66_accuracy: 0.9989 - dense_66_loss: 0.0019 - loss: 0.0021\nEpoch 24: val_loss did not improve from 0.17103\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 168ms/step - dense_65_accuracy: 0.9999 - dense_65_loss: 2.5482e-04 - dense_66_accuracy: 0.9989 - dense_66_loss: 0.0019 - loss: 0.0021 - val_dense_65_accuracy: 0.9352 - val_dense_65_loss: 0.1689 - val_dense_66_accuracy: 0.9969 - val_dense_66_loss: 0.0044 - val_loss: 0.1792 - learning_rate: 1.0000e-05\nEpoch 25/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - dense_65_accuracy: 0.9999 - dense_65_loss: 2.3551e-04 - dense_66_accuracy: 0.9988 - dense_66_loss: 0.0013 - loss: 0.0015\nEpoch 25: val_loss did not improve from 0.17103\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 167ms/step - dense_65_accuracy: 0.9999 - dense_65_loss: 2.3584e-04 - dense_66_accuracy: 0.9988 - dense_66_loss: 0.0012 - loss: 0.0015 - val_dense_65_accuracy: 0.9352 - val_dense_65_loss: 0.1680 - val_dense_66_accuracy: 0.9969 - val_dense_66_loss: 0.0053 - val_loss: 0.1790 - learning_rate: 1.0000e-05\nEpoch 26/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - dense_65_accuracy: 1.0000 - dense_65_loss: 9.1122e-05 - dense_66_accuracy: 0.9995 - dense_66_loss: 0.0010 - loss: 0.0011\nEpoch 26: val_loss did not improve from 0.17103\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 168ms/step - dense_65_accuracy: 1.0000 - dense_65_loss: 9.1352e-05 - dense_66_accuracy: 0.9995 - dense_66_loss: 0.0010 - loss: 0.0011 - val_dense_65_accuracy: 0.9367 - val_dense_65_loss: 0.1688 - val_dense_66_accuracy: 0.9969 - val_dense_66_loss: 0.0048 - val_loss: 0.1795 - learning_rate: 1.0000e-05\nEpoch 27/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - dense_65_accuracy: 1.0000 - dense_65_loss: 1.7039e-04 - dense_66_accuracy: 0.9990 - dense_66_loss: 0.0019 - loss: 0.0021\nEpoch 27: val_loss did not improve from 0.17103\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 168ms/step - dense_65_accuracy: 1.0000 - dense_65_loss: 1.7138e-04 - dense_66_accuracy: 0.9990 - dense_66_loss: 0.0019 - loss: 0.0021 - val_dense_65_accuracy: 0.9367 - val_dense_65_loss: 0.1674 - val_dense_66_accuracy: 0.9969 - val_dense_66_loss: 0.0046 - val_loss: 0.1779 - learning_rate: 1.0000e-05\nEpoch 28/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - dense_65_accuracy: 0.9997 - dense_65_loss: 1.7758e-04 - dense_66_accuracy: 0.9994 - dense_66_loss: 0.0012 - loss: 0.0014\nEpoch 28: val_loss did not improve from 0.17103\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 168ms/step - dense_65_accuracy: 0.9997 - dense_65_loss: 1.7769e-04 - dense_66_accuracy: 0.9994 - dense_66_loss: 0.0012 - loss: 0.0014 - val_dense_65_accuracy: 0.9367 - val_dense_65_loss: 0.1688 - val_dense_66_accuracy: 0.9969 - val_dense_66_loss: 0.0055 - val_loss: 0.1803 - learning_rate: 1.0000e-05\nEpoch 29/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - dense_65_accuracy: 0.9999 - dense_65_loss: 2.5079e-04 - dense_66_accuracy: 0.9993 - dense_66_loss: 0.0014 - loss: 0.0016\nEpoch 29: val_loss did not improve from 0.17103\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 169ms/step - dense_65_accuracy: 0.9999 - dense_65_loss: 2.5093e-04 - dense_66_accuracy: 0.9993 - dense_66_loss: 0.0014 - loss: 0.0016 - val_dense_65_accuracy: 0.9352 - val_dense_65_loss: 0.1693 - val_dense_66_accuracy: 0.9969 - val_dense_66_loss: 0.0055 - val_loss: 0.1807 - learning_rate: 1.0000e-05\nEpoch 30/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - dense_65_accuracy: 1.0000 - dense_65_loss: 1.3347e-04 - dense_66_accuracy: 0.9985 - dense_66_loss: 0.0012 - loss: 0.0013\nEpoch 30: val_loss did not improve from 0.17103\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 168ms/step - dense_65_accuracy: 1.0000 - dense_65_loss: 1.3375e-04 - dense_66_accuracy: 0.9985 - dense_66_loss: 0.0012 - loss: 0.0013 - val_dense_65_accuracy: 0.9352 - val_dense_65_loss: 0.1702 - val_dense_66_accuracy: 0.9969 - val_dense_66_loss: 0.0043 - val_loss: 0.1805 - learning_rate: 1.0000e-05\nEpoch 31/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - dense_65_accuracy: 0.9996 - dense_65_loss: 4.0584e-04 - dense_66_accuracy: 0.9994 - dense_66_loss: 8.5860e-04 - loss: 0.0013\nEpoch 31: val_loss did not improve from 0.17103\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 168ms/step - dense_65_accuracy: 0.9996 - dense_65_loss: 4.0602e-04 - dense_66_accuracy: 0.9994 - dense_66_loss: 8.5882e-04 - loss: 0.0013 - val_dense_65_accuracy: 0.9367 - val_dense_65_loss: 0.1709 - val_dense_66_accuracy: 0.9969 - val_dense_66_loss: 0.0054 - val_loss: 0.1822 - learning_rate: 1.0000e-05\nEpoch 32/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - dense_65_accuracy: 0.9996 - dense_65_loss: 5.8994e-04 - dense_66_accuracy: 0.9992 - dense_66_loss: 8.6006e-04 - loss: 0.0014\nEpoch 32: val_loss did not improve from 0.17103\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 168ms/step - dense_65_accuracy: 0.9996 - dense_65_loss: 5.8928e-04 - dense_66_accuracy: 0.9992 - dense_66_loss: 8.6148e-04 - loss: 0.0014 - val_dense_65_accuracy: 0.9367 - val_dense_65_loss: 0.1705 - val_dense_66_accuracy: 0.9954 - val_dense_66_loss: 0.0055 - val_loss: 0.1821 - learning_rate: 1.0000e-05\nEpoch 33/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - dense_65_accuracy: 1.0000 - dense_65_loss: 1.7906e-04 - dense_66_accuracy: 0.9997 - dense_66_loss: 5.6997e-04 - loss: 7.4903e-04\nEpoch 33: val_loss did not improve from 0.17103\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 168ms/step - dense_65_accuracy: 1.0000 - dense_65_loss: 1.7892e-04 - dense_66_accuracy: 0.9997 - dense_66_loss: 5.7056e-04 - loss: 7.4948e-04 - val_dense_65_accuracy: 0.9367 - val_dense_65_loss: 0.1714 - val_dense_66_accuracy: 0.9969 - val_dense_66_loss: 0.0050 - val_loss: 0.1827 - learning_rate: 1.0000e-05\nEpoch 34/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - dense_65_accuracy: 0.9997 - dense_65_loss: 3.5372e-04 - dense_66_accuracy: 0.9992 - dense_66_loss: 9.8118e-04 - loss: 0.0013\nEpoch 34: val_loss did not improve from 0.17103\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 168ms/step - dense_65_accuracy: 0.9997 - dense_65_loss: 3.5383e-04 - dense_66_accuracy: 0.9992 - dense_66_loss: 9.8174e-04 - loss: 0.0013 - val_dense_65_accuracy: 0.9352 - val_dense_65_loss: 0.1705 - val_dense_66_accuracy: 0.9969 - val_dense_66_loss: 0.0043 - val_loss: 0.1809 - learning_rate: 1.0000e-05\nEpoch 35/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - dense_65_accuracy: 1.0000 - dense_65_loss: 1.9566e-04 - dense_66_accuracy: 0.9997 - dense_66_loss: 7.1152e-04 - loss: 9.0704e-04\nEpoch 35: val_loss did not improve from 0.17103\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 168ms/step - dense_65_accuracy: 1.0000 - dense_65_loss: 1.9628e-04 - dense_66_accuracy: 0.9997 - dense_66_loss: 7.1417e-04 - loss: 9.1018e-04 - val_dense_65_accuracy: 0.9352 - val_dense_65_loss: 0.1701 - val_dense_66_accuracy: 0.9969 - val_dense_66_loss: 0.0043 - val_loss: 0.1804 - learning_rate: 1.0000e-05\nEpoch 36/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - dense_65_accuracy: 1.0000 - dense_65_loss: 1.9561e-04 - dense_66_accuracy: 0.9991 - dense_66_loss: 0.0010 - loss: 0.0012\nEpoch 36: val_loss did not improve from 0.17103\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 167ms/step - dense_65_accuracy: 1.0000 - dense_65_loss: 1.9528e-04 - dense_66_accuracy: 0.9991 - dense_66_loss: 0.0010 - loss: 0.0012 - val_dense_65_accuracy: 0.9336 - val_dense_65_loss: 0.1715 - val_dense_66_accuracy: 0.9969 - val_dense_66_loss: 0.0052 - val_loss: 0.1828 - learning_rate: 1.0000e-05\nEpoch 37/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - dense_65_accuracy: 0.9999 - dense_65_loss: 1.3391e-04 - dense_66_accuracy: 0.9997 - dense_66_loss: 6.8157e-04 - loss: 8.1549e-04\nEpoch 37: val_loss did not improve from 0.17103\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 168ms/step - dense_65_accuracy: 0.9999 - dense_65_loss: 1.3407e-04 - dense_66_accuracy: 0.9997 - dense_66_loss: 6.8178e-04 - loss: 8.1587e-04 - val_dense_65_accuracy: 0.9352 - val_dense_65_loss: 0.1706 - val_dense_66_accuracy: 0.9969 - val_dense_66_loss: 0.0045 - val_loss: 0.1811 - learning_rate: 1.0000e-05\nEpoch 38/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - dense_65_accuracy: 0.9997 - dense_65_loss: 5.8655e-04 - dense_66_accuracy: 0.9993 - dense_66_loss: 7.5892e-04 - loss: 0.0013\nEpoch 38: val_loss did not improve from 0.17103\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 169ms/step - dense_65_accuracy: 0.9997 - dense_65_loss: 5.8617e-04 - dense_66_accuracy: 0.9993 - dense_66_loss: 7.5869e-04 - loss: 0.0013 - val_dense_65_accuracy: 0.9383 - val_dense_65_loss: 0.1686 - val_dense_66_accuracy: 0.9969 - val_dense_66_loss: 0.0046 - val_loss: 0.1790 - learning_rate: 1.0000e-05\nEpoch 39/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - dense_65_accuracy: 0.9998 - dense_65_loss: 1.8011e-04 - dense_66_accuracy: 0.9985 - dense_66_loss: 0.0019 - loss: 0.0021\nEpoch 39: val_loss did not improve from 0.17103\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 168ms/step - dense_65_accuracy: 0.9998 - dense_65_loss: 1.7988e-04 - dense_66_accuracy: 0.9985 - dense_66_loss: 0.0019 - loss: 0.0021 - val_dense_65_accuracy: 0.9367 - val_dense_65_loss: 0.1691 - val_dense_66_accuracy: 0.9969 - val_dense_66_loss: 0.0052 - val_loss: 0.1803 - learning_rate: 1.0000e-05\nEpoch 40/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - dense_65_accuracy: 0.9999 - dense_65_loss: 2.4637e-04 - dense_66_accuracy: 0.9992 - dense_66_loss: 0.0012 - loss: 0.0014\nEpoch 40: val_loss did not improve from 0.17103\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 168ms/step - dense_65_accuracy: 0.9998 - dense_65_loss: 2.4681e-04 - dense_66_accuracy: 0.9992 - dense_66_loss: 0.0012 - loss: 0.0014 - val_dense_65_accuracy: 0.9367 - val_dense_65_loss: 0.1704 - val_dense_66_accuracy: 0.9954 - val_dense_66_loss: 0.0051 - val_loss: 0.1814 - learning_rate: 1.0000e-05\nEpoch 41/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - dense_65_accuracy: 0.9994 - dense_65_loss: 0.0013 - dense_66_accuracy: 0.9991 - dense_66_loss: 0.0011 - loss: 0.0024\nEpoch 41: val_loss did not improve from 0.17103\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 168ms/step - dense_65_accuracy: 0.9994 - dense_65_loss: 0.0013 - dense_66_accuracy: 0.9991 - dense_66_loss: 0.0011 - loss: 0.0024 - val_dense_65_accuracy: 0.9367 - val_dense_65_loss: 0.1729 - val_dense_66_accuracy: 0.9969 - val_dense_66_loss: 0.0053 - val_loss: 0.1844 - learning_rate: 1.0000e-05\nEpoch 42/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - dense_65_accuracy: 1.0000 - dense_65_loss: 8.8948e-05 - dense_66_accuracy: 0.9997 - dense_66_loss: 8.4028e-04 - loss: 9.2915e-04\nEpoch 42: val_loss did not improve from 0.17103\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 168ms/step - dense_65_accuracy: 1.0000 - dense_65_loss: 8.8899e-05 - dense_66_accuracy: 0.9997 - dense_66_loss: 8.4028e-04 - loss: 9.2902e-04 - val_dense_65_accuracy: 0.9367 - val_dense_65_loss: 0.1717 - val_dense_66_accuracy: 0.9954 - val_dense_66_loss: 0.0049 - val_loss: 0.1827 - learning_rate: 1.0000e-05\nEpoch 43/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - dense_65_accuracy: 0.9997 - dense_65_loss: 4.9292e-04 - dense_66_accuracy: 1.0000 - dense_66_loss: 5.9805e-04 - loss: 0.0011\nEpoch 43: val_loss did not improve from 0.17103\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 168ms/step - dense_65_accuracy: 0.9997 - dense_65_loss: 4.9252e-04 - dense_66_accuracy: 1.0000 - dense_66_loss: 5.9760e-04 - loss: 0.0011 - val_dense_65_accuracy: 0.9352 - val_dense_65_loss: 0.1695 - val_dense_66_accuracy: 0.9954 - val_dense_66_loss: 0.0046 - val_loss: 0.1799 - learning_rate: 1.0000e-05\nEpoch 44/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - dense_65_accuracy: 0.9998 - dense_65_loss: 4.6044e-04 - dense_66_accuracy: 0.9999 - dense_66_loss: 6.0724e-04 - loss: 0.0011\nEpoch 44: val_loss did not improve from 0.17103\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 168ms/step - dense_65_accuracy: 0.9998 - dense_65_loss: 4.6041e-04 - dense_66_accuracy: 0.9999 - dense_66_loss: 6.0677e-04 - loss: 0.0011 - val_dense_65_accuracy: 0.9383 - val_dense_65_loss: 0.1701 - val_dense_66_accuracy: 0.9969 - val_dense_66_loss: 0.0046 - val_loss: 0.1806 - learning_rate: 1.0000e-05\nEpoch 45/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - dense_65_accuracy: 0.9997 - dense_65_loss: 4.3741e-04 - dense_66_accuracy: 0.9996 - dense_66_loss: 8.2121e-04 - loss: 0.0013\nEpoch 45: val_loss did not improve from 0.17103\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 168ms/step - dense_65_accuracy: 0.9997 - dense_65_loss: 4.3698e-04 - dense_66_accuracy: 0.9996 - dense_66_loss: 8.2100e-04 - loss: 0.0013 - val_dense_65_accuracy: 0.9383 - val_dense_65_loss: 0.1698 - val_dense_66_accuracy: 0.9969 - val_dense_66_loss: 0.0046 - val_loss: 0.1805 - learning_rate: 1.0000e-05\nEpoch 46/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - dense_65_accuracy: 0.9997 - dense_65_loss: 3.7600e-04 - dense_66_accuracy: 0.9998 - dense_66_loss: 8.2264e-04 - loss: 0.0012\nEpoch 46: val_loss did not improve from 0.17103\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 167ms/step - dense_65_accuracy: 0.9997 - dense_65_loss: 3.7586e-04 - dense_66_accuracy: 0.9998 - dense_66_loss: 8.2299e-04 - loss: 0.0012 - val_dense_65_accuracy: 0.9383 - val_dense_65_loss: 0.1699 - val_dense_66_accuracy: 0.9969 - val_dense_66_loss: 0.0044 - val_loss: 0.1803 - learning_rate: 1.0000e-05\nEpoch 47/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - dense_65_accuracy: 0.9998 - dense_65_loss: 2.6334e-04 - dense_66_accuracy: 0.9996 - dense_66_loss: 0.0012 - loss: 0.0015\nEpoch 47: val_loss did not improve from 0.17103\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 168ms/step - dense_65_accuracy: 0.9998 - dense_65_loss: 2.6450e-04 - dense_66_accuracy: 0.9996 - dense_66_loss: 0.0012 - loss: 0.0015 - val_dense_65_accuracy: 0.9367 - val_dense_65_loss: 0.1700 - val_dense_66_accuracy: 0.9954 - val_dense_66_loss: 0.0053 - val_loss: 0.1813 - learning_rate: 1.0000e-05\nEpoch 48/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - dense_65_accuracy: 0.9997 - dense_65_loss: 3.3293e-04 - dense_66_accuracy: 0.9990 - dense_66_loss: 0.0018 - loss: 0.0021\nEpoch 48: val_loss did not improve from 0.17103\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 168ms/step - dense_65_accuracy: 0.9997 - dense_65_loss: 3.3265e-04 - dense_66_accuracy: 0.9990 - dense_66_loss: 0.0017 - loss: 0.0021 - val_dense_65_accuracy: 0.9367 - val_dense_65_loss: 0.1698 - val_dense_66_accuracy: 0.9969 - val_dense_66_loss: 0.0046 - val_loss: 0.1804 - learning_rate: 1.0000e-05\nEpoch 49/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - dense_65_accuracy: 1.0000 - dense_65_loss: 8.7555e-05 - dense_66_accuracy: 0.9977 - dense_66_loss: 0.0024 - loss: 0.0025\nEpoch 49: val_loss did not improve from 0.17103\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 168ms/step - dense_65_accuracy: 1.0000 - dense_65_loss: 8.7737e-05 - dense_66_accuracy: 0.9977 - dense_66_loss: 0.0024 - loss: 0.0025 - val_dense_65_accuracy: 0.9367 - val_dense_65_loss: 0.1696 - val_dense_66_accuracy: 0.9969 - val_dense_66_loss: 0.0051 - val_loss: 0.1808 - learning_rate: 1.0000e-05\nEpoch 50/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - dense_65_accuracy: 0.9999 - dense_65_loss: 2.6894e-04 - dense_66_accuracy: 0.9995 - dense_66_loss: 0.0016 - loss: 0.0019\nEpoch 50: val_loss did not improve from 0.17103\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 168ms/step - dense_65_accuracy: 0.9999 - dense_65_loss: 2.6913e-04 - dense_66_accuracy: 0.9995 - dense_66_loss: 0.0016 - loss: 0.0019 - val_dense_65_accuracy: 0.9383 - val_dense_65_loss: 0.1706 - val_dense_66_accuracy: 0.9969 - val_dense_66_loss: 0.0048 - val_loss: 0.1815 - learning_rate: 1.0000e-05\nEpoch 51/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - dense_65_accuracy: 0.9998 - dense_65_loss: 4.4891e-04 - dense_66_accuracy: 0.9998 - dense_66_loss: 7.6721e-04 - loss: 0.0012\nEpoch 51: val_loss did not improve from 0.17103\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 168ms/step - dense_65_accuracy: 0.9998 - dense_65_loss: 4.4933e-04 - dense_66_accuracy: 0.9998 - dense_66_loss: 7.6674e-04 - loss: 0.0012 - val_dense_65_accuracy: 0.9367 - val_dense_65_loss: 0.1720 - val_dense_66_accuracy: 0.9969 - val_dense_66_loss: 0.0047 - val_loss: 0.1828 - learning_rate: 1.0000e-05\nEpoch 52/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - dense_65_accuracy: 0.9999 - dense_65_loss: 1.8610e-04 - dense_66_accuracy: 0.9995 - dense_66_loss: 9.5927e-04 - loss: 0.0011\nEpoch 52: val_loss did not improve from 0.17103\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 167ms/step - dense_65_accuracy: 0.9999 - dense_65_loss: 1.8642e-04 - dense_66_accuracy: 0.9995 - dense_66_loss: 9.5912e-04 - loss: 0.0011 - val_dense_65_accuracy: 0.9367 - val_dense_65_loss: 0.1722 - val_dense_66_accuracy: 0.9969 - val_dense_66_loss: 0.0048 - val_loss: 0.1833 - learning_rate: 1.0000e-05\nEpoch 53/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - dense_65_accuracy: 1.0000 - dense_65_loss: 2.0471e-04 - dense_66_accuracy: 0.9995 - dense_66_loss: 9.9548e-04 - loss: 0.0012\nEpoch 53: val_loss did not improve from 0.17103\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 168ms/step - dense_65_accuracy: 1.0000 - dense_65_loss: 2.0484e-04 - dense_66_accuracy: 0.9995 - dense_66_loss: 9.9516e-04 - loss: 0.0012 - val_dense_65_accuracy: 0.9367 - val_dense_65_loss: 0.1727 - val_dense_66_accuracy: 0.9969 - val_dense_66_loss: 0.0045 - val_loss: 0.1834 - learning_rate: 1.0000e-05\nEpoch 54/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - dense_65_accuracy: 1.0000 - dense_65_loss: 9.8565e-05 - dense_66_accuracy: 0.9996 - dense_66_loss: 9.8677e-04 - loss: 0.0011\nEpoch 54: val_loss did not improve from 0.17103\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 168ms/step - dense_65_accuracy: 1.0000 - dense_65_loss: 9.8665e-05 - dense_66_accuracy: 0.9996 - dense_66_loss: 9.8623e-04 - loss: 0.0011 - val_dense_65_accuracy: 0.9367 - val_dense_65_loss: 0.1737 - val_dense_66_accuracy: 0.9969 - val_dense_66_loss: 0.0049 - val_loss: 0.1848 - learning_rate: 1.0000e-05\nEpoch 55/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - dense_65_accuracy: 0.9997 - dense_65_loss: 7.2347e-04 - dense_66_accuracy: 1.0000 - dense_66_loss: 4.7597e-04 - loss: 0.0012\nEpoch 55: val_loss did not improve from 0.17103\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 168ms/step - dense_65_accuracy: 0.9997 - dense_65_loss: 7.2510e-04 - dense_66_accuracy: 1.0000 - dense_66_loss: 4.7663e-04 - loss: 0.0012 - val_dense_65_accuracy: 0.9398 - val_dense_65_loss: 0.1734 - val_dense_66_accuracy: 0.9969 - val_dense_66_loss: 0.0049 - val_loss: 0.1846 - learning_rate: 1.0000e-05\nEpoch 56/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - dense_65_accuracy: 0.9999 - dense_65_loss: 2.1035e-04 - dense_66_accuracy: 0.9994 - dense_66_loss: 9.6379e-04 - loss: 0.0012\nEpoch 56: val_loss did not improve from 0.17103\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 169ms/step - dense_65_accuracy: 0.9999 - dense_65_loss: 2.1085e-04 - dense_66_accuracy: 0.9994 - dense_66_loss: 9.6645e-04 - loss: 0.0012 - val_dense_65_accuracy: 0.9383 - val_dense_65_loss: 0.1742 - val_dense_66_accuracy: 0.9954 - val_dense_66_loss: 0.0047 - val_loss: 0.1853 - learning_rate: 1.0000e-05\nEpoch 57/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - dense_65_accuracy: 0.9998 - dense_65_loss: 2.0535e-04 - dense_66_accuracy: 0.9996 - dense_66_loss: 8.6914e-04 - loss: 0.0011\nEpoch 57: val_loss did not improve from 0.17103\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 168ms/step - dense_65_accuracy: 0.9998 - dense_65_loss: 2.0554e-04 - dense_66_accuracy: 0.9996 - dense_66_loss: 8.6942e-04 - loss: 0.0011 - val_dense_65_accuracy: 0.9367 - val_dense_65_loss: 0.1743 - val_dense_66_accuracy: 0.9954 - val_dense_66_loss: 0.0050 - val_loss: 0.1856 - learning_rate: 1.0000e-05\nEpoch 58/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - dense_65_accuracy: 0.9999 - dense_65_loss: 1.9609e-04 - dense_66_accuracy: 1.0000 - dense_66_loss: 4.4458e-04 - loss: 6.4055e-04\nEpoch 58: val_loss did not improve from 0.17103\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 168ms/step - dense_65_accuracy: 0.9999 - dense_65_loss: 1.9622e-04 - dense_66_accuracy: 1.0000 - dense_66_loss: 4.4570e-04 - loss: 6.4166e-04 - val_dense_65_accuracy: 0.9367 - val_dense_65_loss: 0.1751 - val_dense_66_accuracy: 0.9954 - val_dense_66_loss: 0.0052 - val_loss: 0.1865 - learning_rate: 1.0000e-05\nEpoch 59/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - dense_65_accuracy: 1.0000 - dense_65_loss: 1.1000e-04 - dense_66_accuracy: 0.9995 - dense_66_loss: 9.4139e-04 - loss: 0.0011\nEpoch 59: val_loss did not improve from 0.17103\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 169ms/step - dense_65_accuracy: 1.0000 - dense_65_loss: 1.1001e-04 - dense_66_accuracy: 0.9995 - dense_66_loss: 9.4136e-04 - loss: 0.0011 - val_dense_65_accuracy: 0.9367 - val_dense_65_loss: 0.1745 - val_dense_66_accuracy: 0.9954 - val_dense_66_loss: 0.0053 - val_loss: 0.1861 - learning_rate: 1.0000e-05\nEpoch 60/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - dense_65_accuracy: 0.9996 - dense_65_loss: 6.7591e-04 - dense_66_accuracy: 0.9995 - dense_66_loss: 6.3605e-04 - loss: 0.0013\nEpoch 60: val_loss did not improve from 0.17103\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 168ms/step - dense_65_accuracy: 0.9996 - dense_65_loss: 6.7639e-04 - dense_66_accuracy: 0.9995 - dense_66_loss: 6.3707e-04 - loss: 0.0013 - val_dense_65_accuracy: 0.9383 - val_dense_65_loss: 0.1749 - val_dense_66_accuracy: 0.9954 - val_dense_66_loss: 0.0050 - val_loss: 0.1861 - learning_rate: 1.0000e-05\nEpoch 60: early stopping\nRestoring model weights from the end of the best epoch: 1.\n","output_type":"stream"}],"execution_count":116},{"cell_type":"code","source":"model.evaluate([X_test_s, X_test_h1], [y_test_s, y_test_h1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T07:17:32.240886Z","iopub.execute_input":"2025-03-05T07:17:32.241247Z","iopub.status.idle":"2025-03-05T07:17:33.962437Z","shell.execute_reply.started":"2025-03-05T07:17:32.241209Z","shell.execute_reply":"2025-03-05T07:17:33.961568Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - dense_65_accuracy: 0.9592 - dense_65_loss: 0.1253 - dense_66_accuracy: 0.9897 - dense_66_loss: 0.0138 - loss: 0.1393\n","output_type":"stream"},{"execution_count":117,"output_type":"execute_result","data":{"text/plain":"[0.13699503242969513,\n 0.12142693251371384,\n 0.013077711686491966,\n 0.9580246806144714,\n 0.9925925731658936]"},"metadata":{}}],"execution_count":117},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''random_indices = np.random.choice(2430, 2003, replace=False)\n\nX_test_s1 = X_test_s1[random_indices]\ny_test_s1 = y_test_s1[random_indices]\n\nX_test_s1.shape, y_test_s1.shape, X_test_s.shape, y_test_s.shape'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''#X_train.shape, y_train.shape, X_test.shape, y_test.shape, \nX_train_s.shape,X_test_s.shape, y_train_s.shape,y_test_s.shape, X_test_s1.shape, y_test_s1.shape'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T05:38:09.922023Z","iopub.execute_input":"2025-02-05T05:38:09.922449Z","iopub.status.idle":"2025-02-05T05:38:09.928043Z","shell.execute_reply.started":"2025-02-05T05:38:09.922391Z","shell.execute_reply":"2025-02-05T05:38:09.927039Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"'#X_train.shape, y_train.shape, X_test.shape, y_test.shape, \\nX_train_s.shape,X_test_s.shape, y_train_s.shape,y_test_s.shape, X_test_s1.shape, y_test_s1.shape'"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"'''print(X_train_h.shape, y_train_h.shape, X_test_h.shape, y_test_h.shape,\n#X_train.shape, y_train.shape, X_test.shape, y_test.shape,\nX_train_s.shape,X_test_s.shape, X_test_s1.shape, y_train_s.shape,y_test_s.shape, y_test_s1.shape)'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T05:38:13.303324Z","iopub.execute_input":"2025-02-05T05:38:13.303846Z","iopub.status.idle":"2025-02-05T05:38:13.308532Z","shell.execute_reply.started":"2025-02-05T05:38:13.303812Z","shell.execute_reply":"2025-02-05T05:38:13.307701Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"'print(X_train_h.shape, y_train_h.shape, X_test_h.shape, y_test_h.shape,\\n#X_train.shape, y_train.shape, X_test.shape, y_test.shape,\\nX_train_s.shape,X_test_s.shape, X_test_s1.shape, y_train_s.shape,y_test_s.shape, y_test_s1.shape)'"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Multi-branch fusion attention (MFA) module**","metadata":{}},{"cell_type":"code","source":"#### Multi-branch fusion attention (MFA) module #####\n\nclass DeeperGlobalLocalAttentionLayer1(layers.Layer):\n    def __init__(self, units, activation='sigmoid', dropout_rate=0.2, use_scale=True, axis=-1, **kwargs):\n        super(DeeperGlobalLocalAttentionLayer1, self).__init__(**kwargs)\n        self.units = units\n        self.activation = activation\n        self.dropout_rate = dropout_rate\n        self.use_scale = use_scale\n        self.axis = axis\n\n    def build(self, input_shape):\n        _, _, _, channels = input_shape\n        self.global_conv1 = layers.Conv2D(filters=self.units, kernel_size=(1, 1), activation=self.activation)\n        self.global_avg_pooling1 = layers.GlobalAveragePooling2D()\n        \n        self.global_conv2 = layers.Conv2D(filters=self.units, kernel_size=(1, 1), activation=self.activation)\n        self.global_avg_pooling2 = layers.GlobalMaxPooling2D()\n        \n        self.global_conv3 = layers.Conv2D(filters=self.units, kernel_size=(1, 1), activation=self.activation)\n        self.global_avg_pooling3 = layers.GlobalAveragePooling2D()\n        \n        self.global_conv4 = layers.Conv2D(filters=self.units, kernel_size=(1, 1), activation=self.activation)\n        self.global_avg_pooling4 = layers.GlobalMaxPooling2D()\n        \n        self.concat1 = layers.Add()\n        self.concat2 = layers.Add()\n        self.concat3 = layers.Add()\n        self.concat4 = layers.Add()\n        self.concat5 = layers.Concatenate(axis=-1)\n        \n        self.global_attention = layers.Dense(units=self.units, activation=self.activation)\n        \n        self.local_conv1 = layers.Conv2D(filters=self.units, kernel_size=(1, 1), activation=self.activation)\n        self.local_conv2 = layers.Conv2D(filters=self.units, kernel_size=(1, 1), activation=self.activation)\n        self.concat6 = layers.Add()\n        \n        if self.use_scale:\n            self.global_scale = self.add_weight(shape=(1, 1, 1, 1), initializer='ones', trainable=True, name='global_scale')\n            self.local_scale = self.add_weight(shape=(1, 1, 1, self.units), initializer='ones', trainable=True, name='local_scale')\n        \n        super(DeeperGlobalLocalAttentionLayer1, self).build(input_shape)\n\n    def call(self, inputs, training=None):\n        ##### Hierarchical Information Fusion Attention(HIFA) ######\n        \n        global_attention1 = self.global_conv1(inputs)\n        global_avg1 = self.global_avg_pooling1(global_attention1)\n        \n        global_attention2 = self.global_conv2(global_attention1)\n        global_avg2 = self.global_avg_pooling2(global_attention2)\n        \n        global_concat1 = self.concat1([global_avg1, global_avg2])\n        global_attention_concat1 = self.concat2([global_attention1, global_attention2])\n        \n        global_attention3 = self.global_conv3(global_attention_concat1)\n        global_avg3 = self.global_avg_pooling3(global_attention3)\n        \n        global_attention4 = self.global_conv4(global_attention3)\n        global_avg4 = self.global_avg_pooling4(global_attention4)\n        \n        global_concat2 = self.concat3([global_avg3, global_avg4])\n        global_attention_concat2 = self.concat4([global_attention3, global_attention4])\n        \n        global_avg_concat = self.concat5([global_concat1, global_concat2])\n        \n        global_attention = self.global_attention(global_avg_concat)\n        global_attention = tf.expand_dims(tf.expand_dims(global_attention, 1), 1)\n\n        ##### Channel-wise Local Information Attention (CLIA) ######\n        \n        local_attention1 = self.local_conv1(inputs)\n        local_attention1 = tf.reduce_mean(local_attention1, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        local_attention2 = self.local_conv2(local_attention1)\n        local_attention2 = tf.reduce_mean(local_attention2, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        \n        local_attention = self.concat6([local_attention1, local_attention2])\n        \n        # Scale Global and Local Attention\n        if self.use_scale:\n            global_attention *= self.global_scale\n            local_attention *= self.local_scale\n\n        # Combine Global and Local Attention\n        attention = tf.sigmoid(global_attention + local_attention)\n        return attention\n\n    def get_config(self):\n        config = super(DeeperGlobalLocalAttentionLayer1, self).get_config()\n        config.update({'units': self.units, 'activation': self.activation, 'dropout_rate': self.dropout_rate,\n                       'use_scale': self.use_scale})\n        return config\n\nclass DeeperAttentionLayer1(layers.Layer):\n    def __init__(self, units=64, use_scale=True, **kwargs):\n        super(DeeperAttentionLayer1, self).__init__(**kwargs)\n        self.units = units\n        self.use_scale = use_scale\n\n    def build(self, input_shape):\n        _, H, W, C = input_shape\n        self.alpha = self.add_weight(shape=(1, 1, 1, C), initializer='ones', trainable=True, name='alpha')\n        self.deeper_global_local_attention = DeeperGlobalLocalAttentionLayer1(units=self.units, activation='sigmoid', \n                                                                              dropout_rate=0.2,  # You can adjust the dropout rate\n                                                                              use_scale=self.use_scale)\n        super(DeeperAttentionLayer1, self).build(input_shape)\n\n    def call(self, inputs, training=None):\n        attention = self.deeper_global_local_attention(inputs, training=training)\n        attention_feature = inputs * attention * self.alpha\n        return attention_feature\n\n    def get_config(self):\n        config = super(DeeperAttentionLayer1, self).get_config()\n        config.update({'units': self.units, 'use_scale': self.use_scale})\n        return config\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Multimodal information fusion attention (MIFA)**","metadata":{}},{"cell_type":"code","source":"########## Multimodal information fusion attention (MIFA) ###############\n\n\n\nclass GlobalMinPooling2D(layers.Layer):\n    def __init__(self, **kwargs):\n        super(GlobalMinPooling2D, self).__init__(**kwargs)\n\n    def call(self, inputs):\n        return tf.reduce_min(inputs, axis=[1, 2])\n\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0], input_shape[-1])\n\n    def get_config(self):\n        config = super(GlobalMinPooling2D, self).get_config()\n        return config\n\n\nclass DeeperGlobalLocalAttentionLayer(layers.Layer):\n    def __init__(self, units, activation='sigmoid', dropout_rate=0.2, use_scale=True, axis=-1, **kwargs):\n        super(DeeperGlobalLocalAttentionLayer, self).__init__(**kwargs)\n        self.units = units\n        self.activation = activation\n        self.dropout_rate = dropout_rate\n        self.use_scale = use_scale\n        self.axis = axis\n\n    def build(self, input_shapes):\n        input_shape1, input_shape2 = input_shapes\n        _, _, _, channels1 = input_shape1\n        _, _, _, channels2 = input_shape2\n        \n        self.global_min_pooling1 = GlobalMinPooling2D()\n        self.global_avg_pooling1 = layers.GlobalAveragePooling2D()\n        self.global_max_pooling1 = layers.GlobalMaxPooling2D()\n        \n        self.global_attention = layers.Dense(units=self.units, activation=self.activation)\n        \n        self.global_min_pooling2 = GlobalMinPooling2D()\n        self.global_avg_pooling2 = layers.GlobalAveragePooling2D()\n        self.global_max_pooling2 = layers.GlobalMaxPooling2D()\n        \n        #self.global_attention2 = layers.Dense(units=self.units, activation=self.activation)\n        \n        \n        self.concat = layers.Add()\n        #self.global_attention3 = layers.Dense(units=self.units, activation=self.activation)\n        \n        self.local_conv1 = layers.Conv2D(filters=self.units, kernel_size=(1, 1), activation=self.activation)\n        self.local_conv2 = layers.Conv2D(filters=self.units, kernel_size=(1, 1), activation=self.activation)\n        \n        \n        \n        self.concat2 = layers.Add()\n        #self.local_conv5 = layers.Conv2D(filters=self.units, kernel_size=(1, 1), activation=self.activation)\n        \n        if self.use_scale:\n            self.global_scale = self.add_weight(shape=(1, 1, 1, 1), initializer='ones', trainable=True, name='global_scale')\n            self.local_scale = self.add_weight(shape=(1, 1, 1, self.units), initializer='ones', trainable=True, name='local_scale')\n        \n        super(DeeperGlobalLocalAttentionLayer, self).build(input_shapes)\n\n    def call(self, inputs, training=None):\n        inputs1, inputs2 = inputs\n\n        #########  Multimodal Global Information Fusion Attention (MGIFA) #########\n        global_min1 = self.global_min_pooling1(inputs1)\n        global_avg1 = self.global_avg_pooling1(inputs1)\n        global_max1 = self.global_max_pooling1(inputs1)\n\n        global_min2 = self.global_min_pooling2(inputs2)\n        global_avg2 = self.global_avg_pooling2(inputs2)\n        global_max2 = self.global_max_pooling2(inputs2)\n\n        concat_min = self.concat([global_min1, global_min2])\n        concat_avg = self.concat([global_avg1, global_avg2])\n        concat_max = self.concat([global_max1, global_max2])\n        \n        concat_min = self.global_attention(concat_min)\n        concat_avg = self.global_attention(concat_avg)\n        concat_max = self.global_attention(concat_max)\n        \n        concat_global_attention = self.concat([concat_min, concat_avg, concat_max])\n        \n        #global_attention = self.global_attention3(concat_global_attention)\n        \n        global_attention = tf.expand_dims(tf.expand_dims(concat_global_attention, 1), 1)\n\n        #########  Multimodal Local Information Fusion Attention (MLIFA) #########\n        \n        local_conv1 = self.local_conv1(inputs1)\n        local_min1 = tf.reduce_min(local_conv1, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        local_avg1 = tf.reduce_mean(local_conv1, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        local_max1 = tf.reduce_max(local_conv1, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        \n        local_conv2 = self.local_conv2(inputs2)\n        local_min2 = tf.reduce_min(local_conv2, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        local_avg2 = tf.reduce_mean(local_conv2, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        local_max2 = tf.reduce_max(local_conv2, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        \n        local_concat_min = self.concat2([local_min1, local_min2])\n        local_concat_avg = self.concat2([local_avg1, local_avg2])\n        local_concat_max = self.concat2([local_max1, local_max2])\n\n        local_attention = self.concat2([local_concat_min, local_concat_avg, local_concat_max])\n        \n        \n        # Scale Global and Local Attention\n        if self.use_scale:\n            global_attention *= self.global_scale\n            local_attention *= self.local_scale\n\n        # Combine Global and Local Attention\n        attention = tf.sigmoid(global_attention + local_attention)\n        return attention\n\n    def get_config(self):\n        config = super(DeeperGlobalLocalAttentionLayer, self).get_config()\n        config.update({'units': self.units, 'activation': self.activation, 'dropout_rate': self.dropout_rate,\n                       'use_scale': self.use_scale})\n        return config\n\nclass DeeperAttentionLayer(layers.Layer):\n    def __init__(self, units=64, use_scale=True,axis=-1, **kwargs):\n        super(DeeperAttentionLayer, self).__init__(**kwargs)\n        self.units = units\n        self.use_scale = use_scale\n        self.axis = axis \n\n    def build(self, input_shapes):\n        input_shape1, input_shape2 = input_shapes\n        _, H, W, C1 = input_shape1\n        _, H, W, C2 = input_shape2\n        \n        self.alpha1 = self.add_weight(shape=(1, 1, 1, C1), initializer='ones', trainable=True, name='alpha1')\n        self.alpha2 = self.add_weight(shape=(1, 1, 1, C2), initializer='ones', trainable=True, name='alpha2')\n        \n        self.deeper_global_local_attention = DeeperGlobalLocalAttentionLayer(units=self.units, activation='sigmoid', \n                                                                              dropout_rate=0.2,  # You can adjust the dropout rate\n                                                                              use_scale=self.use_scale)\n        #self.concat3 = layers.Add()\n        #self.concat4 = layers.Add()\n        \n        super(DeeperAttentionLayer, self).build(input_shapes)\n\n    def call(self, inputs, training=None):\n        inputs1, inputs2 = inputs\n        attention = self.deeper_global_local_attention([inputs1, inputs2], training=training)\n        \n        #inputs_concat = self.concat3([inputs1, inputs2])\n        #alpha_concat = self.concat4([self.alpha1, self.alpha2])\n        \n        attention_feature1 = inputs1 * attention * self.alpha1\n        attention_feature2 = inputs2 * attention * self.alpha2\n        \n        return attention_feature1, attention_feature2\n\n    def get_config(self):\n        config = super(DeeperAttentionLayer, self).get_config()\n        config.update({'units': self.units, 'use_scale': self.use_scale})\n        return config\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### RRA block ########\n\ndef RGSA(x, filters, strides=(1, 1), use_projection=False):\n    shortcut = x\n\n    # Define the first convolutional layer of the block\n    \n    x = Conv2D(filters=filters, kernel_size=(3, 3), strides=strides, padding='same', \n               #activation = 'relu'\n\n              )(x)\n    x = DeeperAttentionLayer1(units=filters, use_scale=True)(x)\n    x = BatchNormalization()(x)\n    x = tf.keras.layers.Activation('relu')(x)\n\n    # Define the second convolutional layer of the block\n    \n    x = Conv2D(filters=filters, kernel_size=(3, 3), padding='same')(x)\n    x = DeeperAttentionLayer1(units=filters, use_scale=True)(x)\n    \n    x = BatchNormalization()(x)\n\n    # If the stride is not (1, 1), the dimensions need to be adjusted\n    if strides != (1, 1) or use_projection:\n        \n        shortcut = Conv2D(filters=filters, kernel_size=(1, 1), strides=strides, padding='same')(shortcut)\n        shortcut = BatchNormalization()(shortcut)\n\n    # Add the shortcut (identity connection)\n    \n    x = tf.keras.layers.add([x, shortcut])\n    \n    x = tf.keras.layers.Activation('relu')(x)\n    return x\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**DRIFA-Net**","metadata":{}},{"cell_type":"code","source":"def residual_GLC_branch1(inputs1, inputs2):\n    \n    x1 = Conv2D(filters=64, kernel_size=(7, 7), strides=(2, 2), padding='same')(inputs1)\n    x1 = DeeperAttentionLayer1(units=64, use_scale=True)(x1) ## MFA ####\n    x1 = BatchNormalization()(x1)\n    x1 = tf.keras.layers.Activation('relu')(x1)\n    x1 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x1)\n    \n    x2 = Conv2D(filters=64, kernel_size=(7, 7), strides=(2, 2), padding='same')(inputs2)\n    x2 = DeeperAttentionLayer1(units=64, use_scale=True)(x2) ## MFA ####\n    x2 = BatchNormalization()(x2)\n    x2 = tf.keras.layers.Activation('relu')(x2)\n    x2 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x2)\n    \n\n    x1 = RGSA(x1, filters=64)\n    x1 = tf.keras.layers.Dropout(0.25)(x1, training = True)  ## MCD ####\n    x1 = DeeperAttentionLayer1(units=64, use_scale=True)(x1) ## MFA ####\n\n    x2 = RGSA(x2, filters=64)\n    x2 = tf.keras.layers.Dropout(0.25)(x2, training = True)  ## MCD ####\n    x2 = DeeperAttentionLayer1(units=64, use_scale=True)(x2)\n    \n    x1, x2 = DeeperAttentionLayer(units=64, use_scale=True)([x1, x2])  ## MIFA ####\n    \n    x1 = RGSA(x1, filters=64)\n    x1 = tf.keras.layers.Dropout(0.25)(x1, training = True)  ## MCD ####\n    x1 = DeeperAttentionLayer1(units=64, use_scale=True)(x1) ## MFA ####\n    \n    x2 = RGSA(x2, filters=64)\n    x2 = tf.keras.layers.Dropout(0.25)(x2, training = True)  ## MCD ####\n    x2 = DeeperAttentionLayer1(units=64, use_scale=True)(x2)\n\n    x1, x2 = DeeperAttentionLayer(units=64, use_scale=True)([x1, x2])  ## MIFA ####\n    \n    x1 = RGSA(x1, filters=128, strides=(2, 2), use_projection=True)\n    x1 = tf.keras.layers.Dropout(0.25)(x1, training = True)  ## MCD ####\n    x1 = DeeperAttentionLayer1(units=128, use_scale=True)(x1) ## MFA ####\n\n    x2 = RGSA(x2, filters=128, strides=(2, 2), use_projection=True)\n    x2 = tf.keras.layers.Dropout(0.25)(x2, training = True)  ## MCD ####\n    x2 = DeeperAttentionLayer1(units=128, use_scale=True)(x2)\n\n    x1, x2 = DeeperAttentionLayer(units=128, use_scale=True)([x1, x2])  ## MIFA ####\n    \n    x1 = RGSA(x1, filters=128)\n    x1 = tf.keras.layers.Dropout(0.25)(x1, training = True)  ## MCD ####\n    x1 = DeeperAttentionLayer1(units=128, use_scale=True)(x1)\n  \n    x2 = RGSA(x2, filters=128)\n    x2 = tf.keras.layers.Dropout(0.25)(x2, training = True)  ## MCD ####\n    x2 = DeeperAttentionLayer1(units=128, use_scale=True)(x2)\n\n    x1, x2 = DeeperAttentionLayer(units=128, use_scale=True)([x1, x2])  ## MIFA ####\n    \n    x1 = RGSA(x1, filters=256, strides=(2, 2), use_projection=True)\n    x1 = tf.keras.layers.Dropout(0.25)(x1, training = True)  ## MCD ####\n    x1 = DeeperAttentionLayer1(units=256, use_scale=True)(x1)\n    \n    x2 = RGSA(x2, filters=256, strides=(2, 2), use_projection=True)\n    x2 = tf.keras.layers.Dropout(0.25)(x2, training = True)  ## MCD ####\n    x2 = DeeperAttentionLayer1(units=256, use_scale=True)(x2)\n\n    x1, x2 = DeeperAttentionLayer(units=256, use_scale=True)([x1, x2])  ## MIFA ####\n    \n    \n    x1 = RGSA(x1, filters=256)\n    x1 = tf.keras.layers.Dropout(0.25)(x1, training = True)  ## MCD ####\n    x1 = DeeperAttentionLayer1(units=256, use_scale=True)(x1)\n    \n    x2 = RGSA(x2, filters=256)\n    x2 = tf.keras.layers.Dropout(0.25)(x2, training = True)  ## MCD ####\n    x2 = DeeperAttentionLayer1(units=256, use_scale=True)(x2)\n    \n    x1, x2 = DeeperAttentionLayer(units=256, use_scale=True)([x1, x2])  ## MIFA ####\n\n    x1 = RGSA(x1, filters=512, strides=(2, 2), use_projection=True)\n    x1 = DeeperAttentionLayer1(units=512, use_scale=True)(x1)\n    \n    x2 = RGSA(x2, filters=512, strides=(2, 2), use_projection=True)\n    x2 = DeeperAttentionLayer1(units=512, use_scale=True)(x2)\n\n    x1, x2 = DeeperAttentionLayer(units=512, use_scale=True)([x1, x2])  ## MIFA ####\n    \n    x1 = RGSA(x1, filters=512)\n    x2 = RGSA(x2, filters=512)\n    x1, x2 = DeeperAttentionLayer(units=512, use_scale=True)([x1, x2])\n    \n    return x1, x2","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#def build_resnet18(input_shape=(128, 128, 3), num_classes=2):\ninput_shape=(128, 128, 3)\ninputs1 = Input(shape=input_shape)\ninputs2 = Input(shape=input_shape)\n\n\n\n#input_data = Input(shape=input_shape, name='input_data')\n# Initial convolutional layer\n\nx1, x2 = residual_GLC_branch1(inputs1, inputs2)\n#print('x:',x.shape)\n\ncon = tf.keras.layers.Concatenate(axis=-1)([x1, x2])\n\ncon = tf.keras.layers.Dropout(0.25)(con, training = True)  ## MCD ####\n\nx = GlobalAveragePooling2D()(con)\nprint('GlobalAveragePooling2D x:',x.shape)\n\noutputs1 = Dense(5, activation='softmax')(x)\noutputs2 = Dense(7, activation='softmax')(x)\n\n# Create the model\nmodel = Model([inputs1, inputs2], [outputs1, outputs2])\n#return model\nprint(model.summary())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\n\ninitial_gamma = 0.5\n\noptimizer = Adam(learning_rate=0.001)\n# Compile the model with the custom optimizer\nmodel.compile(optimizer=optimizer,\n              loss=['categorical_crossentropy', 'categorical_crossentropy'],\n              loss_weights=[initial_gamma, (1 -  initial_gamma)],\n              metrics=['accuracy', 'accuracy'])\n\n\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\ndef checkpoint_callback():\n\n    checkpoint_filepath = 'best1_model_cer_skin_lung.keras'\n\n    model_checkpoint_callback= ModelCheckpoint(filepath=checkpoint_filepath,\n                           save_weights_only=False,\n                           #frequency='epoch',\n                           monitor='val_loss',\n                           save_best_only=True,\n                            mode='min',\n                           verbose=0)\n\n    return model_checkpoint_callback\n\ndef early_stopping(patience):\n    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, verbose=1)\n    return es_callback\n\n\n\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n                              patience=5, min_lr=0.00001)\n\ncheckpoint_callback = checkpoint_callback()\n\nearly_stopping = early_stopping(patience=100)\ncallbacks = [checkpoint_callback, early_stopping, reduce_lr]\n            \n\n# Fit the model with callbacks\nhistory = model.fit([X_train_s, X_train_h], [y_train_s, y_train_h],\n                    epochs=200,\n                    validation_split=0.2, verbose=1,\n                    shuffle=True,\n                    callbacks=callbacks) # UpdateGammaCallback\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.evaluate([X_test_s, X_test_h1], [y_test_s, y_test_h1])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nmodel.evaluate([X_test_s1, X_test_h], [y_test_s1, y_test_h])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, Model\nimport tensorflow.keras.backend as K\n\ndef dct_2d(x):\n    return tf.signal.dct(tf.signal.dct(x, type=2, norm='ortho'), type=2, norm='ortho', axis=2)\n\nclass DCTAttentionNonLocalBlock(Model):\n    def __init__(self, in_channels, intermediate_channels, dct_threshold=0.1):\n        super(DCTAttentionNonLocalBlock, self).__init__()\n        self.in_channels = in_channels\n        self.intermediate_channels = intermediate_channels\n        self.dct_threshold = dct_threshold\n        \n        self.theta = layers.Conv2D(intermediate_channels, kernel_size=1, use_bias=False)\n        self.phi = layers.Conv2D(intermediate_channels, kernel_size=1, use_bias=False)\n        self.g = layers.Conv2D(intermediate_channels, kernel_size=1, use_bias=False)\n        self.W = layers.Conv2D(in_channels, kernel_size=1, use_bias=False)\n        \n        self.attn_refine = tf.keras.Sequential([\n            layers.Conv2D(in_channels, kernel_size=3, padding='same', use_bias=False),\n            layers.ReLU(),\n            layers.Conv2D(in_channels, kernel_size=3, padding='same', use_bias=False)\n        ])\n        \n        self.channel_match_conv = layers.Conv2D(intermediate_channels, kernel_size=1)\n        self.dct_attention_conv = layers.Conv2D(intermediate_channels, kernel_size=1)\n        self.reduce_channels_dct = layers.Conv2D(intermediate_channels, kernel_size=1)\n        \n        self.relative_bias_dct = tf.Variable(tf.zeros((1, 1, 1, 1)), trainable=True)\n        self.relative_bias_non_local = tf.Variable(tf.zeros((1, 1, 1, 1)), trainable=True)\n    \n    def dct_transform(self, inputs):\n        dct_input = dct_2d(inputs)\n        \n        gap = tf.reduce_mean(dct_input, axis=[1, 2], keepdims=True)\n        gmp = tf.reduce_max(dct_input, axis=[1, 2], keepdims=True)\n        gmp_min = tf.reduce_min(dct_input, axis=[1, 2], keepdims=True)\n        \n        pooled_features1 = gap + gmp + gmp_min\n        pooled_features2 = gmp - (gap - gmp_min)\n        pooled_features = pooled_features1 + pooled_features2\n        \n        return pooled_features\n    \n    def frequency_attention(self, dct_input):\n        threshold_idx = int(self.dct_threshold * tf.shape(dct_input)[-1])\n        mask = tf.ones_like(dct_input)\n        #mask = tf.tensor_scatter_nd_update(mask, [[..., threshold_idx:]], tf.zeros_like(dct_input[..., threshold_idx:]))\n        mask = tf.concat([tf.ones_like(dct_input[..., :threshold_idx]), tf.zeros_like(dct_input[..., threshold_idx:])], axis=-1)\n\n        return dct_input * mask\n\n    #@tf.function\n    def call(self, inputs):\n        shape = tf.shape(inputs)  #  Fix: Store shape in a variable\n        batch_size = shape[0]\n        height = shape[1]\n        width = shape[2]\n        in_channels = shape[3]\n        \n        dct_input = self.dct_transform(inputs)\n    \n        '''def call(self, inputs):\n        batch_size, height, width, in_channels = tf.shape(inputs)\n        \n        dct_input = self.dct_transform(inputs)'''\n    \n        dct_attention = self.frequency_attention(dct_input) + self.relative_bias_dct\n        \n        theta_x = tf.reshape(self.theta(inputs), [batch_size, -1, self.intermediate_channels])\n        phi_x = tf.reshape(self.phi(inputs), [batch_size, -1, self.intermediate_channels])\n        g_x = tf.reshape(self.g(inputs), [batch_size, -1, self.intermediate_channels])\n        \n        f = tf.matmul(theta_x, phi_x, transpose_b=True)\n        f_softmax = tf.nn.softmax(f, axis=-1)\n        y = tf.matmul(f_softmax, g_x)\n        \n        y = tf.reshape(y, [batch_size, height, width, self.intermediate_channels])\n        \n        refined_attention = self.attn_refine(inputs) + self.relative_bias_non_local\n        refined_attention = self.channel_match_conv(refined_attention)\n        \n        scale_factor = tf.sigmoid(refined_attention)\n        refined_dct_attention = self.reduce_channels_dct(self.dct_transform(dct_attention))\n        dct_attention = self.reduce_channels_dct(dct_attention)\n      \n        \n        y1 = y + (y * scale_factor) + refined_attention\n        y2 = y - (y * scale_factor) - refined_attention\n        y = y1 + y2 + refined_dct_attention + dct_attention\n        \n        y = self.W(y)\n        return tf.nn.relu(inputs + y)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''### RRA block ########\n\ndef RGSA(x, filters, strides=(1, 1), use_projection=False):\n    shortcut = x\n\n    # Define the first convolutional layer of the block\n    \n    #x = Conv2D(filters=filters, kernel_size=(3, 3), strides=strides, padding='same')(x)\n    x = tf.keras.layers.DepthwiseConv2D(kernel_size=(3, 3), strides=strides,  padding=\"same\", depth_multiplier=1)(x)\n    x = Conv2D(filters, 1, padding=\"same\")(x)\n    \n    #x = DeeperAttentionLayer1(units=filters, use_scale=True)(x)\n    dct_attention_block = DCTAttentionNonLocalBlock(filters, filters/2)\n    x = dct_attention_block(x)\n    print('dct 1:',x.shape)\n    x = BatchNormalization()(x)\n    x = tf.keras.layers.Activation('relu')(x)\n\n    # Define the second convolutional layer of the block\n    \n    #x = Conv2D(filters=filters, kernel_size=(3, 3), padding='same')(x)\n    x = tf.keras.layers.DepthwiseConv2D(kernel_size=(3, 3), padding=\"same\", depth_multiplier=1)(x)\n    x = Conv2D(filters, 1, padding=\"same\")(x)\n    \n    #x = DeeperAttentionLayer1(units=filters, use_scale=True)(x)\n    dct_attention_block = DCTAttentionNonLocalBlock(filters, filters/2)\n    x = dct_attention_block(x)\n    print('dct 2:',x.shape)\n    \n    \n    x = BatchNormalization()(x)\n\n    # If the stride is not (1, 1), the dimensions need to be adjusted\n    if strides != (1, 1) or use_projection:\n        \n        shortcut = Conv2D(filters=filters, kernel_size=(1, 1), strides=strides, padding='same')(shortcut)\n        shortcut = BatchNormalization()(shortcut)\n\n    # Add the shortcut (identity connection)\n    \n    x = tf.keras.layers.add([x, shortcut])\n    \n    x = tf.keras.layers.Activation('relu')(x)\n    return x'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers\n\n# Define DCT Transformation\ndef dct_2d(x):\n    return tf.signal.dct(tf.signal.dct(x, type=2, norm='ortho'), type=2, norm='ortho', axis=2)\n\n# DCT Attention Block\ndef dct_transform(inputs):\n    dct_input = dct_2d(inputs)\n    gap = tf.reduce_mean(dct_input, axis=[1, 2], keepdims=True)\n    gmp = tf.reduce_max(dct_input, axis=[1, 2], keepdims=True)\n    gmp_min = tf.reduce_min(dct_input, axis=[1, 2], keepdims=True)\n    \n    pooled_features1 = gap + gmp + gmp_min\n    pooled_features2 = gmp - (gap - gmp_min)\n    pooled_features = pooled_features1 + pooled_features2\n    \n    return pooled_features\n\n# Frequency Attention\ndef frequency_attention(dct_input, dct_threshold):\n    threshold_idx = int(dct_threshold * tf.shape(dct_input)[-1])\n    mask = tf.concat([tf.ones_like(dct_input[..., :threshold_idx]), tf.zeros_like(dct_input[..., threshold_idx:])], axis=-1)\n    return dct_input * mask\n\n# Main DCT Attention Non-Local Block\ndef dct_attention_non_local_block(inputs, in_channels, intermediate_channels, dct_threshold=0.1):\n    batch_size, height, width, _ = tf.unstack(tf.shape(inputs))\n    \n    # Layers\n    theta = layers.Conv2D(intermediate_channels, kernel_size=1, use_bias=False)(inputs)\n    phi = layers.Conv2D(intermediate_channels, kernel_size=1, use_bias=False)(inputs)\n    g = layers.Conv2D(intermediate_channels, kernel_size=1, use_bias=False)(inputs)\n    W = layers.Conv2D(in_channels, kernel_size=1, use_bias=False)\n    \n    attn_refine = tf.keras.Sequential([\n        layers.Conv2D(in_channels, kernel_size=3, padding='same', use_bias=False),\n        layers.ReLU(),\n        layers.Conv2D(in_channels, kernel_size=3, padding='same', use_bias=False)\n    ])\n    \n    channel_match_conv = layers.Conv2D(intermediate_channels, kernel_size=1)\n    reduce_channels_dct = layers.Conv2D(intermediate_channels, kernel_size=1)\n    \n    relative_bias_dct = tf.Variable(tf.zeros((1, 1, 1, 1)), trainable=True)\n    relative_bias_non_local = tf.Variable(tf.zeros((1, 1, 1, 1)), trainable=True)\n    \n    dct_input = dct_transform(inputs)\n    dct_attention = frequency_attention(dct_input, dct_threshold) + relative_bias_dct\n    \n    theta_x = tf.reshape(theta, [batch_size, -1, intermediate_channels])\n    phi_x = tf.reshape(phi, [batch_size, -1, intermediate_channels])\n    g_x = tf.reshape(g, [batch_size, -1, intermediate_channels])\n    \n    f = tf.matmul(theta_x, phi_x, transpose_b=True)\n    f_softmax = tf.nn.softmax(f, axis=-1)\n    y = tf.matmul(f_softmax, g_x)\n    y = tf.reshape(y, [batch_size, height, width, intermediate_channels])\n    \n    refined_attention = attn_refine(inputs) + relative_bias_non_local\n    refined_attention = channel_match_conv(refined_attention)\n    \n    scale_factor = tf.sigmoid(refined_attention)\n    refined_dct_attention = reduce_channels_dct(dct_transform(dct_attention))\n    dct_attention = reduce_channels_dct(dct_attention)\n    \n    y1 = y + (y * scale_factor) + refined_attention\n    y2 = y - (y * scale_factor) - refined_attention\n    y = y1 + y2 + refined_dct_attention + dct_attention\n    \n    y = W(y)\n    return tf.nn.relu(inputs + y)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def dct_attention_non_local_block(inputs, in_channels, intermediate_channels, dct_threshold=0.1):\n    batch_size = tf.shape(inputs)[0]  # Extract batch size dynamically\n    height = tf.shape(inputs)[1]\n    width = tf.shape(inputs)[2]\n\n    # Layers\n    theta = layers.Conv2D(intermediate_channels, kernel_size=1, use_bias=False)(inputs)\n    phi = layers.Conv2D(intermediate_channels, kernel_size=1, use_bias=False)(inputs)\n    g = layers.Conv2D(intermediate_channels, kernel_size=1, use_bias=False)(inputs)\n    W = layers.Conv2D(in_channels, kernel_size=1, use_bias=False)\n\n    attn_refine = tf.keras.Sequential([\n        layers.Conv2D(in_channels, kernel_size=3, padding='same', use_bias=False),\n        layers.ReLU(),\n        layers.Conv2D(in_channels, kernel_size=3, padding='same', use_bias=False)\n    ])\n\n    channel_match_conv = layers.Conv2D(intermediate_channels, kernel_size=1)\n    reduce_channels_dct = layers.Conv2D(intermediate_channels, kernel_size=1)\n\n    relative_bias_dct = tf.Variable(tf.zeros((1, 1, 1, 1)), trainable=True)\n    relative_bias_non_local = tf.Variable(tf.zeros((1, 1, 1, 1)), trainable=True)\n\n    dct_input = dct_transform(inputs)\n    dct_attention = frequency_attention(dct_input, dct_threshold) + relative_bias_dct\n\n    theta_x = tf.reshape(theta, [batch_size, -1, intermediate_channels])\n    phi_x = tf.reshape(phi, [batch_size, -1, intermediate_channels])\n    g_x = tf.reshape(g, [batch_size, -1, intermediate_channels])\n\n    f = tf.matmul(theta_x, phi_x, transpose_b=True)\n    f_softmax = tf.nn.softmax(f, axis=-1)\n    y = tf.matmul(f_softmax, g_x)\n    y = tf.reshape(y, [batch_size, height, width, intermediate_channels])\n\n    refined_attention = attn_refine(inputs) + relative_bias_non_local\n    refined_attention = channel_match_conv(refined_attention)\n\n    scale_factor = tf.sigmoid(refined_attention)\n    refined_dct_attention = reduce_channels_dct(dct_transform(dct_attention))\n    dct_attention = reduce_channels_dct(dct_attention)\n\n    y1 = y + (y * scale_factor) + refined_attention\n    y2 = y - (y * scale_factor) - refined_attention\n    y = y1 + y2 + refined_dct_attention + dct_attention\n\n    y = W(y)\n    return tf.nn.relu(inputs + y)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers\n\n# Define DCT Transformation\ndef dct_2d(x):\n    return tf.signal.dct(tf.signal.dct(x, type=2, norm='ortho'), type=2, norm='ortho', axis=2)\n\n# DCT Attention Block\ndef dct_transform(inputs):\n    dct_input = dct_2d(inputs)\n    gap = tf.reduce_mean(dct_input, axis=[1, 2], keepdims=True)\n    gmp = tf.reduce_max(dct_input, axis=[1, 2], keepdims=True)\n    gmp_min = tf.reduce_min(dct_input, axis=[1, 2], keepdims=True)\n    \n    pooled_features1 = gap + gmp + gmp_min\n    pooled_features2 = gmp - (gap - gmp_min)\n    pooled_features = pooled_features1 + pooled_features2\n    \n    return pooled_features\n\n# Frequency Attention\ndef frequency_attention(dct_input, dct_threshold):\n    threshold_idx = int(dct_threshold * tf.shape(dct_input)[-1])\n    mask = tf.concat([tf.ones_like(dct_input[..., :threshold_idx]), tf.zeros_like(dct_input[..., threshold_idx:])], axis=-1)\n    return dct_input * mask\n\n# Main DCT Attention Non-Local Block\ndef dct_attention_non_local_block(inputs, in_channels, intermediate_channels, dct_threshold=0.1):\n    shape = tf.shape(inputs)\n    batch_size, height, width = shape[0], shape[1], shape[2]\n\n    # Layers\n    theta = layers.Conv2D(intermediate_channels, kernel_size=1, use_bias=False)(inputs)\n    phi = layers.Conv2D(intermediate_channels, kernel_size=1, use_bias=False)(inputs)\n    g = layers.Conv2D(intermediate_channels, kernel_size=1, use_bias=False)(inputs)\n    W = layers.Conv2D(in_channels, kernel_size=1, use_bias=False)\n\n    attn_refine = tf.keras.Sequential([\n        layers.Conv2D(in_channels, kernel_size=3, padding='same', use_bias=False),\n        layers.ReLU(),\n        layers.Conv2D(in_channels, kernel_size=3, padding='same', use_bias=False)\n    ])\n    \n    channel_match_conv = layers.Conv2D(intermediate_channels, kernel_size=1)\n    reduce_channels_dct = layers.Conv2D(intermediate_channels, kernel_size=1)\n\n    relative_bias_dct = tf.Variable(tf.zeros((1, 1, 1, 1)), trainable=True)\n    relative_bias_non_local = tf.Variable(tf.zeros((1, 1, 1, 1)), trainable=True)\n\n    dct_input = dct_transform(inputs)\n    dct_attention = frequency_attention(dct_input, dct_threshold) + relative_bias_dct\n\n    theta_x = tf.reshape(theta, [batch_size, height * width, intermediate_channels])\n    phi_x = tf.reshape(phi, [batch_size, height * width, intermediate_channels])\n    g_x = tf.reshape(g, [batch_size, height * width, intermediate_channels])\n\n    f = tf.matmul(theta_x, phi_x, transpose_b=True)\n    f_softmax = tf.nn.softmax(f, axis=-1)\n    y = tf.matmul(f_softmax, g_x)\n    y = tf.reshape(y, [batch_size, height, width, intermediate_channels])\n\n    refined_attention = attn_refine(inputs) + relative_bias_non_local\n    refined_attention = channel_match_conv(refined_attention)\n\n    scale_factor = tf.sigmoid(refined_attention)\n    refined_dct_attention = reduce_channels_dct(dct_transform(dct_attention))\n    dct_attention = reduce_channels_dct(dct_attention)\n\n    y1 = y + (y * scale_factor) + refined_attention\n    y2 = y - (y * scale_factor) - refined_attention\n    y = y1 + y2 + refined_dct_attention + dct_attention\n\n    y = W(y)\n    return tf.nn.relu(inputs + y)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Conv2D, DepthwiseConv2D, BatchNormalization, Activation, Add\n\ndef RGSA(x, filters, strides=(1, 1), use_projection=False):\n    shortcut = x  # Save the original input for residual connection\n\n    # First Depthwise and Pointwise Convolution\n    x = DepthwiseConv2D(kernel_size=(3, 3), strides=strides, padding=\"same\", depth_multiplier=1)(x)\n    x = Conv2D(filters, kernel_size=1, padding=\"same\")(x)\n    \n    # Apply DCT Attention Block\n    x = dct_attention_non_local_block(x, filters, filters)\n    print('DCT 1:', x.shape)\n\n    # Normalization and Activation\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    # Second Depthwise and Pointwise Convolution\n    x = DepthwiseConv2D(kernel_size=(3, 3), padding=\"same\", depth_multiplier=1)(x)\n    x = Conv2D(filters, kernel_size=1, padding=\"same\")(x)\n\n    # Apply DCT Attention Block Again\n    x = dct_attention_non_local_block(x, filters, filters)\n    print('DCT 2:', x.shape)\n\n    # Normalization\n    x = BatchNormalization()(x)\n\n    # Adjust Shortcut if Needed\n    if strides != (1, 1) or use_projection:\n        shortcut = Conv2D(filters=filters, kernel_size=(1, 1), strides=strides, padding='same')(shortcut)\n        shortcut = BatchNormalization()(shortcut)\n\n    # Residual Connection\n    x = Add()([x, shortcut])\n\n    # Final Activation\n    x = Activation('relu')(x)\n\n    return x\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers\n\n# Define DCT Transformation\ndef dct_2d(x):\n    return tf.signal.dct(tf.signal.dct(x, type=2, norm='ortho'), type=2, norm='ortho', axis=2)\n\n# DCT Attention Block\ndef dct_transform(inputs):\n    dct_input = dct_2d(inputs)\n    gap = tf.reduce_mean(dct_input, axis=[1, 2], keepdims=True)\n    gmp = tf.reduce_max(dct_input, axis=[1, 2], keepdims=True)\n    gmp_min = tf.reduce_min(dct_input, axis=[1, 2], keepdims=True)\n    \n    pooled_features1 = gap + gmp + gmp_min\n    pooled_features2 = gmp - (gap - gmp_min)\n    pooled_features = pooled_features1 + pooled_features2\n    \n    return pooled_features\n\n# Frequency Attention\ndef frequency_attention(dct_input, dct_threshold):\n    threshold_idx = int(dct_threshold * tf.shape(dct_input)[-1])\n    mask = tf.concat([tf.ones_like(dct_input[..., :threshold_idx]), tf.zeros_like(dct_input[..., threshold_idx:])], axis=-1)\n    return dct_input * mask\n\n# Main DCT Attention Non-Local Block\ndef dct_attention_non_local_block(inputs, in_channels, intermediate_channels, dct_threshold=0.1):\n    shape = tf.shape(inputs)\n    batch_size, height, width = shape[0], shape[1], shape[2]\n\n    # Layers\n    theta = layers.Conv2D(intermediate_channels, kernel_size=1, use_bias=False)(inputs)\n    phi = layers.Conv2D(intermediate_channels, kernel_size=1, use_bias=False)(inputs)\n    g = layers.Conv2D(intermediate_channels, kernel_size=1, use_bias=False)(inputs)\n    W = layers.Conv2D(in_channels, kernel_size=1, use_bias=False)\n\n    attn_refine = tf.keras.Sequential([\n        layers.Conv2D(in_channels, kernel_size=3, padding='same', use_bias=False),\n        layers.ReLU(),\n        layers.Conv2D(in_channels, kernel_size=3, padding='same', use_bias=False)\n    ])\n    \n    channel_match_conv = layers.Conv2D(intermediate_channels, kernel_size=1)\n    reduce_channels_dct = layers.Conv2D(intermediate_channels, kernel_size=1)\n\n    relative_bias_dct = tf.Variable(tf.zeros((1, 1, 1, 1)), trainable=True)\n    relative_bias_non_local = tf.Variable(tf.zeros((1, 1, 1, 1)), trainable=True)\n\n    dct_input = dct_transform(inputs)\n    dct_attention = frequency_attention(dct_input, dct_threshold) + relative_bias_dct\n\n    theta_x = tf.reshape(theta, [batch_size, height * width, intermediate_channels])\n    phi_x = tf.reshape(phi, [batch_size, height * width, intermediate_channels])\n    g_x = tf.reshape(g, [batch_size, height * width, intermediate_channels])\n\n    f = tf.matmul(theta_x, phi_x, transpose_b=True)\n    f_softmax = tf.nn.softmax(f, axis=-1)\n    y = tf.matmul(f_softmax, g_x)\n    y = tf.reshape(y, [batch_size, height, width, intermediate_channels])\n\n    refined_attention = attn_refine(inputs) + relative_bias_non_local\n    refined_attention = channel_match_conv(refined_attention)\n\n    scale_factor = tf.sigmoid(refined_attention)\n    refined_dct_attention = reduce_channels_dct(dct_transform(dct_attention))\n    dct_attention = reduce_channels_dct(dct_attention)\n\n    print('refined_dct_attention shape:', refined_dct_attention.shape)\n    print('dct_attention shape:', dct_attention.shape)\n    \n    y1 = y + (y * scale_factor) + refined_attention\n    y2 = y - (y * scale_factor) - refined_attention\n    y = y1 + y2 + refined_dct_attention + dct_attention\n\n    y = W(y)\n    return tf.nn.relu(inputs + y)\n\n# RGSA Block with DCT Attention\nfrom tensorflow.keras.layers import DepthwiseConv2D, BatchNormalization, Activation, Add\n\ndef RGSA(x, filters, strides=(1, 1), use_projection=False):\n    shortcut = x  # Save the original input for residual connection\n\n    # First Depthwise and Pointwise Convolution\n    x = DepthwiseConv2D(kernel_size=(3, 3), strides=strides, padding=\"same\", depth_multiplier=1)(x)\n    x = Conv2D(filters, kernel_size=1, padding=\"same\")(x)\n    \n    # Apply DCT Attention Block\n    x = dct_attention_non_local_block(x, filters, filters)\n    print('DCT 1:', x.shape)\n\n    # Normalization and Activation\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    # Second Depthwise and Pointwise Convolution\n    x = DepthwiseConv2D(kernel_size=(3, 3), padding=\"same\", depth_multiplier=1)(x)\n    x = Conv2D(filters, kernel_size=1, padding=\"same\")(x)\n\n    # Apply DCT Attention Block Again\n    x = dct_attention_non_local_block(x, filters, filters)\n    print('DCT 2:', x.shape)\n\n    # Normalization\n    x = BatchNormalization()(x)\n\n    # Adjust Shortcut if Needed\n    if strides != (1, 1) or use_projection:\n        shortcut = Conv2D(filters=filters, kernel_size=(1, 1), strides=strides, padding='same')(shortcut)\n        shortcut = BatchNormalization()(shortcut)\n\n    # Residual Connection\n    x = Add()([x, shortcut])\n\n    # Final Activation\n    x = Activation('relu')(x)\n\n    return x\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers\n\n\n\nclass DCTAttentionNonLocalBlock(layers.Layer):\n    def __init__(self, in_channels, intermediate_channels, dct_threshold=0.1):\n        super(DCTAttentionNonLocalBlock, self).__init__()\n        self.in_channels = in_channels\n        self.intermediate_channels = intermediate_channels\n        self.dct_threshold = dct_threshold\n        \n        # Define your layers here\n        self.theta = layers.Conv2D(intermediate_channels, kernel_size=1, use_bias=False)\n        self.phi = layers.Conv2D(intermediate_channels, kernel_size=1, use_bias=False)\n        self.g = layers.Conv2D(intermediate_channels, kernel_size=1, use_bias=False)\n        self.W = layers.Conv2D(in_channels, kernel_size=1, use_bias=False)\n        \n        self.attn_refine = tf.keras.Sequential([\n            layers.Conv2D(in_channels, kernel_size=3, padding='same', use_bias=False),\n            layers.ReLU(),\n            layers.Conv2D(in_channels, kernel_size=3, padding='same', use_bias=False)\n        ])\n        \n        self.channel_match_conv = layers.Conv2D(intermediate_channels, kernel_size=1)\n        self.reduce_channels_dct = layers.Conv2D(intermediate_channels, kernel_size=1)\n\n        self.relative_bias_dct = tf.Variable(tf.zeros((1, 1, 1, 1)), trainable=True)\n        self.relative_bias_non_local = tf.Variable(tf.zeros((1, 1, 1, 1)), trainable=True)\n\n    def call(self, inputs):\n        # Get the shape of the input\n        shape = tf.shape(inputs)\n        batch_size, height, width = shape[0], shape[1], shape[2]\n\n        # Process the inputs\n        dct_input = dct_transform(inputs)  # Apply DCT transform\n        dct_attention = frequency_attention(dct_input, self.dct_threshold) + self.relative_bias_dct\n\n        theta_x = tf.reshape(self.theta(inputs), [batch_size, height * width, self.intermediate_channels])\n        phi_x = tf.reshape(self.phi(inputs), [batch_size, height * width, self.intermediate_channels])\n        g_x = tf.reshape(self.g(inputs), [batch_size, height * width, self.intermediate_channels])\n\n        f = tf.matmul(theta_x, phi_x, transpose_b=True)\n        f_softmax = tf.nn.softmax(f, axis=-1)\n        y = tf.matmul(f_softmax, g_x)\n        y = tf.reshape(y, [batch_size, height, width, self.intermediate_channels])\n\n        refined_attention = self.attn_refine(inputs) + self.relative_bias_non_local\n        refined_attention = self.channel_match_conv(refined_attention)\n\n        scale_factor = tf.sigmoid(refined_attention)\n        refined_dct_attention = self.reduce_channels_dct(dct_transform(dct_attention))\n        dct_attention = self.reduce_channels_dct(dct_attention)\n\n        y1 = y + (y * scale_factor) + refined_attention\n        y2 = y - (y * scale_factor) - refined_attention\n        y = y1 + y2 + refined_dct_attention + dct_attention\n\n        y = self.W(y)\n        return tf.nn.relu(inputs + y)\n\n# Usage example\ndef RGSA(x, filters, strides=(1, 1), use_projection=False):\n    shortcut = x  # Save the original input for residual connection\n\n    # First Depthwise and Pointwise Convolution\n    x = layers.DepthwiseConv2D(kernel_size=(3, 3), strides=strides, padding=\"same\", depth_multiplier=1)(x)\n    x = layers.Conv2D(filters, kernel_size=1, padding=\"same\")(x)\n    \n    # Apply DCT Attention Block\n    x = DCTAttentionNonLocalBlock(filters, filters)(x)\n    print('DCT 1:', x.shape)\n\n    # Normalization and Activation\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation('relu')(x)\n\n    # Second Depthwise and Pointwise Convolution\n    x = layers.DepthwiseConv2D(kernel_size=(3, 3), padding=\"same\", depth_multiplier=1)(x)\n    x = layers.Conv2D(filters, kernel_size=1, padding=\"same\")(x)\n\n    # Apply DCT Attention Block Again\n    x = DCTAttentionNonLocalBlock(filters, filters)(x)\n    print('DCT 2:', x.shape)\n\n    # Normalization\n    x = layers.BatchNormalization()(x)\n\n    # Adjust Shortcut if Needed\n    if strides != (1, 1) or use_projection:\n        shortcut = layers.Conv2D(filters=filters, kernel_size=(1, 1), strides=strides, padding='same')(shortcut)\n        shortcut = layers.BatchNormalization()(shortcut)\n\n    # Residual Connection\n    x = layers.Add()([x, shortcut])\n\n    # Final Activation\n    x = layers.Activation('relu')(x)\n\n    return x\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers\n\n'''def dct_2d(x):\n    return tf.signal.dct(tf.signal.dct(x, type=2, norm='ortho'), type=2, norm='ortho', axis=2)\n'''\n\n'''def dct_2d(x):\n    \"\"\"\n    Applies the 2D Discrete Cosine Transform (DCT) along the spatial dimensions (height, width).\n    \"\"\"\n    batch_size, height, width, channels = tf.shape(x)[0], tf.shape(x)[1], tf.shape(x)[2], tf.shape(x)[3]\n\n    # Reshape to make height and width combined in the last dimension\n    x_reshaped = tf.reshape(x, (batch_size, height * width, channels))\n\n    # Apply DCT along the last dimension (axis=-1) for both height and width\n    dct_input = tf.signal.dct(x_reshaped, type=2, norm='ortho', axis=-1)\n\n    # Reshape back to the original shape\n    dct_input = tf.reshape(dct_input, (batch_size, height, width, channels))\n    \n    return dct_input'''\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, Input\n\n\nclass FrequencyTransformLayer(layers.Layer):\n    def __init__(self, transform_type='dct', **kwargs):\n        super(FrequencyTransformLayer, self).__init__(**kwargs)\n        self.transform_type = transform_type\n\n    def call(self, inputs):\n        # Ensure inputs are float32 for DCT and FFT operations\n        inputs = tf.cast(inputs, tf.float32)\n\n        if self.transform_type == 'dct':\n            # Apply 2D DCT along the last axis\n            input_shape = tf.shape(inputs)\n            flattened_inputs = tf.reshape(inputs, [-1, input_shape[-1]])  # Flatten along the last axis\n            dct_transformed = tf.signal.dct(flattened_inputs, type=2, norm='ortho')\n            return tf.reshape(dct_transformed, input_shape)  # Reshape back to original dimensions\n        elif self.transform_type == 'fft':\n            # Apply 2D FFT and return the magnitude\n            fft_transformed = tf.signal.fft2d(tf.cast(inputs, tf.complex64))\n            return tf.math.abs(fft_transformed)\n        else:\n            raise ValueError(\"Unsupported transform type. Choose 'dct' or 'fft'.\")\n\n    def get_config(self):\n        config = super(FrequencyTransformLayer, self).get_config()\n        config.update({'transform_type': self.transform_type})\n        return config\n\n    \n# DCT Attention Block\ndef dct_transform(inputs):\n    #self.dct_transform = FrequencyTransformLayer(transform_type='dct')\n    dct_input = FrequencyTransformLayer(transform_type='dct')(inputs)\n    #dct_input = dct_2d(inputs)\n    gap = tf.reduce_mean(dct_input, axis=[1, 2], keepdims=True)\n    gmp = tf.reduce_max(dct_input, axis=[1, 2], keepdims=True)\n    gmp_min = tf.reduce_min(dct_input, axis=[1, 2], keepdims=True)\n    \n    pooled_features1 = gap + gmp + gmp_min\n    pooled_features2 = gmp - (gap - gmp_min)\n    pooled_features = pooled_features1 + pooled_features2\n    \n    return pooled_features\n# Frequency Attention\ndef frequency_attention(dct_input, dct_threshold):\n    threshold_idx = int(dct_threshold * tf.shape(dct_input)[-1])\n    mask = tf.concat([tf.ones_like(dct_input[..., :threshold_idx]), tf.zeros_like(dct_input[..., threshold_idx:])], axis=-1)\n    return dct_input * mask\n\n\nclass DCTAttentionNonLocalBlock(layers.Layer):\n    def __init__(self, in_channels, intermediate_channels, dct_threshold=0.1):\n        super(DCTAttentionNonLocalBlock, self).__init__()\n        self.in_channels = in_channels\n        self.intermediate_channels = intermediate_channels\n        self.dct_threshold = dct_threshold\n        \n        # Define your layers here\n        self.theta = layers.Conv2D(intermediate_channels, kernel_size=1, use_bias=False)\n        self.phi = layers.Conv2D(intermediate_channels, kernel_size=1, use_bias=False)\n        self.g = layers.Conv2D(intermediate_channels, kernel_size=1, use_bias=False)\n        self.W = layers.Conv2D(in_channels, kernel_size=1, use_bias=False)\n        \n        self.attn_refine = tf.keras.Sequential([\n            layers.Conv2D(in_channels, kernel_size=3, padding='same', use_bias=False),\n            layers.ReLU(),\n            layers.Conv2D(in_channels, kernel_size=3, padding='same', use_bias=False)\n        ])\n        \n        self.channel_match_conv = layers.Conv2D(intermediate_channels, kernel_size=1)\n        self.reduce_channels_dct = layers.Conv2D(intermediate_channels, kernel_size=1)\n\n        self.relative_bias_dct = tf.Variable(tf.zeros((1, 1, 1, 1)), trainable=True)\n        self.relative_bias_non_local = tf.Variable(tf.zeros((1, 1, 1, 1)), trainable=True)\n\n    def call(self, inputs):\n        # Get the shape of the input\n        shape = tf.shape(inputs)\n        batch_size, height, width = shape[0], shape[1], shape[2]\n\n        # Process the inputs\n        dct_input = dct_transform(inputs)  # Apply DCT transform\n        dct_attention = frequency_attention(dct_input, self.dct_threshold) + self.relative_bias_dct\n\n        theta_x = tf.reshape(self.theta(inputs), [batch_size, height * width, self.intermediate_channels])\n        phi_x = tf.reshape(self.phi(inputs), [batch_size, height * width, self.intermediate_channels])\n        g_x = tf.reshape(self.g(inputs), [batch_size, height * width, self.intermediate_channels])\n\n        f = tf.matmul(theta_x, phi_x, transpose_b=True)\n        f_softmax = tf.nn.softmax(f, axis=-1)\n        y = tf.matmul(f_softmax, g_x)\n        y = tf.reshape(y, [batch_size, height, width, self.intermediate_channels])\n\n        refined_attention = self.attn_refine(inputs) + self.relative_bias_non_local\n        refined_attention = self.channel_match_conv(refined_attention)\n\n        scale_factor = tf.sigmoid(refined_attention)\n        refined_dct_attention = self.reduce_channels_dct(dct_transform(dct_attention))\n        dct_attention = self.reduce_channels_dct(dct_attention)\n\n        y1 = y + (y * scale_factor) + refined_attention\n        y2 = y - (y * scale_factor) - refined_attention\n        y = y1 + y2 + refined_dct_attention + dct_attention\n\n        y = self.W(y)\n        return tf.nn.relu(inputs + y)\n\n    def compute_output_shape(self, input_shape):\n        \"\"\"\n        Manually compute the output shape of the layer.\n        The output shape will be the same as the input shape.\n        \"\"\"\n        # The output will have the same dimensions as the input (batch_size, height, width, channels)\n        batch_size = input_shape[0]\n        height = input_shape[1]\n        width = input_shape[2]\n        channels = self.in_channels\n        return (batch_size, height, width, channels)\n\n# Usage example\ndef RGSA(x, filters, strides=(1, 1), use_projection=False):\n    shortcut = x  # Save the original input for residual connection\n\n    # First Depthwise and Pointwise Convolution\n    x = layers.DepthwiseConv2D(kernel_size=(3, 3), strides=strides, padding=\"same\", depth_multiplier=1)(x)\n    x = layers.Conv2D(filters, kernel_size=1, padding=\"same\")(x)\n    \n    # Apply DCT Attention Block\n    x = DCTAttentionNonLocalBlock(filters, filters)(x)\n    #print('DCT 1:', x.shape)\n\n    # Normalization and Activation\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation('relu')(x)\n\n    # Second Depthwise and Pointwise Convolution\n    x = layers.DepthwiseConv2D(kernel_size=(3, 3), padding=\"same\", depth_multiplier=1)(x)\n    x = layers.Conv2D(filters, kernel_size=1, padding=\"same\")(x)\n\n    # Apply DCT Attention Block Again\n    x = DCTAttentionNonLocalBlock(filters, filters)(x)\n    #print('DCT 2:', x.shape)\n\n    # Normalization\n    x = layers.BatchNormalization()(x)\n\n    # Adjust Shortcut if Needed\n    if strides != (1, 1) or use_projection:\n        shortcut = layers.Conv2D(filters=filters, kernel_size=(1, 1), strides=strides, padding='same')(shortcut)\n        shortcut = layers.BatchNormalization()(shortcut)\n\n    # Residual Connection\n    x = layers.Add()([x, shortcut])\n\n    # Final Activation\n    x = layers.Activation('relu')(x)\n\n    return x\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''#### Multi-branch fusion attention (MFA) module #####\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.signal import fft2d, dct\n\nclass GlobalMinPooling2D(layers.Layer):\n    def __init__(self, **kwargs):\n        super(GlobalMinPooling2D, self).__init__(**kwargs)\n\n    def call(self, inputs):\n        return tf.reduce_min(inputs, axis=[1, 2])\n\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0], input_shape[-1])\n\n    def get_config(self):\n        config = super(GlobalMinPooling2D, self).get_config()\n        return config\n\nclass DeeperGlobalLocalAttentionLayer1(layers.Layer):\n    def __init__(self, units, activation='sigmoid', dropout_rate=0.2, use_scale=True, axis=-1, **kwargs):\n        super(DeeperGlobalLocalAttentionLayer1, self).__init__(**kwargs)\n        self.units = units\n        self.activation = activation\n        self.dropout_rate = dropout_rate\n        self.use_scale = use_scale\n        self.axis = axis\n\n    def build(self, input_shape):\n        _, _, _, channels = input_shape\n        \n        \n        self.global_conv1 = layers.Conv2D(filters=self.units, kernel_size=(1, 1), activation=self.activation)\n        self.global_avg_pooling1 = layers.GlobalAveragePooling2D()\n        \n        self.global_conv2 = layers.Conv2D(filters=self.units, kernel_size=(1, 1), activation=self.activation)\n        self.global_avg_pooling2 = layers.GlobalMaxPooling2D()\n        \n        self.global_conv_3 = layers.Conv2D(filters=self.units, kernel_size=(1, 1), activation=self.activation)\n        self.global_avg_pooling_3 = GlobalMinPooling2D()\n        \n        \n        self.global_conv3 = layers.Conv2D(filters=self.units, kernel_size=(1, 1), activation=self.activation)\n        self.global_avg_pooling3 = layers.GlobalAveragePooling2D()\n        \n        self.global_conv4 = layers.Conv2D(filters=self.units, kernel_size=(1, 1), activation=self.activation)\n        self.global_avg_pooling4 = layers.GlobalMaxPooling2D()\n\n        self.global_conv_4 = layers.Conv2D(filters=self.units, kernel_size=(1, 1), activation=self.activation)\n        self.global_avg_pooling_4 = GlobalMinPooling2D()\n        \n        \n        self.concat1 = layers.Add()\n        self.concat2 = layers.Add()\n        self.concat3 = layers.Add()\n        self.concat4 = layers.Add()\n        self.concat_3 = layers.Add()\n        self.concat_4 = layers.Add()\n        \n        self.concat5 = layers.Concatenate(axis=-1)\n        \n        self.global_attention = layers.Dense(units=self.units, activation=self.activation)\n\n        #self.local_dsc = layers.DepthwiseConv2D(kernel_size=(3, 3), padding=\"same\", depth_multiplier=1)\n        self.local_conv1 = layers.Conv2D(filters=self.units, kernel_size=(1, 1), activation=self.activation)\n        self.local_conv2 = layers.Conv2D(filters=self.units, kernel_size=(1, 1), activation=self.activation)\n        self.concat6 = layers.Add()\n        \n        if self.use_scale:\n            self.global_scale = self.add_weight(shape=(1, 1, 1, 1), initializer='ones', trainable=True, name='global_scale')\n            self.local_scale = self.add_weight(shape=(1, 1, 1, self.units), initializer='ones', trainable=True, name='local_scale')\n        \n        super(DeeperGlobalLocalAttentionLayer1, self).build(input_shape)\n\n    def call(self, inputs, training=None):\n        ##### Hierarchical Information Fusion Attention(HIFA) ######\n        \n        global_attention1 = self.global_conv1(inputs)\n        global_avg1 = self.global_avg_pooling1(global_attention1)\n        \n        global_attention2 = self.global_conv2(global_attention1)\n        global_avg2 = self.global_avg_pooling2(global_attention2)\n\n        global_attention_3 = self.global_conv_3(global_attention1)\n        global_avg_3 = self.global_avg_pooling_3(global_attention_3)\n        \n        \n        global_concat1 = self.concat1([global_avg1, global_avg2, global_avg_3])\n        global_sub1 = global_avg2 - global_avg1 - global_avg_3\n        global_concat1 = global_concat1 + global_sub1\n        \n        global_attention_concat1 = self.concat2([global_attention1, global_attention2, global_attention_3])\n        \n        global_sub_1 = global_attention2 - global_attention1 - global_attention_3\n\n        global_attention_concat1 = global_attention_concat1 + global_sub_1\n        \n        global_attention3 = self.global_conv3(global_attention_concat1)\n        global_avg3 = self.global_avg_pooling3(global_attention3)\n        \n        global_attention4 = self.global_conv4(global_attention3)\n        global_avg4 = self.global_avg_pooling4(global_attention4)\n\n        global_attention_4 = self.global_conv_3(global_attention3)\n        global_avg_4 = self.global_avg_pooling_3(global_attention_4)\n        \n        \n        global_concat2 = self.concat3([global_avg3, global_avg4, global_avg_4])\n        global_sub2 = global_avg4 - global_avg3 - global_avg_4\n        global_concat2 = global_concat2 + global_sub2\n\n        \n        #global_attention_concat2 = self.concat4([global_attention3, global_attention4, global_attention_4])\n        #global_sub_2 = global_attention4 - global_attention3 - global_attention_4\n\n        #global_attention_concat2 = global_attention_concat2 + global_sub_2\n\n        \n        \n        global_avg_concat = self.concat5([global_concat1, global_concat2])\n        \n        global_attention = self.global_attention(global_avg_concat)\n        global_attention = tf.expand_dims(tf.expand_dims(global_attention, 1), 1)\n\n        ##### frequency domain Local Information learning Attention ######\n        \n        input_shape = tf.shape(inputs)\n\n        \n        flattened_inputs = tf.reshape(inputs, [-1, input_shape[-1]])  # Flatten along the last axis\n        dct_transformed = tf.signal.dct(flattened_inputs, type=2, norm='ortho')\n        dct_transformed = tf.reshape(dct_transformed, input_shape)  # Reshape back to original dimensions\n        \n        local_attention1 = self.local_conv1(dct_transformed)\n        local_attention1 = tf.reduce_mean(local_attention1, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        local_attention2 = self.local_conv2(local_attention1)\n        local_attention2 = tf.reduce_mean(local_attention2, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n\n        local_attention3 = self.local_conv1(inputs)\n        local_attention3 = tf.reduce_max(local_attention3, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        local_attention4 = self.local_conv2(local_attention3)\n        local_attention4 = tf.reduce_max(local_attention4, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n\n        local_attention5 = self.local_conv1(inputs)\n        local_attention5 = tf.reduce_mean(local_attention5, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        local_attention6 = self.local_conv2(local_attention5)\n        local_attention6 = tf.reduce_mean(local_attention6, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n\n        local_attention7 = self.local_conv1(dct_transformed)\n        local_attention7 = tf.reduce_max(local_attention7, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        local_attention8 = self.local_conv2(local_attention7)\n        local_attention8 = tf.reduce_max(local_attention8, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n\n        \n        \n        #local_attention = self.concat6([local_attention1, local_attention2, local_attention3, local_attention4, \n         #                              local_attention5, local_attention6, local_attention7, local_attention8])\n        \n        local_attention_avg = (self.concat6([local_attention1, local_attention2, local_attention5, local_attention6])) \n        local_attention_max = self.concat6([local_attention3, local_attention4, local_attention7, local_attention8]) \n\n        local_attention = L.Activation(\"relu\")(self.concat6([local_attention_avg, local_attention_max])) \n        \n        # Scale Global and Local Attention\n        if self.use_scale:\n            global_attention *= self.global_scale\n            local_attention *= self.local_scale\n\n        # Combine Global and Local Attention\n        attention = tf.sigmoid(global_attention + local_attention)\n        return attention\n\n    def get_config(self):\n        config = super(DeeperGlobalLocalAttentionLayer1, self).get_config()\n        config.update({'units': self.units, 'activation': self.activation, 'dropout_rate': self.dropout_rate,\n                       'use_scale': self.use_scale})\n        return config\n\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Code strating here for MICCAI","metadata":{}},{"cell_type":"code","source":"#### Multi-branch fusion attention (MFA) module #####\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.signal import fft2d, dct\n\nclass GlobalMinPooling2D(layers.Layer):\n    def __init__(self, **kwargs):\n        super(GlobalMinPooling2D, self).__init__(**kwargs)\n\n    def call(self, inputs):\n        return tf.reduce_min(inputs, axis=[1, 2])\n\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0], input_shape[-1])\n\n    def get_config(self):\n        config = super(GlobalMinPooling2D, self).get_config()\n        return config\n\nclass DeeperGlobalLocalAttentionLayer1(layers.Layer):\n    def __init__(self, units, activation='sigmoid', dropout_rate=0.2, use_scale=True, axis=-1, **kwargs):\n        super(DeeperGlobalLocalAttentionLayer1, self).__init__(**kwargs)\n        self.units = units\n        self.activation = activation\n        self.dropout_rate = dropout_rate\n        self.use_scale = use_scale\n        self.axis = axis\n\n    def build(self, input_shape):\n        _, _, _, channels = input_shape\n        \n        \n        self.global_conv1 = layers.DepthwiseConv2D(kernel_size=(1, 1), activation=self.activation)\n        self.global_avg_pooling1 = layers.GlobalAveragePooling2D()\n        \n        self.global_conv2 = layers.DepthwiseConv2D(kernel_size=(1, 1), activation=self.activation)\n        self.global_avg_pooling2 = layers.GlobalMaxPooling2D()\n        \n        self.global_conv_3 = layers.DepthwiseConv2D(kernel_size=(1, 1), activation=self.activation)\n        self.global_avg_pooling_3 = GlobalMinPooling2D()\n        \n        \n        self.global_conv3 = layers.DepthwiseConv2D(kernel_size=(1, 1), activation=self.activation)\n        self.global_avg_pooling3 = layers.GlobalAveragePooling2D()\n        \n        self.global_conv4 = layers.DepthwiseConv2D(kernel_size=(1, 1), activation=self.activation)\n        self.global_avg_pooling4 = layers.GlobalMaxPooling2D()\n\n        self.global_conv_4 = layers.DepthwiseConv2D(kernel_size=(1, 1), activation=self.activation)\n        self.global_avg_pooling_4 = GlobalMinPooling2D()\n        \n        \n        self.concat1 = layers.Add()\n        self.concat2 = layers.Add()\n        self.concat3 = layers.Add()\n        self.concat4 = layers.Add()\n        self.concat_3 = layers.Add()\n        self.concat_4 = layers.Add()\n        \n        self.concat5 = layers.Concatenate(axis=-1)\n        \n        self.global_attention = layers.Dense(units=self.units, activation=self.activation)\n\n        #self.local_dsc = layers.DepthwiseConv2D(kernel_size=(3, 3), padding=\"same\", depth_multiplier=1)\n        self.local_conv1 = layers.DepthwiseConv2D(kernel_size=(1, 1), activation=self.activation)\n        self.local_conv2 = layers.DepthwiseConv2D(kernel_size=(1, 1), activation=self.activation)\n        \n        self.concat6 = layers.Add()\n        \n        if self.use_scale:\n            self.global_scale = self.add_weight(shape=(1, 1, 1, 1), initializer=tf.keras.initializers.HeNormal(), trainable=True, name='global_scale')\n            self.local_scale = self.add_weight(shape=(1, 1, 1, self.units), initializer=tf.keras.initializers.HeNormal(), trainable=True, name='local_scale')\n        \n        super(DeeperGlobalLocalAttentionLayer1, self).build(input_shape)\n\n    def call(self, inputs, training=None):\n        ##### Hierarchical Information Fusion Attention(HIFA) ######\n        \n        global_attention1 = self.global_conv1(inputs)\n        global_avg1 = self.global_avg_pooling1(global_attention1)\n        \n        global_attention2 = self.global_conv2(global_attention1)\n        global_avg2 = self.global_avg_pooling2(global_attention2)\n\n        global_attention_3 = self.global_conv_3(global_attention1)\n        global_avg_3 = self.global_avg_pooling_3(global_attention_3)\n        \n        \n        global_concat1 = self.concat1([global_avg1, global_avg2, global_avg_3])\n        global_sub1 = global_avg2 - global_avg1 - global_avg_3\n        global_concat1 = global_concat1 + global_sub1\n        \n        global_attention_concat1 = self.concat2([global_attention1, global_attention2, global_attention_3])\n        \n        global_sub_1 = global_attention2 - global_attention1 - global_attention_3\n\n        global_attention_concat1 = global_attention_concat1 + global_sub_1\n        \n        global_attention3 = self.global_conv3(global_attention_concat1)\n        global_avg3 = self.global_avg_pooling3(global_attention3)\n        \n        global_attention4 = self.global_conv4(global_attention3)\n        global_avg4 = self.global_avg_pooling4(global_attention4)\n\n        global_attention_4 = self.global_conv_3(global_attention3)\n        global_avg_4 = self.global_avg_pooling_3(global_attention_4)\n        \n        \n        global_concat2 = self.concat3([global_avg3, global_avg4, global_avg_4])\n        global_sub2 = global_avg4 - global_avg3 - global_avg_4\n        global_concat2 = global_concat2 + global_sub2\n\n        \n        #global_attention_concat2 = self.concat4([global_attention3, global_attention4, global_attention_4])\n        #global_sub_2 = global_attention4 - global_attention3 - global_attention_4\n\n        #global_attention_concat2 = global_attention_concat2 + global_sub_2\n\n        \n        \n        global_avg_concat = self.concat5([global_concat1, global_concat2])\n        \n        global_attention = self.global_attention(global_avg_concat)\n        #global_attention = L.Activation(\"relu\")(tf.expand_dims(tf.expand_dims(global_attention, 1), 1))\n        \n        global_attention = tf.expand_dims(tf.expand_dims(global_attention, 1), 1)\n\n        ##### frequency domain Local Information learning Attention ######\n        \n        input_shape = tf.shape(inputs)\n\n        \n        flattened_inputs = tf.reshape(inputs, [-1, input_shape[-1]])  # Flatten along the last axis\n        dct_transformed = tf.signal.dct(flattened_inputs, type=2, norm='ortho')\n        dct_transformed = tf.reshape(dct_transformed, input_shape)  # Reshape back to original dimensions\n        \n        local_attention1 = self.local_conv1(dct_transformed)\n        local_attention1 = tf.reduce_mean(local_attention1, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        local_attention2 = self.local_conv2(local_attention1)\n        local_attention2 = tf.reduce_mean(local_attention2, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n\n        '''local_attention3 = self.local_conv1(inputs)\n        local_attention3 = tf.reduce_max(local_attention3, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        local_attention4 = self.local_conv2(local_attention3)\n        local_attention4 = tf.reduce_max(local_attention4, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n\n        local_attention5 = self.local_conv1(inputs)\n        local_attention5 = tf.reduce_mean(local_attention5, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        local_attention6 = self.local_conv2(local_attention5)\n        local_attention6 = tf.reduce_mean(local_attention6, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions'''\n\n        local_attention7 = self.local_conv1(dct_transformed)\n        local_attention7 = tf.reduce_max(local_attention7, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        local_attention8 = self.local_conv2(local_attention7)\n        local_attention8 = tf.reduce_max(local_attention8, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n\n        local_attention9 = self.local_conv1(dct_transformed)\n        local_attention9 = tf.reduce_min(local_attention9, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        local_attention10 = self.local_conv2(local_attention9)\n        local_attention10 = tf.reduce_min(local_attention10, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n\n\n        \n        \n        #local_attention = self.concat6([local_attention1, local_attention2, local_attention3, local_attention4, \n         #                              local_attention5, local_attention6, local_attention7, local_attention8])\n        \n        local_attention_avg = (self.concat6([local_attention1, local_attention2])) \n        local_attention_max = self.concat6([local_attention7, local_attention8]) \n        local_attention_min = self.concat6([local_attention9, local_attention10]) \n\n        local_attention = self.concat6([local_attention_avg, local_attention_max, local_attention_min]) \n        \n        # Scale Global and Local Attention\n        if self.use_scale:\n            global_attention *= self.global_scale\n            local_attention *= self.local_scale\n\n        # Combine Global and Local Attention\n        attention = tf.sigmoid(global_attention + local_attention)\n        return attention\n\n    def get_config(self):\n        config = super(DeeperGlobalLocalAttentionLayer1, self).get_config()\n        config.update({'units': self.units, 'activation': self.activation, 'dropout_rate': self.dropout_rate,\n                       'use_scale': self.use_scale})\n        return config\n\n'''class DeeperAttentionLayer1(layers.Layer):\n    def __init__(self, units=64, use_scale=True, **kwargs):\n        super(DeeperAttentionLayer1, self).__init__(**kwargs)\n        self.units = units\n        self.use_scale = use_scale\n\n    def build(self, input_shape):\n        _, H, W, C = input_shape\n        self.alpha = self.add_weight(shape=(1, 1, 1, C), initializer='ones', trainable=True, name='alpha')\n        self.deeper_global_local_attention = DeeperGlobalLocalAttentionLayer1(units=self.units, activation='sigmoid', \n                                                                              dropout_rate=0.2,  # You can adjust the dropout rate\n                                                                              use_scale=self.use_scale)\n        super(DeeperAttentionLayer1, self).build(input_shape)\n\n    def call(self, inputs, training=None):\n        attention = self.deeper_global_local_attention(inputs, training=training)\n        attention_feature = inputs * attention * self.alpha\n        return attention_feature\n\n    def get_config(self):\n        config = super(DeeperAttentionLayer1, self).get_config()\n        config.update({'units': self.units, 'use_scale': self.use_scale})\n        return config'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T07:41:17.902541Z","iopub.execute_input":"2025-03-04T07:41:17.902850Z","iopub.status.idle":"2025-03-04T07:41:17.925216Z","shell.execute_reply.started":"2025-03-04T07:41:17.902829Z","shell.execute_reply":"2025-03-04T07:41:17.924476Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"\"class DeeperAttentionLayer1(layers.Layer):\\n    def __init__(self, units=64, use_scale=True, **kwargs):\\n        super(DeeperAttentionLayer1, self).__init__(**kwargs)\\n        self.units = units\\n        self.use_scale = use_scale\\n\\n    def build(self, input_shape):\\n        _, H, W, C = input_shape\\n        self.alpha = self.add_weight(shape=(1, 1, 1, C), initializer='ones', trainable=True, name='alpha')\\n        self.deeper_global_local_attention = DeeperGlobalLocalAttentionLayer1(units=self.units, activation='sigmoid', \\n                                                                              dropout_rate=0.2,  # You can adjust the dropout rate\\n                                                                              use_scale=self.use_scale)\\n        super(DeeperAttentionLayer1, self).build(input_shape)\\n\\n    def call(self, inputs, training=None):\\n        attention = self.deeper_global_local_attention(inputs, training=training)\\n        attention_feature = inputs * attention * self.alpha\\n        return attention_feature\\n\\n    def get_config(self):\\n        config = super(DeeperAttentionLayer1, self).get_config()\\n        config.update({'units': self.units, 'use_scale': self.use_scale})\\n        return config\""},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers\n\n# Frequency Transform Layer\n'''class FrequencyTransformLayer(layers.Layer):\n    def __init__(self, transform_type='dct', **kwargs):\n        super(FrequencyTransformLayer, self).__init__(**kwargs)\n        self.transform_type = transform_type\n\n    def call(self, inputs):\n        # Ensure inputs are float32 for DCT and FFT operations\n        #inputs = tf.cast(inputs, tf.float32)\n\n        if self.transform_type == 'dct':\n            # Apply 2D DCT along the last axis\n            input_shape = tf.shape(inputs)\n            flattened_inputs = tf.reshape(inputs, [-1, input_shape[-1]])  # Flatten along the last axis\n            dct_transformed = tf.signal.dct(flattened_inputs, type=2, norm='ortho', axis = -1)\n            return tf.reshape(dct_transformed, input_shape)  # Reshape back to original dimensions\n        elif self.transform_type == 'fft':\n            # Apply 2D FFT and return the magnitude\n            fft_transformed = tf.signal.fft2d(tf.cast(inputs, tf.complex64))\n            return tf.math.abs(fft_transformed)\n        else:\n            raise ValueError(\"Unsupported transform type. Choose 'dct' or 'fft'.\")\n\n    def get_config(self):\n        config = super(FrequencyTransformLayer, self).get_config()\n        config.update({'transform_type': self.transform_type})\n        return config'''\n\n# DCT Transform Function\n'''def dct_transform(inputs):\n    # Apply DCT transform to the inputs\n    dct_input = FrequencyTransformLayer(transform_type='dct')(inputs)  # Use FrequencyTransformLayer for DCT\n    \n    # Apply global average pooling (GAP), global max pooling (GMP), and global min pooling (GMP_MIN)\n    gap = tf.reduce_mean(dct_input, axis=[1, 2], keepdims=True)\n    gmp = tf.reduce_max(dct_input, axis=[1, 2], keepdims=True)\n    gmp_min = tf.reduce_min(dct_input, axis=[1, 2], keepdims=True)\n    \n    # Combine the pooled features\n    pooled_features1 = gap + gmp + gmp_min\n    pooled_features2 = gmp - (gap - gmp_min)\n    pooled_features = pooled_features1 + pooled_features2\n    \n    return pooled_features'''\n\n\n\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\n# 1D DCT from scratch using TensorFlow\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\n# 1D DCT from scratch using TensorFlow\n'''def dct1d(x):\n    \"\"\"1D Discrete Cosine Transform.\"\"\"\n    N = tf.shape(x)[0]\n    k = tf.range(N, dtype=tf.float32)  # Create range for k\n    n = tf.range(N, dtype=tf.float32)  # Create range for n\n    # Using broadcasting to calculate the DCT\n    X = tf.reduce_sum(x * tf.cos(np.pi / N * (n + 0.5)[:, None] * k[None, :]), axis=0)\n    return X\n\n# 2D DCT from scratch using TensorFlow\ndef dct2d(x):\n    \"\"\"2D Discrete Cosine Transform (DCT). Applies DCT row-wise and column-wise.\"\"\"\n    # Apply DCT to each row (along the last axis)\n    x_dct_rows = tf.map_fn(dct1d, x, dtype=tf.float32)\n    # Apply DCT to each column (along the second last axis)\n    x_dct = tf.map_fn(lambda x: dct1d(tf.transpose(x)), x_dct_rows, dtype=tf.float32)\n    return x_dct\n\n\nimport tensorflow as tf\n\n# 1D DCT from scratch using TensorFlow\ndef dct1d(x):\n    \"\"\"1D Discrete Cosine Transform.\"\"\"\n    N = tf.shape(x)[0]\n    k = tf.range(N, dtype=tf.int32)  # Create range for k\n    n = tf.range(N, dtype=tf.int32)  # Create range for n\n    pi = tf.constant(3, dtype=tf.int32)  # Explicitly use TensorFlow constant for pi\n    \n    # Using broadcasting to calculate the DCT\n    X = tf.reduce_sum(x * tf.cos(3 / N * (n + 0.5)[:, None] * k[None, :]), axis=0)\n    return X\n\n# 2D DCT from scratch using TensorFlow\ndef dct2d(x):\n    \"\"\"2D Discrete Cosine Transform (DCT). Applies DCT row-wise and column-wise.\"\"\"\n    # Apply DCT to each row (along the last axis)\n    x_dct_rows = tf.map_fn(dct1d, x, dtype=tf.int32)\n    # Apply DCT to each column (along the second last axis)\n    x_dct = tf.map_fn(lambda x: dct1d(tf.transpose(x)), x_dct_rows, dtype=tf.int32)\n    return x_dct\n\n# Custom FrequencyTransformLayer class\nclass FrequencyTransformLayer(layers.Layer):\n    def __init__(self, transform_type='dct', **kwargs):\n        super(FrequencyTransformLayer, self).__init__(**kwargs)\n        self.transform_type = transform_type\n\n    def call(self, inputs):\n        # Ensure inputs are float32 for DCT and FFT operations\n        inputs = tf.cast(inputs, tf.float32)\n\n        if self.transform_type == 'dct':\n            # Apply 2D DCT from scratch (using custom dct2d function)\n            input_shape = tf.shape(inputs)\n            flattened_inputs = tf.reshape(inputs, [-1, input_shape[-1]])  # Flatten along the last axis\n            \n            # Perform the 2D DCT using the custom scratch implementation\n            dct_transformed = dct2d(flattened_inputs)  # Perform 2D DCT\n            \n            # Reshape back to the original dimensions\n            return tf.reshape(dct_transformed, input_shape)  # Reshape back to original dimensions\n\n        elif self.transform_type == 'fft':\n            # Apply 2D FFT and return the magnitude\n            fft_transformed = tf.signal.fft2d(tf.cast(inputs, tf.complex64))\n            return tf.math.abs(fft_transformed)\n\n        else:\n            raise ValueError(\"Unsupported transform type. Choose 'dct' or 'fft'.\")\n\n    def get_config(self):\n        config = super(FrequencyTransformLayer, self).get_config()\n        config.update({'transform_type': self.transform_type})\n        return config\n'''\n\n\n\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Layer\n\n'''class DCTTransformLayer(Layer):\n    def __init__(self, **kwargs):\n        super(DCTTransformLayer, self).__init__(**kwargs)\n        # Optionally initialize any additional parameters here\n        #self.frequency_transform = FrequencyTransformLayer(transform_type='dct')\n\n    def build(self, input_shape):\n        # This method is used for any initialization specific to the layer, such as creating weights or other components\n        pass\n\n    def call(self, inputs):\n        # Apply DCT transform to the inputs\n        #dct_input = self.frequency_transform(inputs)  # Using self to reference the FrequencyTransformLayer instance\n        \n        # Apply global average pooling (GAP), global max pooling (GMP), and global min pooling (GMP_MIN)\n        gap = tf.reduce_mean(inputs, axis=[1, 2], keepdims=True)\n        gmp = tf.reduce_max(inputs, axis=[1, 2], keepdims=True)\n        gmp_min = tf.reduce_min(inputs, axis=[1, 2], keepdims=True)\n        \n        # Combine the pooled features\n        pooled_features1 = gap + gmp + gmp_min\n        pooled_features2 = gmp - (gap - gmp_min)\n        pooled_features = pooled_features1 + pooled_features2\n        \n        return pooled_features\n\n\n\n\n# Frequency Attention\nimport tensorflow as tf\n\n\n# DCT Attention Non-Local Block\nclass DCTAttentionNonLocalBlock(layers.Layer):\n    def __init__(self, in_channels, intermediate_channels, dct_threshold=0.1):\n        super(DCTAttentionNonLocalBlock, self).__init__()\n        self.in_channels = in_channels\n        self.intermediate_channels = intermediate_channels\n        self.dct_threshold = dct_threshold\n\n\n        #self.dct_transform_layer = DCTTransformLayer()\n\n        \n        \n        # Define your layers here\n        self.theta = layers.Conv2D(intermediate_channels, kernel_size=1, use_bias=False)\n        self.phi = layers.Conv2D(intermediate_channels, kernel_size=1, use_bias=False)\n        self.g = layers.Conv2D(intermediate_channels, kernel_size=1, use_bias=False)\n        self.W = layers.Conv2D(in_channels, kernel_size=1, use_bias=False)\n        \n        self.attn_refine = tf.keras.Sequential([\n            layers.Conv2D(in_channels, kernel_size=3, padding='same', use_bias=False),\n            layers.ReLU(),\n            layers.Conv2D(in_channels, kernel_size=3, padding='same', use_bias=False)\n        ])\n        \n        self.channel_match_conv = layers.Conv2D(intermediate_channels, kernel_size=1)\n        self.reduce_channels_dct = layers.Conv2D(intermediate_channels, kernel_size=1)\n\n        self.relative_bias_dct = tf.Variable(tf.zeros((1, 1, 1, 1)), trainable=True)\n        self.relative_bias_non_local = tf.Variable(tf.zeros((1, 1, 1, 1)), trainable=True)\n\n    def call(self, inputs):\n        # Get the shape of the input\n        shape = tf.shape(inputs)\n        batch_size, height, width = shape[0], shape[1], shape[2]\n\n        # Process the inputs\n        # Global attention\n        gap = tf.reduce_mean(inputs, axis=[1, 2], keepdims=True)\n        gmp = tf.reduce_max(inputs, axis=[1, 2], keepdims=True)\n        gmp_min = tf.reduce_min(inputs, axis=[1, 2], keepdims=True)\n        \n        # Combine the pooled features\n        pooled_features1 = gap + gmp + gmp_min\n        pooled_features2 = gmp - (gap - gmp_min)\n        \n        #pooled_features = pooled_features1 + pooled_features2\n        \n        #dct_input = self.dct_transform_layer(inputs)  # Apply DCT transform\n        #dct_input = dct_transform(inputs)  # Apply DCT transform\n\n        \n        dct_attention = tf.sigmoid((pooled_features1 + pooled_features2) * self.relative_bias_dct) #frequency_attention(dct_input, self.dct_threshold) + self.relative_bias_dct\n\n        theta_x = tf.reshape(self.theta(inputs), [batch_size, height * width, self.intermediate_channels])\n        phi_x = tf.reshape(self.phi(inputs), [batch_size, height * width, self.intermediate_channels])\n        g_x = tf.reshape(self.g(inputs), [batch_size, height * width, self.intermediate_channels])\n\n        f = tf.matmul(theta_x, phi_x, transpose_b=True)\n        f_softmax = tf.nn.softmax(f, axis=-1)\n        y = tf.matmul(f_softmax, g_x)\n        y = tf.reshape(y, [batch_size, height, width, self.intermediate_channels])\n\n        refined_attention = self.attn_refine(inputs) + self.relative_bias_non_local\n        refined_attention = self.channel_match_conv(refined_attention)\n\n        scale_factor = tf.sigmoid(refined_attention)\n        #refined_dct_attention = self.reduce_channels_dct(dct_transform(dct_attention))\n        dct_attention = self.reduce_channels_dct(dct_attention)\n\n        y1 = y + (y * scale_factor) + refined_attention\n        y2 = y - (y * scale_factor) - refined_attention\n        y = tf.sigmoid(y1 + y2 + dct_attention)\n\n        y = self.W(y)\n        return tf.nn.relu(inputs + y)\n\n    def compute_output_shape(self, input_shape):\n        \"\"\"\n        Manually compute the output shape of the layer.\n        The output shape will be the same as the input shape.\n        \"\"\"\n        # The output will have the same dimensions as the input (batch_size, height, width, channels)\n        batch_size = input_shape[0]\n        height = input_shape[1]\n        width = input_shape[2]\n        channels = self.in_channels\n        return (batch_size, height, width, channels)\n'''\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\n'''# Custom Self-Attention Layer\nclass SelfAttention(layers.Layer):\n    def __init__(self, channels, **kwargs):\n        super(SelfAttention, self).__init__(**kwargs)\n        self.channels = channels\n        self.query_conv = layers.Conv2D(channels // 8, kernel_size=1)\n        self.key_conv = layers.Conv2D(channels // 8, kernel_size=1)\n        self.value_conv = layers.Conv2D(channels, kernel_size=1)\n        self.attn_conv = layers.Conv2D(channels, kernel_size=1)\n\n    def call(self, inputs):\n        # Generate Q, K, V matrices\n        query = self.query_conv(inputs)\n        key = self.key_conv(inputs)\n        value = self.value_conv(inputs)\n\n        # Perform dot product between query and key (self-attention)\n        attn_map = tf.matmul(query, key, transpose_b=True)\n        attn_map = tf.nn.softmax(attn_map, axis=-1)\n\n        # Weighted sum of value vectors\n        attn_out = tf.matmul(attn_map, value)\n\n        # Final attention output\n        attn_out = self.attn_conv(attn_out)\n        return attn_out + inputs  # Add residual connection\n'''\n# Custom Attention Block with Global Pooling\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\nclass GlobalMinPooling2D(layers.Layer):\n    def __init__(self, **kwargs):\n        super(GlobalMinPooling2D, self).__init__(**kwargs)\n\n    def call(self, inputs):\n        return tf.reduce_min(inputs, axis=[1, 2])\n\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0], input_shape[-1])\n\n    def get_config(self):\n        config = super(GlobalMinPooling2D, self).get_config()\n        return config\n\n\nclass AttentionBlock(layers.Layer):\n    def __init__(self, channels, reduction_ratio=8, use_scale = True, **kwargs):\n        super(AttentionBlock, self).__init__(**kwargs)\n        self.channels = channels\n\n        # Global Pooling (Average, Max, and Min)\n        self.gap = layers.GlobalAveragePooling2D()\n        self.gmp = layers.GlobalMaxPooling2D()\n        self.gmin = GlobalMinPooling2D()  #layers.Lambda(lambda x: tf.reduce_min(x, axis=[1, 2], keepdims=False))  # Min pooling\n\n        # Efficient Channel Attention (Replaces Dense Layer)\n        self.channel_attn = layers.DepthwiseConv2D(kernel_size=1)\n        self.channel_attn_out = layers.DepthwiseConv2D(kernel_size=1)\n\n        # Spatial Attention (Lightweight Conv)\n        #self.spatial_attn = layers.Conv2D(1, kernel_size=3, padding=\"same\", activation=\"sigmoid\")\n\n        # Batch Normalization\n        self.batch_norm = layers.BatchNormalization()\n        \n        '''self.weight1 = self.add_weight(\n            shape=(1,1,1,1), initializer=initializers.HeNormal(), trainable=True, name=\"weight1\"\n        )\n        self.weight2 = self.add_weight(\n            shape=(1,1,1,1), initializer=initializers.RandomNormal(mean=0.0, stddev=0.02), trainable=True, name=\"weight2\"\n        )'''\n        \n        #self.weight1 = self.add_weight(shape=(1, 1, 1, 1),  initializer=tf.keras.initializers.HeNormal(), trainable=True, name='weight1', \n         #                                     constraint=tf.keras.constraints.MaxNorm(2.0))\n        \n        self.weight1 = self.add_weight(shape=(1, 1, 1, 1),  initializer=tf.keras.initializers.HeNormal(), trainable=True, name='weight2', \n                                              constraint=tf.keras.constraints.MaxNorm(2.0))\n\n        self.weight2 = self.add_weight(shape=(1, 1, 1, 1),  initializer=tf.keras.initializers.RandomNormal(mean=1.0, stddev=0.02), trainable=True, name='weight3', \n                                              constraint=tf.keras.constraints.MaxNorm(2.0))\n\n        self.weight3 = self.add_weight(shape=(1, 1, 1, 1), initializer='ones', trainable=True, name='alpha2')\n        \n            \n        self.dropout = layers.Dropout(0.25)\n        self.bn = layers.BatchNormalization()\n\n        self.use_scale = use_scale\n        self.local_attn = DeeperGlobalLocalAttentionLayer1(units=self.channels, activation='sigmoid', \n                                                                              dropout_rate=0.2,  # You can adjust the dropout rate\n                                                                              use_scale=self.use_scale)\n\n        \n    def call(self, inputs):\n        inputs1, inputs2 = inputs\n        \n        # Compute Global Average, Max, and Min Pooling\n        gap_out = self.gap(inputs1)\n        gmp_out = self.gmp(inputs1)\n        gmin_out = self.gmin(inputs1)\n\n        # Combine pooling features\n        combined_pool1 = gap_out + gmp_out + gmin_out\n        combined_pool2 = gmp_out - (gap_out - gmin_out)\n        \n        # Normalize for stability\n        #combined_pool1 = tf.nn.l2_normalize(combined_pool1, axis=-1)\n        \n        combined_pool1 = tf.expand_dims(tf.expand_dims(combined_pool1, 1), 1) \n        combined_pool2 = tf.expand_dims(tf.expand_dims(combined_pool2, 1), 1)\n\n        combined_pool1 = self.bn(combined_pool1)\n        combined_pool1 = self.dropout(combined_pool1)\n\n        combined_pool2 = self.bn(combined_pool2)\n        combined_pool2 = self.dropout(combined_pool2)\n        \n        combined_pool1 = combined_pool1 * self.weight1\n        combined_pool2 = combined_pool2 * self.weight1\n        combined_pool = tf.sigmoid(combined_pool1 + combined_pool2) \n        \n\n        #combined_pool = tf.sigmoid(combined_pool1 + combined_pool2) \n        \n        #combined_pool = inputs * combined_pool * self.weight3\n\n        # Compute Channel Attention\n        channel_attn = self.channel_attn(inputs2)\n        gap = tf.reduce_mean(channel_attn, axis=[1, 2], keepdims=True)\n        gmp = tf.reduce_max(channel_attn, axis=[1, 2], keepdims=True)\n        gmp_min = tf.reduce_min(channel_attn, axis=[1, 2], keepdims=True)\n\n        pooled_features1 = gap + gmp + gmp_min\n        pooled_features2 = gmp - (gap - gmp_min)\n\n        pooled_features = pooled_features1 + pooled_features2\n        \n        channel_attn = self.channel_attn_out(pooled_features)  # Sigmoid attention map\n        #print('channel_attn shape:', channel_attn.shape)\n        channel_attn = channel_attn + pooled_features\n        channel_attn = self.batch_norm(channel_attn)\n        \n\n        channel_attn = self.bn(channel_attn)\n        channel_attn = self.dropout(channel_attn)\n\n        channel_attn = channel_attn * self.weight2\n        channel_attn = tf.sigmoid(channel_attn) \n\n        \n        local1 = self.local_attn(inputs1)\n        local2 = self.local_attn(inputs2)\n        \n        \n        # Apply Cross Attention Maps\n        attention1 =  tf.sigmoid(combined_pool + local2) \n        attention2 =  tf.sigmoid(channel_attn + local1) \n        \n        att1 = inputs1 * attention2 * self.weight3\n        att2 = inputs2 * attention1 * self.weight3\n        #print('output attention shape:', output.shape)\n\n        return att1, att2\n\n\n# Residual Attention Block (Using Spatial Attention)\n'''class ResidualAttentionBlock(layers.Layer):\n    def __init__(self, in_channels, **kwargs):\n        super(ResidualAttentionBlock, self).__init__(**kwargs)\n        self.in_channels = in_channels\n        self.attn_block = AttentionBlock(in_channels)\n\n        # Convolution layers for feature refinement\n        #self.dwc = layers.DepthwiseConv2D(kernel_size=(3, 3), padding=\"same\", depth_multiplier=1)\n        self.conv1 = layers.Conv2D(in_channels, kernel_size=1, padding=\"same\")\n        self.relu = layers.ReLU()\n        self.conv2 = layers.Conv2D(in_channels, kernel_size=1, padding=\"same\")\n\n    def call(self, inputs):\n        # Apply attention mechanism\n        attn_out = self.attn_block(inputs)\n\n        # Refining features\n        #x = self.dwc(attn_out)\n        x = self.conv1(attn_out)\n        x = self.relu(x)\n        #x = self.dwc(x)\n        x = self.conv2(x)\n\n        # Add residual connection\n        return inputs * tf.sigmoid(x + attn_out)  # Residual connection for better feature learning\n'''\n\nfrom tensorflow.keras import layers\n\ndef multi_kernel_groupwise_conv(x, filters, groups=8, strides=1):\n    # 1x1 Group-wise Convolution\n    conv1x1 = layers.Conv2D(filters, kernel_size=1, groups=groups, strides=strides, padding=\"same\")(x)\n\n    # 3x3 Group-wise Convolution\n    conv3x3 = layers.DepthwiseConv2D(kernel_size=3, strides=strides, padding=\"same\")(x)\n\n    # 5x5 Group-wise Convolution\n    conv5x5 = layers.DepthwiseConv2D(kernel_size=5, strides=strides, padding=\"same\")(x)\n\n    # Depthwise 3x3 Group-wise Convolution\n    #depthwise3x3 = layers.DepthwiseConv2D(kernel_size=3, strides=strides, padding=\"same\")(x)\n    \n    # Concatenate all outputs along the channel axis\n    x1 = layers.Concatenate()([conv1x1, conv3x3, conv5x5])\n    x1 = layers.Conv2D(filters, kernel_size=1, groups=groups, padding=\"same\")(x1)\n\n    x = layers.Conv2D(filters=filters, kernel_size=(1, 1), strides=strides, padding='same')(x)\n    x = layers.Add()([x, x1])\n    x = layers.Activation('relu')(x)\n    \n    return x\n\n\n# Usage example\ndef RGSA(x, filters, strides=(1, 1), use_projection=False):\n    shortcut = x  # Save the original input for residual connection\n\n    # First Depthwise and Pointwise Convolution\n    #x = layers.DepthwiseConv2D(kernel_size=(3, 3), strides=strides, padding=\"same\", depth_multiplier=1)(x)\n    \n    #x = layers.Conv2D(filters, kernel_size=3, strides=strides, padding=\"same\")(x)\n\n    #x = layers.DepthwiseConv2D(kernel_size=3, strides=strides, padding=\"same\")(x)\n\n    #x = layers.Conv2D(filters=x.shape[-1], kernel_size=1, groups=4, padding=\"same\")(x)\n\n    # Depthwise Convolution\n    #x = layers.DepthwiseConv2D(kernel_size=3, strides=strides, padding=\"same\")(x)\n    \n    # Apply DCT Attention Block\n    #x = DCTAttentionNonLocalBlock(filters, filters)(x)\n    #x = AttentionBlock(filters)(x)\n\n    x = multi_kernel_groupwise_conv(x, filters=filters, groups=8, strides=strides)\n    \n    # Normalization and Activation\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation('relu')(x)\n\n    # Second Depthwise and Pointwise Convolution\n    x = layers.Conv2D(filters, kernel_size=(3, 3), padding=\"same\")(x)\n    #x = layers.Conv2D(filters, kernel_size=3, padding=\"same\")(x)\n\n    # Apply DCT Attention Block Again\n    #x = DCTAttentionNonLocalBlock(filters, filters)(x)\n    #x = AttentionBlock(filters)(x)\n\n    # Normalization\n    x = layers.BatchNormalization()(x)\n\n    # Adjust Shortcut if Needed\n    if strides != (1, 1) or use_projection:\n        shortcut = layers.Conv2D(filters=filters, kernel_size=(1, 1), strides=strides, padding='same')(shortcut)\n        shortcut = layers.BatchNormalization()(shortcut)\n\n    # Residual Connection\n    x = layers.Add()([x, shortcut])\n\n    # Final Activation\n    x = layers.Activation('relu')(x)\n\n    return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T07:41:18.359515Z","iopub.execute_input":"2025-03-04T07:41:18.359811Z","iopub.status.idle":"2025-03-04T07:41:18.380584Z","shell.execute_reply.started":"2025-03-04T07:41:18.359791Z","shell.execute_reply":"2025-03-04T07:41:18.379777Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def residual_GLC_branch1(inputs1, inputs2):\n    \n    x1 = Conv2D(filters=64, kernel_size=(7, 7), strides=(2, 2), padding='same')(inputs1)\n    #x1 = DeeperAttentionLayer1(units=64, use_scale=True)(x1) ## MFA ####\n    #x1 = AttentionBlock(64)(x1)\n    x1 = BatchNormalization()(x1)\n    x1 = tf.keras.layers.Activation('relu')(x1)\n    x1 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x1)\n    \n    x2 = Conv2D(filters=64, kernel_size=(7, 7), strides=(2, 2), padding='same')(inputs2)\n    #x2 = DeeperAttentionLayer1(units=64, use_scale=True)(x2) ## MFA ####\n    #x2 = AttentionBlock(64)(x2)\n    x2 = BatchNormalization()(x2)\n    x2 = tf.keras.layers.Activation('relu')(x2)\n    x2 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x2)\n    \n\n    x1 = RGSA(x1, filters=64)\n    x1 = tf.keras.layers.Dropout(0.25)(x1, training = True)  ## MCD ####\n    #x1 = DeeperAttentionLayer1(units=64, use_scale=True)(x1) ## MFA ####\n    #x1 = AttentionBlock(64)(x1)\n\n    x2 = RGSA(x2, filters=64)\n    x2 = tf.keras.layers.Dropout(0.25)(x2, training = True)  ## MCD ####\n    #x2 = DeeperAttentionLayer1(units=64, use_scale=True)(x2)\n    #x2 = AttentionBlock(64)(x2)\n    \n    x1, x2 = AttentionBlock(channels = 64)([x1, x2])  ## MIFA ####\n    \n    x1 = RGSA(x1, filters=64)\n    x1 = tf.keras.layers.Dropout(0.25)(x1, training = True)  ## MCD ####\n    #x1 = DeeperAttentionLayer1(units=64, use_scale=True)(x1) ## MFA ####\n    #x1 = AttentionBlock(64)(x1)\n    \n    x2 = RGSA(x2, filters=64)\n    x2 = tf.keras.layers.Dropout(0.25)(x2, training = True)  ## MCD ####\n    #x2 = DeeperAttentionLayer1(units=64, use_scale=True)(x2)\n    #x2 = AttentionBlock(64)(x2)\n\n    x1, x2 = AttentionBlock(channels = 64)([x1, x2])  ## MIFA ####\n    \n    x1 = RGSA(x1, filters=128, strides=(2, 2), use_projection=True)\n    x1 = tf.keras.layers.Dropout(0.25)(x1, training = True)  ## MCD ####\n    #x1 = DeeperAttentionLayer1(units=128, use_scale=True)(x1) ## MFA ####\n    #x1 = AttentionBlock(128)(x1)\n\n    x2 = RGSA(x2, filters=128, strides=(2, 2), use_projection=True)\n    x2 = tf.keras.layers.Dropout(0.25)(x2, training = True)  ## MCD ####\n    #x2 = DeeperAttentionLayer1(units=128, use_scale=True)(x2)\n    #x2 = AttentionBlock(128)(x2)\n\n    x1, x2 = AttentionBlock(channels = 128)([x1, x2])  ## MIFA ####\n    \n    x1 = RGSA(x1, filters=128)\n    x1 = tf.keras.layers.Dropout(0.25)(x1, training = True)  ## MCD ####\n    #x1 = DeeperAttentionLayer1(units=128, use_scale=True)(x1)\n    #x1 = AttentionBlock(128)(x1)\n  \n    x2 = RGSA(x2, filters=128)\n    x2 = tf.keras.layers.Dropout(0.25)(x2, training = True)  ## MCD ####\n    #x2 = DeeperAttentionLayer1(units=128, use_scale=True)(x2)\n    #x2 = AttentionBlock(128)(x2)\n\n    x1, x2 = AttentionBlock(channels = 128)([x1, x2])  ## MIFA ####\n    \n    x1 = RGSA(x1, filters=256, strides=(2, 2), use_projection=True)\n    x1 = tf.keras.layers.Dropout(0.25)(x1, training = True)  ## MCD ####\n    #x1 = DeeperAttentionLayer1(units=256, use_scale=True)(x1)\n    #x1 = AttentionBlock(256)(x1)\n    \n    x2 = RGSA(x2, filters=256, strides=(2, 2), use_projection=True)\n    x2 = tf.keras.layers.Dropout(0.25)(x2, training = True)  ## MCD ####\n    #x2 = DeeperAttentionLayer1(units=256, use_scale=True)(x2)\n    #x2 = AttentionBlock(256)(x2)\n\n    x1, x2 = AttentionBlock(channels = 256)([x1, x2])  ## MIFA ####\n    \n    \n    x1 = RGSA(x1, filters=256)\n    x1 = tf.keras.layers.Dropout(0.25)(x1, training = True)  ## MCD ####\n    #x1 = DeeperAttentionLayer1(units=256, use_scale=True)(x1)\n    #x1 = AttentionBlock(256)(x1)\n    \n    x2 = RGSA(x2, filters=256)\n    x2 = tf.keras.layers.Dropout(0.25)(x2, training = True)  ## MCD ####\n    #x2 = DeeperAttentionLayer1(units=256, use_scale=True)(x2)\n    #x2 = AttentionBlock(256)(x2)\n    \n    x1, x2 = AttentionBlock(channels = 256)([x1, x2])  ## MIFA ####\n\n    x1 = RGSA(x1, filters=512, strides=(2, 2), use_projection=True)\n    #x1 = DeeperAttentionLayer1(units=512, use_scale=True)(x1)\n    #x1 = AttentionBlock(512)(x1)\n    \n    \n    x2 = RGSA(x2, filters=512, strides=(2, 2), use_projection=True)\n    #x2 = DeeperAttentionLayer1(units=512, use_scale=True)(x2)\n    #x2 = AttentionBlock(512)(x2)\n\n    x1, x2 = AttentionBlock(channels = 512)([x1, x2])  ## MIFA ####\n    \n    x1 = RGSA(x1, filters=512)\n    x2 = RGSA(x2, filters=512)\n    x1, x2 = AttentionBlock(channels = 512)([x1, x2])\n    \n    return x1, x2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T07:41:27.827825Z","iopub.execute_input":"2025-03-04T07:41:27.828146Z","iopub.status.idle":"2025-03-04T07:41:27.839538Z","shell.execute_reply.started":"2025-03-04T07:41:27.828123Z","shell.execute_reply":"2025-03-04T07:41:27.838467Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#def build_resnet18(input_shape=(128, 128, 3), num_classes=2):\ninput_shape=(128, 128, 3)\ninputs1 = Input(shape=input_shape)\ninputs2 = Input(shape=input_shape)\n\nimport tensorflow.keras.layers as L\n\n#input_data = Input(shape=input_shape, name='input_data')\n# Initial convolutional layer\n\nx1, x2 = residual_GLC_branch1(inputs1, inputs2)\n#print('x:',x.shape)\n\ncon = tf.keras.layers.Concatenate(axis=-1)([x1, x2])\n\ncon = tf.keras.layers.Dropout(0.25)(con, training = True)  ## MCD ####\n\nx = GlobalAveragePooling2D()(con)\nprint('GlobalAveragePooling2D x:',x.shape)\n\noutputs1 = Dense(5, activation='softmax')(x)\noutputs2 = Dense(7, activation='softmax')(x)\n\n# Create the model\nmodel = Model([inputs1, inputs2], [outputs1, outputs2])\n#return model\nprint(model.summary())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T07:42:01.542735Z","iopub.execute_input":"2025-03-04T07:42:01.543106Z","iopub.status.idle":"2025-03-04T07:42:09.471388Z","execution_failed":"2025-03-04T08:37:35.947Z"}},"outputs":[{"name":"stdout","text":"GlobalAveragePooling2D x: (None, 1024)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m16,106,028\u001b[0m (61.44 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">16,106,028</span> (61.44 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m26,880\u001b[0m (105.00 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">26,880</span> (105.00 KB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"None\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nimport tensorflow as tf\n\n# Hyperparameters\ninitial_gamma = 0.5\nlearning_rate = 1e-2\noptimizer = Adam(learning_rate=0.001)\n\n# Compile the model initially\nmodel.compile(optimizer=optimizer,\n              loss=['categorical_crossentropy', 'categorical_crossentropy'],\n              loss_weights=[initial_gamma, (1 - initial_gamma)],\n              metrics=['accuracy', 'accuracy'])\n\ndef checkpoint_callback():\n    checkpoint_filepath = 'best1_model_cer_skin_lung.keras'\n    return ModelCheckpoint(filepath=checkpoint_filepath,\n                           save_weights_only=False,\n                           monitor='val_loss',\n                           save_best_only=True,\n                           mode='min',\n                           verbose=1)\n\ndef early_stopping(patience):\n    return EarlyStopping(monitor='val_loss', restore_best_weights=True, patience=60, verbose=1)\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n                              patience=50, verbose=1, min_lr=0.00001)\n\ncallbacks = [checkpoint_callback(), early_stopping(patience=100), reduce_lr]\n\n# Alternating Training Loop\nnum_epochs = 200\nfor epoch in range(num_epochs):\n    print(f\"Epoch {epoch+1}/{num_epochs}\")\n    \n    if epoch % 2 == 0:\n        # Train on first modality only (freeze second modality)\n        for layer in model.layers:\n            layer.trainable = True  # Unfreeze all layers\n        \n        # Set loss weights: train skin modality (set lung modality loss weight to 0)\n        model.compile(optimizer=optimizer,\n                      loss=['categorical_crossentropy', 'categorical_crossentropy'],\n                      loss_weights=[1.0, 0.0],\n                      metrics=['accuracy', 'accuracy'])\n        \n        history = model.fit([X_train_s, X_train_h], [y_train_s, y_train_h],\n                            epochs=10,\n                            validation_data=([X_val_s, X_val_h1], [y_val_s, y_val_h1]),\n                            verbose=1,\n                            shuffle=True,\n                            callbacks=callbacks)\n    \n    else:\n        # Train on second modality only (freeze first modality)\n        for layer in model.layers:\n            layer.trainable = True  # Unfreeze all layers\n        \n        # Set loss weights: train lung modality (set skin modality loss weight to 0)\n        model.compile(optimizer=optimizer,\n                      loss=['categorical_crossentropy', 'categorical_crossentropy'],\n                      loss_weights=[0.0, 1.0],\n                      metrics=['accuracy', 'accuracy'])\n        \n        history = model.fit([X_train_s, X_train_h], [y_train_s, y_train_h],\n                            epochs=1,\n                            validation_data=([X_val_s, X_val_h1], [y_val_s, y_val_h1]),\n                            verbose=1,\n                            shuffle=True,\n                            callbacks=callbacks)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T07:54:03.299895Z","iopub.execute_input":"2025-03-04T07:54:03.300276Z","execution_failed":"2025-03-04T08:37:35.947Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 643ms/step - dense_18_accuracy: 0.5902 - dense_18_loss: 1.1879 - dense_19_accuracy: 0.0187 - dense_19_loss: 0.0000e+00 - loss: 1.1879\nEpoch 1: val_loss improved from inf to 2.39414, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m408s\u001b[0m 778ms/step - dense_18_accuracy: 0.5906 - dense_18_loss: 1.1867 - dense_19_accuracy: 0.0187 - dense_19_loss: 0.0000e+00 - loss: 1.1867 - val_dense_18_accuracy: 0.3395 - val_dense_18_loss: 2.4609 - val_dense_19_accuracy: 0.0278 - val_dense_19_loss: 0.0000e+00 - val_loss: 2.3941 - learning_rate: 0.0010\nRestoring model weights from the end of the best epoch: 1.\nEpoch 2/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 627ms/step - dense_18_accuracy: 0.4835 - dense_18_loss: 0.0000e+00 - dense_19_accuracy: 0.6284 - dense_19_loss: 1.1892 - loss: 1.1892\nEpoch 1: val_loss improved from 2.39414 to 1.84427, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m381s\u001b[0m 758ms/step - dense_18_accuracy: 0.4832 - dense_18_loss: 0.0000e+00 - dense_19_accuracy: 0.6285 - dense_19_loss: 1.1885 - loss: 1.1885 - val_dense_18_accuracy: 0.2423 - val_dense_18_loss: 0.0000e+00 - val_dense_19_accuracy: 0.2870 - val_dense_19_loss: 1.8528 - val_loss: 1.8443 - learning_rate: 0.0010\nRestoring model weights from the end of the best epoch: 1.\nEpoch 3/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576ms/step - dense_18_accuracy: 0.6994 - dense_18_loss: 0.8218 - dense_19_accuracy: 0.6629 - dense_19_loss: 0.0000e+00 - loss: 0.8218\nEpoch 1: val_loss did not improve from 1.84427\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m351s\u001b[0m 684ms/step - dense_18_accuracy: 0.6996 - dense_18_loss: 0.8214 - dense_19_accuracy: 0.6628 - dense_19_loss: 0.0000e+00 - loss: 0.8214 - val_dense_18_accuracy: 0.4491 - val_dense_18_loss: 2.0441 - val_dense_19_accuracy: 0.6296 - val_dense_19_loss: 0.0000e+00 - val_loss: 2.0685 - learning_rate: 0.0010\nRestoring model weights from the end of the best epoch: 1.\nEpoch 4/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 564ms/step - dense_18_accuracy: 0.6632 - dense_18_loss: 0.0000e+00 - dense_19_accuracy: 0.6826 - dense_19_loss: 0.9085 - loss: 0.9085\nEpoch 1: val_loss improved from 1.84427 to 1.55364, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m349s\u001b[0m 682ms/step - dense_18_accuracy: 0.6631 - dense_18_loss: 0.0000e+00 - dense_19_accuracy: 0.6826 - dense_19_loss: 0.9084 - loss: 0.9084 - val_dense_18_accuracy: 0.4568 - val_dense_18_loss: 0.0000e+00 - val_dense_19_accuracy: 0.5340 - val_dense_19_loss: 1.5909 - val_loss: 1.5536 - learning_rate: 0.0010\nRestoring model weights from the end of the best epoch: 1.\nEpoch 5/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 583ms/step - dense_18_accuracy: 0.7784 - dense_18_loss: 0.6382 - dense_19_accuracy: 0.6590 - dense_19_loss: 0.0000e+00 - loss: 0.6382\nEpoch 1: val_loss improved from 1.55364 to 1.07050, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m356s\u001b[0m 701ms/step - dense_18_accuracy: 0.7785 - dense_18_loss: 0.6380 - dense_19_accuracy: 0.6590 - dense_19_loss: 0.0000e+00 - loss: 0.6380 - val_dense_18_accuracy: 0.6142 - val_dense_18_loss: 1.0807 - val_dense_19_accuracy: 0.6065 - val_dense_19_loss: 0.0000e+00 - val_loss: 1.0705 - learning_rate: 0.0010\nRestoring model weights from the end of the best epoch: 1.\nEpoch 6/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 569ms/step - dense_18_accuracy: 0.7239 - dense_18_loss: 0.0000e+00 - dense_19_accuracy: 0.6794 - dense_19_loss: 0.8820 - loss: 0.8820\nEpoch 1: val_loss did not improve from 1.07050\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m354s\u001b[0m 680ms/step - dense_18_accuracy: 0.7237 - dense_18_loss: 0.0000e+00 - dense_19_accuracy: 0.6795 - dense_19_loss: 0.8819 - loss: 0.8819 - val_dense_18_accuracy: 0.4321 - val_dense_18_loss: 0.0000e+00 - val_dense_19_accuracy: 0.5494 - val_dense_19_loss: 1.2704 - val_loss: 1.2511 - learning_rate: 0.0010\nRestoring model weights from the end of the best epoch: 1.\nEpoch 7/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 600ms/step - dense_18_accuracy: 0.7859 - dense_18_loss: 0.5750 - dense_19_accuracy: 0.6933 - dense_19_loss: 0.0000e+00 - loss: 0.5750\nEpoch 1: val_loss improved from 1.07050 to 0.97201, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m367s\u001b[0m 725ms/step - dense_18_accuracy: 0.7860 - dense_18_loss: 0.5748 - dense_19_accuracy: 0.6933 - dense_19_loss: 0.0000e+00 - loss: 0.5748 - val_dense_18_accuracy: 0.5988 - val_dense_18_loss: 0.9640 - val_dense_19_accuracy: 0.4398 - val_dense_19_loss: 0.0000e+00 - val_loss: 0.9720 - learning_rate: 0.0010\nRestoring model weights from the end of the best epoch: 1.\nEpoch 8/200\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train_ct = X_train_s\nX_train_mri = X_train_h\ny_train_ct = y_train_s \ny_train_mri = y_train_h\n\nX_val_ct = X_val_s\nX_val_mri = X_val_h\ny_val_ct = y_val_s\ny_val_mri = y_val_h","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T07:42:18.812656Z","iopub.execute_input":"2025-03-04T07:42:18.813003Z","iopub.status.idle":"2025-03-04T07:42:18.817249Z","shell.execute_reply.started":"2025-03-04T07:42:18.812975Z","shell.execute_reply":"2025-03-04T07:42:18.816235Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n\n# Optimizer and learning rate setup\ninitial_gamma1 = 0.25\ninitial_gamma2 = 0.25\ninitial_gamma3 = 0.25\nlearning_rate = 1e-2\noptimizer = Adam(learning_rate=learning_rate)\n\n# Compile the model with the custom optimizer\nmodel.compile(optimizer=optimizer,\n              loss=['categorical_crossentropy', 'categorical_crossentropy', 'categorical_crossentropy', 'categorical_crossentropy'],\n              loss_weights=[initial_gamma1, initial_gamma2, initial_gamma3, (1 - (initial_gamma1 + initial_gamma2 + initial_gamma3))],\n              metrics=['accuracy', 'accuracy', 'accuracy', 'accuracy'])\n\n# Checkpoint callback\ndef checkpoint_callback():\n    checkpoint_filepath = 'best1_model_cer_skin_lung.keras'\n    model_checkpoint_callback = ModelCheckpoint(filepath=checkpoint_filepath,\n                                                save_weights_only=False,\n                                                monitor='val_loss',\n                                                save_best_only=True,\n                                                mode='min',\n                                                verbose=1)\n    return model_checkpoint_callback\n\n# Early stopping callback\ndef early_stopping(patience):\n    es_callback = EarlyStopping(monitor='val_loss', restore_best_weights=True, patience=patience, verbose=1)\n    return es_callback\n\n# Reduce learning rate callback\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n                              patience=50, verbose=1, min_lr=0.00001)\n\n# Callbacks list\ncheckpoint_callback = checkpoint_callback()\nearly_stopping = early_stopping(patience=100)\ncallbacks = [checkpoint_callback, early_stopping, reduce_lr]\n\nimport tensorflow as tf\nimport numpy as np\n\n# Updated generator to ensure outputs are TensorFlow-compatible\ndef alternating_training_data_generator(X_train_s, X_train_h, X_train_ct, X_train_mri, y_train_s, y_train_h, y_train_ct, y_train_mri, batch_size):\n    while True:\n        # Shuffle training data (if required)\n        indices_s = np.random.permutation(len(X_train_s))\n        indices_h = np.random.permutation(len(X_train_h))\n        indices_ct = np.random.permutation(len(X_train_ct))\n        indices_mri = np.random.permutation(len(X_train_mri))\n\n        for i in range(0, len(X_train_s), batch_size):\n            # Alternating between modalities\n            if i % 4 == 0:\n                # Train with skin modality (X_train_s, y_train_s)\n                X_batch = X_train_s[indices_s[i:i+batch_size]]\n                y_batch = y_train_s[indices_s[i:i+batch_size]]\n                # Feed all modalities, fill others with zeros\n                X_batch_all = [X_batch, np.zeros_like(X_batch), np.zeros_like(X_batch), np.zeros_like(X_batch)]\n                y_batch_all = [y_batch, np.zeros_like(y_batch), np.zeros_like(y_batch), np.zeros_like(y_batch)]\n            elif i % 4 == 1:\n                # Train with lung modality (X_train_h, y_train_h)\n                X_batch = X_train_h[indices_h[i:i+batch_size]]\n                y_batch = y_train_h[indices_h[i:i+batch_size]]\n                # Fill others with zeros\n                X_batch_all = [np.zeros_like(X_batch), X_batch, np.zeros_like(X_batch), np.zeros_like(X_batch)]\n                y_batch_all = [np.zeros_like(y_batch), y_batch, np.zeros_like(y_batch), np.zeros_like(y_batch)]\n            elif i % 4 == 2:\n                # Train with CT modality (X_train_ct, y_train_ct)\n                X_batch = X_train_ct[indices_ct[i:i+batch_size]]\n                y_batch = y_train_ct[indices_ct[i:i+batch_size]]\n                # Fill others with zeros\n                X_batch_all = [np.zeros_like(X_batch), np.zeros_like(X_batch), X_batch, np.zeros_like(X_batch)]\n                y_batch_all = [np.zeros_like(y_batch), np.zeros_like(y_batch), y_batch, np.zeros_like(y_batch)]\n            else:\n                # Train with MRI modality (X_train_mri, y_train_mri)\n                X_batch = X_train_mri[indices_mri[i:i+batch_size]]\n                y_batch = y_train_mri[indices_mri[i:i+batch_size]]\n                # Fill others with zeros\n                X_batch_all = [np.zeros_like(X_batch), np.zeros_like(X_batch), np.zeros_like(X_batch), X_batch]\n                y_batch_all = [np.zeros_like(y_batch), np.zeros_like(y_batch), np.zeros_like(y_batch), y_batch]\n\n            # Convert to TensorFlow Tensors before yielding\n            yield (tf.convert_to_tensor(X_batch_all), tf.convert_to_tensor(y_batch_all))  # Return as tuple of tensors\n\n\n# Updated validation data generator with same structure\ndef alternating_validation_data_generator(X_val_s, X_val_h, X_val_ct, X_val_mri, y_val_s, y_val_h, y_val_ct, y_val_mri, batch_size):\n    while True:\n        # Shuffle validation data (if required)\n        indices_s = np.random.permutation(len(X_val_s))\n        indices_h = np.random.permutation(len(X_val_h))\n        indices_ct = np.random.permutation(len(X_val_ct))\n        indices_mri = np.random.permutation(len(X_val_mri))\n\n        for i in range(0, len(X_val_s), batch_size):\n            # Alternating between modalities\n            if i % 4 == 0:\n                # Validation with skin modality (X_val_s, y_val_s)\n                X_batch = X_val_s[indices_s[i:i+batch_size]]\n                y_batch = y_val_s[indices_s[i:i+batch_size]]\n                # You need to feed all modalities, fill others with zeros\n                X_batch_all = [X_batch, np.zeros_like(X_batch), np.zeros_like(X_batch), np.zeros_like(X_batch)]\n                y_batch_all = [y_batch, np.zeros_like(y_batch), np.zeros_like(y_batch), np.zeros_like(y_batch)]\n            elif i % 4 == 1:\n                # Validation with lung modality (X_val_h, y_val_h)\n                X_batch = X_val_h[indices_h[i:i+batch_size]]\n                y_batch = y_val_h[indices_h[i:i+batch_size]]\n                # Fill others with zeros\n                X_batch_all = [np.zeros_like(X_batch), X_batch, np.zeros_like(X_batch), np.zeros_like(X_batch)]\n                y_batch_all = [np.zeros_like(y_batch), y_batch, np.zeros_like(y_batch), np.zeros_like(y_batch)]\n            elif i % 4 == 2:\n                # Validation with CT modality (X_val_ct, y_val_ct)\n                X_batch = X_val_ct[indices_ct[i:i+batch_size]]\n                y_batch = y_val_ct[indices_ct[i:i+batch_size]]\n                # Fill others with zeros\n                X_batch_all = [np.zeros_like(X_batch), np.zeros_like(X_batch), X_batch, np.zeros_like(X_batch)]\n                y_batch_all = [np.zeros_like(y_batch), np.zeros_like(y_batch), y_batch, np.zeros_like(y_batch)]\n            else:\n                # Validation with MRI modality (X_val_mri, y_val_mri)\n                X_batch = X_val_mri[indices_mri[i:i+batch_size]]\n                y_batch = y_val_mri[indices_mri[i:i+batch_size]]\n                # Fill others with zeros\n                X_batch_all = [np.zeros_like(X_batch), np.zeros_like(X_batch), np.zeros_like(X_batch), X_batch]\n                y_batch_all = [np.zeros_like(y_batch), np.zeros_like(y_batch), np.zeros_like(y_batch), y_batch]\n\n            # Convert to TensorFlow Tensors before yielding\n            yield (tf.convert_to_tensor(X_batch_all), tf.convert_to_tensor(y_batch_all))  # Return as tuple of tensors\n\n\n# In model.fit, update the validation data to use the validation data generator\nhistory = model.fit(\n    alternating_training_data_generator(X_train_s, X_train_h, X_train_ct, X_train_mri, y_train_s, y_train_h, y_train_ct, y_train_mri, batch_size=32),\n    epochs=200,\n    validation_data=alternating_validation_data_generator(X_val_s, X_val_h, X_val_ct, X_val_mri, y_val_s, y_val_h, y_val_ct, y_val_mri, batch_size=32),\n    steps_per_epoch=len(X_train_s) // 32,  # Adjust based on your batch size and dataset length\n    validation_steps=len(X_val_s) // 32,  # Same logic for validation\n    callbacks=callbacks\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T07:46:26.547661Z","iopub.execute_input":"2025-03-04T07:46:26.548181Z","iopub.status.idle":"2025-03-04T07:46:31.385698Z","shell.execute_reply.started":"2025-03-04T07:46:26.548140Z","shell.execute_reply":"2025-03-04T07:46:31.384406Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/200\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-660a45656297>\u001b[0m in \u001b[0;36m<cell line: 136>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;31m# In model.fit, update the validation data to use the validation data generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m    137\u001b[0m     \u001b[0malternating_training_data_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_ct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_mri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_ct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_mri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/layers/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    161\u001b[0m             \u001b[0;34mf'Layer \"{layer_name}\" expects {len(input_spec)} input(s),'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;34mf\" but it received {len(inputs)} input tensors. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Layer \"functional_1\" expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'data:0' shape=(None, 32, 128, 128, 3) dtype=int32>]"],"ename":"ValueError","evalue":"Layer \"functional_1\" expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'data:0' shape=(None, 32, 128, 128, 3) dtype=int32>]","output_type":"error"}],"execution_count":27},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\n\n# Updated generator for two modalities (Skin and Lung)\ndef alternating_training_data_generator(X_train_s, X_train_h, y_train_s, y_train_h, batch_size):\n    while True:\n        # Shuffle training data (if required)\n        indices_s = np.random.permutation(len(X_train_s))\n        indices_h = np.random.permutation(len(X_train_h))\n\n        for i in range(0, len(X_train_s), batch_size):\n            # Alternating between two modalities\n            if i % 2 == 0:\n                # Train with skin modality (X_train_s, y_train_s)\n                X_batch = X_train_s[indices_s[i:i+batch_size]]\n                y_batch = y_train_s[indices_s[i:i+batch_size]]\n                # Feed all modalities, fill the second with zeros\n                X_batch_all = [X_batch, np.zeros_like(X_batch)]  # Skin and empty lung\n                y_batch_all = [y_batch, np.zeros_like(y_batch)]  # Skin and empty lung\n            else:\n                # Train with lung modality (X_train_h, y_train_h)\n                X_batch = X_train_h[indices_h[i:i+batch_size]]\n                y_batch = y_train_h[indices_h[i:i+batch_size]]\n                # Fill the first modality with zeros\n                X_batch_all = [np.zeros_like(X_batch), X_batch]  # Empty skin and lung\n                y_batch_all = [np.zeros_like(y_batch), y_batch]  # Empty skin and lung\n\n            # Convert to TensorFlow tensors before yielding\n            yield (tf.convert_to_tensor(X_batch_all), tf.convert_to_tensor(y_batch_all))  # Return as tuple of tensors\n\n\n# Updated validation data generator for two modalities\ndef alternating_validation_data_generator(X_val_s, X_val_h, y_val_s, y_val_h, batch_size):\n    while True:\n        # Shuffle validation data (if required)\n        indices_s = np.random.permutation(len(X_val_s))\n        indices_h = np.random.permutation(len(X_val_h))\n\n        for i in range(0, len(X_val_s), batch_size):\n            # Alternating between two modalities\n            if i % 2 == 0:\n                # Validation with skin modality (X_val_s, y_val_s)\n                X_batch = X_val_s[indices_s[i:i+batch_size]]\n                y_batch = y_val_s[indices_s[i:i+batch_size]]\n                # Feed all modalities, fill the second with zeros\n                X_batch_all = [X_batch, np.zeros_like(X_batch)]  # Skin and empty lung\n                y_batch_all = [y_batch, np.zeros_like(y_batch)]  # Skin and empty lung\n            else:\n                # Validation with lung modality (X_val_h, y_val_h)\n                X_batch = X_val_h[indices_h[i:i+batch_size]]\n                y_batch = y_val_h[indices_h[i:i+batch_size]]\n                # Fill the first modality with zeros\n                X_batch_all = [np.zeros_like(X_batch), X_batch]  # Empty skin and lung\n                y_batch_all = [np.zeros_like(y_batch), y_batch]  # Empty skin and lung\n\n            # Convert to TensorFlow tensors before yielding\n            yield (tf.convert_to_tensor(X_batch_all), tf.convert_to_tensor(y_batch_all))  # Return as tuple of tensors\n\n\n# In model.fit, update the validation data to use the validation data generator\nhistory = model.fit(\n    alternating_training_data_generator(X_train_s, X_train_h, y_train_s, y_train_h, batch_size=32),\n    epochs=200,\n    validation_data=alternating_validation_data_generator(X_val_s, X_val_h, y_val_s, y_val_h, batch_size=32),\n    steps_per_epoch=len(X_train_s) // 32,  # Adjust based on your batch size and dataset length\n    validation_steps=len(X_val_s) // 32,  # Same logic for validation\n    callbacks=callbacks\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T07:47:49.224732Z","iopub.execute_input":"2025-03-04T07:47:49.225106Z","iopub.status.idle":"2025-03-04T07:47:51.670457Z","shell.execute_reply.started":"2025-03-04T07:47:49.225076Z","shell.execute_reply":"2025-03-04T07:47:51.669167Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/200\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-cb544c2b3e9a>\u001b[0m in \u001b[0;36m<cell line: 61>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;31m# In model.fit, update the validation data to use the validation data generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0malternating_training_data_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/layers/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    161\u001b[0m             \u001b[0;34mf'Layer \"{layer_name}\" expects {len(input_spec)} input(s),'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;34mf\" but it received {len(inputs)} input tensors. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Layer \"functional_1\" expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'data:0' shape=(None, 32, 128, 128, 3) dtype=int32>]"],"ename":"ValueError","evalue":"Layer \"functional_1\" expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'data:0' shape=(None, 32, 128, 128, 3) dtype=int32>]","output_type":"error"}],"execution_count":28},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nimport tensorflow as tf\n\ninitial_gamma = 0.5\nlearning_rate = 1e-2\noptimizer = Adam(learning_rate=0.001)\n\n# Compile the model initially\nmodel.compile(optimizer=optimizer,\n              loss=['categorical_crossentropy', 'categorical_crossentropy'],\n              loss_weights=[initial_gamma, (1 - initial_gamma)],\n              metrics=['accuracy', 'accuracy'])\n\ndef checkpoint_callback():\n    checkpoint_filepath = 'best1_model_cer_skin_lung.keras'\n    return ModelCheckpoint(filepath=checkpoint_filepath,\n                           save_weights_only=False,\n                           monitor='val_loss',\n                           save_best_only=True,\n                           mode='min',\n                           verbose=1)\n\ndef early_stopping(patience):\n    return EarlyStopping(monitor='val_loss', restore_best_weights=True, patience=60, verbose=1)\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n                              patience=50, verbose=1, min_lr=0.00001)\n\ncallbacks = [checkpoint_callback(), early_stopping(patience=100), reduce_lr]\n\n# Alternating Training Loop\nnum_epochs = 200\nfor epoch in range(num_epochs):\n    print(f\"Epoch {epoch+1}/{num_epochs}\")\n    if epoch % 2 == 0:\n        # Train on first modality only (freeze second modality)\n        for layer in model.layers:\n            layer.trainable = True  # Unfreeze all layers\n        model.compile(optimizer=optimizer,\n                      loss=['categorical_crossentropy', 'categorical_crossentropy'],\n                      loss_weights=[1.0, 0.0],\n                      metrics=['accuracy', 'accuracy'])\n        history = model.fit([X_train_s, X_train_h], [y_train_s, y_train_h],\n                            epochs=1,\n                            validation_data=([X_val_s, X_val_h1], [y_val_s, y_val_h1]), verbose=1,\n                            shuffle=True,\n                            callbacks=callbacks)\n    else:\n        # Train on second modality only (freeze first modality)\n        for layer in model.layers:\n            layer.trainable = True  # Unfreeze all layers\n        model.compile(optimizer=optimizer,\n                      loss=['categorical_crossentropy', 'categorical_crossentropy'],\n                      loss_weights=[0.0, 1.0],\n                      metrics=['accuracy', 'accuracy'])\n        history = model.fit([X_train_s, X_train_h], [y_train_s, y_train_h],\n                            epochs=1,\n                            validation_data=([X_val_s, X_val_h1], [y_val_s, y_val_h1]), verbose=1,\n                            shuffle=True,\n                            callbacks=callbacks)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\n\ninitial_gamma1 = 0.25\ninitial_gamma2 = 0.25\ninitial_gamma3 = 0.25\nlearning_rate = 1e-2\noptimizer = Adam(learning_rate=0.001)\n#opt = Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.9, epsilon=None, amsgrad=False)\n# Compile the model with the custom optimizer\nmodel.compile(optimizer=optimizer,\n              loss=['categorical_crossentropy', 'categorical_crossentropy', 'categorical_crossentropy', 'categorical_crossentropy'],\n              loss_weights=[initial_gamma1, initial_gamma2, initial_gamma3, (1 -  (initial_gamma1 + initial_gamma2 + initial_gamma3))],\n              metrics=['accuracy', 'accuracy', 'accuracy', 'accuracy'])\n\n\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\ndef checkpoint_callback():\n\n    checkpoint_filepath = 'best1_model_cer_skin_lung.keras'\n\n    model_checkpoint_callback= ModelCheckpoint(filepath=checkpoint_filepath,\n                           save_weights_only=False,\n                           #frequency='epoch',\n                           monitor='val_loss',\n                           save_best_only=True,\n                            mode='min',\n                           verbose=1)\n\n    return model_checkpoint_callback\n\ndef early_stopping(patience):\n    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', restore_best_weights=True, patience=60, verbose=1)\n    return es_callback\n\n\n\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n                              patience=50, verbose = 1, min_lr=0.00001)\n\ncheckpoint_callback = checkpoint_callback()\n\nearly_stopping = early_stopping(patience=100)\ncallbacks = [checkpoint_callback, early_stopping, reduce_lr]\n            \n\n# Fit the model with callbacks\nhistory = model.fit([X_train_s, X_train_h, X_train_ct, X_train_mri], [y_train_s, y_train_h, y_train_ct, y_train_mri],\n                    epochs=200,\n                    validation_data=([X_val_s, X_val_h1, X_val_ct, X_val_mri], [y_val_s, y_val_h1, y_val_ct, y_val_mri]), verbose=1,\n                    shuffle=True,\n                    callbacks=callbacks) # UpdateGammaCallback","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\n\ninitial_gamma = 0.5\nlearning_rate = 1e-2\noptimizer = Adam(learning_rate=0.001)\n#opt = Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.9, epsilon=None, amsgrad=False)\n# Compile the model with the custom optimizer\nmodel.compile(optimizer=optimizer,\n              loss=['categorical_crossentropy', 'categorical_crossentropy'],\n              loss_weights=[initial_gamma, (1 -  initial_gamma)],\n              metrics=['accuracy', 'accuracy'])\n\n\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\ndef checkpoint_callback():\n\n    checkpoint_filepath = 'best1_model_cer_skin_lung.keras'\n\n    model_checkpoint_callback= ModelCheckpoint(filepath=checkpoint_filepath,\n                           save_weights_only=False,\n                           #frequency='epoch',\n                           monitor='val_loss',\n                           save_best_only=True,\n                            mode='min',\n                           verbose=1)\n\n    return model_checkpoint_callback\n\ndef early_stopping(patience):\n    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', restore_best_weights=True, patience=60, verbose=1)\n    return es_callback\n\n\n\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n                              patience=50, verbose = 1, min_lr=0.00001)\n\ncheckpoint_callback = checkpoint_callback()\n\nearly_stopping = early_stopping(patience=100)\ncallbacks = [checkpoint_callback, early_stopping, reduce_lr]\n            \n\n# Fit the model with callbacks\nhistory = model.fit([X_train_s, X_train_h], [y_train_s, y_train_h],\n                    epochs=200,\n                    validation_data=([X_val_s, X_val_h1], [y_val_s, y_val_h1]), verbose=1,\n                    shuffle=True,\n                    callbacks=callbacks) # UpdateGammaCallback","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T07:12:44.178713Z","iopub.execute_input":"2025-03-04T07:12:44.179055Z","execution_failed":"2025-03-04T07:38:23.648Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 617ms/step - dense_8_accuracy: 0.5753 - dense_8_loss: 0.5614 - dense_9_accuracy: 0.6137 - dense_9_loss: 0.5713 - loss: 1.1328\nEpoch 1: val_loss improved from inf to 2.47334, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m392s\u001b[0m 741ms/step - dense_8_accuracy: 0.5757 - dense_8_loss: 0.5610 - dense_9_accuracy: 0.6139 - dense_9_loss: 0.5710 - loss: 1.1320 - val_dense_8_accuracy: 0.3364 - val_dense_8_loss: 1.1113 - val_dense_9_accuracy: 0.0926 - val_dense_9_loss: 1.3643 - val_loss: 2.4733 - learning_rate: 0.0010\nEpoch 2/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - dense_8_accuracy: 0.7654 - dense_8_loss: 0.3310 - dense_9_accuracy: 0.6764 - dense_9_loss: 0.4535 - loss: 0.7846\nEpoch 2: val_loss improved from 2.47334 to 1.47917, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 168ms/step - dense_8_accuracy: 0.7654 - dense_8_loss: 0.3310 - dense_9_accuracy: 0.6764 - dense_9_loss: 0.4535 - loss: 0.7845 - val_dense_8_accuracy: 0.7284 - val_dense_8_loss: 0.3574 - val_dense_9_accuracy: 0.2685 - val_dense_9_loss: 1.1238 - val_loss: 1.4792 - learning_rate: 0.0010\nEpoch 3/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - dense_8_accuracy: 0.8236 - dense_8_loss: 0.2493 - dense_9_accuracy: 0.6960 - dense_9_loss: 0.4169 - loss: 0.6663\nEpoch 3: val_loss did not improve from 1.47917\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 155ms/step - dense_8_accuracy: 0.8236 - dense_8_loss: 0.2493 - dense_9_accuracy: 0.6960 - dense_9_loss: 0.4169 - loss: 0.6662 - val_dense_8_accuracy: 0.5648 - val_dense_8_loss: 0.7804 - val_dense_9_accuracy: 0.5571 - val_dense_9_loss: 0.8845 - val_loss: 1.6696 - learning_rate: 0.0010\nEpoch 4/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - dense_8_accuracy: 0.8413 - dense_8_loss: 0.2296 - dense_9_accuracy: 0.7158 - dense_9_loss: 0.3933 - loss: 0.6229\nEpoch 4: val_loss improved from 1.47917 to 1.33994, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 164ms/step - dense_8_accuracy: 0.8413 - dense_8_loss: 0.2295 - dense_9_accuracy: 0.7158 - dense_9_loss: 0.3933 - loss: 0.6229 - val_dense_8_accuracy: 0.6944 - val_dense_8_loss: 0.4671 - val_dense_9_accuracy: 0.5123 - val_dense_9_loss: 0.8613 - val_loss: 1.3399 - learning_rate: 0.0010\nEpoch 5/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - dense_8_accuracy: 0.8694 - dense_8_loss: 0.1827 - dense_9_accuracy: 0.7163 - dense_9_loss: 0.3801 - loss: 0.5627\nEpoch 5: val_loss improved from 1.33994 to 0.95884, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 164ms/step - dense_8_accuracy: 0.8694 - dense_8_loss: 0.1827 - dense_9_accuracy: 0.7163 - dense_9_loss: 0.3801 - loss: 0.5627 - val_dense_8_accuracy: 0.6991 - val_dense_8_loss: 0.4256 - val_dense_9_accuracy: 0.6574 - val_dense_9_loss: 0.5275 - val_loss: 0.9588 - learning_rate: 0.0010\nEpoch 6/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - dense_8_accuracy: 0.8848 - dense_8_loss: 0.1665 - dense_9_accuracy: 0.7391 - dense_9_loss: 0.3567 - loss: 0.5232\nEpoch 6: val_loss improved from 0.95884 to 0.72457, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 165ms/step - dense_8_accuracy: 0.8848 - dense_8_loss: 0.1665 - dense_9_accuracy: 0.7391 - dense_9_loss: 0.3567 - loss: 0.5233 - val_dense_8_accuracy: 0.7731 - val_dense_8_loss: 0.3405 - val_dense_9_accuracy: 0.7145 - val_dense_9_loss: 0.3792 - val_loss: 0.7246 - learning_rate: 0.0010\nEpoch 7/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - dense_8_accuracy: 0.9056 - dense_8_loss: 0.1450 - dense_9_accuracy: 0.7362 - dense_9_loss: 0.3556 - loss: 0.5006\nEpoch 7: val_loss did not improve from 0.72457\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 156ms/step - dense_8_accuracy: 0.9056 - dense_8_loss: 0.1450 - dense_9_accuracy: 0.7362 - dense_9_loss: 0.3556 - loss: 0.5006 - val_dense_8_accuracy: 0.8318 - val_dense_8_loss: 0.2425 - val_dense_9_accuracy: 0.3781 - val_dense_9_loss: 0.8568 - val_loss: 1.0980 - learning_rate: 0.0010\nEpoch 8/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - dense_8_accuracy: 0.9154 - dense_8_loss: 0.1188 - dense_9_accuracy: 0.7447 - dense_9_loss: 0.3473 - loss: 0.4661\nEpoch 8: val_loss did not improve from 0.72457\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 155ms/step - dense_8_accuracy: 0.9154 - dense_8_loss: 0.1188 - dense_9_accuracy: 0.7447 - dense_9_loss: 0.3473 - loss: 0.4661 - val_dense_8_accuracy: 0.8410 - val_dense_8_loss: 0.2166 - val_dense_9_accuracy: 0.2948 - val_dense_9_loss: 1.1995 - val_loss: 1.4083 - learning_rate: 0.0010\nEpoch 9/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - dense_8_accuracy: 0.9259 - dense_8_loss: 0.1084 - dense_9_accuracy: 0.7410 - dense_9_loss: 0.3409 - loss: 0.4493\nEpoch 9: val_loss did not improve from 0.72457\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 155ms/step - dense_8_accuracy: 0.9258 - dense_8_loss: 0.1084 - dense_9_accuracy: 0.7411 - dense_9_loss: 0.3409 - loss: 0.4493 - val_dense_8_accuracy: 0.7747 - val_dense_8_loss: 0.2896 - val_dense_9_accuracy: 0.6898 - val_dense_9_loss: 0.5112 - val_loss: 0.8159 - learning_rate: 0.0010\nEpoch 10/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - dense_8_accuracy: 0.9260 - dense_8_loss: 0.1066 - dense_9_accuracy: 0.7523 - dense_9_loss: 0.3296 - loss: 0.4363\nEpoch 10: val_loss did not improve from 0.72457\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 154ms/step - dense_8_accuracy: 0.9260 - dense_8_loss: 0.1066 - dense_9_accuracy: 0.7523 - dense_9_loss: 0.3296 - loss: 0.4363 - val_dense_8_accuracy: 0.7901 - val_dense_8_loss: 0.3057 - val_dense_9_accuracy: 0.5417 - val_dense_9_loss: 0.6069 - val_loss: 0.9185 - learning_rate: 0.0010\nEpoch 11/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - dense_8_accuracy: 0.9315 - dense_8_loss: 0.1035 - dense_9_accuracy: 0.7727 - dense_9_loss: 0.3142 - loss: 0.4177\nEpoch 11: val_loss improved from 0.72457 to 0.67458, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 164ms/step - dense_8_accuracy: 0.9315 - dense_8_loss: 0.1035 - dense_9_accuracy: 0.7726 - dense_9_loss: 0.3142 - loss: 0.4178 - val_dense_8_accuracy: 0.8117 - val_dense_8_loss: 0.2832 - val_dense_9_accuracy: 0.7253 - val_dense_9_loss: 0.3848 - val_loss: 0.6746 - learning_rate: 0.0010\nEpoch 12/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - dense_8_accuracy: 0.9342 - dense_8_loss: 0.0940 - dense_9_accuracy: 0.7666 - dense_9_loss: 0.3134 - loss: 0.4073\nEpoch 12: val_loss did not improve from 0.67458\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 156ms/step - dense_8_accuracy: 0.9342 - dense_8_loss: 0.0939 - dense_9_accuracy: 0.7665 - dense_9_loss: 0.3134 - loss: 0.4073 - val_dense_8_accuracy: 0.8148 - val_dense_8_loss: 0.2545 - val_dense_9_accuracy: 0.6914 - val_dense_9_loss: 0.6097 - val_loss: 0.8700 - learning_rate: 0.0010\nEpoch 13/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - dense_8_accuracy: 0.9377 - dense_8_loss: 0.0845 - dense_9_accuracy: 0.7721 - dense_9_loss: 0.3096 - loss: 0.3941\nEpoch 13: val_loss improved from 0.67458 to 0.58516, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 164ms/step - dense_8_accuracy: 0.9378 - dense_8_loss: 0.0845 - dense_9_accuracy: 0.7721 - dense_9_loss: 0.3097 - loss: 0.3941 - val_dense_8_accuracy: 0.8503 - val_dense_8_loss: 0.2175 - val_dense_9_accuracy: 0.7145 - val_dense_9_loss: 0.3554 - val_loss: 0.5852 - learning_rate: 0.0010\nEpoch 14/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - dense_8_accuracy: 0.9523 - dense_8_loss: 0.0701 - dense_9_accuracy: 0.7695 - dense_9_loss: 0.3083 - loss: 0.3784\nEpoch 14: val_loss did not improve from 0.58516\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 155ms/step - dense_8_accuracy: 0.9523 - dense_8_loss: 0.0701 - dense_9_accuracy: 0.7695 - dense_9_loss: 0.3083 - loss: 0.3784 - val_dense_8_accuracy: 0.8349 - val_dense_8_loss: 0.2398 - val_dense_9_accuracy: 0.6667 - val_dense_9_loss: 0.4620 - val_loss: 0.7051 - learning_rate: 0.0010\nEpoch 15/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - dense_8_accuracy: 0.9602 - dense_8_loss: 0.0582 - dense_9_accuracy: 0.7795 - dense_9_loss: 0.2995 - loss: 0.3577\nEpoch 15: val_loss did not improve from 0.58516\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 155ms/step - dense_8_accuracy: 0.9602 - dense_8_loss: 0.0582 - dense_9_accuracy: 0.7795 - dense_9_loss: 0.2995 - loss: 0.3577 - val_dense_8_accuracy: 0.8719 - val_dense_8_loss: 0.1824 - val_dense_9_accuracy: 0.7037 - val_dense_9_loss: 0.5513 - val_loss: 0.7432 - learning_rate: 0.0010\nEpoch 16/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - dense_8_accuracy: 0.9555 - dense_8_loss: 0.0613 - dense_9_accuracy: 0.7916 - dense_9_loss: 0.2817 - loss: 0.3430\nEpoch 16: val_loss did not improve from 0.58516\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 155ms/step - dense_8_accuracy: 0.9555 - dense_8_loss: 0.0613 - dense_9_accuracy: 0.7916 - dense_9_loss: 0.2818 - loss: 0.3431 - val_dense_8_accuracy: 0.8704 - val_dense_8_loss: 0.2055 - val_dense_9_accuracy: 0.6975 - val_dense_9_loss: 0.4516 - val_loss: 0.6538 - learning_rate: 0.0010\nEpoch 17/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - dense_8_accuracy: 0.9707 - dense_8_loss: 0.0456 - dense_9_accuracy: 0.7989 - dense_9_loss: 0.2782 - loss: 0.3238\nEpoch 17: val_loss improved from 0.58516 to 0.54146, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 164ms/step - dense_8_accuracy: 0.9707 - dense_8_loss: 0.0456 - dense_9_accuracy: 0.7989 - dense_9_loss: 0.2782 - loss: 0.3239 - val_dense_8_accuracy: 0.8827 - val_dense_8_loss: 0.1554 - val_dense_9_accuracy: 0.7392 - val_dense_9_loss: 0.3754 - val_loss: 0.5415 - learning_rate: 0.0010\nEpoch 18/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - dense_8_accuracy: 0.9616 - dense_8_loss: 0.0587 - dense_9_accuracy: 0.7992 - dense_9_loss: 0.2690 - loss: 0.3277\nEpoch 18: val_loss improved from 0.54146 to 0.51980, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 164ms/step - dense_8_accuracy: 0.9616 - dense_8_loss: 0.0587 - dense_9_accuracy: 0.7991 - dense_9_loss: 0.2691 - loss: 0.3277 - val_dense_8_accuracy: 0.8719 - val_dense_8_loss: 0.1878 - val_dense_9_accuracy: 0.7685 - val_dense_9_loss: 0.3242 - val_loss: 0.5198 - learning_rate: 0.0010\nEpoch 19/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - dense_8_accuracy: 0.9684 - dense_8_loss: 0.0484 - dense_9_accuracy: 0.7972 - dense_9_loss: 0.2667 - loss: 0.3151\nEpoch 19: val_loss did not improve from 0.51980\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 156ms/step - dense_8_accuracy: 0.9683 - dense_8_loss: 0.0484 - dense_9_accuracy: 0.7972 - dense_9_loss: 0.2667 - loss: 0.3151 - val_dense_8_accuracy: 0.6605 - val_dense_8_loss: 0.6927 - val_dense_9_accuracy: 0.7361 - val_dense_9_loss: 0.3916 - val_loss: 1.1104 - learning_rate: 0.0010\nEpoch 20/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - dense_8_accuracy: 0.9673 - dense_8_loss: 0.0486 - dense_9_accuracy: 0.8119 - dense_9_loss: 0.2563 - loss: 0.3049\nEpoch 20: val_loss did not improve from 0.51980\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 154ms/step - dense_8_accuracy: 0.9673 - dense_8_loss: 0.0486 - dense_9_accuracy: 0.8119 - dense_9_loss: 0.2564 - loss: 0.3050 - val_dense_8_accuracy: 0.8102 - val_dense_8_loss: 0.2448 - val_dense_9_accuracy: 0.7299 - val_dense_9_loss: 0.3929 - val_loss: 0.6405 - learning_rate: 0.0010\nEpoch 21/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - dense_8_accuracy: 0.9658 - dense_8_loss: 0.0468 - dense_9_accuracy: 0.8218 - dense_9_loss: 0.2458 - loss: 0.2926\nEpoch 21: val_loss did not improve from 0.51980\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 154ms/step - dense_8_accuracy: 0.9659 - dense_8_loss: 0.0468 - dense_9_accuracy: 0.8218 - dense_9_loss: 0.2459 - loss: 0.2926 - val_dense_8_accuracy: 0.8457 - val_dense_8_loss: 0.2329 - val_dense_9_accuracy: 0.5216 - val_dense_9_loss: 0.8678 - val_loss: 1.1026 - learning_rate: 0.0010\nEpoch 22/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - dense_8_accuracy: 0.9756 - dense_8_loss: 0.0343 - dense_9_accuracy: 0.8305 - dense_9_loss: 0.2382 - loss: 0.2725\nEpoch 22: val_loss improved from 0.51980 to 0.48749, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 164ms/step - dense_8_accuracy: 0.9756 - dense_8_loss: 0.0343 - dense_9_accuracy: 0.8305 - dense_9_loss: 0.2382 - loss: 0.2726 - val_dense_8_accuracy: 0.8781 - val_dense_8_loss: 0.2296 - val_dense_9_accuracy: 0.8256 - val_dense_9_loss: 0.2479 - val_loss: 0.4875 - learning_rate: 0.0010\nEpoch 23/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - dense_8_accuracy: 0.9732 - dense_8_loss: 0.0388 - dense_9_accuracy: 0.8279 - dense_9_loss: 0.2287 - loss: 0.2675\nEpoch 23: val_loss did not improve from 0.48749\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 155ms/step - dense_8_accuracy: 0.9732 - dense_8_loss: 0.0388 - dense_9_accuracy: 0.8278 - dense_9_loss: 0.2288 - loss: 0.2675 - val_dense_8_accuracy: 0.7994 - val_dense_8_loss: 0.3025 - val_dense_9_accuracy: 0.7654 - val_dense_9_loss: 0.3373 - val_loss: 0.6479 - learning_rate: 0.0010\nEpoch 24/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - dense_8_accuracy: 0.9702 - dense_8_loss: 0.0427 - dense_9_accuracy: 0.8349 - dense_9_loss: 0.2241 - loss: 0.2668\nEpoch 24: val_loss improved from 0.48749 to 0.46862, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 164ms/step - dense_8_accuracy: 0.9702 - dense_8_loss: 0.0427 - dense_9_accuracy: 0.8349 - dense_9_loss: 0.2241 - loss: 0.2668 - val_dense_8_accuracy: 0.9028 - val_dense_8_loss: 0.1462 - val_dense_9_accuracy: 0.7793 - val_dense_9_loss: 0.3116 - val_loss: 0.4686 - learning_rate: 0.0010\nEpoch 25/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - dense_8_accuracy: 0.9762 - dense_8_loss: 0.0373 - dense_9_accuracy: 0.8486 - dense_9_loss: 0.2028 - loss: 0.2401\nEpoch 25: val_loss did not improve from 0.46862\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 156ms/step - dense_8_accuracy: 0.9761 - dense_8_loss: 0.0373 - dense_9_accuracy: 0.8486 - dense_9_loss: 0.2028 - loss: 0.2401 - val_dense_8_accuracy: 0.9275 - val_dense_8_loss: 0.1130 - val_dense_9_accuracy: 0.5602 - val_dense_9_loss: 0.7433 - val_loss: 0.8574 - learning_rate: 0.0010\nEpoch 26/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - dense_8_accuracy: 0.9819 - dense_8_loss: 0.0262 - dense_9_accuracy: 0.8531 - dense_9_loss: 0.1942 - loss: 0.2204\nEpoch 26: val_loss did not improve from 0.46862\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 156ms/step - dense_8_accuracy: 0.9819 - dense_8_loss: 0.0262 - dense_9_accuracy: 0.8531 - dense_9_loss: 0.1942 - loss: 0.2204 - val_dense_8_accuracy: 0.8920 - val_dense_8_loss: 0.1731 - val_dense_9_accuracy: 0.7377 - val_dense_9_loss: 0.3472 - val_loss: 0.5202 - learning_rate: 0.0010\nEpoch 27/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - dense_8_accuracy: 0.9790 - dense_8_loss: 0.0308 - dense_9_accuracy: 0.8691 - dense_9_loss: 0.1736 - loss: 0.2045\nEpoch 27: val_loss improved from 0.46862 to 0.46776, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 164ms/step - dense_8_accuracy: 0.9790 - dense_8_loss: 0.0308 - dense_9_accuracy: 0.8691 - dense_9_loss: 0.1737 - loss: 0.2045 - val_dense_8_accuracy: 0.8580 - val_dense_8_loss: 0.2316 - val_dense_9_accuracy: 0.8349 - val_dense_9_loss: 0.2250 - val_loss: 0.4678 - learning_rate: 0.0010\nEpoch 28/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - dense_8_accuracy: 0.9836 - dense_8_loss: 0.0243 - dense_9_accuracy: 0.8766 - dense_9_loss: 0.1728 - loss: 0.1971\nEpoch 28: val_loss did not improve from 0.46776\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 155ms/step - dense_8_accuracy: 0.9836 - dense_8_loss: 0.0243 - dense_9_accuracy: 0.8766 - dense_9_loss: 0.1729 - loss: 0.1972 - val_dense_8_accuracy: 0.9136 - val_dense_8_loss: 0.1476 - val_dense_9_accuracy: 0.7330 - val_dense_9_loss: 0.4291 - val_loss: 0.5640 - learning_rate: 0.0010\nEpoch 29/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - dense_8_accuracy: 0.9804 - dense_8_loss: 0.0307 - dense_9_accuracy: 0.8748 - dense_9_loss: 0.1639 - loss: 0.1946\nEpoch 29: val_loss did not improve from 0.46776\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 155ms/step - dense_8_accuracy: 0.9804 - dense_8_loss: 0.0307 - dense_9_accuracy: 0.8747 - dense_9_loss: 0.1639 - loss: 0.1946 - val_dense_8_accuracy: 0.8858 - val_dense_8_loss: 0.2077 - val_dense_9_accuracy: 0.7855 - val_dense_9_loss: 0.3097 - val_loss: 0.5273 - learning_rate: 0.0010\nEpoch 30/200\n\u001b[1m170/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 153ms/step - dense_8_accuracy: 0.9828 - dense_8_loss: 0.0207 - dense_9_accuracy: 0.8942 - dense_9_loss: 0.1452 - loss: 0.1659","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"model.evaluate([X_test_s, X_test_h1], [y_test_s, y_test_h1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T20:51:53.300530Z","iopub.execute_input":"2025-02-05T20:51:53.300791Z","iopub.status.idle":"2025-02-05T20:52:06.150380Z","shell.execute_reply.started":"2025-02-05T20:51:53.300769Z","shell.execute_reply":"2025-02-05T20:52:06.149623Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 489ms/step - dense_104_accuracy: 0.9430 - dense_104_loss: 0.1299 - dense_105_accuracy: 0.9929 - dense_105_loss: 0.0117 - loss: 0.1419\n","output_type":"stream"},{"execution_count":72,"output_type":"execute_result","data":{"text/plain":"[0.1415456086397171,\n 0.12401048839092255,\n 0.014973864890635014,\n 0.9419752955436707,\n 0.9888888597488403]"},"metadata":{}}],"execution_count":72},{"cell_type":"code","source":"history = model.fit([X_train_s, X_train_h], [y_train_s, y_train_h],\n                    epochs=200,\n                    validation_data=([X_val_s, X_val_h1], [y_val_s, y_val_h1]), verbose=1,\n                    shuffle=True,\n                    callbacks=callbacks) # UpdateGammaCallback","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T20:53:05.176975Z","iopub.execute_input":"2025-02-05T20:53:05.177330Z","iopub.status.idle":"2025-02-05T22:23:59.121905Z","shell.execute_reply.started":"2025-02-05T20:53:05.177297Z","shell.execute_reply":"2025-02-05T22:23:59.121095Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - dense_104_accuracy: 0.9968 - dense_104_loss: 0.0049 - dense_105_accuracy: 0.9914 - dense_105_loss: 0.0152 - loss: 0.0201\nEpoch 1: val_loss did not improve from 0.13492\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 165ms/step - dense_104_accuracy: 0.9968 - dense_104_loss: 0.0049 - dense_105_accuracy: 0.9914 - dense_105_loss: 0.0152 - loss: 0.0200 - val_dense_104_accuracy: 0.9228 - val_dense_104_loss: 0.1368 - val_dense_105_accuracy: 0.9645 - val_dense_105_loss: 0.0534 - val_loss: 0.1910 - learning_rate: 0.0010\nEpoch 2/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - dense_104_accuracy: 0.9997 - dense_104_loss: 0.0011 - dense_105_accuracy: 0.9944 - dense_105_loss: 0.0069 - loss: 0.0080\nEpoch 2: val_loss did not improve from 0.13492\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 162ms/step - dense_104_accuracy: 0.9997 - dense_104_loss: 0.0011 - dense_105_accuracy: 0.9944 - dense_105_loss: 0.0069 - loss: 0.0080 - val_dense_104_accuracy: 0.9090 - val_dense_104_loss: 0.1590 - val_dense_105_accuracy: 0.9815 - val_dense_105_loss: 0.0258 - val_loss: 0.1897 - learning_rate: 0.0010\nEpoch 3/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - dense_104_accuracy: 0.9972 - dense_104_loss: 0.0031 - dense_105_accuracy: 0.9952 - dense_105_loss: 0.0073 - loss: 0.0104\nEpoch 3: val_loss did not improve from 0.13492\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 162ms/step - dense_104_accuracy: 0.9972 - dense_104_loss: 0.0031 - dense_105_accuracy: 0.9952 - dense_105_loss: 0.0073 - loss: 0.0104 - val_dense_104_accuracy: 0.9383 - val_dense_104_loss: 0.1215 - val_dense_105_accuracy: 0.9460 - val_dense_105_loss: 0.1146 - val_loss: 0.2436 - learning_rate: 0.0010\nEpoch 4/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - dense_104_accuracy: 0.9984 - dense_104_loss: 0.0032 - dense_105_accuracy: 0.9968 - dense_105_loss: 0.0050 - loss: 0.0083\nEpoch 4: val_loss did not improve from 0.13492\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 163ms/step - dense_104_accuracy: 0.9984 - dense_104_loss: 0.0032 - dense_105_accuracy: 0.9968 - dense_105_loss: 0.0050 - loss: 0.0083 - val_dense_104_accuracy: 0.9352 - val_dense_104_loss: 0.1247 - val_dense_105_accuracy: 0.9907 - val_dense_105_loss: 0.0118 - val_loss: 0.1413 - learning_rate: 0.0010\nEpoch 5/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - dense_104_accuracy: 0.9971 - dense_104_loss: 0.0031 - dense_105_accuracy: 0.9952 - dense_105_loss: 0.0068 - loss: 0.0099\nEpoch 5: val_loss improved from 0.13492 to 0.13076, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 172ms/step - dense_104_accuracy: 0.9971 - dense_104_loss: 0.0031 - dense_105_accuracy: 0.9952 - dense_105_loss: 0.0068 - loss: 0.0099 - val_dense_104_accuracy: 0.9306 - val_dense_104_loss: 0.1082 - val_dense_105_accuracy: 0.9799 - val_dense_105_loss: 0.0179 - val_loss: 0.1308 - learning_rate: 0.0010\nEpoch 6/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - dense_104_accuracy: 0.9997 - dense_104_loss: 8.5316e-04 - dense_105_accuracy: 0.9961 - dense_105_loss: 0.0056 - loss: 0.0064\nEpoch 6: val_loss did not improve from 0.13076\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 163ms/step - dense_104_accuracy: 0.9997 - dense_104_loss: 8.5467e-04 - dense_105_accuracy: 0.9961 - dense_105_loss: 0.0056 - loss: 0.0064 - val_dense_104_accuracy: 0.9105 - val_dense_104_loss: 0.1490 - val_dense_105_accuracy: 0.9707 - val_dense_105_loss: 0.0531 - val_loss: 0.2045 - learning_rate: 0.0010\nEpoch 7/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - dense_104_accuracy: 0.9980 - dense_104_loss: 0.0046 - dense_105_accuracy: 0.9939 - dense_105_loss: 0.0084 - loss: 0.0129\nEpoch 7: val_loss did not improve from 0.13076\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 162ms/step - dense_104_accuracy: 0.9980 - dense_104_loss: 0.0046 - dense_105_accuracy: 0.9939 - dense_105_loss: 0.0084 - loss: 0.0129 - val_dense_104_accuracy: 0.9383 - val_dense_104_loss: 0.1167 - val_dense_105_accuracy: 0.9213 - val_dense_105_loss: 0.1102 - val_loss: 0.2334 - learning_rate: 0.0010\nEpoch 8/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - dense_104_accuracy: 0.9995 - dense_104_loss: 0.0018 - dense_105_accuracy: 0.9909 - dense_105_loss: 0.0145 - loss: 0.0163\nEpoch 8: val_loss did not improve from 0.13076\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 162ms/step - dense_104_accuracy: 0.9995 - dense_104_loss: 0.0018 - dense_105_accuracy: 0.9909 - dense_105_loss: 0.0144 - loss: 0.0162 - val_dense_104_accuracy: 0.9460 - val_dense_104_loss: 0.1064 - val_dense_105_accuracy: 0.9799 - val_dense_105_loss: 0.0307 - val_loss: 0.1395 - learning_rate: 0.0010\nEpoch 9/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - dense_104_accuracy: 0.9980 - dense_104_loss: 0.0031 - dense_105_accuracy: 0.9953 - dense_105_loss: 0.0057 - loss: 0.0088\nEpoch 9: val_loss did not improve from 0.13076\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 162ms/step - dense_104_accuracy: 0.9980 - dense_104_loss: 0.0031 - dense_105_accuracy: 0.9953 - dense_105_loss: 0.0057 - loss: 0.0088 - val_dense_104_accuracy: 0.9167 - val_dense_104_loss: 0.1562 - val_dense_105_accuracy: 0.9028 - val_dense_105_loss: 0.2005 - val_loss: 0.3556 - learning_rate: 0.0010\nEpoch 10/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - dense_104_accuracy: 0.9989 - dense_104_loss: 0.0024 - dense_105_accuracy: 0.9927 - dense_105_loss: 0.0137 - loss: 0.0160\nEpoch 10: val_loss did not improve from 0.13076\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 162ms/step - dense_104_accuracy: 0.9989 - dense_104_loss: 0.0024 - dense_105_accuracy: 0.9927 - dense_105_loss: 0.0137 - loss: 0.0160 - val_dense_104_accuracy: 0.9414 - val_dense_104_loss: 0.1044 - val_dense_105_accuracy: 0.9753 - val_dense_105_loss: 0.0381 - val_loss: 0.1462 - learning_rate: 0.0010\nEpoch 11/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - dense_104_accuracy: 0.9995 - dense_104_loss: 0.0015 - dense_105_accuracy: 0.9949 - dense_105_loss: 0.0072 - loss: 0.0086\nEpoch 11: val_loss did not improve from 0.13076\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 162ms/step - dense_104_accuracy: 0.9995 - dense_104_loss: 0.0015 - dense_105_accuracy: 0.9949 - dense_105_loss: 0.0072 - loss: 0.0086 - val_dense_104_accuracy: 0.9321 - val_dense_104_loss: 0.1346 - val_dense_105_accuracy: 0.9799 - val_dense_105_loss: 0.0282 - val_loss: 0.1686 - learning_rate: 0.0010\nEpoch 12/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - dense_104_accuracy: 0.9987 - dense_104_loss: 0.0015 - dense_105_accuracy: 0.9979 - dense_105_loss: 0.0043 - loss: 0.0058\nEpoch 12: val_loss did not improve from 0.13076\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 163ms/step - dense_104_accuracy: 0.9987 - dense_104_loss: 0.0015 - dense_105_accuracy: 0.9979 - dense_105_loss: 0.0043 - loss: 0.0058 - val_dense_104_accuracy: 0.9290 - val_dense_104_loss: 0.1261 - val_dense_105_accuracy: 0.8519 - val_dense_105_loss: 0.3075 - val_loss: 0.4206 - learning_rate: 0.0010\nEpoch 13/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - dense_104_accuracy: 0.9981 - dense_104_loss: 0.0028 - dense_105_accuracy: 0.9945 - dense_105_loss: 0.0076 - loss: 0.0103\nEpoch 13: val_loss did not improve from 0.13076\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 162ms/step - dense_104_accuracy: 0.9981 - dense_104_loss: 0.0028 - dense_105_accuracy: 0.9945 - dense_105_loss: 0.0076 - loss: 0.0103 - val_dense_104_accuracy: 0.9306 - val_dense_104_loss: 0.1265 - val_dense_105_accuracy: 0.8642 - val_dense_105_loss: 0.3310 - val_loss: 0.4578 - learning_rate: 0.0010\nEpoch 14/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - dense_104_accuracy: 0.9997 - dense_104_loss: 8.8046e-04 - dense_105_accuracy: 0.9909 - dense_105_loss: 0.0132 - loss: 0.0141\nEpoch 14: val_loss did not improve from 0.13076\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 162ms/step - dense_104_accuracy: 0.9997 - dense_104_loss: 8.8258e-04 - dense_105_accuracy: 0.9909 - dense_105_loss: 0.0132 - loss: 0.0141 - val_dense_104_accuracy: 0.9275 - val_dense_104_loss: 0.1270 - val_dense_105_accuracy: 0.7114 - val_dense_105_loss: 0.7439 - val_loss: 0.8398 - learning_rate: 0.0010\nEpoch 15/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - dense_104_accuracy: 0.9990 - dense_104_loss: 0.0015 - dense_105_accuracy: 0.9961 - dense_105_loss: 0.0081 - loss: 0.0096\nEpoch 15: val_loss did not improve from 0.13076\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 162ms/step - dense_104_accuracy: 0.9990 - dense_104_loss: 0.0015 - dense_105_accuracy: 0.9961 - dense_105_loss: 0.0081 - loss: 0.0096 - val_dense_104_accuracy: 0.9151 - val_dense_104_loss: 0.1692 - val_dense_105_accuracy: 0.9877 - val_dense_105_loss: 0.0245 - val_loss: 0.1945 - learning_rate: 0.0010\nEpoch 16/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - dense_104_accuracy: 0.9980 - dense_104_loss: 0.0036 - dense_105_accuracy: 0.9969 - dense_105_loss: 0.0045 - loss: 0.0081\nEpoch 16: val_loss did not improve from 0.13076\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 162ms/step - dense_104_accuracy: 0.9980 - dense_104_loss: 0.0036 - dense_105_accuracy: 0.9969 - dense_105_loss: 0.0045 - loss: 0.0081 - val_dense_104_accuracy: 0.8981 - val_dense_104_loss: 0.1908 - val_dense_105_accuracy: 0.9861 - val_dense_105_loss: 0.0268 - val_loss: 0.2237 - learning_rate: 0.0010\nEpoch 17/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - dense_104_accuracy: 0.9945 - dense_104_loss: 0.0091 - dense_105_accuracy: 0.9957 - dense_105_loss: 0.0063 - loss: 0.0154\nEpoch 17: val_loss did not improve from 0.13076\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 163ms/step - dense_104_accuracy: 0.9945 - dense_104_loss: 0.0091 - dense_105_accuracy: 0.9957 - dense_105_loss: 0.0063 - loss: 0.0154 - val_dense_104_accuracy: 0.9259 - val_dense_104_loss: 0.1320 - val_dense_105_accuracy: 0.9568 - val_dense_105_loss: 0.0837 - val_loss: 0.2172 - learning_rate: 0.0010\nEpoch 18/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - dense_104_accuracy: 0.9969 - dense_104_loss: 0.0043 - dense_105_accuracy: 0.9965 - dense_105_loss: 0.0051 - loss: 0.0094\nEpoch 18: val_loss did not improve from 0.13076\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 162ms/step - dense_104_accuracy: 0.9969 - dense_104_loss: 0.0043 - dense_105_accuracy: 0.9965 - dense_105_loss: 0.0051 - loss: 0.0094 - val_dense_104_accuracy: 0.9167 - val_dense_104_loss: 0.1508 - val_dense_105_accuracy: 0.9815 - val_dense_105_loss: 0.0460 - val_loss: 0.1929 - learning_rate: 0.0010\nEpoch 19/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - dense_104_accuracy: 0.9984 - dense_104_loss: 0.0029 - dense_105_accuracy: 0.9920 - dense_105_loss: 0.0097 - loss: 0.0126\nEpoch 19: val_loss did not improve from 0.13076\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 162ms/step - dense_104_accuracy: 0.9984 - dense_104_loss: 0.0029 - dense_105_accuracy: 0.9920 - dense_105_loss: 0.0097 - loss: 0.0126 - val_dense_104_accuracy: 0.9213 - val_dense_104_loss: 0.1278 - val_dense_105_accuracy: 0.9645 - val_dense_105_loss: 0.0703 - val_loss: 0.2027 - learning_rate: 0.0010\nEpoch 20/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - dense_104_accuracy: 0.9989 - dense_104_loss: 0.0011 - dense_105_accuracy: 0.9933 - dense_105_loss: 0.0113 - loss: 0.0124\nEpoch 20: val_loss did not improve from 0.13076\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 163ms/step - dense_104_accuracy: 0.9989 - dense_104_loss: 0.0011 - dense_105_accuracy: 0.9933 - dense_105_loss: 0.0113 - loss: 0.0124 - val_dense_104_accuracy: 0.9259 - val_dense_104_loss: 0.1455 - val_dense_105_accuracy: 0.9861 - val_dense_105_loss: 0.0226 - val_loss: 0.1735 - learning_rate: 0.0010\nEpoch 21/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - dense_104_accuracy: 0.9990 - dense_104_loss: 0.0020 - dense_105_accuracy: 0.9948 - dense_105_loss: 0.0072 - loss: 0.0093\nEpoch 21: val_loss did not improve from 0.13076\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 162ms/step - dense_104_accuracy: 0.9990 - dense_104_loss: 0.0020 - dense_105_accuracy: 0.9948 - dense_105_loss: 0.0072 - loss: 0.0093 - val_dense_104_accuracy: 0.9259 - val_dense_104_loss: 0.1329 - val_dense_105_accuracy: 0.9815 - val_dense_105_loss: 0.0292 - val_loss: 0.1679 - learning_rate: 0.0010\nEpoch 22/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - dense_104_accuracy: 0.9990 - dense_104_loss: 9.8548e-04 - dense_105_accuracy: 0.9932 - dense_105_loss: 0.0111 - loss: 0.0121\nEpoch 22: val_loss did not improve from 0.13076\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 162ms/step - dense_104_accuracy: 0.9990 - dense_104_loss: 9.8694e-04 - dense_105_accuracy: 0.9932 - dense_105_loss: 0.0111 - loss: 0.0121 - val_dense_104_accuracy: 0.9228 - val_dense_104_loss: 0.1145 - val_dense_105_accuracy: 0.9799 - val_dense_105_loss: 0.0249 - val_loss: 0.1435 - learning_rate: 0.0010\nEpoch 23/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - dense_104_accuracy: 0.9999 - dense_104_loss: 5.9503e-04 - dense_105_accuracy: 0.9969 - dense_105_loss: 0.0044 - loss: 0.0050\nEpoch 23: val_loss did not improve from 0.13076\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 163ms/step - dense_104_accuracy: 0.9999 - dense_104_loss: 5.9515e-04 - dense_105_accuracy: 0.9969 - dense_105_loss: 0.0045 - loss: 0.0050 - val_dense_104_accuracy: 0.9306 - val_dense_104_loss: 0.1204 - val_dense_105_accuracy: 0.9877 - val_dense_105_loss: 0.0215 - val_loss: 0.1399 - learning_rate: 0.0010\nEpoch 24/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - dense_104_accuracy: 0.9993 - dense_104_loss: 0.0013 - dense_105_accuracy: 0.9964 - dense_105_loss: 0.0055 - loss: 0.0068\nEpoch 24: val_loss did not improve from 0.13076\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 163ms/step - dense_104_accuracy: 0.9993 - dense_104_loss: 0.0013 - dense_105_accuracy: 0.9964 - dense_105_loss: 0.0055 - loss: 0.0068 - val_dense_104_accuracy: 0.9383 - val_dense_104_loss: 0.1320 - val_dense_105_accuracy: 0.9552 - val_dense_105_loss: 0.0990 - val_loss: 0.2335 - learning_rate: 0.0010\nEpoch 25/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - dense_104_accuracy: 0.9962 - dense_104_loss: 0.0061 - dense_105_accuracy: 0.9934 - dense_105_loss: 0.0121 - loss: 0.0182\nEpoch 25: val_loss did not improve from 0.13076\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 163ms/step - dense_104_accuracy: 0.9962 - dense_104_loss: 0.0061 - dense_105_accuracy: 0.9934 - dense_105_loss: 0.0121 - loss: 0.0182 - val_dense_104_accuracy: 0.9275 - val_dense_104_loss: 0.1254 - val_dense_105_accuracy: 0.9753 - val_dense_105_loss: 0.0319 - val_loss: 0.1614 - learning_rate: 0.0010\nEpoch 26/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - dense_104_accuracy: 0.9978 - dense_104_loss: 0.0033 - dense_105_accuracy: 0.9950 - dense_105_loss: 0.0075 - loss: 0.0109\nEpoch 26: val_loss did not improve from 0.13076\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 162ms/step - dense_104_accuracy: 0.9978 - dense_104_loss: 0.0033 - dense_105_accuracy: 0.9950 - dense_105_loss: 0.0075 - loss: 0.0109 - val_dense_104_accuracy: 0.9275 - val_dense_104_loss: 0.1454 - val_dense_105_accuracy: 0.9074 - val_dense_105_loss: 0.1856 - val_loss: 0.3305 - learning_rate: 0.0010\nEpoch 27/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - dense_104_accuracy: 0.9987 - dense_104_loss: 0.0022 - dense_105_accuracy: 0.9957 - dense_105_loss: 0.0055 - loss: 0.0077\nEpoch 27: val_loss did not improve from 0.13076\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 161ms/step - dense_104_accuracy: 0.9987 - dense_104_loss: 0.0022 - dense_105_accuracy: 0.9957 - dense_105_loss: 0.0055 - loss: 0.0077 - val_dense_104_accuracy: 0.9167 - val_dense_104_loss: 0.1459 - val_dense_105_accuracy: 0.9105 - val_dense_105_loss: 0.1537 - val_loss: 0.3018 - learning_rate: 0.0010\nEpoch 28/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - dense_104_accuracy: 0.9980 - dense_104_loss: 0.0022 - dense_105_accuracy: 0.9948 - dense_105_loss: 0.0086 - loss: 0.0108\nEpoch 28: val_loss did not improve from 0.13076\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 162ms/step - dense_104_accuracy: 0.9980 - dense_104_loss: 0.0022 - dense_105_accuracy: 0.9948 - dense_105_loss: 0.0086 - loss: 0.0108 - val_dense_104_accuracy: 0.9306 - val_dense_104_loss: 0.1100 - val_dense_105_accuracy: 0.9861 - val_dense_105_loss: 0.0272 - val_loss: 0.1377 - learning_rate: 0.0010\nEpoch 29/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - dense_104_accuracy: 0.9992 - dense_104_loss: 0.0018 - dense_105_accuracy: 0.9957 - dense_105_loss: 0.0058 - loss: 0.0077\nEpoch 29: val_loss did not improve from 0.13076\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 162ms/step - dense_104_accuracy: 0.9991 - dense_104_loss: 0.0018 - dense_105_accuracy: 0.9957 - dense_105_loss: 0.0058 - loss: 0.0077 - val_dense_104_accuracy: 0.9414 - val_dense_104_loss: 0.1214 - val_dense_105_accuracy: 0.8904 - val_dense_105_loss: 0.2703 - val_loss: 0.3729 - learning_rate: 0.0010\nEpoch 30/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - dense_104_accuracy: 0.9990 - dense_104_loss: 0.0019 - dense_105_accuracy: 0.9949 - dense_105_loss: 0.0074 - loss: 0.0093\nEpoch 30: val_loss did not improve from 0.13076\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 162ms/step - dense_104_accuracy: 0.9990 - dense_104_loss: 0.0019 - dense_105_accuracy: 0.9949 - dense_105_loss: 0.0074 - loss: 0.0093 - val_dense_104_accuracy: 0.9244 - val_dense_104_loss: 0.1372 - val_dense_105_accuracy: 0.9861 - val_dense_105_loss: 0.0245 - val_loss: 0.1632 - learning_rate: 0.0010\nEpoch 31/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - dense_104_accuracy: 0.9951 - dense_104_loss: 0.0083 - dense_105_accuracy: 0.9976 - dense_105_loss: 0.0050 - loss: 0.0133\nEpoch 31: val_loss did not improve from 0.13076\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 162ms/step - dense_104_accuracy: 0.9951 - dense_104_loss: 0.0083 - dense_105_accuracy: 0.9976 - dense_105_loss: 0.0050 - loss: 0.0133 - val_dense_104_accuracy: 0.9336 - val_dense_104_loss: 0.1369 - val_dense_105_accuracy: 0.9938 - val_dense_105_loss: 0.0098 - val_loss: 0.1509 - learning_rate: 0.0010\nEpoch 32/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - dense_104_accuracy: 0.9981 - dense_104_loss: 0.0025 - dense_105_accuracy: 0.9973 - dense_105_loss: 0.0038 - loss: 0.0063\nEpoch 32: val_loss did not improve from 0.13076\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 162ms/step - dense_104_accuracy: 0.9981 - dense_104_loss: 0.0025 - dense_105_accuracy: 0.9973 - dense_105_loss: 0.0038 - loss: 0.0063 - val_dense_104_accuracy: 0.9352 - val_dense_104_loss: 0.1307 - val_dense_105_accuracy: 0.8364 - val_dense_105_loss: 0.4327 - val_loss: 0.5628 - learning_rate: 0.0010\nEpoch 33/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - dense_104_accuracy: 0.9986 - dense_104_loss: 0.0035 - dense_105_accuracy: 0.9931 - dense_105_loss: 0.0114 - loss: 0.0149\nEpoch 33: val_loss did not improve from 0.13076\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 162ms/step - dense_104_accuracy: 0.9986 - dense_104_loss: 0.0035 - dense_105_accuracy: 0.9931 - dense_105_loss: 0.0114 - loss: 0.0148 - val_dense_104_accuracy: 0.9259 - val_dense_104_loss: 0.1554 - val_dense_105_accuracy: 0.9830 - val_dense_105_loss: 0.0208 - val_loss: 0.1826 - learning_rate: 0.0010\nEpoch 34/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - dense_104_accuracy: 0.9988 - dense_104_loss: 0.0016 - dense_105_accuracy: 0.9957 - dense_105_loss: 0.0060 - loss: 0.0076\nEpoch 34: val_loss did not improve from 0.13076\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 162ms/step - dense_104_accuracy: 0.9988 - dense_104_loss: 0.0016 - dense_105_accuracy: 0.9957 - dense_105_loss: 0.0060 - loss: 0.0076 - val_dense_104_accuracy: 0.9059 - val_dense_104_loss: 0.1816 - val_dense_105_accuracy: 0.9414 - val_dense_105_loss: 0.0983 - val_loss: 0.2891 - learning_rate: 0.0010\nEpoch 35/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - dense_104_accuracy: 0.9974 - dense_104_loss: 0.0036 - dense_105_accuracy: 0.9951 - dense_105_loss: 0.0075 - loss: 0.0111\nEpoch 35: val_loss did not improve from 0.13076\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 163ms/step - dense_104_accuracy: 0.9974 - dense_104_loss: 0.0036 - dense_105_accuracy: 0.9951 - dense_105_loss: 0.0075 - loss: 0.0111 - val_dense_104_accuracy: 0.9120 - val_dense_104_loss: 0.1682 - val_dense_105_accuracy: 0.9861 - val_dense_105_loss: 0.0235 - val_loss: 0.1985 - learning_rate: 0.0010\nEpoch 36/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - dense_104_accuracy: 0.9996 - dense_104_loss: 8.0837e-04 - dense_105_accuracy: 0.9958 - dense_105_loss: 0.0062 - loss: 0.0070\nEpoch 36: val_loss did not improve from 0.13076\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 162ms/step - dense_104_accuracy: 0.9996 - dense_104_loss: 8.0973e-04 - dense_105_accuracy: 0.9958 - dense_105_loss: 0.0062 - loss: 0.0071 - val_dense_104_accuracy: 0.9321 - val_dense_104_loss: 0.1225 - val_dense_105_accuracy: 0.9722 - val_dense_105_loss: 0.0408 - val_loss: 0.1693 - learning_rate: 0.0010\nEpoch 37/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - dense_104_accuracy: 0.9971 - dense_104_loss: 0.0030 - dense_105_accuracy: 0.9930 - dense_105_loss: 0.0094 - loss: 0.0124\nEpoch 37: val_loss did not improve from 0.13076\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 162ms/step - dense_104_accuracy: 0.9971 - dense_104_loss: 0.0030 - dense_105_accuracy: 0.9930 - dense_105_loss: 0.0094 - loss: 0.0124 - val_dense_104_accuracy: 0.8812 - val_dense_104_loss: 0.2413 - val_dense_105_accuracy: 0.9938 - val_dense_105_loss: 0.0130 - val_loss: 0.2590 - learning_rate: 0.0010\nEpoch 38/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - dense_104_accuracy: 0.9989 - dense_104_loss: 0.0019 - dense_105_accuracy: 0.9950 - dense_105_loss: 0.0072 - loss: 0.0091\nEpoch 38: val_loss did not improve from 0.13076\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 162ms/step - dense_104_accuracy: 0.9989 - dense_104_loss: 0.0019 - dense_105_accuracy: 0.9950 - dense_105_loss: 0.0072 - loss: 0.0091 - val_dense_104_accuracy: 0.9275 - val_dense_104_loss: 0.1239 - val_dense_105_accuracy: 0.9815 - val_dense_105_loss: 0.0288 - val_loss: 0.1552 - learning_rate: 0.0010\nEpoch 39/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - dense_104_accuracy: 0.9987 - dense_104_loss: 0.0022 - dense_105_accuracy: 0.9964 - dense_105_loss: 0.0057 - loss: 0.0078\nEpoch 39: val_loss did not improve from 0.13076\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 161ms/step - dense_104_accuracy: 0.9987 - dense_104_loss: 0.0022 - dense_105_accuracy: 0.9964 - dense_105_loss: 0.0057 - loss: 0.0078 - val_dense_104_accuracy: 0.9244 - val_dense_104_loss: 0.1571 - val_dense_105_accuracy: 0.9938 - val_dense_105_loss: 0.0167 - val_loss: 0.1791 - learning_rate: 0.0010\nEpoch 40/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - dense_104_accuracy: 0.9990 - dense_104_loss: 0.0021 - dense_105_accuracy: 0.9969 - dense_105_loss: 0.0061 - loss: 0.0082\nEpoch 40: val_loss did not improve from 0.13076\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 162ms/step - dense_104_accuracy: 0.9990 - dense_104_loss: 0.0021 - dense_105_accuracy: 0.9969 - dense_105_loss: 0.0061 - loss: 0.0082 - val_dense_104_accuracy: 0.9213 - val_dense_104_loss: 0.1512 - val_dense_105_accuracy: 0.9815 - val_dense_105_loss: 0.0313 - val_loss: 0.1785 - learning_rate: 0.0010\nEpoch 41/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - dense_104_accuracy: 0.9989 - dense_104_loss: 0.0031 - dense_105_accuracy: 0.9928 - dense_105_loss: 0.0120 - loss: 0.0151\nEpoch 41: val_loss did not improve from 0.13076\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 162ms/step - dense_104_accuracy: 0.9989 - dense_104_loss: 0.0031 - dense_105_accuracy: 0.9928 - dense_105_loss: 0.0119 - loss: 0.0151 - val_dense_104_accuracy: 0.9290 - val_dense_104_loss: 0.1308 - val_dense_105_accuracy: 0.9923 - val_dense_105_loss: 0.0155 - val_loss: 0.1483 - learning_rate: 0.0010\nEpoch 42/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - dense_104_accuracy: 0.9999 - dense_104_loss: 4.7903e-04 - dense_105_accuracy: 0.9969 - dense_105_loss: 0.0048 - loss: 0.0053\nEpoch 42: val_loss did not improve from 0.13076\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 162ms/step - dense_104_accuracy: 0.9999 - dense_104_loss: 4.7985e-04 - dense_105_accuracy: 0.9969 - dense_105_loss: 0.0048 - loss: 0.0053 - val_dense_104_accuracy: 0.9244 - val_dense_104_loss: 0.1328 - val_dense_105_accuracy: 0.9907 - val_dense_105_loss: 0.0215 - val_loss: 0.1561 - learning_rate: 0.0010\nEpoch 43/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - dense_104_accuracy: 0.9990 - dense_104_loss: 0.0015 - dense_105_accuracy: 0.9982 - dense_105_loss: 0.0042 - loss: 0.0057\nEpoch 43: val_loss did not improve from 0.13076\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 162ms/step - dense_104_accuracy: 0.9990 - dense_104_loss: 0.0015 - dense_105_accuracy: 0.9982 - dense_105_loss: 0.0042 - loss: 0.0057 - val_dense_104_accuracy: 0.9275 - val_dense_104_loss: 0.1377 - val_dense_105_accuracy: 0.7469 - val_dense_105_loss: 0.8609 - val_loss: 0.9582 - learning_rate: 0.0010\nEpoch 44/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - dense_104_accuracy: 0.9997 - dense_104_loss: 8.4526e-04 - dense_105_accuracy: 0.9954 - dense_105_loss: 0.0063 - loss: 0.0072\nEpoch 44: val_loss did not improve from 0.13076\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 163ms/step - dense_104_accuracy: 0.9997 - dense_104_loss: 8.4592e-04 - dense_105_accuracy: 0.9954 - dense_105_loss: 0.0063 - loss: 0.0072 - val_dense_104_accuracy: 0.9198 - val_dense_104_loss: 0.1791 - val_dense_105_accuracy: 0.9676 - val_dense_105_loss: 0.0654 - val_loss: 0.2505 - learning_rate: 0.0010\nEpoch 45/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - dense_104_accuracy: 0.9976 - dense_104_loss: 0.0036 - dense_105_accuracy: 0.9944 - dense_105_loss: 0.0083 - loss: 0.0119\nEpoch 45: val_loss did not improve from 0.13076\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 163ms/step - dense_104_accuracy: 0.9976 - dense_104_loss: 0.0036 - dense_105_accuracy: 0.9944 - dense_105_loss: 0.0083 - loss: 0.0119 - val_dense_104_accuracy: 0.9182 - val_dense_104_loss: 0.1472 - val_dense_105_accuracy: 0.9877 - val_dense_105_loss: 0.0171 - val_loss: 0.1697 - learning_rate: 0.0010\nEpoch 46/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - dense_104_accuracy: 0.9979 - dense_104_loss: 0.0026 - dense_105_accuracy: 0.9953 - dense_105_loss: 0.0062 - loss: 0.0088\nEpoch 46: val_loss did not improve from 0.13076\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 162ms/step - dense_104_accuracy: 0.9979 - dense_104_loss: 0.0026 - dense_105_accuracy: 0.9953 - dense_105_loss: 0.0062 - loss: 0.0088 - val_dense_104_accuracy: 0.9275 - val_dense_104_loss: 0.1412 - val_dense_105_accuracy: 0.9028 - val_dense_105_loss: 0.1686 - val_loss: 0.3076 - learning_rate: 0.0010\nEpoch 47/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - dense_104_accuracy: 0.9996 - dense_104_loss: 9.1698e-04 - dense_105_accuracy: 0.9955 - dense_105_loss: 0.0066 - loss: 0.0075\nEpoch 47: val_loss did not improve from 0.13076\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 162ms/step - dense_104_accuracy: 0.9996 - dense_104_loss: 9.1701e-04 - dense_105_accuracy: 0.9955 - dense_105_loss: 0.0066 - loss: 0.0075 - val_dense_104_accuracy: 0.9306 - val_dense_104_loss: 0.1157 - val_dense_105_accuracy: 0.9599 - val_dense_105_loss: 0.0668 - val_loss: 0.1867 - learning_rate: 0.0010\nEpoch 48/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - dense_104_accuracy: 0.9994 - dense_104_loss: 9.0138e-04 - dense_105_accuracy: 0.9953 - dense_105_loss: 0.0052 - loss: 0.0061\nEpoch 48: val_loss did not improve from 0.13076\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 162ms/step - dense_104_accuracy: 0.9994 - dense_104_loss: 9.0285e-04 - dense_105_accuracy: 0.9953 - dense_105_loss: 0.0052 - loss: 0.0061 - val_dense_104_accuracy: 0.9213 - val_dense_104_loss: 0.1459 - val_dense_105_accuracy: 0.9398 - val_dense_105_loss: 0.1022 - val_loss: 0.2410 - learning_rate: 0.0010\nEpoch 49/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - dense_104_accuracy: 0.9974 - dense_104_loss: 0.0038 - dense_105_accuracy: 0.9962 - dense_105_loss: 0.0051 - loss: 0.0089\nEpoch 49: val_loss did not improve from 0.13076\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 162ms/step - dense_104_accuracy: 0.9974 - dense_104_loss: 0.0038 - dense_105_accuracy: 0.9962 - dense_105_loss: 0.0052 - loss: 0.0089 - val_dense_104_accuracy: 0.9213 - val_dense_104_loss: 0.1325 - val_dense_105_accuracy: 0.9907 - val_dense_105_loss: 0.0191 - val_loss: 0.1571 - learning_rate: 0.0010\nEpoch 50/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - dense_104_accuracy: 0.9987 - dense_104_loss: 0.0020 - dense_105_accuracy: 0.9959 - dense_105_loss: 0.0056 - loss: 0.0076\nEpoch 50: val_loss did not improve from 0.13076\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 161ms/step - dense_104_accuracy: 0.9987 - dense_104_loss: 0.0020 - dense_105_accuracy: 0.9959 - dense_105_loss: 0.0056 - loss: 0.0076 - val_dense_104_accuracy: 0.9367 - val_dense_104_loss: 0.1221 - val_dense_105_accuracy: 0.9028 - val_dense_105_loss: 0.2304 - val_loss: 0.3571 - learning_rate: 0.0010\nEpoch 51/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - dense_104_accuracy: 0.9984 - dense_104_loss: 0.0034 - dense_105_accuracy: 0.9974 - dense_105_loss: 0.0042 - loss: 0.0077\nEpoch 51: val_loss did not improve from 0.13076\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 162ms/step - dense_104_accuracy: 0.9984 - dense_104_loss: 0.0034 - dense_105_accuracy: 0.9974 - dense_105_loss: 0.0042 - loss: 0.0077 - val_dense_104_accuracy: 0.9151 - val_dense_104_loss: 0.1579 - val_dense_105_accuracy: 0.9043 - val_dense_105_loss: 0.2341 - val_loss: 0.3794 - learning_rate: 0.0010\nEpoch 52/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - dense_104_accuracy: 0.9988 - dense_104_loss: 0.0019 - dense_105_accuracy: 0.9975 - dense_105_loss: 0.0036 - loss: 0.0055\nEpoch 52: val_loss did not improve from 0.13076\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 163ms/step - dense_104_accuracy: 0.9988 - dense_104_loss: 0.0019 - dense_105_accuracy: 0.9975 - dense_105_loss: 0.0036 - loss: 0.0055 - val_dense_104_accuracy: 0.9213 - val_dense_104_loss: 0.1344 - val_dense_105_accuracy: 0.9306 - val_dense_105_loss: 0.1493 - val_loss: 0.2864 - learning_rate: 0.0010\nEpoch 53/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - dense_104_accuracy: 0.9980 - dense_104_loss: 0.0023 - dense_105_accuracy: 0.9948 - dense_105_loss: 0.0071 - loss: 0.0093\nEpoch 53: val_loss did not improve from 0.13076\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 162ms/step - dense_104_accuracy: 0.9980 - dense_104_loss: 0.0023 - dense_105_accuracy: 0.9948 - dense_105_loss: 0.0071 - loss: 0.0093 - val_dense_104_accuracy: 0.9151 - val_dense_104_loss: 0.1468 - val_dense_105_accuracy: 0.9753 - val_dense_105_loss: 0.0415 - val_loss: 0.1949 - learning_rate: 0.0010\nEpoch 54/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - dense_104_accuracy: 0.9975 - dense_104_loss: 0.0040 - dense_105_accuracy: 0.9943 - dense_105_loss: 0.0079 - loss: 0.0118\nEpoch 54: val_loss did not improve from 0.13076\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 162ms/step - dense_104_accuracy: 0.9975 - dense_104_loss: 0.0040 - dense_105_accuracy: 0.9943 - dense_105_loss: 0.0078 - loss: 0.0118 - val_dense_104_accuracy: 0.9244 - val_dense_104_loss: 0.1406 - val_dense_105_accuracy: 0.9830 - val_dense_105_loss: 0.0250 - val_loss: 0.1717 - learning_rate: 0.0010\nEpoch 55/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - dense_104_accuracy: 0.9982 - dense_104_loss: 0.0029 - dense_105_accuracy: 0.9946 - dense_105_loss: 0.0051 - loss: 0.0081\nEpoch 55: val_loss did not improve from 0.13076\n\nEpoch 55: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 162ms/step - dense_104_accuracy: 0.9982 - dense_104_loss: 0.0029 - dense_105_accuracy: 0.9946 - dense_105_loss: 0.0051 - loss: 0.0081 - val_dense_104_accuracy: 0.9167 - val_dense_104_loss: 0.1399 - val_dense_105_accuracy: 0.9861 - val_dense_105_loss: 0.0242 - val_loss: 0.1688 - learning_rate: 0.0010\nEpoch 56/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - dense_104_accuracy: 0.9997 - dense_104_loss: 0.0011 - dense_105_accuracy: 0.9977 - dense_105_loss: 0.0030 - loss: 0.0041\nEpoch 56: val_loss did not improve from 0.13076\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 162ms/step - dense_104_accuracy: 0.9997 - dense_104_loss: 0.0011 - dense_105_accuracy: 0.9977 - dense_105_loss: 0.0030 - loss: 0.0041 - val_dense_104_accuracy: 0.9290 - val_dense_104_loss: 0.1110 - val_dense_105_accuracy: 0.9861 - val_dense_105_loss: 0.0181 - val_loss: 0.1332 - learning_rate: 1.0000e-04\nEpoch 57/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - dense_104_accuracy: 0.9991 - dense_104_loss: 0.0018 - dense_105_accuracy: 0.9989 - dense_105_loss: 0.0015 - loss: 0.0033\nEpoch 57: val_loss improved from 0.13076 to 0.12424, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 171ms/step - dense_104_accuracy: 0.9991 - dense_104_loss: 0.0018 - dense_105_accuracy: 0.9989 - dense_105_loss: 0.0015 - loss: 0.0033 - val_dense_104_accuracy: 0.9367 - val_dense_104_loss: 0.1045 - val_dense_105_accuracy: 0.9861 - val_dense_105_loss: 0.0155 - val_loss: 0.1242 - learning_rate: 1.0000e-04\nEpoch 58/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - dense_104_accuracy: 0.9998 - dense_104_loss: 4.6552e-04 - dense_105_accuracy: 0.9987 - dense_105_loss: 0.0018 - loss: 0.0022\nEpoch 58: val_loss improved from 0.12424 to 0.11809, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 171ms/step - dense_104_accuracy: 0.9998 - dense_104_loss: 4.6556e-04 - dense_105_accuracy: 0.9987 - dense_105_loss: 0.0018 - loss: 0.0022 - val_dense_104_accuracy: 0.9398 - val_dense_104_loss: 0.1018 - val_dense_105_accuracy: 0.9877 - val_dense_105_loss: 0.0122 - val_loss: 0.1181 - learning_rate: 1.0000e-04\nEpoch 59/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - dense_104_accuracy: 0.9997 - dense_104_loss: 3.4021e-04 - dense_105_accuracy: 0.9993 - dense_105_loss: 0.0015 - loss: 0.0019\nEpoch 59: val_loss improved from 0.11809 to 0.11652, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 171ms/step - dense_104_accuracy: 0.9997 - dense_104_loss: 3.4025e-04 - dense_105_accuracy: 0.9993 - dense_105_loss: 0.0015 - loss: 0.0019 - val_dense_104_accuracy: 0.9352 - val_dense_104_loss: 0.1010 - val_dense_105_accuracy: 0.9861 - val_dense_105_loss: 0.0115 - val_loss: 0.1165 - learning_rate: 1.0000e-04\nEpoch 60/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - dense_104_accuracy: 0.9997 - dense_104_loss: 4.4999e-04 - dense_105_accuracy: 0.9994 - dense_105_loss: 9.0902e-04 - loss: 0.0014\nEpoch 60: val_loss did not improve from 0.11652\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 162ms/step - dense_104_accuracy: 0.9997 - dense_104_loss: 4.5194e-04 - dense_105_accuracy: 0.9994 - dense_105_loss: 9.0948e-04 - loss: 0.0014 - val_dense_104_accuracy: 0.9414 - val_dense_104_loss: 0.1003 - val_dense_105_accuracy: 0.9861 - val_dense_105_loss: 0.0171 - val_loss: 0.1215 - learning_rate: 1.0000e-04\nEpoch 61/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 4.1237e-04 - dense_105_accuracy: 0.9994 - dense_105_loss: 0.0012 - loss: 0.0016\nEpoch 61: val_loss did not improve from 0.11652\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 162ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 4.1247e-04 - dense_105_accuracy: 0.9994 - dense_105_loss: 0.0012 - loss: 0.0016 - val_dense_104_accuracy: 0.9352 - val_dense_104_loss: 0.1055 - val_dense_105_accuracy: 0.9877 - val_dense_105_loss: 0.0162 - val_loss: 0.1259 - learning_rate: 1.0000e-04\nEpoch 62/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - dense_104_accuracy: 0.9999 - dense_104_loss: 4.2987e-04 - dense_105_accuracy: 0.9997 - dense_105_loss: 8.8293e-04 - loss: 0.0013\nEpoch 62: val_loss did not improve from 0.11652\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 162ms/step - dense_104_accuracy: 0.9999 - dense_104_loss: 4.3008e-04 - dense_105_accuracy: 0.9997 - dense_105_loss: 8.8417e-04 - loss: 0.0013 - val_dense_104_accuracy: 0.9336 - val_dense_104_loss: 0.1077 - val_dense_105_accuracy: 0.9923 - val_dense_105_loss: 0.0086 - val_loss: 0.1206 - learning_rate: 1.0000e-04\nEpoch 63/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 1.5424e-04 - dense_105_accuracy: 0.9991 - dense_105_loss: 0.0016 - loss: 0.0017\nEpoch 63: val_loss did not improve from 0.11652\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 162ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 1.5472e-04 - dense_105_accuracy: 0.9991 - dense_105_loss: 0.0016 - loss: 0.0017 - val_dense_104_accuracy: 0.9367 - val_dense_104_loss: 0.1034 - val_dense_105_accuracy: 0.9907 - val_dense_105_loss: 0.0101 - val_loss: 0.1176 - learning_rate: 1.0000e-04\nEpoch 64/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - dense_104_accuracy: 0.9996 - dense_104_loss: 4.5458e-04 - dense_105_accuracy: 0.9997 - dense_105_loss: 6.5478e-04 - loss: 0.0011\nEpoch 64: val_loss did not improve from 0.11652\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 161ms/step - dense_104_accuracy: 0.9996 - dense_104_loss: 4.5427e-04 - dense_105_accuracy: 0.9997 - dense_105_loss: 6.5595e-04 - loss: 0.0011 - val_dense_104_accuracy: 0.9336 - val_dense_104_loss: 0.1039 - val_dense_105_accuracy: 0.9877 - val_dense_105_loss: 0.0133 - val_loss: 0.1215 - learning_rate: 1.0000e-04\nEpoch 65/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 1.3120e-04 - dense_105_accuracy: 0.9995 - dense_105_loss: 7.1119e-04 - loss: 8.4239e-04\nEpoch 65: val_loss improved from 0.11652 to 0.11456, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 169ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 1.3143e-04 - dense_105_accuracy: 0.9995 - dense_105_loss: 7.1204e-04 - loss: 8.4346e-04 - val_dense_104_accuracy: 0.9367 - val_dense_104_loss: 0.1046 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0060 - val_loss: 0.1146 - learning_rate: 1.0000e-04\nEpoch 66/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - dense_104_accuracy: 0.9999 - dense_104_loss: 2.0849e-04 - dense_105_accuracy: 0.9989 - dense_105_loss: 0.0012 - loss: 0.0014\nEpoch 66: val_loss did not improve from 0.11456\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 160ms/step - dense_104_accuracy: 0.9999 - dense_104_loss: 2.0897e-04 - dense_105_accuracy: 0.9989 - dense_105_loss: 0.0012 - loss: 0.0014 - val_dense_104_accuracy: 0.9321 - val_dense_104_loss: 0.1092 - val_dense_105_accuracy: 0.9923 - val_dense_105_loss: 0.0075 - val_loss: 0.1209 - learning_rate: 1.0000e-04\nEpoch 67/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - dense_104_accuracy: 0.9997 - dense_104_loss: 2.2856e-04 - dense_105_accuracy: 0.9994 - dense_105_loss: 0.0011 - loss: 0.0014\nEpoch 67: val_loss did not improve from 0.11456\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 160ms/step - dense_104_accuracy: 0.9997 - dense_104_loss: 2.2839e-04 - dense_105_accuracy: 0.9994 - dense_105_loss: 0.0011 - loss: 0.0014 - val_dense_104_accuracy: 0.9367 - val_dense_104_loss: 0.1025 - val_dense_105_accuracy: 0.9892 - val_dense_105_loss: 0.0090 - val_loss: 0.1155 - learning_rate: 1.0000e-04\nEpoch 68/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - dense_104_accuracy: 0.9997 - dense_104_loss: 2.2873e-04 - dense_105_accuracy: 1.0000 - dense_105_loss: 3.5565e-04 - loss: 5.8439e-04\nEpoch 68: val_loss did not improve from 0.11456\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 160ms/step - dense_104_accuracy: 0.9997 - dense_104_loss: 2.2853e-04 - dense_105_accuracy: 1.0000 - dense_105_loss: 3.5580e-04 - loss: 5.8434e-04 - val_dense_104_accuracy: 0.9352 - val_dense_104_loss: 0.1028 - val_dense_105_accuracy: 0.9877 - val_dense_105_loss: 0.0135 - val_loss: 0.1204 - learning_rate: 1.0000e-04\nEpoch 69/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - dense_104_accuracy: 0.9997 - dense_104_loss: 4.8255e-04 - dense_105_accuracy: 0.9998 - dense_105_loss: 6.6212e-04 - loss: 0.0011\nEpoch 69: val_loss did not improve from 0.11456\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 160ms/step - dense_104_accuracy: 0.9997 - dense_104_loss: 4.8220e-04 - dense_105_accuracy: 0.9998 - dense_105_loss: 6.6204e-04 - loss: 0.0011 - val_dense_104_accuracy: 0.9336 - val_dense_104_loss: 0.1032 - val_dense_105_accuracy: 0.9877 - val_dense_105_loss: 0.0102 - val_loss: 0.1175 - learning_rate: 1.0000e-04\nEpoch 70/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - dense_104_accuracy: 0.9998 - dense_104_loss: 3.1084e-04 - dense_105_accuracy: 0.9998 - dense_105_loss: 3.6463e-04 - loss: 6.7547e-04\nEpoch 70: val_loss did not improve from 0.11456\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 160ms/step - dense_104_accuracy: 0.9998 - dense_104_loss: 3.1096e-04 - dense_105_accuracy: 0.9998 - dense_105_loss: 3.6536e-04 - loss: 6.7633e-04 - val_dense_104_accuracy: 0.9336 - val_dense_104_loss: 0.1127 - val_dense_105_accuracy: 0.9892 - val_dense_105_loss: 0.0081 - val_loss: 0.1252 - learning_rate: 1.0000e-04\nEpoch 71/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - dense_104_accuracy: 0.9999 - dense_104_loss: 2.9316e-04 - dense_105_accuracy: 0.9995 - dense_105_loss: 8.6120e-04 - loss: 0.0012\nEpoch 71: val_loss did not improve from 0.11456\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 159ms/step - dense_104_accuracy: 0.9999 - dense_104_loss: 2.9292e-04 - dense_105_accuracy: 0.9995 - dense_105_loss: 8.6125e-04 - loss: 0.0012 - val_dense_104_accuracy: 0.9336 - val_dense_104_loss: 0.1111 - val_dense_105_accuracy: 0.9892 - val_dense_105_loss: 0.0098 - val_loss: 0.1253 - learning_rate: 1.0000e-04\nEpoch 72/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - dense_104_accuracy: 0.9998 - dense_104_loss: 2.5106e-04 - dense_105_accuracy: 0.9993 - dense_105_loss: 0.0010 - loss: 0.0013\nEpoch 72: val_loss improved from 0.11456 to 0.11314, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 169ms/step - dense_104_accuracy: 0.9998 - dense_104_loss: 2.5093e-04 - dense_105_accuracy: 0.9993 - dense_105_loss: 0.0010 - loss: 0.0013 - val_dense_104_accuracy: 0.9414 - val_dense_104_loss: 0.1038 - val_dense_105_accuracy: 0.9938 - val_dense_105_loss: 0.0054 - val_loss: 0.1131 - learning_rate: 1.0000e-04\nEpoch 73/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 9.0750e-05 - dense_105_accuracy: 0.9994 - dense_105_loss: 6.5307e-04 - loss: 7.4381e-04\nEpoch 73: val_loss did not improve from 0.11314\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 160ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 9.0699e-05 - dense_105_accuracy: 0.9994 - dense_105_loss: 6.5275e-04 - loss: 7.4344e-04 - val_dense_104_accuracy: 0.9383 - val_dense_104_loss: 0.1050 - val_dense_105_accuracy: 0.9907 - val_dense_105_loss: 0.0089 - val_loss: 0.1180 - learning_rate: 1.0000e-04\nEpoch 74/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - dense_104_accuracy: 0.9999 - dense_104_loss: 3.0040e-04 - dense_105_accuracy: 0.9998 - dense_105_loss: 4.8933e-04 - loss: 7.8956e-04\nEpoch 74: val_loss improved from 0.11314 to 0.11257, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 169ms/step - dense_104_accuracy: 0.9999 - dense_104_loss: 3.0105e-04 - dense_105_accuracy: 0.9998 - dense_105_loss: 4.9029e-04 - loss: 7.9099e-04 - val_dense_104_accuracy: 0.9383 - val_dense_104_loss: 0.0992 - val_dense_105_accuracy: 0.9923 - val_dense_105_loss: 0.0095 - val_loss: 0.1126 - learning_rate: 1.0000e-04\nEpoch 75/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 1.1890e-04 - dense_105_accuracy: 0.9999 - dense_105_loss: 3.6129e-04 - loss: 4.8018e-04\nEpoch 75: val_loss improved from 0.11257 to 0.10964, saving model to best1_model_cer_skin_lung.keras\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 169ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 1.1897e-04 - dense_105_accuracy: 0.9999 - dense_105_loss: 3.6213e-04 - loss: 4.8109e-04 - val_dense_104_accuracy: 0.9383 - val_dense_104_loss: 0.1014 - val_dense_105_accuracy: 0.9969 - val_dense_105_loss: 0.0044 - val_loss: 0.1096 - learning_rate: 1.0000e-04\nEpoch 76/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 8.2937e-05 - dense_105_accuracy: 0.9999 - dense_105_loss: 3.3487e-04 - loss: 4.1782e-04\nEpoch 76: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 161ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 8.2841e-05 - dense_105_accuracy: 0.9999 - dense_105_loss: 3.3530e-04 - loss: 4.1815e-04 - val_dense_104_accuracy: 0.9352 - val_dense_104_loss: 0.1064 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0045 - val_loss: 0.1150 - learning_rate: 1.0000e-04\nEpoch 77/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 1.5406e-04 - dense_105_accuracy: 0.9993 - dense_105_loss: 8.5878e-04 - loss: 0.0010\nEpoch 77: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 158ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 1.5492e-04 - dense_105_accuracy: 0.9994 - dense_105_loss: 8.5862e-04 - loss: 0.0010 - val_dense_104_accuracy: 0.9414 - val_dense_104_loss: 0.1093 - val_dense_105_accuracy: 0.9923 - val_dense_105_loss: 0.0061 - val_loss: 0.1196 - learning_rate: 1.0000e-04\nEpoch 78/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - dense_104_accuracy: 0.9998 - dense_104_loss: 2.5228e-04 - dense_105_accuracy: 0.9989 - dense_105_loss: 0.0010 - loss: 0.0013\nEpoch 78: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 158ms/step - dense_104_accuracy: 0.9998 - dense_104_loss: 2.5209e-04 - dense_105_accuracy: 0.9989 - dense_105_loss: 0.0010 - loss: 0.0013 - val_dense_104_accuracy: 0.9414 - val_dense_104_loss: 0.1042 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0066 - val_loss: 0.1148 - learning_rate: 1.0000e-04\nEpoch 79/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 6.4768e-05 - dense_105_accuracy: 0.9995 - dense_105_loss: 9.1602e-04 - loss: 9.8080e-04\nEpoch 79: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 158ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 6.4749e-05 - dense_105_accuracy: 0.9995 - dense_105_loss: 9.1598e-04 - loss: 9.8074e-04 - val_dense_104_accuracy: 0.9414 - val_dense_104_loss: 0.1021 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0056 - val_loss: 0.1117 - learning_rate: 1.0000e-04\nEpoch 80/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 1.0102e-04 - dense_105_accuracy: 0.9999 - dense_105_loss: 3.9819e-04 - loss: 4.9735e-04\nEpoch 80: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 159ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 1.0397e-04 - dense_105_accuracy: 0.9999 - dense_105_loss: 3.9884e-04 - loss: 4.9909e-04 - val_dense_104_accuracy: 0.9398 - val_dense_104_loss: 0.1033 - val_dense_105_accuracy: 0.9938 - val_dense_105_loss: 0.0048 - val_loss: 0.1120 - learning_rate: 1.0000e-04\nEpoch 81/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - dense_104_accuracy: 0.9999 - dense_104_loss: 1.6643e-04 - dense_105_accuracy: 0.9996 - dense_105_loss: 8.2581e-04 - loss: 9.9225e-04\nEpoch 81: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 159ms/step - dense_104_accuracy: 0.9999 - dense_104_loss: 1.6634e-04 - dense_105_accuracy: 0.9996 - dense_105_loss: 8.2518e-04 - loss: 9.9152e-04 - val_dense_104_accuracy: 0.9414 - val_dense_104_loss: 0.1024 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0048 - val_loss: 0.1111 - learning_rate: 1.0000e-04\nEpoch 82/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - dense_104_accuracy: 0.9990 - dense_104_loss: 0.0013 - dense_105_accuracy: 0.9999 - dense_105_loss: 3.8349e-04 - loss: 0.0017\nEpoch 82: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 160ms/step - dense_104_accuracy: 0.9990 - dense_104_loss: 0.0013 - dense_105_accuracy: 0.9999 - dense_105_loss: 3.8351e-04 - loss: 0.0017 - val_dense_104_accuracy: 0.9414 - val_dense_104_loss: 0.1063 - val_dense_105_accuracy: 0.9923 - val_dense_105_loss: 0.0052 - val_loss: 0.1155 - learning_rate: 1.0000e-04\nEpoch 83/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 9.5636e-05 - dense_105_accuracy: 0.9997 - dense_105_loss: 4.7282e-04 - loss: 5.6845e-04\nEpoch 83: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 160ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 9.5550e-05 - dense_105_accuracy: 0.9997 - dense_105_loss: 4.7318e-04 - loss: 5.6873e-04 - val_dense_104_accuracy: 0.9414 - val_dense_104_loss: 0.1024 - val_dense_105_accuracy: 0.9923 - val_dense_105_loss: 0.0074 - val_loss: 0.1137 - learning_rate: 1.0000e-04\nEpoch 84/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - dense_104_accuracy: 0.9992 - dense_104_loss: 0.0013 - dense_105_accuracy: 0.9996 - dense_105_loss: 7.4122e-04 - loss: 0.0020\nEpoch 84: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 160ms/step - dense_104_accuracy: 0.9992 - dense_104_loss: 0.0013 - dense_105_accuracy: 0.9996 - dense_105_loss: 7.3999e-04 - loss: 0.0020 - val_dense_104_accuracy: 0.9398 - val_dense_104_loss: 0.1035 - val_dense_105_accuracy: 0.9892 - val_dense_105_loss: 0.0082 - val_loss: 0.1155 - learning_rate: 1.0000e-04\nEpoch 85/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - dense_104_accuracy: 0.9999 - dense_104_loss: 1.3933e-04 - dense_105_accuracy: 0.9996 - dense_105_loss: 4.6596e-04 - loss: 6.0530e-04\nEpoch 85: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 159ms/step - dense_104_accuracy: 0.9999 - dense_104_loss: 1.3992e-04 - dense_105_accuracy: 0.9996 - dense_105_loss: 4.6576e-04 - loss: 6.0569e-04 - val_dense_104_accuracy: 0.9398 - val_dense_104_loss: 0.1071 - val_dense_105_accuracy: 0.9938 - val_dense_105_loss: 0.0062 - val_loss: 0.1174 - learning_rate: 1.0000e-04\nEpoch 86/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 6.5897e-05 - dense_105_accuracy: 1.0000 - dense_105_loss: 2.3088e-04 - loss: 2.9678e-04\nEpoch 86: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 159ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 6.6132e-05 - dense_105_accuracy: 1.0000 - dense_105_loss: 2.3109e-04 - loss: 2.9722e-04 - val_dense_104_accuracy: 0.9383 - val_dense_104_loss: 0.1110 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0048 - val_loss: 0.1200 - learning_rate: 1.0000e-04\nEpoch 87/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 8.1036e-05 - dense_105_accuracy: 0.9996 - dense_105_loss: 6.7447e-04 - loss: 7.5551e-04\nEpoch 87: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 158ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 8.1269e-05 - dense_105_accuracy: 0.9996 - dense_105_loss: 6.7388e-04 - loss: 7.5516e-04 - val_dense_104_accuracy: 0.9367 - val_dense_104_loss: 0.1144 - val_dense_105_accuracy: 0.9923 - val_dense_105_loss: 0.0055 - val_loss: 0.1242 - learning_rate: 1.0000e-04\nEpoch 88/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - dense_104_accuracy: 0.9997 - dense_104_loss: 2.1673e-04 - dense_105_accuracy: 0.9998 - dense_105_loss: 3.8066e-04 - loss: 5.9739e-04\nEpoch 88: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 159ms/step - dense_104_accuracy: 0.9997 - dense_104_loss: 2.1640e-04 - dense_105_accuracy: 0.9998 - dense_105_loss: 3.8100e-04 - loss: 5.9740e-04 - val_dense_104_accuracy: 0.9367 - val_dense_104_loss: 0.1129 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0051 - val_loss: 0.1223 - learning_rate: 1.0000e-04\nEpoch 89/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - dense_104_accuracy: 0.9997 - dense_104_loss: 2.7854e-04 - dense_105_accuracy: 0.9994 - dense_105_loss: 5.7760e-04 - loss: 8.5614e-04\nEpoch 89: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 159ms/step - dense_104_accuracy: 0.9997 - dense_104_loss: 2.7858e-04 - dense_105_accuracy: 0.9994 - dense_105_loss: 5.7726e-04 - loss: 8.5584e-04 - val_dense_104_accuracy: 0.9352 - val_dense_104_loss: 0.1259 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0042 - val_loss: 0.1348 - learning_rate: 1.0000e-04\nEpoch 90/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - dense_104_accuracy: 0.9997 - dense_104_loss: 1.9504e-04 - dense_105_accuracy: 0.9994 - dense_105_loss: 4.9148e-04 - loss: 6.8652e-04\nEpoch 90: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 159ms/step - dense_104_accuracy: 0.9997 - dense_104_loss: 1.9467e-04 - dense_105_accuracy: 0.9994 - dense_105_loss: 4.9154e-04 - loss: 6.8622e-04 - val_dense_104_accuracy: 0.9367 - val_dense_104_loss: 0.1213 - val_dense_105_accuracy: 0.9938 - val_dense_105_loss: 0.0054 - val_loss: 0.1312 - learning_rate: 1.0000e-04\nEpoch 91/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - dense_104_accuracy: 0.9997 - dense_104_loss: 4.4567e-04 - dense_105_accuracy: 0.9994 - dense_105_loss: 5.6111e-04 - loss: 0.0010\nEpoch 91: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 158ms/step - dense_104_accuracy: 0.9997 - dense_104_loss: 4.4546e-04 - dense_105_accuracy: 0.9994 - dense_105_loss: 5.6064e-04 - loss: 0.0010 - val_dense_104_accuracy: 0.9352 - val_dense_104_loss: 0.1231 - val_dense_105_accuracy: 0.9938 - val_dense_105_loss: 0.0066 - val_loss: 0.1343 - learning_rate: 1.0000e-04\nEpoch 92/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 7.0630e-05 - dense_105_accuracy: 0.9998 - dense_105_loss: 2.1197e-04 - loss: 2.8261e-04\nEpoch 92: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 157ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 7.1908e-05 - dense_105_accuracy: 0.9998 - dense_105_loss: 2.1196e-04 - loss: 2.8388e-04 - val_dense_104_accuracy: 0.9352 - val_dense_104_loss: 0.1184 - val_dense_105_accuracy: 0.9938 - val_dense_105_loss: 0.0060 - val_loss: 0.1290 - learning_rate: 1.0000e-04\nEpoch 93/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 1.1354e-04 - dense_105_accuracy: 0.9998 - dense_105_loss: 2.7306e-04 - loss: 3.8661e-04\nEpoch 93: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 158ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 1.1383e-04 - dense_105_accuracy: 0.9998 - dense_105_loss: 2.7346e-04 - loss: 3.8729e-04 - val_dense_104_accuracy: 0.9367 - val_dense_104_loss: 0.1207 - val_dense_105_accuracy: 0.9938 - val_dense_105_loss: 0.0062 - val_loss: 0.1315 - learning_rate: 1.0000e-04\nEpoch 94/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 7.1485e-05 - dense_105_accuracy: 0.9999 - dense_105_loss: 2.0946e-04 - loss: 2.7877e-04\nEpoch 94: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 158ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 7.4913e-05 - dense_105_accuracy: 0.9999 - dense_105_loss: 2.0978e-04 - loss: 2.8035e-04 - val_dense_104_accuracy: 0.9398 - val_dense_104_loss: 0.1246 - val_dense_105_accuracy: 0.9969 - val_dense_105_loss: 0.0046 - val_loss: 0.1339 - learning_rate: 1.0000e-04\nEpoch 95/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 1.6169e-04 - dense_105_accuracy: 0.9999 - dense_105_loss: 1.7867e-04 - loss: 3.4036e-04\nEpoch 95: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 158ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 1.6141e-04 - dense_105_accuracy: 0.9999 - dense_105_loss: 1.7955e-04 - loss: 3.4097e-04 - val_dense_104_accuracy: 0.9367 - val_dense_104_loss: 0.1139 - val_dense_105_accuracy: 0.9969 - val_dense_105_loss: 0.0043 - val_loss: 0.1225 - learning_rate: 1.0000e-04\nEpoch 96/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - dense_104_accuracy: 0.9998 - dense_104_loss: 1.5305e-04 - dense_105_accuracy: 0.9994 - dense_105_loss: 9.2821e-04 - loss: 0.0011\nEpoch 96: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 159ms/step - dense_104_accuracy: 0.9998 - dense_104_loss: 1.5291e-04 - dense_105_accuracy: 0.9994 - dense_105_loss: 9.2803e-04 - loss: 0.0011 - val_dense_104_accuracy: 0.9398 - val_dense_104_loss: 0.1118 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0061 - val_loss: 0.1221 - learning_rate: 1.0000e-04\nEpoch 97/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 2.0028e-05 - dense_105_accuracy: 0.9999 - dense_105_loss: 6.2914e-04 - loss: 6.4917e-04\nEpoch 97: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 158ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 2.0045e-05 - dense_105_accuracy: 0.9999 - dense_105_loss: 6.3030e-04 - loss: 6.5036e-04 - val_dense_104_accuracy: 0.9398 - val_dense_104_loss: 0.1113 - val_dense_105_accuracy: 0.9969 - val_dense_105_loss: 0.0037 - val_loss: 0.1193 - learning_rate: 1.0000e-04\nEpoch 98/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 5.9884e-05 - dense_105_accuracy: 0.9995 - dense_105_loss: 8.3179e-04 - loss: 8.9163e-04\nEpoch 98: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 158ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 6.0299e-05 - dense_105_accuracy: 0.9995 - dense_105_loss: 8.3046e-04 - loss: 8.9066e-04 - val_dense_104_accuracy: 0.9367 - val_dense_104_loss: 0.1130 - val_dense_105_accuracy: 0.9907 - val_dense_105_loss: 0.0065 - val_loss: 0.1238 - learning_rate: 1.0000e-04\nEpoch 99/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 1.0725e-04 - dense_105_accuracy: 1.0000 - dense_105_loss: 3.1529e-04 - loss: 4.2254e-04\nEpoch 99: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 158ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 1.0714e-04 - dense_105_accuracy: 1.0000 - dense_105_loss: 3.1552e-04 - loss: 4.2266e-04 - val_dense_104_accuracy: 0.9383 - val_dense_104_loss: 0.1152 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0051 - val_loss: 0.1246 - learning_rate: 1.0000e-04\nEpoch 100/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 1.1246e-04 - dense_105_accuracy: 0.9997 - dense_105_loss: 4.7801e-04 - loss: 5.9048e-04\nEpoch 100: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 157ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 1.1443e-04 - dense_105_accuracy: 0.9997 - dense_105_loss: 4.7790e-04 - loss: 5.9234e-04 - val_dense_104_accuracy: 0.9306 - val_dense_104_loss: 0.1216 - val_dense_105_accuracy: 0.9923 - val_dense_105_loss: 0.0065 - val_loss: 0.1327 - learning_rate: 1.0000e-04\nEpoch 101/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 3.2118e-05 - dense_105_accuracy: 0.9993 - dense_105_loss: 9.8362e-04 - loss: 0.0010\nEpoch 101: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 157ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 3.2140e-05 - dense_105_accuracy: 0.9993 - dense_105_loss: 9.8339e-04 - loss: 0.0010 - val_dense_104_accuracy: 0.9383 - val_dense_104_loss: 0.1137 - val_dense_105_accuracy: 0.9907 - val_dense_105_loss: 0.0076 - val_loss: 0.1256 - learning_rate: 1.0000e-04\nEpoch 102/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - dense_104_accuracy: 0.9999 - dense_104_loss: 2.2577e-04 - dense_105_accuracy: 0.9999 - dense_105_loss: 3.4073e-04 - loss: 5.6172e-04\nEpoch 102: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 158ms/step - dense_104_accuracy: 0.9999 - dense_104_loss: 2.2596e-04 - dense_105_accuracy: 0.9999 - dense_105_loss: 3.4846e-04 - loss: 5.6490e-04 - val_dense_104_accuracy: 0.9352 - val_dense_104_loss: 0.1132 - val_dense_105_accuracy: 0.9938 - val_dense_105_loss: 0.0048 - val_loss: 0.1223 - learning_rate: 1.0000e-04\nEpoch 103/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - dense_104_accuracy: 0.9998 - dense_104_loss: 2.4758e-04 - dense_105_accuracy: 1.0000 - dense_105_loss: 2.5688e-04 - loss: 5.0446e-04\nEpoch 103: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 157ms/step - dense_104_accuracy: 0.9998 - dense_104_loss: 2.4721e-04 - dense_105_accuracy: 1.0000 - dense_105_loss: 2.5670e-04 - loss: 5.0392e-04 - val_dense_104_accuracy: 0.9321 - val_dense_104_loss: 0.1146 - val_dense_105_accuracy: 0.9907 - val_dense_105_loss: 0.0079 - val_loss: 0.1268 - learning_rate: 1.0000e-04\nEpoch 104/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - dense_104_accuracy: 0.9999 - dense_104_loss: 1.0844e-04 - dense_105_accuracy: 1.0000 - dense_105_loss: 9.2711e-05 - loss: 2.0115e-04\nEpoch 104: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 158ms/step - dense_104_accuracy: 0.9999 - dense_104_loss: 1.0869e-04 - dense_105_accuracy: 1.0000 - dense_105_loss: 9.2994e-05 - loss: 2.0168e-04 - val_dense_104_accuracy: 0.9321 - val_dense_104_loss: 0.1182 - val_dense_105_accuracy: 0.9938 - val_dense_105_loss: 0.0056 - val_loss: 0.1283 - learning_rate: 1.0000e-04\nEpoch 105/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - dense_104_accuracy: 0.9998 - dense_104_loss: 2.9085e-04 - dense_105_accuracy: 0.9998 - dense_105_loss: 4.5919e-04 - loss: 7.5005e-04\nEpoch 105: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 157ms/step - dense_104_accuracy: 0.9998 - dense_104_loss: 2.9098e-04 - dense_105_accuracy: 0.9998 - dense_105_loss: 4.5894e-04 - loss: 7.4994e-04 - val_dense_104_accuracy: 0.9414 - val_dense_104_loss: 0.1095 - val_dense_105_accuracy: 0.9938 - val_dense_105_loss: 0.0037 - val_loss: 0.1173 - learning_rate: 1.0000e-04\nEpoch 106/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - dense_104_accuracy: 0.9998 - dense_104_loss: 2.2635e-04 - dense_105_accuracy: 0.9996 - dense_105_loss: 3.6856e-04 - loss: 5.9491e-04\nEpoch 106: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 157ms/step - dense_104_accuracy: 0.9998 - dense_104_loss: 2.2682e-04 - dense_105_accuracy: 0.9996 - dense_105_loss: 3.6819e-04 - loss: 5.9503e-04 - val_dense_104_accuracy: 0.9367 - val_dense_104_loss: 0.1109 - val_dense_105_accuracy: 0.9938 - val_dense_105_loss: 0.0049 - val_loss: 0.1200 - learning_rate: 1.0000e-04\nEpoch 107/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - dense_104_accuracy: 0.9999 - dense_104_loss: 2.7880e-04 - dense_105_accuracy: 0.9998 - dense_105_loss: 3.3693e-04 - loss: 6.1573e-04\nEpoch 107: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 157ms/step - dense_104_accuracy: 0.9999 - dense_104_loss: 2.7968e-04 - dense_105_accuracy: 0.9998 - dense_105_loss: 3.3702e-04 - loss: 6.1668e-04 - val_dense_104_accuracy: 0.9398 - val_dense_104_loss: 0.1165 - val_dense_105_accuracy: 0.9938 - val_dense_105_loss: 0.0054 - val_loss: 0.1262 - learning_rate: 1.0000e-04\nEpoch 108/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - dense_104_accuracy: 0.9996 - dense_104_loss: 1.7051e-04 - dense_105_accuracy: 1.0000 - dense_105_loss: 8.6249e-05 - loss: 2.5658e-04\nEpoch 108: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 157ms/step - dense_104_accuracy: 0.9996 - dense_104_loss: 1.7023e-04 - dense_105_accuracy: 1.0000 - dense_105_loss: 8.6854e-05 - loss: 2.5672e-04 - val_dense_104_accuracy: 0.9352 - val_dense_104_loss: 0.1184 - val_dense_105_accuracy: 0.9969 - val_dense_105_loss: 0.0039 - val_loss: 0.1267 - learning_rate: 1.0000e-04\nEpoch 109/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 3.3201e-05 - dense_105_accuracy: 1.0000 - dense_105_loss: 6.6991e-05 - loss: 9.9978e-05\nEpoch 109: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 158ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 3.3254e-05 - dense_105_accuracy: 1.0000 - dense_105_loss: 6.7377e-05 - loss: 1.0021e-04 - val_dense_104_accuracy: 0.9336 - val_dense_104_loss: 0.1217 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0049 - val_loss: 0.1311 - learning_rate: 1.0000e-04\nEpoch 110/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 7.9558e-05 - dense_105_accuracy: 0.9996 - dense_105_loss: 5.2395e-04 - loss: 6.0351e-04\nEpoch 110: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 158ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 8.0415e-05 - dense_105_accuracy: 0.9996 - dense_105_loss: 5.2288e-04 - loss: 6.0331e-04 - val_dense_104_accuracy: 0.9336 - val_dense_104_loss: 0.1212 - val_dense_105_accuracy: 0.9938 - val_dense_105_loss: 0.0051 - val_loss: 0.1308 - learning_rate: 1.0000e-04\nEpoch 111/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 2.1220e-05 - dense_105_accuracy: 0.9998 - dense_105_loss: 4.0393e-04 - loss: 4.2509e-04\nEpoch 111: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 158ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 2.1232e-05 - dense_105_accuracy: 0.9998 - dense_105_loss: 4.0522e-04 - loss: 4.2633e-04 - val_dense_104_accuracy: 0.9352 - val_dense_104_loss: 0.1182 - val_dense_105_accuracy: 0.9938 - val_dense_105_loss: 0.0069 - val_loss: 0.1296 - learning_rate: 1.0000e-04\nEpoch 112/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 6.3069e-05 - dense_105_accuracy: 1.0000 - dense_105_loss: 2.0802e-04 - loss: 2.7109e-04\nEpoch 112: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 158ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 6.3478e-05 - dense_105_accuracy: 1.0000 - dense_105_loss: 2.0815e-04 - loss: 2.7163e-04 - val_dense_104_accuracy: 0.9383 - val_dense_104_loss: 0.1159 - val_dense_105_accuracy: 0.9923 - val_dense_105_loss: 0.0090 - val_loss: 0.1292 - learning_rate: 1.0000e-04\nEpoch 113/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 3.2409e-05 - dense_105_accuracy: 0.9996 - dense_105_loss: 6.6667e-04 - loss: 6.9820e-04\nEpoch 113: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 158ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 3.3824e-05 - dense_105_accuracy: 0.9996 - dense_105_loss: 6.6645e-04 - loss: 6.9852e-04 - val_dense_104_accuracy: 0.9398 - val_dense_104_loss: 0.1140 - val_dense_105_accuracy: 0.9969 - val_dense_105_loss: 0.0050 - val_loss: 0.1233 - learning_rate: 1.0000e-04\nEpoch 114/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - dense_104_accuracy: 0.9999 - dense_104_loss: 1.2273e-04 - dense_105_accuracy: 0.9998 - dense_105_loss: 2.6663e-04 - loss: 3.8935e-04\nEpoch 114: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 158ms/step - dense_104_accuracy: 0.9999 - dense_104_loss: 1.2289e-04 - dense_105_accuracy: 0.9998 - dense_105_loss: 2.6694e-04 - loss: 3.8982e-04 - val_dense_104_accuracy: 0.9444 - val_dense_104_loss: 0.1224 - val_dense_105_accuracy: 0.9938 - val_dense_105_loss: 0.0067 - val_loss: 0.1338 - learning_rate: 1.0000e-04\nEpoch 115/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 6.2189e-05 - dense_105_accuracy: 0.9998 - dense_105_loss: 1.6058e-04 - loss: 2.2277e-04\nEpoch 115: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 158ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 6.2132e-05 - dense_105_accuracy: 0.9998 - dense_105_loss: 1.6095e-04 - loss: 2.2309e-04 - val_dense_104_accuracy: 0.9398 - val_dense_104_loss: 0.1257 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0052 - val_loss: 0.1357 - learning_rate: 1.0000e-04\nEpoch 116/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 8.3504e-05 - dense_105_accuracy: 0.9997 - dense_105_loss: 2.8541e-04 - loss: 3.6892e-04\nEpoch 116: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 157ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 8.4230e-05 - dense_105_accuracy: 0.9997 - dense_105_loss: 2.8536e-04 - loss: 3.6960e-04 - val_dense_104_accuracy: 0.9429 - val_dense_104_loss: 0.1214 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0057 - val_loss: 0.1318 - learning_rate: 1.0000e-04\nEpoch 117/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 6.5877e-05 - dense_105_accuracy: 0.9998 - dense_105_loss: 3.3232e-04 - loss: 3.9820e-04\nEpoch 117: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 158ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 6.6079e-05 - dense_105_accuracy: 0.9998 - dense_105_loss: 3.3237e-04 - loss: 3.9846e-04 - val_dense_104_accuracy: 0.9444 - val_dense_104_loss: 0.1244 - val_dense_105_accuracy: 0.9923 - val_dense_105_loss: 0.0069 - val_loss: 0.1360 - learning_rate: 1.0000e-04\nEpoch 118/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 1.0827e-04 - dense_105_accuracy: 0.9998 - dense_105_loss: 4.7125e-04 - loss: 5.7944e-04\nEpoch 118: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 157ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 1.0806e-04 - dense_105_accuracy: 0.9998 - dense_105_loss: 4.7156e-04 - loss: 5.7945e-04 - val_dense_104_accuracy: 0.9429 - val_dense_104_loss: 0.1239 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0041 - val_loss: 0.1327 - learning_rate: 1.0000e-04\nEpoch 119/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 5.0151e-05 - dense_105_accuracy: 0.9995 - dense_105_loss: 6.4161e-04 - loss: 6.9111e-04\nEpoch 119: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 158ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 5.0227e-05 - dense_105_accuracy: 0.9995 - dense_105_loss: 6.4203e-04 - loss: 6.9095e-04 - val_dense_104_accuracy: 0.9414 - val_dense_104_loss: 0.1233 - val_dense_105_accuracy: 0.9938 - val_dense_105_loss: 0.0042 - val_loss: 0.1321 - learning_rate: 1.0000e-04\nEpoch 120/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 1.0672e-04 - dense_105_accuracy: 0.9998 - dense_105_loss: 3.9306e-04 - loss: 4.9978e-04\nEpoch 120: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 157ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 1.0654e-04 - dense_105_accuracy: 0.9998 - dense_105_loss: 3.9295e-04 - loss: 4.9949e-04 - val_dense_104_accuracy: 0.9398 - val_dense_104_loss: 0.1253 - val_dense_105_accuracy: 0.9969 - val_dense_105_loss: 0.0040 - val_loss: 0.1340 - learning_rate: 1.0000e-04\nEpoch 121/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - dense_104_accuracy: 0.9999 - dense_104_loss: 9.0465e-05 - dense_105_accuracy: 0.9994 - dense_105_loss: 0.0011 - loss: 0.0012\nEpoch 121: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 158ms/step - dense_104_accuracy: 0.9999 - dense_104_loss: 9.0657e-05 - dense_105_accuracy: 0.9994 - dense_105_loss: 0.0011 - loss: 0.0012 - val_dense_104_accuracy: 0.9414 - val_dense_104_loss: 0.1198 - val_dense_105_accuracy: 0.9938 - val_dense_105_loss: 0.0051 - val_loss: 0.1295 - learning_rate: 1.0000e-04\nEpoch 122/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 1.8647e-05 - dense_105_accuracy: 0.9998 - dense_105_loss: 3.0728e-04 - loss: 3.2593e-04\nEpoch 122: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 158ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 1.8718e-05 - dense_105_accuracy: 0.9998 - dense_105_loss: 3.0758e-04 - loss: 3.2630e-04 - val_dense_104_accuracy: 0.9429 - val_dense_104_loss: 0.1208 - val_dense_105_accuracy: 0.9969 - val_dense_105_loss: 0.0026 - val_loss: 0.1279 - learning_rate: 1.0000e-04\nEpoch 123/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 3.9356e-05 - dense_105_accuracy: 0.9992 - dense_105_loss: 6.3580e-04 - loss: 6.7515e-04\nEpoch 123: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 157ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 3.9298e-05 - dense_105_accuracy: 0.9992 - dense_105_loss: 6.3532e-04 - loss: 6.7462e-04 - val_dense_104_accuracy: 0.9414 - val_dense_104_loss: 0.1198 - val_dense_105_accuracy: 0.9907 - val_dense_105_loss: 0.0067 - val_loss: 0.1311 - learning_rate: 1.0000e-04\nEpoch 124/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 1.2050e-05 - dense_105_accuracy: 0.9994 - dense_105_loss: 5.7836e-04 - loss: 5.9040e-04\nEpoch 124: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 157ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 1.2056e-05 - dense_105_accuracy: 0.9994 - dense_105_loss: 5.7698e-04 - loss: 5.8902e-04 - val_dense_104_accuracy: 0.9414 - val_dense_104_loss: 0.1208 - val_dense_105_accuracy: 0.9938 - val_dense_105_loss: 0.0048 - val_loss: 0.1302 - learning_rate: 1.0000e-04\nEpoch 125/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 3.4118e-05 - dense_105_accuracy: 0.9998 - dense_105_loss: 5.0259e-04 - loss: 5.3555e-04\nEpoch 125: val_loss did not improve from 0.10964\n\nEpoch 125: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 158ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 3.5980e-05 - dense_105_accuracy: 0.9998 - dense_105_loss: 5.0274e-04 - loss: 5.3640e-04 - val_dense_104_accuracy: 0.9414 - val_dense_104_loss: 0.1188 - val_dense_105_accuracy: 0.9923 - val_dense_105_loss: 0.0082 - val_loss: 0.1316 - learning_rate: 1.0000e-04\nEpoch 126/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 1.2042e-04 - dense_105_accuracy: 0.9999 - dense_105_loss: 2.0441e-04 - loss: 3.2483e-04\nEpoch 126: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 157ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 1.2027e-04 - dense_105_accuracy: 0.9999 - dense_105_loss: 2.0551e-04 - loss: 3.2578e-04 - val_dense_104_accuracy: 0.9398 - val_dense_104_loss: 0.1195 - val_dense_105_accuracy: 0.9923 - val_dense_105_loss: 0.0052 - val_loss: 0.1292 - learning_rate: 1.0000e-05\nEpoch 127/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - dense_104_accuracy: 0.9999 - dense_104_loss: 6.8841e-05 - dense_105_accuracy: 0.9994 - dense_105_loss: 4.1546e-04 - loss: 4.8430e-04\nEpoch 127: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 157ms/step - dense_104_accuracy: 0.9999 - dense_104_loss: 6.8886e-05 - dense_105_accuracy: 0.9994 - dense_105_loss: 4.1494e-04 - loss: 4.8383e-04 - val_dense_104_accuracy: 0.9398 - val_dense_104_loss: 0.1189 - val_dense_105_accuracy: 0.9938 - val_dense_105_loss: 0.0051 - val_loss: 0.1285 - learning_rate: 1.0000e-05\nEpoch 128/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 6.2230e-05 - dense_105_accuracy: 0.9997 - dense_105_loss: 5.3350e-04 - loss: 5.9573e-04\nEpoch 128: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 157ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 6.2246e-05 - dense_105_accuracy: 0.9997 - dense_105_loss: 5.3295e-04 - loss: 5.9521e-04 - val_dense_104_accuracy: 0.9429 - val_dense_104_loss: 0.1211 - val_dense_105_accuracy: 0.9938 - val_dense_105_loss: 0.0049 - val_loss: 0.1306 - learning_rate: 1.0000e-05\nEpoch 129/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - dense_104_accuracy: 0.9999 - dense_104_loss: 1.3789e-04 - dense_105_accuracy: 0.9998 - dense_105_loss: 5.7278e-04 - loss: 7.1068e-04\nEpoch 129: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 157ms/step - dense_104_accuracy: 0.9999 - dense_104_loss: 1.3793e-04 - dense_105_accuracy: 0.9998 - dense_105_loss: 5.7266e-04 - loss: 7.1060e-04 - val_dense_104_accuracy: 0.9414 - val_dense_104_loss: 0.1215 - val_dense_105_accuracy: 0.9938 - val_dense_105_loss: 0.0047 - val_loss: 0.1309 - learning_rate: 1.0000e-05\nEpoch 130/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 1.1012e-05 - dense_105_accuracy: 0.9999 - dense_105_loss: 1.3176e-04 - loss: 1.4277e-04\nEpoch 130: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 158ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 1.1029e-05 - dense_105_accuracy: 0.9999 - dense_105_loss: 1.3183e-04 - loss: 1.4285e-04 - val_dense_104_accuracy: 0.9414 - val_dense_104_loss: 0.1210 - val_dense_105_accuracy: 0.9938 - val_dense_105_loss: 0.0052 - val_loss: 0.1308 - learning_rate: 1.0000e-05\nEpoch 131/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 2.9091e-05 - dense_105_accuracy: 1.0000 - dense_105_loss: 1.3898e-04 - loss: 1.6807e-04\nEpoch 131: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 158ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 2.9069e-05 - dense_105_accuracy: 1.0000 - dense_105_loss: 1.3898e-04 - loss: 1.6805e-04 - val_dense_104_accuracy: 0.9414 - val_dense_104_loss: 0.1217 - val_dense_105_accuracy: 0.9923 - val_dense_105_loss: 0.0051 - val_loss: 0.1314 - learning_rate: 1.0000e-05\nEpoch 132/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 2.4895e-04 - dense_105_accuracy: 1.0000 - dense_105_loss: 1.4939e-04 - loss: 3.9834e-04\nEpoch 132: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 156ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 2.4827e-04 - dense_105_accuracy: 1.0000 - dense_105_loss: 1.4940e-04 - loss: 3.9767e-04 - val_dense_104_accuracy: 0.9398 - val_dense_104_loss: 0.1230 - val_dense_105_accuracy: 0.9923 - val_dense_105_loss: 0.0052 - val_loss: 0.1328 - learning_rate: 1.0000e-05\nEpoch 133/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 7.7331e-05 - dense_105_accuracy: 0.9998 - dense_105_loss: 2.7772e-04 - loss: 3.5506e-04\nEpoch 133: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 159ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 7.7848e-05 - dense_105_accuracy: 0.9998 - dense_105_loss: 2.7764e-04 - loss: 3.5549e-04 - val_dense_104_accuracy: 0.9414 - val_dense_104_loss: 0.1201 - val_dense_105_accuracy: 0.9923 - val_dense_105_loss: 0.0052 - val_loss: 0.1299 - learning_rate: 1.0000e-05\nEpoch 134/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 5.2039e-05 - dense_105_accuracy: 1.0000 - dense_105_loss: 3.9542e-04 - loss: 4.4746e-04\nEpoch 134: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 159ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 5.2044e-05 - dense_105_accuracy: 1.0000 - dense_105_loss: 3.9497e-04 - loss: 4.4702e-04 - val_dense_104_accuracy: 0.9398 - val_dense_104_loss: 0.1213 - val_dense_105_accuracy: 0.9938 - val_dense_105_loss: 0.0049 - val_loss: 0.1308 - learning_rate: 1.0000e-05\nEpoch 135/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 1.5661e-04 - dense_105_accuracy: 1.0000 - dense_105_loss: 1.3626e-04 - loss: 2.9286e-04\nEpoch 135: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 159ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 1.5642e-04 - dense_105_accuracy: 1.0000 - dense_105_loss: 1.3625e-04 - loss: 2.9268e-04 - val_dense_104_accuracy: 0.9398 - val_dense_104_loss: 0.1214 - val_dense_105_accuracy: 0.9923 - val_dense_105_loss: 0.0051 - val_loss: 0.1310 - learning_rate: 1.0000e-05\nEpoch 135: early stopping\nRestoring model weights from the end of the best epoch: 75.\n","output_type":"stream"}],"execution_count":73},{"cell_type":"markdown","source":"### best result till date","metadata":{}},{"cell_type":"code","source":"model.evaluate([X_test_s, X_test_h1], [y_test_s, y_test_h1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T22:23:59.123457Z","iopub.execute_input":"2025-02-05T22:23:59.123688Z","iopub.status.idle":"2025-02-05T22:24:00.576943Z","shell.execute_reply.started":"2025-02-05T22:23:59.123669Z","shell.execute_reply":"2025-02-05T22:24:00.576278Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - dense_104_accuracy: 0.9606 - dense_104_loss: 0.1143 - dense_105_accuracy: 0.9971 - dense_105_loss: 0.0032 - loss: 0.1176\n","output_type":"stream"},{"execution_count":74,"output_type":"execute_result","data":{"text/plain":"[0.11255320906639099,\n 0.10341478139162064,\n 0.006329158786684275,\n 0.9580246806144714,\n 0.9950617551803589]"},"metadata":{}}],"execution_count":74},{"cell_type":"code","source":"history = model.fit([X_train_s, X_train_h], [y_train_s, y_train_h],\n                    epochs=200,\n                    validation_data=([X_val_s, X_val_h1], [y_val_s, y_val_h1]), verbose=1,\n                    shuffle=True,\n                    callbacks=callbacks) # UpdateGammaCallback","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T22:24:00.578256Z","iopub.execute_input":"2025-02-05T22:24:00.578512Z","iopub.status.idle":"2025-02-05T23:03:57.508498Z","shell.execute_reply.started":"2025-02-05T22:24:00.578491Z","shell.execute_reply":"2025-02-05T23:03:57.507515Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - dense_104_accuracy: 0.9995 - dense_104_loss: 3.2398e-04 - dense_105_accuracy: 0.9996 - dense_105_loss: 6.8908e-04 - loss: 0.0010\nEpoch 1: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 162ms/step - dense_104_accuracy: 0.9995 - dense_104_loss: 3.2353e-04 - dense_105_accuracy: 0.9996 - dense_105_loss: 6.8950e-04 - loss: 0.0010 - val_dense_104_accuracy: 0.9383 - val_dense_104_loss: 0.1028 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0045 - val_loss: 0.1113 - learning_rate: 1.0000e-05\nEpoch 2/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 6.5308e-05 - dense_105_accuracy: 0.9994 - dense_105_loss: 6.5318e-04 - loss: 7.1849e-04\nEpoch 2: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 158ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 6.5281e-05 - dense_105_accuracy: 0.9994 - dense_105_loss: 6.5330e-04 - loss: 7.1859e-04 - val_dense_104_accuracy: 0.9367 - val_dense_104_loss: 0.1038 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0053 - val_loss: 0.1131 - learning_rate: 1.0000e-05\nEpoch 3/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 2.8769e-04 - dense_105_accuracy: 0.9993 - dense_105_loss: 8.5643e-04 - loss: 0.0011\nEpoch 3: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 157ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 2.8722e-04 - dense_105_accuracy: 0.9993 - dense_105_loss: 8.5680e-04 - loss: 0.0011 - val_dense_104_accuracy: 0.9383 - val_dense_104_loss: 0.1055 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0047 - val_loss: 0.1142 - learning_rate: 1.0000e-05\nEpoch 4/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - dense_104_accuracy: 0.9999 - dense_104_loss: 1.2551e-04 - dense_105_accuracy: 0.9998 - dense_105_loss: 3.6591e-04 - loss: 4.9140e-04\nEpoch 4: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 158ms/step - dense_104_accuracy: 0.9999 - dense_104_loss: 1.2552e-04 - dense_105_accuracy: 0.9998 - dense_105_loss: 3.6583e-04 - loss: 4.9130e-04 - val_dense_104_accuracy: 0.9383 - val_dense_104_loss: 0.1057 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0051 - val_loss: 0.1147 - learning_rate: 1.0000e-05\nEpoch 5/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - dense_104_accuracy: 0.9999 - dense_104_loss: 1.8915e-04 - dense_105_accuracy: 0.9998 - dense_105_loss: 4.0123e-04 - loss: 5.9036e-04\nEpoch 5: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 158ms/step - dense_104_accuracy: 0.9999 - dense_104_loss: 1.8923e-04 - dense_105_accuracy: 0.9998 - dense_105_loss: 4.0207e-04 - loss: 5.9128e-04 - val_dense_104_accuracy: 0.9414 - val_dense_104_loss: 0.1048 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0052 - val_loss: 0.1139 - learning_rate: 1.0000e-05\nEpoch 6/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 7.4652e-05 - dense_105_accuracy: 1.0000 - dense_105_loss: 3.5814e-04 - loss: 4.3280e-04\nEpoch 6: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 158ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 7.4780e-05 - dense_105_accuracy: 1.0000 - dense_105_loss: 3.5833e-04 - loss: 4.3312e-04 - val_dense_104_accuracy: 0.9383 - val_dense_104_loss: 0.1062 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0053 - val_loss: 0.1154 - learning_rate: 1.0000e-05\nEpoch 7/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 1.3276e-04 - dense_105_accuracy: 0.9994 - dense_105_loss: 6.8845e-04 - loss: 8.2122e-04\nEpoch 7: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 157ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 1.3311e-04 - dense_105_accuracy: 0.9994 - dense_105_loss: 6.8878e-04 - loss: 8.2190e-04 - val_dense_104_accuracy: 0.9398 - val_dense_104_loss: 0.1047 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0051 - val_loss: 0.1137 - learning_rate: 1.0000e-05\nEpoch 8/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - dense_104_accuracy: 0.9997 - dense_104_loss: 4.9992e-04 - dense_105_accuracy: 0.9998 - dense_105_loss: 3.1647e-04 - loss: 8.1617e-04\nEpoch 8: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 158ms/step - dense_104_accuracy: 0.9997 - dense_104_loss: 4.9949e-04 - dense_105_accuracy: 0.9998 - dense_105_loss: 3.1752e-04 - loss: 8.1656e-04 - val_dense_104_accuracy: 0.9383 - val_dense_104_loss: 0.1050 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0050 - val_loss: 0.1140 - learning_rate: 1.0000e-05\nEpoch 9/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - dense_104_accuracy: 0.9997 - dense_104_loss: 2.3950e-04 - dense_105_accuracy: 1.0000 - dense_105_loss: 2.7498e-04 - loss: 5.1366e-04\nEpoch 9: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 158ms/step - dense_104_accuracy: 0.9997 - dense_104_loss: 2.4049e-04 - dense_105_accuracy: 1.0000 - dense_105_loss: 2.7459e-04 - loss: 5.1345e-04 - val_dense_104_accuracy: 0.9398 - val_dense_104_loss: 0.1049 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0053 - val_loss: 0.1142 - learning_rate: 1.0000e-05\nEpoch 10/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 1.7637e-04 - dense_105_accuracy: 0.9994 - dense_105_loss: 4.5950e-04 - loss: 6.3588e-04\nEpoch 10: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 159ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 1.7617e-04 - dense_105_accuracy: 0.9994 - dense_105_loss: 4.5979e-04 - loss: 6.3598e-04 - val_dense_104_accuracy: 0.9398 - val_dense_104_loss: 0.1034 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0048 - val_loss: 0.1122 - learning_rate: 1.0000e-05\nEpoch 11/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 1.2457e-04 - dense_105_accuracy: 0.9993 - dense_105_loss: 0.0017 - loss: 0.0018\nEpoch 11: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 160ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 1.2453e-04 - dense_105_accuracy: 0.9993 - dense_105_loss: 0.0016 - loss: 0.0018 - val_dense_104_accuracy: 0.9398 - val_dense_104_loss: 0.1029 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0051 - val_loss: 0.1119 - learning_rate: 1.0000e-05\nEpoch 12/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 1.2875e-04 - dense_105_accuracy: 0.9995 - dense_105_loss: 9.0640e-04 - loss: 0.0010\nEpoch 12: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 160ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 1.2870e-04 - dense_105_accuracy: 0.9995 - dense_105_loss: 9.0751e-04 - loss: 0.0010 - val_dense_104_accuracy: 0.9383 - val_dense_104_loss: 0.1017 - val_dense_105_accuracy: 0.9938 - val_dense_105_loss: 0.0060 - val_loss: 0.1116 - learning_rate: 1.0000e-05\nEpoch 13/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 5.2092e-05 - dense_105_accuracy: 0.9997 - dense_105_loss: 5.3115e-04 - loss: 5.8324e-04\nEpoch 13: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 159ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 5.2246e-05 - dense_105_accuracy: 0.9997 - dense_105_loss: 5.3136e-04 - loss: 5.8361e-04 - val_dense_104_accuracy: 0.9367 - val_dense_104_loss: 0.1044 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0055 - val_loss: 0.1139 - learning_rate: 1.0000e-05\nEpoch 14/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 2.5297e-05 - dense_105_accuracy: 0.9997 - dense_105_loss: 4.2912e-04 - loss: 4.5442e-04\nEpoch 14: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 158ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 2.5345e-05 - dense_105_accuracy: 0.9997 - dense_105_loss: 4.2955e-04 - loss: 4.5490e-04 - val_dense_104_accuracy: 0.9383 - val_dense_104_loss: 0.1048 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0052 - val_loss: 0.1141 - learning_rate: 1.0000e-05\nEpoch 15/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 9.7722e-05 - dense_105_accuracy: 0.9992 - dense_105_loss: 8.3496e-04 - loss: 9.3268e-04\nEpoch 15: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 158ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 9.8085e-05 - dense_105_accuracy: 0.9992 - dense_105_loss: 8.3378e-04 - loss: 9.3188e-04 - val_dense_104_accuracy: 0.9383 - val_dense_104_loss: 0.1050 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0044 - val_loss: 0.1134 - learning_rate: 1.0000e-05\nEpoch 16/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 1.6216e-04 - dense_105_accuracy: 1.0000 - dense_105_loss: 2.1031e-04 - loss: 3.7247e-04\nEpoch 16: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 160ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 1.6194e-04 - dense_105_accuracy: 1.0000 - dense_105_loss: 2.1031e-04 - loss: 3.7225e-04 - val_dense_104_accuracy: 0.9398 - val_dense_104_loss: 0.1047 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0052 - val_loss: 0.1138 - learning_rate: 1.0000e-05\nEpoch 17/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 1.7405e-04 - dense_105_accuracy: 0.9998 - dense_105_loss: 3.3270e-04 - loss: 5.0647e-04\nEpoch 17: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 159ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 1.7385e-04 - dense_105_accuracy: 0.9998 - dense_105_loss: 3.3336e-04 - loss: 5.0663e-04 - val_dense_104_accuracy: 0.9414 - val_dense_104_loss: 0.1035 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0049 - val_loss: 0.1124 - learning_rate: 1.0000e-05\nEpoch 18/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 4.9294e-05 - dense_105_accuracy: 0.9997 - dense_105_loss: 3.6512e-04 - loss: 4.1438e-04\nEpoch 18: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 159ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 4.9329e-05 - dense_105_accuracy: 0.9997 - dense_105_loss: 3.6525e-04 - loss: 4.1452e-04 - val_dense_104_accuracy: 0.9398 - val_dense_104_loss: 0.1050 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0054 - val_loss: 0.1144 - learning_rate: 1.0000e-05\nEpoch 19/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - dense_104_accuracy: 0.9999 - dense_104_loss: 7.8707e-05 - dense_105_accuracy: 0.9994 - dense_105_loss: 8.5282e-04 - loss: 9.3153e-04\nEpoch 19: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 159ms/step - dense_104_accuracy: 0.9999 - dense_104_loss: 7.9023e-05 - dense_105_accuracy: 0.9994 - dense_105_loss: 8.5316e-04 - loss: 9.3219e-04 - val_dense_104_accuracy: 0.9398 - val_dense_104_loss: 0.1054 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0053 - val_loss: 0.1147 - learning_rate: 1.0000e-05\nEpoch 20/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - dense_104_accuracy: 0.9999 - dense_104_loss: 2.2074e-04 - dense_105_accuracy: 1.0000 - dense_105_loss: 1.9328e-04 - loss: 4.1402e-04\nEpoch 20: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 159ms/step - dense_104_accuracy: 0.9999 - dense_104_loss: 2.2090e-04 - dense_105_accuracy: 1.0000 - dense_105_loss: 1.9356e-04 - loss: 4.1447e-04 - val_dense_104_accuracy: 0.9398 - val_dense_104_loss: 0.1052 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0052 - val_loss: 0.1144 - learning_rate: 1.0000e-05\nEpoch 21/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 8.8492e-05 - dense_105_accuracy: 0.9998 - dense_105_loss: 4.4145e-04 - loss: 5.2995e-04\nEpoch 21: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 161ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 8.8633e-05 - dense_105_accuracy: 0.9998 - dense_105_loss: 4.4267e-04 - loss: 5.3132e-04 - val_dense_104_accuracy: 0.9383 - val_dense_104_loss: 0.1042 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0051 - val_loss: 0.1133 - learning_rate: 1.0000e-05\nEpoch 22/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 1.0016e-04 - dense_105_accuracy: 0.9994 - dense_105_loss: 6.6364e-04 - loss: 7.6380e-04\nEpoch 22: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 158ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 1.0008e-04 - dense_105_accuracy: 0.9994 - dense_105_loss: 6.6406e-04 - loss: 7.6414e-04 - val_dense_104_accuracy: 0.9383 - val_dense_104_loss: 0.1061 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0053 - val_loss: 0.1154 - learning_rate: 1.0000e-05\nEpoch 23/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - dense_104_accuracy: 0.9999 - dense_104_loss: 1.3387e-04 - dense_105_accuracy: 0.9996 - dense_105_loss: 5.4148e-04 - loss: 6.7535e-04\nEpoch 23: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 158ms/step - dense_104_accuracy: 0.9999 - dense_104_loss: 1.3404e-04 - dense_105_accuracy: 0.9996 - dense_105_loss: 5.4142e-04 - loss: 6.7547e-04 - val_dense_104_accuracy: 0.9398 - val_dense_104_loss: 0.1032 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0051 - val_loss: 0.1122 - learning_rate: 1.0000e-05\nEpoch 24/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - dense_104_accuracy: 0.9998 - dense_104_loss: 2.2975e-04 - dense_105_accuracy: 0.9999 - dense_105_loss: 3.3263e-04 - loss: 5.6238e-04\nEpoch 24: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 159ms/step - dense_104_accuracy: 0.9998 - dense_104_loss: 2.2974e-04 - dense_105_accuracy: 0.9999 - dense_105_loss: 3.3302e-04 - loss: 5.6278e-04 - val_dense_104_accuracy: 0.9352 - val_dense_104_loss: 0.1058 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0052 - val_loss: 0.1151 - learning_rate: 1.0000e-05\nEpoch 25/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - dense_104_accuracy: 0.9999 - dense_104_loss: 1.3435e-04 - dense_105_accuracy: 1.0000 - dense_105_loss: 2.5397e-04 - loss: 3.8832e-04\nEpoch 25: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 159ms/step - dense_104_accuracy: 0.9999 - dense_104_loss: 1.3426e-04 - dense_105_accuracy: 1.0000 - dense_105_loss: 2.5455e-04 - loss: 3.8881e-04 - val_dense_104_accuracy: 0.9321 - val_dense_104_loss: 0.1084 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0051 - val_loss: 0.1176 - learning_rate: 1.0000e-05\nEpoch 26/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - dense_104_accuracy: 0.9998 - dense_104_loss: 5.5881e-04 - dense_105_accuracy: 0.9998 - dense_105_loss: 3.6577e-04 - loss: 9.2458e-04\nEpoch 26: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 159ms/step - dense_104_accuracy: 0.9998 - dense_104_loss: 5.5828e-04 - dense_105_accuracy: 0.9998 - dense_105_loss: 3.6589e-04 - loss: 9.2418e-04 - val_dense_104_accuracy: 0.9336 - val_dense_104_loss: 0.1065 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0049 - val_loss: 0.1155 - learning_rate: 1.0000e-05\nEpoch 27/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 9.2899e-05 - dense_105_accuracy: 0.9997 - dense_105_loss: 3.7935e-04 - loss: 4.7225e-04\nEpoch 27: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 160ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 9.2906e-05 - dense_105_accuracy: 0.9997 - dense_105_loss: 3.7915e-04 - loss: 4.7205e-04 - val_dense_104_accuracy: 0.9352 - val_dense_104_loss: 0.1081 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0048 - val_loss: 0.1170 - learning_rate: 1.0000e-05\nEpoch 28/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 3.0437e-05 - dense_105_accuracy: 0.9997 - dense_105_loss: 4.2723e-04 - loss: 4.5763e-04\nEpoch 28: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 158ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 3.0551e-05 - dense_105_accuracy: 0.9997 - dense_105_loss: 4.2702e-04 - loss: 4.5749e-04 - val_dense_104_accuracy: 0.9336 - val_dense_104_loss: 0.1063 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0048 - val_loss: 0.1151 - learning_rate: 1.0000e-05\nEpoch 29/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 7.7425e-05 - dense_105_accuracy: 1.0000 - dense_105_loss: 1.9069e-04 - loss: 2.6811e-04\nEpoch 29: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 157ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 7.7464e-05 - dense_105_accuracy: 1.0000 - dense_105_loss: 1.9077e-04 - loss: 2.6823e-04 - val_dense_104_accuracy: 0.9352 - val_dense_104_loss: 0.1079 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0048 - val_loss: 0.1167 - learning_rate: 1.0000e-05\nEpoch 30/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - dense_104_accuracy: 0.9999 - dense_104_loss: 1.5808e-04 - dense_105_accuracy: 0.9995 - dense_105_loss: 7.0833e-04 - loss: 8.6459e-04\nEpoch 30: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 159ms/step - dense_104_accuracy: 0.9999 - dense_104_loss: 1.6105e-04 - dense_105_accuracy: 0.9995 - dense_105_loss: 7.0831e-04 - loss: 8.6573e-04 - val_dense_104_accuracy: 0.9336 - val_dense_104_loss: 0.1094 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0053 - val_loss: 0.1188 - learning_rate: 1.0000e-05\nEpoch 31/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 6.1430e-05 - dense_105_accuracy: 0.9995 - dense_105_loss: 5.0292e-04 - loss: 5.6436e-04\nEpoch 31: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 159ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 6.1729e-05 - dense_105_accuracy: 0.9995 - dense_105_loss: 5.0280e-04 - loss: 5.6454e-04 - val_dense_104_accuracy: 0.9321 - val_dense_104_loss: 0.1113 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0048 - val_loss: 0.1204 - learning_rate: 1.0000e-05\nEpoch 32/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - dense_104_accuracy: 0.9992 - dense_104_loss: 7.0344e-04 - dense_105_accuracy: 0.9999 - dense_105_loss: 4.0576e-04 - loss: 0.0011\nEpoch 32: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 158ms/step - dense_104_accuracy: 0.9992 - dense_104_loss: 7.0147e-04 - dense_105_accuracy: 0.9999 - dense_105_loss: 4.0548e-04 - loss: 0.0011 - val_dense_104_accuracy: 0.9336 - val_dense_104_loss: 0.1102 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0050 - val_loss: 0.1194 - learning_rate: 1.0000e-05\nEpoch 33/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 6.7303e-05 - dense_105_accuracy: 0.9998 - dense_105_loss: 5.1901e-04 - loss: 5.8630e-04\nEpoch 33: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 160ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 6.7339e-05 - dense_105_accuracy: 0.9998 - dense_105_loss: 5.1981e-04 - loss: 5.8712e-04 - val_dense_104_accuracy: 0.9321 - val_dense_104_loss: 0.1092 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0049 - val_loss: 0.1182 - learning_rate: 1.0000e-05\nEpoch 34/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 4.1648e-05 - dense_105_accuracy: 0.9998 - dense_105_loss: 5.5940e-04 - loss: 6.0105e-04\nEpoch 34: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 158ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 4.1732e-05 - dense_105_accuracy: 0.9998 - dense_105_loss: 5.5941e-04 - loss: 6.0115e-04 - val_dense_104_accuracy: 0.9336 - val_dense_104_loss: 0.1101 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0048 - val_loss: 0.1191 - learning_rate: 1.0000e-05\nEpoch 35/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 7.9112e-05 - dense_105_accuracy: 0.9997 - dense_105_loss: 4.6382e-04 - loss: 5.4294e-04\nEpoch 35: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 158ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 7.9830e-05 - dense_105_accuracy: 0.9997 - dense_105_loss: 4.6349e-04 - loss: 5.4333e-04 - val_dense_104_accuracy: 0.9336 - val_dense_104_loss: 0.1082 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0053 - val_loss: 0.1176 - learning_rate: 1.0000e-05\nEpoch 36/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 5.9553e-05 - dense_105_accuracy: 0.9998 - dense_105_loss: 3.4565e-04 - loss: 4.0245e-04\nEpoch 36: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 158ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 6.3996e-05 - dense_105_accuracy: 0.9998 - dense_105_loss: 3.4573e-04 - loss: 4.0423e-04 - val_dense_104_accuracy: 0.9352 - val_dense_104_loss: 0.1081 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0053 - val_loss: 0.1176 - learning_rate: 1.0000e-05\nEpoch 37/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 1.1106e-04 - dense_105_accuracy: 0.9994 - dense_105_loss: 0.0026 - loss: 0.0027\nEpoch 37: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 159ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 1.1158e-04 - dense_105_accuracy: 0.9994 - dense_105_loss: 0.0025 - loss: 0.0027 - val_dense_104_accuracy: 0.9352 - val_dense_104_loss: 0.1085 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0050 - val_loss: 0.1177 - learning_rate: 1.0000e-05\nEpoch 38/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - dense_104_accuracy: 0.9998 - dense_104_loss: 3.1400e-04 - dense_105_accuracy: 0.9990 - dense_105_loss: 0.0011 - loss: 0.0014\nEpoch 38: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 158ms/step - dense_104_accuracy: 0.9998 - dense_104_loss: 3.1360e-04 - dense_105_accuracy: 0.9990 - dense_105_loss: 0.0011 - loss: 0.0014 - val_dense_104_accuracy: 0.9336 - val_dense_104_loss: 0.1092 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0049 - val_loss: 0.1182 - learning_rate: 1.0000e-05\nEpoch 39/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 1.0601e-04 - dense_105_accuracy: 0.9989 - dense_105_loss: 9.5888e-04 - loss: 0.0011\nEpoch 39: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 160ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 1.0605e-04 - dense_105_accuracy: 0.9989 - dense_105_loss: 9.5841e-04 - loss: 0.0011 - val_dense_104_accuracy: 0.9336 - val_dense_104_loss: 0.1094 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0045 - val_loss: 0.1180 - learning_rate: 1.0000e-05\nEpoch 40/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - dense_104_accuracy: 0.9998 - dense_104_loss: 2.6569e-04 - dense_105_accuracy: 0.9998 - dense_105_loss: 8.9011e-04 - loss: 0.0012\nEpoch 40: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 158ms/step - dense_104_accuracy: 0.9998 - dense_104_loss: 2.6543e-04 - dense_105_accuracy: 0.9998 - dense_105_loss: 8.8915e-04 - loss: 0.0012 - val_dense_104_accuracy: 0.9352 - val_dense_104_loss: 0.1086 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0048 - val_loss: 0.1175 - learning_rate: 1.0000e-05\nEpoch 41/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - dense_104_accuracy: 0.9997 - dense_104_loss: 4.7005e-04 - dense_105_accuracy: 0.9995 - dense_105_loss: 4.7689e-04 - loss: 9.4695e-04\nEpoch 41: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 158ms/step - dense_104_accuracy: 0.9997 - dense_104_loss: 4.6963e-04 - dense_105_accuracy: 0.9995 - dense_105_loss: 4.7704e-04 - loss: 9.4668e-04 - val_dense_104_accuracy: 0.9352 - val_dense_104_loss: 0.1084 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0049 - val_loss: 0.1175 - learning_rate: 1.0000e-05\nEpoch 42/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 4.3808e-05 - dense_105_accuracy: 1.0000 - dense_105_loss: 2.0700e-04 - loss: 2.5081e-04\nEpoch 42: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 160ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 4.3829e-05 - dense_105_accuracy: 1.0000 - dense_105_loss: 2.0719e-04 - loss: 2.5103e-04 - val_dense_104_accuracy: 0.9336 - val_dense_104_loss: 0.1095 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0048 - val_loss: 0.1185 - learning_rate: 1.0000e-05\nEpoch 43/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 5.0897e-05 - dense_105_accuracy: 0.9999 - dense_105_loss: 3.1788e-04 - loss: 3.6878e-04\nEpoch 43: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 158ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 5.0904e-05 - dense_105_accuracy: 0.9999 - dense_105_loss: 3.1929e-04 - loss: 3.7021e-04 - val_dense_104_accuracy: 0.9383 - val_dense_104_loss: 0.1051 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0047 - val_loss: 0.1138 - learning_rate: 1.0000e-05\nEpoch 44/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 1.3215e-04 - dense_105_accuracy: 0.9996 - dense_105_loss: 5.9132e-04 - loss: 7.2348e-04\nEpoch 44: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 158ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 1.3250e-04 - dense_105_accuracy: 0.9996 - dense_105_loss: 5.9120e-04 - loss: 7.2372e-04 - val_dense_104_accuracy: 0.9336 - val_dense_104_loss: 0.1091 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0049 - val_loss: 0.1181 - learning_rate: 1.0000e-05\nEpoch 45/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - dense_104_accuracy: 0.9999 - dense_104_loss: 3.8594e-04 - dense_105_accuracy: 0.9994 - dense_105_loss: 6.7591e-04 - loss: 0.0011\nEpoch 45: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 160ms/step - dense_104_accuracy: 0.9999 - dense_104_loss: 3.8647e-04 - dense_105_accuracy: 0.9994 - dense_105_loss: 6.7620e-04 - loss: 0.0011 - val_dense_104_accuracy: 0.9367 - val_dense_104_loss: 0.1091 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0048 - val_loss: 0.1181 - learning_rate: 1.0000e-05\nEpoch 46/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 9.1563e-05 - dense_105_accuracy: 0.9997 - dense_105_loss: 5.0947e-04 - loss: 6.0070e-04\nEpoch 46: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 158ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 9.1966e-05 - dense_105_accuracy: 0.9997 - dense_105_loss: 5.0929e-04 - loss: 6.0060e-04 - val_dense_104_accuracy: 0.9352 - val_dense_104_loss: 0.1078 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0048 - val_loss: 0.1167 - learning_rate: 1.0000e-05\nEpoch 47/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 4.1185e-05 - dense_105_accuracy: 0.9999 - dense_105_loss: 1.4481e-04 - loss: 1.8600e-04\nEpoch 47: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 158ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 4.1240e-05 - dense_105_accuracy: 0.9999 - dense_105_loss: 1.4519e-04 - loss: 1.8643e-04 - val_dense_104_accuracy: 0.9336 - val_dense_104_loss: 0.1093 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0051 - val_loss: 0.1186 - learning_rate: 1.0000e-05\nEpoch 48/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 2.9720e-05 - dense_105_accuracy: 0.9998 - dense_105_loss: 6.3616e-04 - loss: 6.6589e-04\nEpoch 48: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 158ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 2.9724e-05 - dense_105_accuracy: 0.9998 - dense_105_loss: 6.3584e-04 - loss: 6.6558e-04 - val_dense_104_accuracy: 0.9336 - val_dense_104_loss: 0.1102 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0049 - val_loss: 0.1192 - learning_rate: 1.0000e-05\nEpoch 49/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 2.7742e-05 - dense_105_accuracy: 0.9997 - dense_105_loss: 3.8517e-04 - loss: 4.1292e-04\nEpoch 49: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 159ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 2.7725e-05 - dense_105_accuracy: 0.9997 - dense_105_loss: 3.8535e-04 - loss: 4.1309e-04 - val_dense_104_accuracy: 0.9383 - val_dense_104_loss: 0.1089 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0048 - val_loss: 0.1179 - learning_rate: 1.0000e-05\nEpoch 50/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - dense_104_accuracy: 0.9999 - dense_104_loss: 9.4014e-05 - dense_105_accuracy: 0.9998 - dense_105_loss: 4.2712e-04 - loss: 5.2102e-04\nEpoch 50: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 158ms/step - dense_104_accuracy: 0.9999 - dense_104_loss: 9.4122e-05 - dense_105_accuracy: 0.9998 - dense_105_loss: 4.2730e-04 - loss: 5.2120e-04 - val_dense_104_accuracy: 0.9336 - val_dense_104_loss: 0.1096 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0050 - val_loss: 0.1188 - learning_rate: 1.0000e-05\nEpoch 51/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - dense_104_accuracy: 0.9994 - dense_104_loss: 0.0017 - dense_105_accuracy: 1.0000 - dense_105_loss: 1.8582e-04 - loss: 0.0019\nEpoch 51: val_loss did not improve from 0.10964\n\nEpoch 51: ReduceLROnPlateau reducing learning rate to 1e-05.\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 160ms/step - dense_104_accuracy: 0.9994 - dense_104_loss: 0.0017 - dense_105_accuracy: 1.0000 - dense_105_loss: 1.8644e-04 - loss: 0.0019 - val_dense_104_accuracy: 0.9321 - val_dense_104_loss: 0.1085 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0048 - val_loss: 0.1175 - learning_rate: 1.0000e-05\nEpoch 52/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - dense_104_accuracy: 0.9997 - dense_104_loss: 6.4034e-04 - dense_105_accuracy: 1.0000 - dense_105_loss: 1.0586e-04 - loss: 7.4601e-04\nEpoch 52: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 158ms/step - dense_104_accuracy: 0.9997 - dense_104_loss: 6.3889e-04 - dense_105_accuracy: 1.0000 - dense_105_loss: 1.0622e-04 - loss: 7.4474e-04 - val_dense_104_accuracy: 0.9336 - val_dense_104_loss: 0.1092 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0050 - val_loss: 0.1183 - learning_rate: 1.0000e-05\nEpoch 53/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 3.0252e-05 - dense_105_accuracy: 1.0000 - dense_105_loss: 1.8093e-04 - loss: 2.1119e-04\nEpoch 53: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 158ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 3.0251e-05 - dense_105_accuracy: 1.0000 - dense_105_loss: 1.8091e-04 - loss: 2.1117e-04 - val_dense_104_accuracy: 0.9321 - val_dense_104_loss: 0.1082 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0049 - val_loss: 0.1172 - learning_rate: 1.0000e-05\nEpoch 54/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 4.2784e-05 - dense_105_accuracy: 0.9998 - dense_105_loss: 2.9782e-04 - loss: 3.4060e-04\nEpoch 54: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 159ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 4.2822e-05 - dense_105_accuracy: 0.9998 - dense_105_loss: 2.9833e-04 - loss: 3.4116e-04 - val_dense_104_accuracy: 0.9367 - val_dense_104_loss: 0.1066 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0050 - val_loss: 0.1157 - learning_rate: 1.0000e-05\nEpoch 55/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 4.7637e-05 - dense_105_accuracy: 0.9995 - dense_105_loss: 5.1861e-04 - loss: 5.6625e-04\nEpoch 55: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 159ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 4.7666e-05 - dense_105_accuracy: 0.9995 - dense_105_loss: 5.1894e-04 - loss: 5.6660e-04 - val_dense_104_accuracy: 0.9336 - val_dense_104_loss: 0.1070 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0049 - val_loss: 0.1160 - learning_rate: 1.0000e-05\nEpoch 56/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - dense_104_accuracy: 0.9997 - dense_104_loss: 2.5416e-04 - dense_105_accuracy: 0.9996 - dense_105_loss: 6.1842e-04 - loss: 8.7259e-04\nEpoch 56: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 159ms/step - dense_104_accuracy: 0.9997 - dense_104_loss: 2.5433e-04 - dense_105_accuracy: 0.9996 - dense_105_loss: 6.1778e-04 - loss: 8.7212e-04 - val_dense_104_accuracy: 0.9336 - val_dense_104_loss: 0.1077 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0048 - val_loss: 0.1165 - learning_rate: 1.0000e-05\nEpoch 57/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 6.9883e-05 - dense_105_accuracy: 0.9997 - dense_105_loss: 2.9878e-04 - loss: 3.6867e-04\nEpoch 57: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 159ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 6.9828e-05 - dense_105_accuracy: 0.9997 - dense_105_loss: 2.9901e-04 - loss: 3.6883e-04 - val_dense_104_accuracy: 0.9336 - val_dense_104_loss: 0.1074 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0048 - val_loss: 0.1162 - learning_rate: 1.0000e-05\nEpoch 58/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 3.6386e-05 - dense_105_accuracy: 0.9997 - dense_105_loss: 5.2205e-04 - loss: 5.5845e-04\nEpoch 58: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 159ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 3.6434e-05 - dense_105_accuracy: 0.9997 - dense_105_loss: 5.2305e-04 - loss: 5.5950e-04 - val_dense_104_accuracy: 0.9383 - val_dense_104_loss: 0.1067 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0046 - val_loss: 0.1154 - learning_rate: 1.0000e-05\nEpoch 59/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - dense_104_accuracy: 0.9999 - dense_104_loss: 1.1463e-04 - dense_105_accuracy: 1.0000 - dense_105_loss: 3.0808e-04 - loss: 4.2249e-04\nEpoch 59: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 158ms/step - dense_104_accuracy: 0.9999 - dense_104_loss: 1.1515e-04 - dense_105_accuracy: 1.0000 - dense_105_loss: 3.0797e-04 - loss: 4.2267e-04 - val_dense_104_accuracy: 0.9352 - val_dense_104_loss: 0.1070 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0048 - val_loss: 0.1158 - learning_rate: 1.0000e-05\nEpoch 60/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 2.9010e-05 - dense_105_accuracy: 0.9996 - dense_105_loss: 5.5472e-04 - loss: 5.6331e-04\nEpoch 60: val_loss did not improve from 0.10964\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 159ms/step - dense_104_accuracy: 1.0000 - dense_104_loss: 2.9108e-05 - dense_105_accuracy: 0.9996 - dense_105_loss: 5.8758e-04 - loss: 5.7601e-04 - val_dense_104_accuracy: 0.9352 - val_dense_104_loss: 0.1073 - val_dense_105_accuracy: 0.9954 - val_dense_105_loss: 0.0049 - val_loss: 0.1163 - learning_rate: 1.0000e-05\nEpoch 60: early stopping\nRestoring model weights from the end of the best epoch: 1.\n","output_type":"stream"}],"execution_count":75},{"cell_type":"code","source":"model.evaluate([X_test_s, X_test_h1], [y_test_s, y_test_h1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T23:03:57.509994Z","iopub.execute_input":"2025-02-05T23:03:57.510375Z","iopub.status.idle":"2025-02-05T23:03:58.972436Z","shell.execute_reply.started":"2025-02-05T23:03:57.510331Z","shell.execute_reply":"2025-02-05T23:03:58.971549Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - dense_104_accuracy: 0.9607 - dense_104_loss: 0.1170 - dense_105_accuracy: 0.9985 - dense_105_loss: 0.0029 - loss: 0.1201\n","output_type":"stream"},{"execution_count":76,"output_type":"execute_result","data":{"text/plain":"[0.11490587145090103,\n 0.10549081861972809,\n 0.006610536947846413,\n 0.9592592716217041,\n 0.9962962865829468]"},"metadata":{}}],"execution_count":76},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.evaluate([X_test_s, X_test_h1], [y_test_s, y_test_h1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T17:47:15.006667Z","iopub.execute_input":"2025-02-05T17:47:15.006999Z","iopub.status.idle":"2025-02-05T17:47:28.421195Z","shell.execute_reply.started":"2025-02-05T17:47:15.006976Z","shell.execute_reply":"2025-02-05T17:47:28.420518Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 511ms/step - dense_38_accuracy: 0.9435 - dense_38_loss: 0.1542 - dense_39_accuracy: 0.9685 - dense_39_loss: 0.0282 - loss: 0.1826\n","output_type":"stream"},{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"[0.19918939471244812,\n 0.17125782370567322,\n 0.02569425478577614,\n 0.9395061731338501,\n 0.9728395342826843]"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"history = model.fit([X_train_s, X_train_h], [y_train_s, y_train_h],\n                    epochs=200,\n                    validation_data=([X_val_s, X_val_h1], [y_val_s, y_val_h1]), verbose=1,\n                    shuffle=True,\n                    callbacks=callbacks) # UpdateGammaCallback","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T17:47:55.541893Z","iopub.execute_input":"2025-02-05T17:47:55.542260Z","iopub.status.idle":"2025-02-05T18:01:38.845595Z","shell.execute_reply.started":"2025-02-05T17:47:55.542208Z","shell.execute_reply":"2025-02-05T18:01:38.844558Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - dense_38_accuracy: 1.0000 - dense_38_loss: 2.3909e-04 - dense_39_accuracy: 0.9995 - dense_39_loss: 8.7616e-04 - loss: 0.0011\nEpoch 1: val_loss did not improve from 0.18486\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 110ms/step - dense_38_accuracy: 1.0000 - dense_38_loss: 2.4674e-04 - dense_39_accuracy: 0.9995 - dense_39_loss: 8.7750e-04 - loss: 0.0011 - val_dense_38_accuracy: 0.9336 - val_dense_38_loss: 0.1523 - val_dense_39_accuracy: 0.9784 - val_dense_39_loss: 0.0415 - val_loss: 0.1912 - learning_rate: 1.0000e-05\nEpoch 2/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - dense_38_accuracy: 0.9999 - dense_38_loss: 2.5351e-04 - dense_39_accuracy: 0.9989 - dense_39_loss: 0.0015 - loss: 0.0017\nEpoch 2: val_loss did not improve from 0.18486\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 109ms/step - dense_38_accuracy: 0.9999 - dense_38_loss: 2.5335e-04 - dense_39_accuracy: 0.9989 - dense_39_loss: 0.0015 - loss: 0.0017 - val_dense_38_accuracy: 0.9306 - val_dense_38_loss: 0.1603 - val_dense_39_accuracy: 0.9769 - val_dense_39_loss: 0.0447 - val_loss: 0.2024 - learning_rate: 1.0000e-05\nEpoch 3/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - dense_38_accuracy: 0.9999 - dense_38_loss: 3.3232e-04 - dense_39_accuracy: 0.9991 - dense_39_loss: 0.0016 - loss: 0.0019\nEpoch 3: val_loss did not improve from 0.18486\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 109ms/step - dense_38_accuracy: 0.9999 - dense_38_loss: 3.3213e-04 - dense_39_accuracy: 0.9991 - dense_39_loss: 0.0016 - loss: 0.0019 - val_dense_38_accuracy: 0.9306 - val_dense_38_loss: 0.1583 - val_dense_39_accuracy: 0.9753 - val_dense_39_loss: 0.0429 - val_loss: 0.1988 - learning_rate: 1.0000e-05\nEpoch 4/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - dense_38_accuracy: 1.0000 - dense_38_loss: 1.5193e-04 - dense_39_accuracy: 0.9992 - dense_39_loss: 9.9367e-04 - loss: 0.0011\nEpoch 4: val_loss did not improve from 0.18486\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 109ms/step - dense_38_accuracy: 1.0000 - dense_38_loss: 1.5194e-04 - dense_39_accuracy: 0.9992 - dense_39_loss: 9.9502e-04 - loss: 0.0011 - val_dense_38_accuracy: 0.9306 - val_dense_38_loss: 0.1590 - val_dense_39_accuracy: 0.9753 - val_dense_39_loss: 0.0424 - val_loss: 0.1992 - learning_rate: 1.0000e-05\nEpoch 5/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - dense_38_accuracy: 0.9998 - dense_38_loss: 1.5178e-04 - dense_39_accuracy: 0.9979 - dense_39_loss: 0.0020 - loss: 0.0021\nEpoch 5: val_loss did not improve from 0.18486\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 109ms/step - dense_38_accuracy: 0.9998 - dense_38_loss: 1.5166e-04 - dense_39_accuracy: 0.9979 - dense_39_loss: 0.0019 - loss: 0.0021 - val_dense_38_accuracy: 0.9290 - val_dense_38_loss: 0.1560 - val_dense_39_accuracy: 0.9753 - val_dense_39_loss: 0.0432 - val_loss: 0.1969 - learning_rate: 1.0000e-05\nEpoch 6/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - dense_38_accuracy: 0.9996 - dense_38_loss: 3.5866e-04 - dense_39_accuracy: 0.9992 - dense_39_loss: 0.0013 - loss: 0.0017\nEpoch 6: val_loss did not improve from 0.18486\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 108ms/step - dense_38_accuracy: 0.9996 - dense_38_loss: 3.5841e-04 - dense_39_accuracy: 0.9992 - dense_39_loss: 0.0013 - loss: 0.0017 - val_dense_38_accuracy: 0.9321 - val_dense_38_loss: 0.1573 - val_dense_39_accuracy: 0.9753 - val_dense_39_loss: 0.0429 - val_loss: 0.1978 - learning_rate: 1.0000e-05\nEpoch 7/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - dense_38_accuracy: 0.9998 - dense_38_loss: 3.6023e-04 - dense_39_accuracy: 0.9996 - dense_39_loss: 8.7846e-04 - loss: 0.0012\nEpoch 7: val_loss did not improve from 0.18486\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 109ms/step - dense_38_accuracy: 0.9998 - dense_38_loss: 3.6006e-04 - dense_39_accuracy: 0.9996 - dense_39_loss: 8.7962e-04 - loss: 0.0012 - val_dense_38_accuracy: 0.9321 - val_dense_38_loss: 0.1570 - val_dense_39_accuracy: 0.9753 - val_dense_39_loss: 0.0441 - val_loss: 0.1988 - learning_rate: 1.0000e-05\nEpoch 8/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - dense_38_accuracy: 1.0000 - dense_38_loss: 1.5923e-04 - dense_39_accuracy: 0.9988 - dense_39_loss: 0.0017 - loss: 0.0018\nEpoch 8: val_loss did not improve from 0.18486\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 109ms/step - dense_38_accuracy: 1.0000 - dense_38_loss: 1.5927e-04 - dense_39_accuracy: 0.9988 - dense_39_loss: 0.0017 - loss: 0.0018 - val_dense_38_accuracy: 0.9321 - val_dense_38_loss: 0.1593 - val_dense_39_accuracy: 0.9738 - val_dense_39_loss: 0.0416 - val_loss: 0.1987 - learning_rate: 1.0000e-05\nEpoch 9/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - dense_38_accuracy: 0.9999 - dense_38_loss: 9.2777e-05 - dense_39_accuracy: 0.9991 - dense_39_loss: 0.0021 - loss: 0.0022\nEpoch 9: val_loss did not improve from 0.18486\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 108ms/step - dense_38_accuracy: 0.9999 - dense_38_loss: 9.2817e-05 - dense_39_accuracy: 0.9991 - dense_39_loss: 0.0021 - loss: 0.0022 - val_dense_38_accuracy: 0.9321 - val_dense_38_loss: 0.1575 - val_dense_39_accuracy: 0.9769 - val_dense_39_loss: 0.0422 - val_loss: 0.1980 - learning_rate: 1.0000e-05\nEpoch 10/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - dense_38_accuracy: 1.0000 - dense_38_loss: 9.3116e-05 - dense_39_accuracy: 0.9997 - dense_39_loss: 0.0012 - loss: 0.0013\nEpoch 10: val_loss did not improve from 0.18486\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 109ms/step - dense_38_accuracy: 1.0000 - dense_38_loss: 9.3202e-05 - dense_39_accuracy: 0.9997 - dense_39_loss: 0.0012 - loss: 0.0013 - val_dense_38_accuracy: 0.9336 - val_dense_38_loss: 0.1573 - val_dense_39_accuracy: 0.9784 - val_dense_39_loss: 0.0401 - val_loss: 0.1957 - learning_rate: 1.0000e-05\nEpoch 11/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - dense_38_accuracy: 1.0000 - dense_38_loss: 1.1318e-04 - dense_39_accuracy: 0.9997 - dense_39_loss: 4.7420e-04 - loss: 5.8724e-04\nEpoch 11: val_loss did not improve from 0.18486\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 108ms/step - dense_38_accuracy: 1.0000 - dense_38_loss: 1.1300e-04 - dense_39_accuracy: 0.9997 - dense_39_loss: 4.7570e-04 - loss: 5.8843e-04 - val_dense_38_accuracy: 0.9321 - val_dense_38_loss: 0.1562 - val_dense_39_accuracy: 0.9784 - val_dense_39_loss: 0.0386 - val_loss: 0.1932 - learning_rate: 1.0000e-05\nEpoch 12/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - dense_38_accuracy: 0.9998 - dense_38_loss: 2.4803e-04 - dense_39_accuracy: 0.9973 - dense_39_loss: 0.0027 - loss: 0.0029\nEpoch 12: val_loss did not improve from 0.18486\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 108ms/step - dense_38_accuracy: 0.9998 - dense_38_loss: 2.4896e-04 - dense_39_accuracy: 0.9973 - dense_39_loss: 0.0027 - loss: 0.0029 - val_dense_38_accuracy: 0.9352 - val_dense_38_loss: 0.1553 - val_dense_39_accuracy: 0.9784 - val_dense_39_loss: 0.0385 - val_loss: 0.1925 - learning_rate: 1.0000e-05\nEpoch 13/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - dense_38_accuracy: 1.0000 - dense_38_loss: 6.0050e-05 - dense_39_accuracy: 0.9992 - dense_39_loss: 0.0013 - loss: 0.0013\nEpoch 13: val_loss did not improve from 0.18486\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 109ms/step - dense_38_accuracy: 1.0000 - dense_38_loss: 6.0243e-05 - dense_39_accuracy: 0.9992 - dense_39_loss: 0.0013 - loss: 0.0013 - val_dense_38_accuracy: 0.9336 - val_dense_38_loss: 0.1554 - val_dense_39_accuracy: 0.9784 - val_dense_39_loss: 0.0372 - val_loss: 0.1912 - learning_rate: 1.0000e-05\nEpoch 14/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - dense_38_accuracy: 1.0000 - dense_38_loss: 1.3134e-04 - dense_39_accuracy: 0.9998 - dense_39_loss: 6.3181e-04 - loss: 7.6316e-04\nEpoch 14: val_loss did not improve from 0.18486\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 108ms/step - dense_38_accuracy: 1.0000 - dense_38_loss: 1.3135e-04 - dense_39_accuracy: 0.9998 - dense_39_loss: 6.3219e-04 - loss: 7.6356e-04 - val_dense_38_accuracy: 0.9306 - val_dense_38_loss: 0.1575 - val_dense_39_accuracy: 0.9769 - val_dense_39_loss: 0.0365 - val_loss: 0.1932 - learning_rate: 1.0000e-05\nEpoch 15/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - dense_38_accuracy: 1.0000 - dense_38_loss: 9.8827e-05 - dense_39_accuracy: 0.9988 - dense_39_loss: 0.0018 - loss: 0.0019\nEpoch 15: val_loss did not improve from 0.18486\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 109ms/step - dense_38_accuracy: 1.0000 - dense_38_loss: 9.8957e-05 - dense_39_accuracy: 0.9988 - dense_39_loss: 0.0018 - loss: 0.0019 - val_dense_38_accuracy: 0.9306 - val_dense_38_loss: 0.1583 - val_dense_39_accuracy: 0.9769 - val_dense_39_loss: 0.0376 - val_loss: 0.1951 - learning_rate: 1.0000e-05\nEpoch 16/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - dense_38_accuracy: 0.9999 - dense_38_loss: 2.0836e-04 - dense_39_accuracy: 0.9997 - dense_39_loss: 7.4295e-04 - loss: 9.5127e-04\nEpoch 16: val_loss did not improve from 0.18486\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 108ms/step - dense_38_accuracy: 0.9999 - dense_38_loss: 2.0859e-04 - dense_39_accuracy: 0.9997 - dense_39_loss: 7.4420e-04 - loss: 9.5271e-04 - val_dense_38_accuracy: 0.9306 - val_dense_38_loss: 0.1558 - val_dense_39_accuracy: 0.9769 - val_dense_39_loss: 0.0356 - val_loss: 0.1901 - learning_rate: 1.0000e-05\nEpoch 17/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - dense_38_accuracy: 0.9998 - dense_38_loss: 1.6427e-04 - dense_39_accuracy: 0.9995 - dense_39_loss: 9.9979e-04 - loss: 0.0012\nEpoch 17: val_loss did not improve from 0.18486\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 109ms/step - dense_38_accuracy: 0.9998 - dense_38_loss: 1.6466e-04 - dense_39_accuracy: 0.9995 - dense_39_loss: 9.9993e-04 - loss: 0.0012 - val_dense_38_accuracy: 0.9306 - val_dense_38_loss: 0.1576 - val_dense_39_accuracy: 0.9769 - val_dense_39_loss: 0.0369 - val_loss: 0.1932 - learning_rate: 1.0000e-05\nEpoch 18/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - dense_38_accuracy: 0.9998 - dense_38_loss: 2.0892e-04 - dense_39_accuracy: 0.9999 - dense_39_loss: 5.0925e-04 - loss: 7.1813e-04\nEpoch 18: val_loss did not improve from 0.18486\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 108ms/step - dense_38_accuracy: 0.9998 - dense_38_loss: 2.0876e-04 - dense_39_accuracy: 0.9999 - dense_39_loss: 5.1092e-04 - loss: 7.1960e-04 - val_dense_38_accuracy: 0.9290 - val_dense_38_loss: 0.1590 - val_dense_39_accuracy: 0.9769 - val_dense_39_loss: 0.0383 - val_loss: 0.1956 - learning_rate: 1.0000e-05\nEpoch 19/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - dense_38_accuracy: 0.9999 - dense_38_loss: 1.3436e-04 - dense_39_accuracy: 0.9995 - dense_39_loss: 7.5972e-04 - loss: 8.9408e-04\nEpoch 19: val_loss did not improve from 0.18486\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 108ms/step - dense_38_accuracy: 0.9999 - dense_38_loss: 1.3467e-04 - dense_39_accuracy: 0.9995 - dense_39_loss: 7.5981e-04 - loss: 8.9449e-04 - val_dense_38_accuracy: 0.9306 - val_dense_38_loss: 0.1601 - val_dense_39_accuracy: 0.9769 - val_dense_39_loss: 0.0378 - val_loss: 0.1963 - learning_rate: 1.0000e-05\nEpoch 20/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - dense_38_accuracy: 0.9998 - dense_38_loss: 2.3102e-04 - dense_39_accuracy: 0.9995 - dense_39_loss: 7.8175e-04 - loss: 0.0010\nEpoch 20: val_loss did not improve from 0.18486\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 108ms/step - dense_38_accuracy: 0.9998 - dense_38_loss: 2.3111e-04 - dense_39_accuracy: 0.9995 - dense_39_loss: 7.8153e-04 - loss: 0.0010 - val_dense_38_accuracy: 0.9306 - val_dense_38_loss: 0.1605 - val_dense_39_accuracy: 0.9784 - val_dense_39_loss: 0.0378 - val_loss: 0.1969 - learning_rate: 1.0000e-05\nEpoch 21/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - dense_38_accuracy: 0.9995 - dense_38_loss: 9.4593e-04 - dense_39_accuracy: 0.9992 - dense_39_loss: 0.0011 - loss: 0.0020\nEpoch 21: val_loss did not improve from 0.18486\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 109ms/step - dense_38_accuracy: 0.9995 - dense_38_loss: 9.4333e-04 - dense_39_accuracy: 0.9992 - dense_39_loss: 0.0011 - loss: 0.0020 - val_dense_38_accuracy: 0.9306 - val_dense_38_loss: 0.1575 - val_dense_39_accuracy: 0.9784 - val_dense_39_loss: 0.0386 - val_loss: 0.1945 - learning_rate: 1.0000e-05\nEpoch 22/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - dense_38_accuracy: 1.0000 - dense_38_loss: 4.1977e-05 - dense_39_accuracy: 0.9995 - dense_39_loss: 6.6065e-04 - loss: 7.0260e-04\nEpoch 22: val_loss did not improve from 0.18486\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 108ms/step - dense_38_accuracy: 1.0000 - dense_38_loss: 4.2047e-05 - dense_39_accuracy: 0.9995 - dense_39_loss: 6.6103e-04 - loss: 7.0305e-04 - val_dense_38_accuracy: 0.9290 - val_dense_38_loss: 0.1595 - val_dense_39_accuracy: 0.9753 - val_dense_39_loss: 0.0396 - val_loss: 0.1978 - learning_rate: 1.0000e-05\nEpoch 23/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - dense_38_accuracy: 0.9998 - dense_38_loss: 6.6067e-04 - dense_39_accuracy: 0.9999 - dense_39_loss: 3.6784e-04 - loss: 0.0010\nEpoch 23: val_loss did not improve from 0.18486\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 109ms/step - dense_38_accuracy: 0.9998 - dense_38_loss: 6.6017e-04 - dense_39_accuracy: 0.9999 - dense_39_loss: 3.6872e-04 - loss: 0.0010 - val_dense_38_accuracy: 0.9306 - val_dense_38_loss: 0.1627 - val_dense_39_accuracy: 0.9784 - val_dense_39_loss: 0.0382 - val_loss: 0.2001 - learning_rate: 1.0000e-05\nEpoch 24/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - dense_38_accuracy: 0.9995 - dense_38_loss: 7.0639e-04 - dense_39_accuracy: 0.9993 - dense_39_loss: 8.6864e-04 - loss: 0.0016\nEpoch 24: val_loss did not improve from 0.18486\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 108ms/step - dense_38_accuracy: 0.9995 - dense_38_loss: 7.0607e-04 - dense_39_accuracy: 0.9993 - dense_39_loss: 8.6847e-04 - loss: 0.0016 - val_dense_38_accuracy: 0.9275 - val_dense_38_loss: 0.1650 - val_dense_39_accuracy: 0.9769 - val_dense_39_loss: 0.0378 - val_loss: 0.2023 - learning_rate: 1.0000e-05\nEpoch 25/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - dense_38_accuracy: 0.9999 - dense_38_loss: 1.6463e-04 - dense_39_accuracy: 0.9995 - dense_39_loss: 7.0153e-04 - loss: 8.6617e-04\nEpoch 25: val_loss did not improve from 0.18486\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 108ms/step - dense_38_accuracy: 0.9999 - dense_38_loss: 1.6494e-04 - dense_39_accuracy: 0.9995 - dense_39_loss: 7.0167e-04 - loss: 8.6663e-04 - val_dense_38_accuracy: 0.9275 - val_dense_38_loss: 0.1618 - val_dense_39_accuracy: 0.9769 - val_dense_39_loss: 0.0365 - val_loss: 0.1979 - learning_rate: 1.0000e-05\nEpoch 26/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - dense_38_accuracy: 0.9996 - dense_38_loss: 4.5126e-04 - dense_39_accuracy: 0.9999 - dense_39_loss: 3.6970e-04 - loss: 8.2093e-04\nEpoch 26: val_loss did not improve from 0.18486\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 108ms/step - dense_38_accuracy: 0.9996 - dense_38_loss: 4.5164e-04 - dense_39_accuracy: 0.9999 - dense_39_loss: 3.7011e-04 - loss: 8.2168e-04 - val_dense_38_accuracy: 0.9275 - val_dense_38_loss: 0.1628 - val_dense_39_accuracy: 0.9753 - val_dense_39_loss: 0.0374 - val_loss: 0.1995 - learning_rate: 1.0000e-05\nEpoch 27/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - dense_38_accuracy: 0.9999 - dense_38_loss: 1.8880e-04 - dense_39_accuracy: 0.9996 - dense_39_loss: 0.0013 - loss: 0.0015\nEpoch 27: val_loss did not improve from 0.18486\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 109ms/step - dense_38_accuracy: 0.9999 - dense_38_loss: 1.9043e-04 - dense_39_accuracy: 0.9996 - dense_39_loss: 0.0013 - loss: 0.0015 - val_dense_38_accuracy: 0.9259 - val_dense_38_loss: 0.1605 - val_dense_39_accuracy: 0.9769 - val_dense_39_loss: 0.0387 - val_loss: 0.1987 - learning_rate: 1.0000e-05\nEpoch 28/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - dense_38_accuracy: 1.0000 - dense_38_loss: 8.0666e-05 - dense_39_accuracy: 0.9997 - dense_39_loss: 8.5988e-04 - loss: 9.4056e-04\nEpoch 28: val_loss did not improve from 0.18486\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 109ms/step - dense_38_accuracy: 1.0000 - dense_38_loss: 8.0740e-05 - dense_39_accuracy: 0.9997 - dense_39_loss: 8.6007e-04 - loss: 9.4082e-04 - val_dense_38_accuracy: 0.9275 - val_dense_38_loss: 0.1646 - val_dense_39_accuracy: 0.9769 - val_dense_39_loss: 0.0346 - val_loss: 0.1991 - learning_rate: 1.0000e-05\nEpoch 29/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - dense_38_accuracy: 0.9995 - dense_38_loss: 5.9616e-04 - dense_39_accuracy: 0.9997 - dense_39_loss: 7.5533e-04 - loss: 0.0014\nEpoch 29: val_loss did not improve from 0.18486\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 108ms/step - dense_38_accuracy: 0.9995 - dense_38_loss: 5.9539e-04 - dense_39_accuracy: 0.9997 - dense_39_loss: 7.5594e-04 - loss: 0.0014 - val_dense_38_accuracy: 0.9259 - val_dense_38_loss: 0.1645 - val_dense_39_accuracy: 0.9769 - val_dense_39_loss: 0.0357 - val_loss: 0.2000 - learning_rate: 1.0000e-05\nEpoch 30/200\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - dense_38_accuracy: 1.0000 - dense_38_loss: 1.7274e-04 - dense_39_accuracy: 0.9992 - dense_39_loss: 9.1091e-04 - loss: 0.0011\nEpoch 30: val_loss did not improve from 0.18486\n\u001b[1m251/251\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 108ms/step - dense_38_accuracy: 1.0000 - dense_38_loss: 1.7302e-04 - dense_39_accuracy: 0.9992 - dense_39_loss: 9.1199e-04 - loss: 0.0011 - val_dense_38_accuracy: 0.9244 - val_dense_38_loss: 0.1690 - val_dense_39_accuracy: 0.9784 - val_dense_39_loss: 0.0339 - val_loss: 0.2030 - learning_rate: 1.0000e-05\nEpoch 30: early stopping\nRestoring model weights from the end of the best epoch: 1.\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"model.evaluate([X_test_s, X_test_h1], [y_test_s, y_test_h1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T18:01:40.036618Z","iopub.execute_input":"2025-02-05T18:01:40.036895Z","iopub.status.idle":"2025-02-05T18:01:41.197144Z","shell.execute_reply.started":"2025-02-05T18:01:40.036874Z","shell.execute_reply":"2025-02-05T18:01:41.196364Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - dense_38_accuracy: 0.9413 - dense_38_loss: 0.1599 - dense_39_accuracy: 0.9687 - dense_39_loss: 0.0289 - loss: 0.1889\n","output_type":"stream"},{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"[0.20872877538204193,\n 0.18076977133750916,\n 0.02600208856165409,\n 0.9382715821266174,\n 0.9740740656852722]"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#def build_resnet18(input_shape=(128, 128, 3), num_classes=2):\ninput_shape=(128, 128, 3)\ninputs1 = Input(shape=input_shape)\ninputs2 = Input(shape=input_shape)\n\nimport tensorflow.keras.layers as L\n\n#input_data = Input(shape=input_shape, name='input_data')\n# Initial convolutional layer\n\nx1, x2 = residual_GLC_branch1(inputs1, inputs2)\n#print('x:',x.shape)\n\ncon = tf.keras.layers.Concatenate(axis=-1)([x1, x2])\n\ncon = tf.keras.layers.Dropout(0.25)(con, training = True)  ## MCD ####\n\nx = GlobalAveragePooling2D()(con)\nprint('GlobalAveragePooling2D x:',x.shape)\n\noutputs1 = Dense(5, activation='softmax')(x)\noutputs2 = Dense(7, activation='softmax')(x)\n\n# Create the model\nmodel = Model([inputs1, inputs2], [outputs1, outputs2])\n#return model\nprint(model.summary())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\n\ninitial_gamma = 0.5\n\noptimizer = Adam(learning_rate=0.001)\n# Compile the model with the custom optimizer\nmodel.compile(optimizer=optimizer,\n              loss=['categorical_crossentropy', 'categorical_crossentropy'],\n              loss_weights=[initial_gamma, (1 -  initial_gamma)],\n              metrics=['accuracy', 'accuracy'])\n\n\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\ndef checkpoint_callback():\n\n    checkpoint_filepath = 'best1_model_cer_skin_lung.keras'\n\n    model_checkpoint_callback= ModelCheckpoint(filepath=checkpoint_filepath,\n                           save_weights_only=False,\n                           #frequency='epoch',\n                           monitor='val_loss',\n                           save_best_only=True,\n                            mode='min',\n                           verbose=1)\n\n    return model_checkpoint_callback\n\ndef early_stopping(patience):\n    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, verbose=1)\n    return es_callback\n\n\n\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n                              patience=5, min_lr=0.00001)\n\ncheckpoint_callback = checkpoint_callback()\n\nearly_stopping = early_stopping(patience=15)\ncallbacks = [checkpoint_callback, early_stopping, reduce_lr]\n            \n\n# Fit the model with callbacks\nhistory = model.fit([X_train_s, X_train_h], [y_train_s, y_train_h],\n                    epochs=200,\n                    validation_split=0.2, verbose=1,\n                    shuffle=True,\n                    callbacks=callbacks) # UpdateGammaCallback","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.evaluate([X_test_s, X_test_h1], [y_test_s, y_test_h1])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.evaluate([X_test_s, X_test_h1], [y_test_s, y_test_h1])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.evaluate([X_test_s1, X_test_h], [y_test_s1, y_test_h])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred = model.predict([X_test_s1, X_test_h]) \n\ny_pred_binary1 = y_pred[0] >= 0.5\ny_pred_binary_pgd_test1 = np.array(y_pred_binary1, dtype='int32')\n\nprint('y_pred_binary_pgd_test1:', y_pred_binary_pgd_test1.shape)\n\ny_pred_binary2 = y_pred[1] >= 0.5\ny_pred_binary_pgd_test2 = np.array(y_pred_binary2, dtype='int32')\n\nprint('y_pred_binary_pgd_test2:', y_pred_binary_pgd_test2.shape)\n\n#y_test_s, y_test_h\n# Calculate evaluation metrics for the current epsilon\ny_test_categorical1 = y_test_s1\ny_test_categorical2 = y_test_h\n\n## Task 1:\nprint('skin cancer classification:')\naccuracy = accuracy_score(y_pred_binary_pgd_test1, y_test_categorical1) * 100\nprecision = precision_score(y_pred_binary_pgd_test1, y_test_categorical1, average='macro') * 100\nrecall = recall_score(y_pred_binary_pgd_test1, y_test_categorical1, average='macro') * 100\nf1 = f1_score(y_pred_binary_pgd_test1, y_test_categorical1, average='macro') * 100\n#auc = roc_auc_score(y_pred, y_train_categorical, multi_class='ovr') * 100\nprint('accuracy:', accuracy)\nprint('precision:', precision)\nprint('recall:', recall)\nprint('f1:', f1)\n\n## Task 2:\nprint('Cervical cancer classification:')\naccuracy = accuracy_score(y_pred_binary_pgd_test2, y_test_categorical2) * 100\nprecision = precision_score(y_pred_binary_pgd_test2, y_test_categorical2, average='macro') * 100\nrecall = recall_score(y_pred_binary_pgd_test2, y_test_categorical2, average='macro') * 100\nf1 = f1_score(y_pred_binary_pgd_test2, y_test_categorical2, average='macro') * 100\nprint('accuracy:', accuracy)\nprint('precision:', precision)\nprint('recall:', recall)\nprint('f1:', f1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit([X_train_s, X_train_h], [y_train_s, y_train_h],\n                    epochs=100,\n                    validation_split=0.2, verbose=1,\n                    shuffle=True,\n                   callbacks=callbacks)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.evaluate([X_test_s, X_test_h1], [y_test_s, y_test_h1])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\n\nmodel1 = load_model('/kaggle/working/best1_model_cer_skin_lung.keras', custom_objects={'DeeperAttentionLayer1': DeeperAttentionLayer1,\n                                                                         'DeeperAttentionLayer': DeeperAttentionLayer\n                                                                  })\nmodel1.evaluate([X_test_s, X_test_h1], [y_test_s, y_test_h1])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.evaluate([X_test_s1, X_test_h], [y_test_s1, y_test_h])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred = model.predict([X_test_s1, X_test_h]) \n\ny_pred_binary1 = y_pred[0] >= 0.5\ny_pred_binary_pgd_test1 = np.array(y_pred_binary1, dtype='int32')\n\nprint('y_pred_binary_pgd_test1:', y_pred_binary_pgd_test1.shape)\n\ny_pred_binary2 = y_pred[1] >= 0.5\ny_pred_binary_pgd_test2 = np.array(y_pred_binary2, dtype='int32')\n\nprint('y_pred_binary_pgd_test2:', y_pred_binary_pgd_test2.shape)\n\n#y_test_s, y_test_h\n# Calculate evaluation metrics for the current epsilon\ny_test_categorical1 = y_test_s1\ny_test_categorical2 = y_test_h\n\n## Task 1:\nprint('skin cancer classification:')\naccuracy = accuracy_score(y_pred_binary_pgd_test1, y_test_categorical1) * 100\nprecision = precision_score(y_pred_binary_pgd_test1, y_test_categorical1, average='macro') * 100\nrecall = recall_score(y_pred_binary_pgd_test1, y_test_categorical1, average='macro') * 100\nf1 = f1_score(y_pred_binary_pgd_test1, y_test_categorical1, average='macro') * 100\n#auc = roc_auc_score(y_pred, y_train_categorical, multi_class='ovr') * 100\nprint('accuracy:', accuracy)\nprint('precision:', precision)\nprint('recall:', recall)\nprint('f1:', f1)\n\n## Task 2:\nprint('Cervical cancer classification:')\naccuracy = accuracy_score(y_pred_binary_pgd_test2, y_test_categorical2) * 100\nprecision = precision_score(y_pred_binary_pgd_test2, y_test_categorical2, average='macro') * 100\nrecall = recall_score(y_pred_binary_pgd_test2, y_test_categorical2, average='macro') * 100\nf1 = f1_score(y_pred_binary_pgd_test2, y_test_categorical2, average='macro') * 100\nprint('accuracy:', accuracy)\nprint('precision:', precision)\nprint('recall:', recall)\nprint('f1:', f1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred = model.predict([X_test_s, X_test_h1]) \n\ny_pred_binary1 = y_pred[0] >= 0.5\ny_pred_binary_pgd_test1 = np.array(y_pred_binary1, dtype='int32')\n\nprint('y_pred_binary_pgd_test1:', y_pred_binary_pgd_test1.shape)\n\ny_pred_binary2 = y_pred[1] >= 0.5\ny_pred_binary_pgd_test2 = np.array(y_pred_binary2, dtype='int32')\n\nprint('y_pred_binary_pgd_test2:', y_pred_binary_pgd_test2.shape)\n\n#y_test_s, y_test_h\n# Calculate evaluation metrics for the current epsilon\ny_test_categorical1 = y_test_s\ny_test_categorical2 = y_test_h1\n\n## Task 1:\nprint('skin cancer classification:')\naccuracy = accuracy_score(y_pred_binary_pgd_test1, y_test_categorical1) * 100\nprecision = precision_score(y_pred_binary_pgd_test1, y_test_categorical1, average='macro') * 100\nrecall = recall_score(y_pred_binary_pgd_test1, y_test_categorical1, average='macro') * 100\nf1 = f1_score(y_pred_binary_pgd_test1, y_test_categorical1, average='macro') * 100\n#auc = roc_auc_score(y_pred, y_train_categorical, multi_class='ovr') * 100\nprint('accuracy:', accuracy)\nprint('precision:', precision)\nprint('recall:', recall)\nprint('f1:', f1)\n\n## Task 2:\nprint('Cervical cancer classification:')\naccuracy = accuracy_score(y_pred_binary_pgd_test2, y_test_categorical2) * 100\nprecision = precision_score(y_pred_binary_pgd_test2, y_test_categorical2, average='macro') * 100\nrecall = recall_score(y_pred_binary_pgd_test2, y_test_categorical2, average='macro') * 100\nf1 = f1_score(y_pred_binary_pgd_test2, y_test_categorical2, average='macro') * 100\nprint('accuracy:', accuracy)\nprint('precision:', precision)\nprint('recall:', recall)\nprint('f1:', f1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save('best_model_ever.keras')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}