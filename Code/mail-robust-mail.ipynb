{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7957692,"sourceType":"datasetVersion","datasetId":4680816},{"sourceId":7957702,"sourceType":"datasetVersion","datasetId":4680825},{"sourceId":10653461,"sourceType":"datasetVersion","datasetId":6597049}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import cv2\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nsns.set_style('whitegrid')\nfrom sklearn.metrics import confusion_matrix , classification_report\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense , Flatten , Conv2D , MaxPooling2D , Dropout , Activation , BatchNormalization\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import Adam , Adamax\nfrom tensorflow.keras import regularizers\n\n#Warnings\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\n#tf.keras.mixed_precision.set_global_policy('mixed_float16')\n\nimport tensorflow as tf\nimport numpy as np\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D, BatchNormalization, ReLU, Add\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.losses import KLDivergence\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.datasets import cifar10\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.applications import DenseNet121, ResNet50V2\nfrom tensorflow.keras.layers import GlobalAveragePooling2D\nimport copy\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D, BatchNormalization, ReLU, Add\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.losses import KLDivergence\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.datasets import cifar10\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.applications import DenseNet169, MobileNetV2, ResNet50, EfficientNetB0\nfrom tensorflow.keras.layers import GlobalAveragePooling2D\nimport copy\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Modality 1","metadata":{}},{"cell_type":"code","source":"X_train_h = np.load('x_train.npy')\ny_train_h = np.load('y_train.npy')\nX_test_h = np.load('x_test.npy')\ny_test_h = np.load('y_test.npy')\n\nX_val_h = np.load('x_val.npy')\ny_val_h = np.load('y_val.npy')\n\n\nX_train_h.shape, y_train_h.shape, X_test_h.shape, y_test_h.shape, X_val_h.shape, y_val_h.shape\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"random_indices = np.random.choice(1001, 810, replace=False)\n\nX_test_h1 = X_test_h[random_indices]\ny_test_h1 = y_test_h[random_indices]\n\nX_test_h1.shape, y_test_h1.shape, X_test_h.shape, y_test_h.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"random_indices = np.random.choice(1002, 648, replace=False)\n\nX_val_h1 = X_val_h[random_indices]\ny_val_h1 = y_val_h[random_indices]\n\nX_test_h1.shape, y_test_h1.shape, X_test_h.shape, y_test_h.shape, X_val_h1.shape, y_val_h1.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#X_train_s.shape,X_test_s.shape, y_train_s.shape,y_test_s.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train_h.shape, y_train_h.shape, X_test_h.shape, y_test_h.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Modality 2","metadata":{}},{"cell_type":"code","source":"X_train_s = np.load('X.npy')\ny_train_s = np.load('Y.npy')\n\nX_train_s.shape, y_train_s.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train_s, X_test_s, y_train_s, y_test_s = train_test_split(X_train_s, y_train_s, test_size=0.2, random_state=42)\nX_train_s, X_val_s, y_train_s, y_val_s = train_test_split(X_train_s, y_train_s, test_size=0.2, random_state=42)\n\nX_train_s.shape,X_test_s.shape, y_train_s.shape,y_test_s.shape, y_val_s.shape,y_val_s.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''import numpy as np\nimport cv2\n\ndef rotate_image(image, angle):\n    \"\"\"\n    Rotate the image by the specified angle.\n    \"\"\"\n    center = tuple(np.array(image.shape[1::-1]) / 2)\n    rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n    rotated_image = cv2.warpAffine(image, rotation_matrix, image.shape[1::-1], flags=cv2.INTER_LINEAR)\n    return rotated_image\n\ndef translate_image(image, tx, ty):\n    \"\"\"\n    Translate the image by the specified translation parameters.\n    \"\"\"\n    translation_matrix = np.float32([[1, 0, tx], [0, 1, ty]])\n    translated_image = cv2.warpAffine(image, translation_matrix, image.shape[1::-1])\n    return translated_image\n\n# Example data\n#X_train = np.random.rand(100, 28, 28)  # Assuming 100 images of size 28x28\n#y_train = np.random.randint(0, 10, 100)  # Assuming 100 labels\n\n# Augmentation parameters\nrotation_angles = [20]\ntranslations = [(5, 5)]\n\naugmented_X_train = []\naugmented_y_train = []\n\nfor image, label in zip(X_train_s, y_train_s):\n    # Original image\n    #augmented_X_train.append(image)\n    #augmented_y_train.append(label)\n\n    # Augment with rotations\n    for angle in rotation_angles:\n        rotated_image = rotate_image(image, angle)\n        augmented_X_train.append(rotated_image)\n        augmented_y_train.append(label)\n\n    # Augment with translations\n    for tx, ty in translations:\n        translated_image = translate_image(image, tx, ty)\n        augmented_X_train.append(translated_image)\n        augmented_y_train.append(label)\n\n# Convert lists to numpy arrays\naugmented_X_train = np.array(augmented_X_train)\naugmented_y_train = np.array(augmented_y_train)\n\n# Shuffle the data\nshuffle_indices = np.random.permutation(len(augmented_X_train))\naugmented_X_train = augmented_X_train[shuffle_indices]\naugmented_y_train = augmented_y_train[shuffle_indices]\naugmented_X_train.shape, augmented_y_train.shape\n# Now, augmented_X_train and augmented_y_train contain the augmented dataset.'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport cv2\n\ndef rotate_image(image, angle):\n    \"\"\"\n    Rotate the image by the specified angle.\n    \"\"\"\n    center = tuple(np.array(image.shape[1::-1]) / 2)\n    rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n    rotated_image = cv2.warpAffine(image, rotation_matrix, image.shape[1::-1], flags=cv2.INTER_LINEAR)\n    return rotated_image\n\ndef translate_image(image, tx, ty):\n    \"\"\"\n    Translate the image by the specified translation parameters.\n    \"\"\"\n    translation_matrix = np.float32([[1, 0, tx], [0, 1, ty]])\n    translated_image = cv2.warpAffine(image, translation_matrix, image.shape[1::-1])\n    return translated_image\n\ndef apply_gaussian_blur(image, kernel_size=3):\n    \"\"\"\n    Apply Gaussian Blur to the image to reduce noise and improve generalization.\n    \"\"\"\n    blurred_image = cv2.GaussianBlur(image, (kernel_size, kernel_size), 0)\n    return blurred_image\n\n# Augmentation parameters\nrotation_angles = [20]\ntranslations = [(5, 5)]\nkernel_sizes = [3]  # Gaussian Blur kernel sizes\n\naugmented_X_train = []\naugmented_y_train = []\n\nfor image, label in zip(X_train_s, y_train_s):\n    # Augment with rotations\n    for angle in rotation_angles:\n        rotated_image = rotate_image(image, angle)\n        augmented_X_train.append(rotated_image)\n        augmented_y_train.append(label)\n\n    # Augment with translations\n    for tx, ty in translations:\n        translated_image = translate_image(image, tx, ty)\n        augmented_X_train.append(translated_image)\n        augmented_y_train.append(label)\n\n    # Augment with Gaussian Blur\n    for kernel_size in kernel_sizes:\n        blurred_image = apply_gaussian_blur(image, kernel_size)\n        augmented_X_train.append(blurred_image)\n        augmented_y_train.append(label)\n\n# Convert lists to numpy arrays\naugmented_X_train = np.array(augmented_X_train)\naugmented_y_train = np.array(augmented_y_train)\n\n# Shuffle the data\nshuffle_indices = np.random.permutation(len(augmented_X_train))\naugmented_X_train = augmented_X_train[shuffle_indices]\naugmented_y_train = augmented_y_train[shuffle_indices]\naugmented_X_train.shape, augmented_y_train.shape\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"random_indices = np.random.choice(7773, 5421, replace=False)\n\naugmented_X_train = augmented_X_train[random_indices]\naugmented_y_train = augmented_y_train[random_indices]\n\naugmented_X_train.shape, augmented_y_train.shape\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train_s = np.concatenate((X_train_s, augmented_X_train), axis=0)\ny_train_s = np.concatenate((y_train_s, augmented_y_train), axis=0)\nX_train_s.shape, y_train_s.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''X_train_s = np.concatenate((X_train_s, X_train_s, X_train_s), axis=0)\ny_train_s = np.concatenate((y_train_s, y_train_s, y_train_s), axis=0)\nX_train_s.shape, y_train_s.shape'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''X_test_s1 = np.concatenate((X_test_s, X_test_s, X_test_s), axis=0)\ny_test_s1 = np.concatenate((y_test_s, y_test_s, y_test_s), axis=0)\nX_test_s1.shape, y_test_s1.shape'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train_s.shape, y_train_s.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"augmented_X_train.shape, augmented_y_train.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#### Multi-branch fusion attention (MFA) module #####\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.signal import fft2d, dct\n\nclass GlobalMinPooling2D(layers.Layer):\n    def __init__(self, **kwargs):\n        super(GlobalMinPooling2D, self).__init__(**kwargs)\n\n    def call(self, inputs):\n        return tf.reduce_min(inputs, axis=[1, 2])\n\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0], input_shape[-1])\n\n    def get_config(self):\n        config = super(GlobalMinPooling2D, self).get_config()\n        return config\n\nclass DeeperGlobalLocalAttentionLayer1(layers.Layer):\n    def __init__(self, units, activation='sigmoid', dropout_rate=0.2, use_scale=True, axis=-1, **kwargs):\n        super(DeeperGlobalLocalAttentionLayer1, self).__init__(**kwargs)\n        self.units = units\n        self.activation = activation\n        self.dropout_rate = dropout_rate\n        self.use_scale = use_scale\n        self.axis = axis\n\n    def build(self, input_shape):\n        _, _, _, channels = input_shape\n        \n        \n        self.global_conv1 = layers.DepthwiseConv2D(kernel_size=(1, 1), activation=self.activation)\n        self.global_avg_pooling1 = layers.GlobalAveragePooling2D()\n        \n        self.global_conv2 = layers.DepthwiseConv2D(kernel_size=(1, 1), activation=self.activation)\n        self.global_avg_pooling2 = layers.GlobalMaxPooling2D()\n        \n        self.global_conv_3 = layers.DepthwiseConv2D(kernel_size=(1, 1), activation=self.activation)\n        self.global_avg_pooling_3 = GlobalMinPooling2D()\n        \n        \n        self.global_conv3 = layers.DepthwiseConv2D(kernel_size=(1, 1), activation=self.activation)\n        self.global_avg_pooling3 = layers.GlobalAveragePooling2D()\n        \n        self.global_conv4 = layers.DepthwiseConv2D(kernel_size=(1, 1), activation=self.activation)\n        self.global_avg_pooling4 = layers.GlobalMaxPooling2D()\n\n        self.global_conv_4 = layers.DepthwiseConv2D(kernel_size=(1, 1), activation=self.activation)\n        self.global_avg_pooling_4 = GlobalMinPooling2D()\n        \n        \n        self.concat1 = layers.Add()\n        self.concat2 = layers.Add()\n        self.concat3 = layers.Add()\n        self.concat4 = layers.Add()\n        self.concat_3 = layers.Add()\n        self.concat_4 = layers.Add()\n        \n        self.concat5 = layers.Concatenate(axis=-1)\n        \n        self.global_attention = layers.Dense(units=self.units, activation=self.activation)\n\n        #self.local_dsc = layers.DepthwiseConv2D(kernel_size=(3, 3), padding=\"same\", depth_multiplier=1)\n        self.local_conv1 = layers.DepthwiseConv2D(kernel_size=(1, 1), activation=self.activation)\n        self.local_conv2 = layers.DepthwiseConv2D(kernel_size=(1, 1), activation=self.activation)\n        \n        self.concat6 = layers.Add()\n        \n        if self.use_scale:\n            self.global_scale = self.add_weight(shape=(1, 1, 1, 1), initializer=tf.keras.initializers.HeNormal(), trainable=True, name='global_scale')\n            self.local_scale = self.add_weight(shape=(1, 1, 1, self.units), initializer=tf.keras.initializers.HeNormal(), trainable=True, name='local_scale')\n        \n        super(DeeperGlobalLocalAttentionLayer1, self).build(input_shape)\n\n    def call(self, inputs, training=None):\n        ##### Hierarchical Information Fusion Attention(HIFA) ######\n        \n        if not tf.sparse.is_sparse(inputs):\n            inputs = tf.sparse.from_dense(inputs)\n        \n        global_attention1 = self.global_conv1(inputs)\n        global_avg1 = self.global_avg_pooling1(global_attention1)\n        \n        global_attention2 = self.global_conv2(global_attention1)\n        global_avg2 = self.global_avg_pooling2(global_attention2)\n\n        global_attention_3 = self.global_conv_3(global_attention1)\n        global_avg_3 = self.global_avg_pooling_3(global_attention_3)\n        \n        \n        global_concat1 = self.concat1([global_avg1, global_avg2, global_avg_3])\n        global_sub1 = global_avg2 - global_avg1 - global_avg_3\n        global_concat1 = global_concat1 + global_sub1\n        \n        global_attention_concat1 = self.concat2([global_attention1, global_attention2, global_attention_3])\n        \n        global_sub_1 = global_attention2 - global_attention1 - global_attention_3\n\n        global_attention_concat1 = global_attention_concat1 + global_sub_1\n        \n        global_attention3 = self.global_conv3(global_attention_concat1)\n        global_avg3 = self.global_avg_pooling3(global_attention3)\n        \n        global_attention4 = self.global_conv4(global_attention3)\n        global_avg4 = self.global_avg_pooling4(global_attention4)\n\n        global_attention_4 = self.global_conv_3(global_attention3)\n        global_avg_4 = self.global_avg_pooling_3(global_attention_4)\n        \n        \n        global_concat2 = self.concat3([global_avg3, global_avg4, global_avg_4])\n        global_sub2 = global_avg4 - global_avg3 - global_avg_4\n        global_concat2 = global_concat2 + global_sub2\n\n        \n        #global_attention_concat2 = self.concat4([global_attention3, global_attention4, global_attention_4])\n        #global_sub_2 = global_attention4 - global_attention3 - global_attention_4\n\n        #global_attention_concat2 = global_attention_concat2 + global_sub_2\n\n        \n        \n        global_avg_concat = self.concat5([global_concat1, global_concat2])\n        \n        global_attention = self.global_attention(global_avg_concat)\n\n        if tf.sparse.is_sparse(global_attention):\n            global_attention = tf.sparse.to_dense(global_attention)\n            \n        #global_attention = L.Activation(\"relu\")(tf.expand_dims(tf.expand_dims(global_attention, 1), 1))\n        \n        global_attention = tf.expand_dims(tf.expand_dims(global_attention, 1), 1)\n\n        ##### frequency domain Local Information learning Attention ######\n        \n        input_shape = tf.shape(inputs)\n\n        \n        flattened_inputs = tf.reshape(inputs, [-1, input_shape[-1]])  # Flatten along the last axis\n        dct_transformed = tf.signal.dct(flattened_inputs, type=2, norm='ortho')\n        dct_transformed = tf.reshape(dct_transformed, input_shape)  # Reshape back to original dimensions\n        \n        local_attention1 = self.local_conv1(dct_transformed)\n        local_attention1 = tf.reduce_mean(local_attention1, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        local_attention2 = self.local_conv2(local_attention1)\n        local_attention2 = tf.reduce_mean(local_attention2, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n\n        '''local_attention3 = self.local_conv1(inputs)\n        local_attention3 = tf.reduce_max(local_attention3, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        local_attention4 = self.local_conv2(local_attention3)\n        local_attention4 = tf.reduce_max(local_attention4, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n\n        local_attention5 = self.local_conv1(inputs)\n        local_attention5 = tf.reduce_mean(local_attention5, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        local_attention6 = self.local_conv2(local_attention5)\n        local_attention6 = tf.reduce_mean(local_attention6, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions'''\n\n        local_attention7 = self.local_conv1(dct_transformed)\n        local_attention7 = tf.reduce_max(local_attention7, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        local_attention8 = self.local_conv2(local_attention7)\n        local_attention8 = tf.reduce_max(local_attention8, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n\n        local_attention9 = self.local_conv1(dct_transformed)\n        local_attention9 = tf.reduce_min(local_attention9, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        local_attention10 = self.local_conv2(local_attention9)\n        local_attention10 = tf.reduce_min(local_attention10, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n\n\n        \n        \n        #local_attention = self.concat6([local_attention1, local_attention2, local_attention3, local_attention4, \n         #                              local_attention5, local_attention6, local_attention7, local_attention8])\n        \n        local_attention_avg = (self.concat6([local_attention1, local_attention2])) \n        local_attention_max = self.concat6([local_attention7, local_attention8]) \n        local_attention_min = self.concat6([local_attention9, local_attention10]) \n\n        local_attention = self.concat6([local_attention_avg, local_attention_max, local_attention_min]) \n        \n        # Scale Global and Local Attention\n        if self.use_scale:\n            global_attention *= self.global_scale\n            local_attention *= self.local_scale\n\n        # Combine Global and Local Attention\n        attention = tf.sigmoid(global_attention + local_attention)\n        return attention\n\n    def get_config(self):\n        config = super(DeeperGlobalLocalAttentionLayer1, self).get_config()\n        config.update({'units': self.units, 'activation': self.activation, 'dropout_rate': self.dropout_rate,\n                       'use_scale': self.use_scale})\n        return config\n\n'''class DeeperAttentionLayer1(layers.Layer):\n    def __init__(self, units=64, use_scale=True, **kwargs):\n        super(DeeperAttentionLayer1, self).__init__(**kwargs)\n        self.units = units\n        self.use_scale = use_scale\n\n    def build(self, input_shape):\n        _, H, W, C = input_shape\n        self.alpha = self.add_weight(shape=(1, 1, 1, C), initializer='ones', trainable=True, name='alpha')\n        self.deeper_global_local_attention = DeeperGlobalLocalAttentionLayer1(units=self.units, activation='sigmoid', \n                                                                              dropout_rate=0.2,  # You can adjust the dropout rate\n                                                                              use_scale=self.use_scale)\n        super(DeeperAttentionLayer1, self).build(input_shape)\n\n    def call(self, inputs, training=None):\n        attention = self.deeper_global_local_attention(inputs, training=training)\n        attention_feature = inputs * attention * self.alpha\n        return attention_feature\n\n    def get_config(self):\n        config = super(DeeperAttentionLayer1, self).get_config()\n        config.update({'units': self.units, 'use_scale': self.use_scale})\n        return config'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#### Multi-branch fusion attention (MFA) module #####\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.signal import fft2d, dct\n\nclass GlobalMinPooling2D(layers.Layer):\n    def __init__(self, **kwargs):\n        super(GlobalMinPooling2D, self).__init__(**kwargs)\n\n    def call(self, inputs):\n        return tf.reduce_min(inputs, axis=[1, 2])\n\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0], input_shape[-1])\n\n    def get_config(self):\n        config = super(GlobalMinPooling2D, self).get_config()\n        return config\n\nclass DeeperGlobalLocalAttentionLayer1(layers.Layer):\n    def __init__(self, units, activation='sigmoid', dropout_rate=0.2, use_scale=True, axis=-1, **kwargs):\n        super(DeeperGlobalLocalAttentionLayer1, self).__init__(**kwargs)\n        self.units = units\n        self.activation = activation\n        self.dropout_rate = dropout_rate\n        self.use_scale = use_scale\n        self.axis = axis\n\n    def build(self, input_shape):\n        _, _, _, channels = input_shape\n        \n        \n        self.global_conv1 = layers.Conv2D(filters = channels, kernel_size=(1, 1), activation=self.activation, dilation_rate=1)\n        self.global_avg_pooling1 = layers.GlobalAveragePooling2D()\n        \n        #self.global_conv2 = layers.SeparableConv2D(filters = self.units, kernel_size=(1, 1), activation=self.activation)\n        self.global_conv2 = layers.DepthwiseConv2D(kernel_size=(3, 3), activation=self.activation, dilation_rate=1, padding=\"same\")\n\n        self.global_avg_pooling2 = layers.GlobalMaxPooling2D()\n        \n        self.global_conv_3 = layers.DepthwiseConv2D(kernel_size=(5, 5), activation=self.activation, dilation_rate=1, padding=\"same\")\n        self.global_avg_pooling_3 = GlobalMinPooling2D()\n\n        self.global_conv4 = layers.Conv2D(filters = channels, kernel_size=(5, 5), activation=self.activation, groups=8, padding=\"same\")\n        #self.global_conv5 = layers.Conv2D(filters = channels, kernel_size=(1, 1), activation=self.activation, padding=\"same\")\n        \n        \n        '''self.global_conv3 = layers.DepthwiseConv2D(kernel_size=(1, 1), activation=self.activation)\n        self.global_avg_pooling3 = layers.GlobalAveragePooling2D()\n        \n        self.global_conv4 = layers.DepthwiseConv2D(kernel_size=(1, 1), activation=self.activation)\n        self.global_avg_pooling4 = layers.GlobalMaxPooling2D()\n\n        self.global_conv_4 = layers.DepthwiseConv2D(kernel_size=(1, 1), activation=self.activation)\n        self.global_avg_pooling_4 = GlobalMinPooling2D()'''\n        \n        \n        self.concat1 = layers.Add()\n        self.concat2 = layers.Add()\n        self.concat3 = layers.Add()\n        self.concat4 = layers.Add()\n        self.concat_3 = layers.Add()\n        self.concat_4 = layers.Add()\n        \n        self.concat5 = layers.Concatenate(axis=-1)\n        \n        self.global_attention = layers.Dense(units=self.units, activation=self.activation)\n\n        #self.local_dsc = layers.DepthwiseConv2D(kernel_size=(3, 3), padding=\"same\", depth_multiplier=1)\n        self.local_conv1 = layers.DepthwiseConv2D(kernel_size=(1, 1), activation=self.activation)\n        self.local_conv2 = layers.DepthwiseConv2D(kernel_size=(1, 1), activation=self.activation)\n        self.local_conv3 = layers.DepthwiseConv2D(kernel_size=(1, 1), activation=self.activation)\n\n        self.local_conv4 = layers.DepthwiseConv2D(kernel_size=(1, 1), activation=self.activation)\n        self.batch_norm = layers.LayerNormalization()\n        \n        self.concat6 = layers.Add()\n        \n        if self.use_scale:\n            self.global_scale = self.add_weight(shape=(1, 1, 1, 1), initializer=tf.keras.initializers.HeNormal(), trainable=True, name='global_scale')\n            self.local_scale = self.add_weight(shape=(1, 1, 1, self.units), initializer=tf.keras.initializers.HeNormal(), trainable=True, name='local_scale')\n        \n        super(DeeperGlobalLocalAttentionLayer1, self).build(input_shape)\n\n    def call(self, inputs, training=None):\n        ##### Hierarchical Information Fusion Attention(HIFA) ######\n\n        input_shape = tf.shape(inputs)\n        \n        flattened_inputs = tf.reshape(inputs, [-1, input_shape[-1]])  # Flatten along the last axis\n        dct_transformed = tf.signal.dct(flattened_inputs, type=2, norm='ortho')\n        dct_transformed = tf.reshape(dct_transformed, input_shape)  # Reshape back to original dimensions\n        \n        \n        global_attention1 = self.global_conv1(dct_transformed)\n        global_avg1 = self.global_avg_pooling1(global_attention1)\n        global_avg2 = self.global_avg_pooling2(global_attention1)\n        global_avg3 = self.global_avg_pooling_3(global_attention1)\n\n        '''global_concat1 = self.concat1([global_avg1, global_avg2, global_avg3])\n        global_sub1 = global_avg2 - global_avg1 - global_avg3\n        global_concat1 = global_concat1 + global_sub1\n        '''\n\n        dct_transformed_ga = self.concat1([inputs, global_attention1]) #global_attention1 + dct_transformed\n        \n        global_attention2 = self.global_conv2(dct_transformed_ga)\n        global_avg4 = self.global_avg_pooling1(global_attention2)\n        global_avg5 = self.global_avg_pooling2(global_attention2)\n        global_avg6 = self.global_avg_pooling_3(global_attention2)\n\n        '''global_concat2 = self.concat1([global_avg4, global_avg5, global_avg6])\n        global_sub2 = global_avg5 - global_avg4 - global_avg6\n        global_concat2 = global_concat2 + global_sub2\n        '''\n\n        \n        #dct_transformed_ga = self.global_conv4(dct_transformed_ga)\n        #dct_transformed_ga = self.global_conv4(dct_transformed_ga)\n        dct_transformed_ga2 =  self.concat1([dct_transformed_ga, global_attention2]) #dct_transformed_ga + global_attention2\n\n        '''x1 = layers.Concatenate()([conv1x1, conv3x3, conv5x5])\n        x1 = layers.Conv2D(filters, kernel_size=1, groups=groups, padding=\"same\")(x1)\n\n        x = layers.Conv2D(filters=filters, kernel_size=(1, 1), strides=strides, padding='same')(x)\n        x = layers.Add()([x, x1])\n        x = layers.Activation('relu')(x)\n        '''\n        \n        global_attention3 = self.global_conv_3(dct_transformed_ga2)\n        global_avg7 = self.global_avg_pooling1(global_attention3)\n        global_avg8 = self.global_avg_pooling2(global_attention3)\n        global_avg9 = self.global_avg_pooling_3(global_attention3)\n\n        global_concat_avg = self.concat1([global_avg1, global_avg4, global_avg7])\n        global_concat_max = self.concat1([global_avg2, global_avg5, global_avg8])\n        global_concat_min = self.concat1([global_avg3, global_avg6, global_avg9])\n\n        global_concat_add = global_concat_avg + global_concat_max + global_concat_min \n        global_concat_sub = global_concat_max - global_concat_avg - global_concat_min \n        \n\n        global_avg_concat = global_concat_add + global_concat_sub  #self.concat5([global_concat_add, global_concat_sub])\n        global_attention = self.global_attention(global_avg_concat)\n        \n        global_attention = tf.expand_dims(tf.expand_dims(global_attention, 1), 1)\n\n        ##### frequency domain Local Information learning Attention ######\n        \n        input_shape = tf.shape(inputs)\n        batch_size, height, width, channels = inputs.shape\n        \n        flattened_inputs = tf.reshape(inputs, [-1, input_shape[-1]])  # Flatten along the last axis\n        dct_transformed = tf.signal.dct(flattened_inputs, type=2, norm='ortho')\n        dct_transformed = tf.reshape(dct_transformed, input_shape)  # Reshape back to original dimensions\n        \n        local_attention1 = self.local_conv1(dct_transformed)\n        local_attention1 = tf.reduce_mean(local_attention1, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        #local_max1 = tf.reduce_max(local_attention1, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        #local_min1 = tf.reduce_min(local_attention1, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        #local_attention_1 = local_avg1 + local_max1 + local_min1\n        \n        local_attention2 = self.local_conv2(dct_transformed)\n        #local_avg2 = tf.reduce_mean(local_attention2, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        local_attention2 = tf.reduce_max(local_attention2, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        #local_min2 = tf.reduce_min(local_attention2, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        #local_attention_2 = local_avg2 + local_max2 + local_min2\n\n        #local_attention2 = tf.reduce_mean(local_attention2, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n\n        local_attention3 = self.local_conv3(dct_transformed)\n        #local_avg3 = tf.reduce_mean(local_attention3, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        #local_max3 = tf.reduce_max(local_attention3, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        local_attention3 = tf.reduce_min(local_attention3, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        #local_attention_3 = local_avg3 + local_max3 + local_min3\n\n\n        \n        \n\n        '''#local_attention1 = tf.reshape(local_attention1, [batch_size, -1, self.units])\n        local_attention2 = tf.reshape(local_attention2, [batch_size, self.units, -1])\n        local_attention3 = tf.reshape(local_attention3, [batch_size, -1, self.units])\n\n        f = tf.matmul(local_attention1, local_attention2)\n        f_softmax = tf.nn.softmax(f, -1)\n        y = tf.matmul(f_softmax, local_attention3)\n        y = tf.reshape(y, [batch_size, height, width, self.units])\n\n        y = self.local_conv4(y)\n        y = self.batch_norm(y)'''\n\n\n        '''local_attention1 = tf.cast(local_attention1, tf.float32)\n        local_attention2 = tf.cast(local_attention2, tf.float32)\n        local_attention3 = tf.cast(local_attention3, tf.float32)\n\n        local_attention1_flattened = tf.reshape(local_attention1, [batch_size, height * width, channels])\n        local_attention2_flattened = tf.reshape(local_attention2, [batch_size, height * width, channels])\n        local_attention3_flattened = tf.reshape(local_attention3, [batch_size, height * width, channels])'''\n        \n        \n        \n        # Step 2: Perform the matrix multiplication and softmax operation\n        f = tf.matmul(local_attention1, local_attention2, transpose_b=True)  # Shape: (batch_size, height * width, height * width)\n        f_softmax = tf.nn.softmax(f, axis=-1)  # Softmax along the second axis (the spatial dimension)\n        \n        # Step 3: Multiply with local_attention3 and reshape\n        y = tf.matmul(f_softmax, local_attention3)  # Shape: (batch_size, height * width, channels)\n        #y = tf.reshape(y, [batch_size, height, width, channels])  # Reshaping to the original spatial dimensions\n        \n        # Step 4: Apply convolution and batch normalization\n        y = self.local_conv4(y)  # Convolution layer (you may have to define self.local_conv4 as a Conv2D layer)\n        y = self.batch_norm(y)  # Batch normalization\n        \n\n        \n\n        \n\n        '''local_attention3 = self.local_conv1(inputs)\n        local_attention3 = tf.reduce_max(local_attention3, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        local_attention4 = self.local_conv2(local_attention3)\n        local_attention4 = tf.reduce_max(local_attention4, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n\n        local_attention5 = self.local_conv1(inputs)\n        local_attention5 = tf.reduce_mean(local_attention5, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        local_attention6 = self.local_conv2(local_attention5)\n        local_attention6 = tf.reduce_mean(local_attention6, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions'''\n\n        \n\n        '''local_attention7 = self.local_conv1(dct_transformed)\n        local_attention7 = tf.reduce_max(local_attention7, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        local_attention8 = self.local_conv2(local_attention7)\n        local_attention8 = tf.reduce_max(local_attention8, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n\n        local_attention9 = self.local_conv1(dct_transformed)\n        local_attention9 = tf.reduce_min(local_attention9, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        local_attention10 = self.local_conv2(local_attention9)\n        local_attention10 = tf.reduce_min(local_attention10, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n\n\n        \n        \n        #local_attention = self.concat6([local_attention1, local_attention2, local_attention3, local_attention4, \n         #                              local_attention5, local_attention6, local_attention7, local_attention8])\n        \n        local_attention_avg = (self.concat6([local_attention1, local_attention2])) \n        local_attention_max = self.concat6([local_attention7, local_attention8]) \n        local_attention_min = self.concat6([local_attention9, local_attention10]) \n\n        local_attention = self.concat6([local_attention_avg, local_attention_max, local_attention_min]) '''\n        \n        # Scale Global and Local Attention\n        if self.use_scale:\n            global_attention *= self.global_scale\n            y *= self.local_scale\n\n        # Combine Global and Local Attention\n        attention = tf.sigmoid(global_attention + y)\n        return attention\n\n    def get_config(self):\n        config = super(DeeperGlobalLocalAttentionLayer1, self).get_config()\n        config.update({'units': self.units, 'activation': self.activation, 'dropout_rate': self.dropout_rate,\n                       'use_scale': self.use_scale})\n        return config\n\n'''class DeeperAttentionLayer1(layers.Layer):\n    def __init__(self, units=64, use_scale=True, **kwargs):\n        super(DeeperAttentionLayer1, self).__init__(**kwargs)\n        self.units = units\n        self.use_scale = use_scale\n\n    def build(self, input_shape):\n        _, H, W, C = input_shape\n        self.alpha = self.add_weight(shape=(1, 1, 1, C), initializer='ones', trainable=True, name='alpha')\n        self.deeper_global_local_attention = DeeperGlobalLocalAttentionLayer1(units=self.units, activation='sigmoid', \n                                                                              dropout_rate=0.2,  # You can adjust the dropout rate\n                                                                              use_scale=self.use_scale)\n        super(DeeperAttentionLayer1, self).build(input_shape)\n\n    def call(self, inputs, training=None):\n        attention = self.deeper_global_local_attention(inputs, training=training)\n        attention_feature = inputs * attention * self.alpha\n        return attention_feature\n\n    def get_config(self):\n        config = super(DeeperAttentionLayer1, self).get_config()\n        config.update({'units': self.units, 'use_scale': self.use_scale})\n        return config'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers\n\n# Frequency Transform Layer\n'''class FrequencyTransformLayer(layers.Layer):\n    def __init__(self, transform_type='dct', **kwargs):\n        super(FrequencyTransformLayer, self).__init__(**kwargs)\n        self.transform_type = transform_type\n\n    def call(self, inputs):\n        # Ensure inputs are float32 for DCT and FFT operations\n        #inputs = tf.cast(inputs, tf.float32)\n\n        if self.transform_type == 'dct':\n            # Apply 2D DCT along the last axis\n            input_shape = tf.shape(inputs)\n            flattened_inputs = tf.reshape(inputs, [-1, input_shape[-1]])  # Flatten along the last axis\n            dct_transformed = tf.signal.dct(flattened_inputs, type=2, norm='ortho', axis = -1)\n            return tf.reshape(dct_transformed, input_shape)  # Reshape back to original dimensions\n        elif self.transform_type == 'fft':\n            # Apply 2D FFT and return the magnitude\n            fft_transformed = tf.signal.fft2d(tf.cast(inputs, tf.complex64))\n            return tf.math.abs(fft_transformed)\n        else:\n            raise ValueError(\"Unsupported transform type. Choose 'dct' or 'fft'.\")\n\n    def get_config(self):\n        config = super(FrequencyTransformLayer, self).get_config()\n        config.update({'transform_type': self.transform_type})\n        return config'''\n\n# DCT Transform Function\n'''def dct_transform(inputs):\n    # Apply DCT transform to the inputs\n    dct_input = FrequencyTransformLayer(transform_type='dct')(inputs)  # Use FrequencyTransformLayer for DCT\n    \n    # Apply global average pooling (GAP), global max pooling (GMP), and global min pooling (GMP_MIN)\n    gap = tf.reduce_mean(dct_input, axis=[1, 2], keepdims=True)\n    gmp = tf.reduce_max(dct_input, axis=[1, 2], keepdims=True)\n    gmp_min = tf.reduce_min(dct_input, axis=[1, 2], keepdims=True)\n    \n    # Combine the pooled features\n    pooled_features1 = gap + gmp + gmp_min\n    pooled_features2 = gmp - (gap - gmp_min)\n    pooled_features = pooled_features1 + pooled_features2\n    \n    return pooled_features'''\n\n\n\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\n# 1D DCT from scratch using TensorFlow\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\n# 1D DCT from scratch using TensorFlow\n'''def dct1d(x):\n    \"\"\"1D Discrete Cosine Transform.\"\"\"\n    N = tf.shape(x)[0]\n    k = tf.range(N, dtype=tf.float32)  # Create range for k\n    n = tf.range(N, dtype=tf.float32)  # Create range for n\n    # Using broadcasting to calculate the DCT\n    X = tf.reduce_sum(x * tf.cos(np.pi / N * (n + 0.5)[:, None] * k[None, :]), axis=0)\n    return X\n\n# 2D DCT from scratch using TensorFlow\ndef dct2d(x):\n    \"\"\"2D Discrete Cosine Transform (DCT). Applies DCT row-wise and column-wise.\"\"\"\n    # Apply DCT to each row (along the last axis)\n    x_dct_rows = tf.map_fn(dct1d, x, dtype=tf.float32)\n    # Apply DCT to each column (along the second last axis)\n    x_dct = tf.map_fn(lambda x: dct1d(tf.transpose(x)), x_dct_rows, dtype=tf.float32)\n    return x_dct\n\n\nimport tensorflow as tf\n\n# 1D DCT from scratch using TensorFlow\ndef dct1d(x):\n    \"\"\"1D Discrete Cosine Transform.\"\"\"\n    N = tf.shape(x)[0]\n    k = tf.range(N, dtype=tf.int32)  # Create range for k\n    n = tf.range(N, dtype=tf.int32)  # Create range for n\n    pi = tf.constant(3, dtype=tf.int32)  # Explicitly use TensorFlow constant for pi\n    \n    # Using broadcasting to calculate the DCT\n    X = tf.reduce_sum(x * tf.cos(3 / N * (n + 0.5)[:, None] * k[None, :]), axis=0)\n    return X\n\n# 2D DCT from scratch using TensorFlow\ndef dct2d(x):\n    \"\"\"2D Discrete Cosine Transform (DCT). Applies DCT row-wise and column-wise.\"\"\"\n    # Apply DCT to each row (along the last axis)\n    x_dct_rows = tf.map_fn(dct1d, x, dtype=tf.int32)\n    # Apply DCT to each column (along the second last axis)\n    x_dct = tf.map_fn(lambda x: dct1d(tf.transpose(x)), x_dct_rows, dtype=tf.int32)\n    return x_dct\n\n# Custom FrequencyTransformLayer class\nclass FrequencyTransformLayer(layers.Layer):\n    def __init__(self, transform_type='dct', **kwargs):\n        super(FrequencyTransformLayer, self).__init__(**kwargs)\n        self.transform_type = transform_type\n\n    def call(self, inputs):\n        # Ensure inputs are float32 for DCT and FFT operations\n        inputs = tf.cast(inputs, tf.float32)\n\n        if self.transform_type == 'dct':\n            # Apply 2D DCT from scratch (using custom dct2d function)\n            input_shape = tf.shape(inputs)\n            flattened_inputs = tf.reshape(inputs, [-1, input_shape[-1]])  # Flatten along the last axis\n            \n            # Perform the 2D DCT using the custom scratch implementation\n            dct_transformed = dct2d(flattened_inputs)  # Perform 2D DCT\n            \n            # Reshape back to the original dimensions\n            return tf.reshape(dct_transformed, input_shape)  # Reshape back to original dimensions\n\n        elif self.transform_type == 'fft':\n            # Apply 2D FFT and return the magnitude\n            fft_transformed = tf.signal.fft2d(tf.cast(inputs, tf.complex64))\n            return tf.math.abs(fft_transformed)\n\n        else:\n            raise ValueError(\"Unsupported transform type. Choose 'dct' or 'fft'.\")\n\n    def get_config(self):\n        config = super(FrequencyTransformLayer, self).get_config()\n        config.update({'transform_type': self.transform_type})\n        return config\n'''\n\n\n\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Layer\n\n'''class DCTTransformLayer(Layer):\n    def __init__(self, **kwargs):\n        super(DCTTransformLayer, self).__init__(**kwargs)\n        # Optionally initialize any additional parameters here\n        #self.frequency_transform = FrequencyTransformLayer(transform_type='dct')\n\n    def build(self, input_shape):\n        # This method is used for any initialization specific to the layer, such as creating weights or other components\n        pass\n\n    def call(self, inputs):\n        # Apply DCT transform to the inputs\n        #dct_input = self.frequency_transform(inputs)  # Using self to reference the FrequencyTransformLayer instance\n        \n        # Apply global average pooling (GAP), global max pooling (GMP), and global min pooling (GMP_MIN)\n        gap = tf.reduce_mean(inputs, axis=[1, 2], keepdims=True)\n        gmp = tf.reduce_max(inputs, axis=[1, 2], keepdims=True)\n        gmp_min = tf.reduce_min(inputs, axis=[1, 2], keepdims=True)\n        \n        # Combine the pooled features\n        pooled_features1 = gap + gmp + gmp_min\n        pooled_features2 = gmp - (gap - gmp_min)\n        pooled_features = pooled_features1 + pooled_features2\n        \n        return pooled_features\n\n\n\n\n# Frequency Attention\nimport tensorflow as tf\n\n\n# DCT Attention Non-Local Block\nclass DCTAttentionNonLocalBlock(layers.Layer):\n    def __init__(self, in_channels, intermediate_channels, dct_threshold=0.1):\n        super(DCTAttentionNonLocalBlock, self).__init__()\n        self.in_channels = in_channels\n        self.intermediate_channels = intermediate_channels\n        self.dct_threshold = dct_threshold\n\n\n        #self.dct_transform_layer = DCTTransformLayer()\n\n        \n        \n        # Define your layers here\n        self.theta = layers.Conv2D(intermediate_channels, kernel_size=1, use_bias=False)\n        self.phi = layers.Conv2D(intermediate_channels, kernel_size=1, use_bias=False)\n        self.g = layers.Conv2D(intermediate_channels, kernel_size=1, use_bias=False)\n        self.W = layers.Conv2D(in_channels, kernel_size=1, use_bias=False)\n        \n        self.attn_refine = tf.keras.Sequential([\n            layers.Conv2D(in_channels, kernel_size=3, padding='same', use_bias=False),\n            layers.ReLU(),\n            layers.Conv2D(in_channels, kernel_size=3, padding='same', use_bias=False)\n        ])\n        \n        self.channel_match_conv = layers.Conv2D(intermediate_channels, kernel_size=1)\n        self.reduce_channels_dct = layers.Conv2D(intermediate_channels, kernel_size=1)\n\n        self.relative_bias_dct = tf.Variable(tf.zeros((1, 1, 1, 1)), trainable=True)\n        self.relative_bias_non_local = tf.Variable(tf.zeros((1, 1, 1, 1)), trainable=True)\n\n    def call(self, inputs):\n        # Get the shape of the input\n        shape = tf.shape(inputs)\n        batch_size, height, width = shape[0], shape[1], shape[2]\n\n        # Process the inputs\n        # Global attention\n        gap = tf.reduce_mean(inputs, axis=[1, 2], keepdims=True)\n        gmp = tf.reduce_max(inputs, axis=[1, 2], keepdims=True)\n        gmp_min = tf.reduce_min(inputs, axis=[1, 2], keepdims=True)\n        \n        # Combine the pooled features\n        pooled_features1 = gap + gmp + gmp_min\n        pooled_features2 = gmp - (gap - gmp_min)\n        \n        #pooled_features = pooled_features1 + pooled_features2\n        \n        #dct_input = self.dct_transform_layer(inputs)  # Apply DCT transform\n        #dct_input = dct_transform(inputs)  # Apply DCT transform\n\n        \n        dct_attention = tf.sigmoid((pooled_features1 + pooled_features2) * self.relative_bias_dct) #frequency_attention(dct_input, self.dct_threshold) + self.relative_bias_dct\n\n        theta_x = tf.reshape(self.theta(inputs), [batch_size, height * width, self.intermediate_channels])\n        phi_x = tf.reshape(self.phi(inputs), [batch_size, height * width, self.intermediate_channels])\n        g_x = tf.reshape(self.g(inputs), [batch_size, height * width, self.intermediate_channels])\n\n        f = tf.matmul(theta_x, phi_x, transpose_b=True)\n        f_softmax = tf.nn.softmax(f, axis=-1)\n        y = tf.matmul(f_softmax, g_x)\n        y = tf.reshape(y, [batch_size, height, width, self.intermediate_channels])\n\n        refined_attention = self.attn_refine(inputs) + self.relative_bias_non_local\n        refined_attention = self.channel_match_conv(refined_attention)\n\n        scale_factor = tf.sigmoid(refined_attention)\n        #refined_dct_attention = self.reduce_channels_dct(dct_transform(dct_attention))\n        dct_attention = self.reduce_channels_dct(dct_attention)\n\n        y1 = y + (y * scale_factor) + refined_attention\n        y2 = y - (y * scale_factor) - refined_attention\n        y = tf.sigmoid(y1 + y2 + dct_attention)\n\n        y = self.W(y)\n        return tf.nn.relu(inputs + y)\n\n    def compute_output_shape(self, input_shape):\n        \"\"\"\n        Manually compute the output shape of the layer.\n        The output shape will be the same as the input shape.\n        \"\"\"\n        # The output will have the same dimensions as the input (batch_size, height, width, channels)\n        batch_size = input_shape[0]\n        height = input_shape[1]\n        width = input_shape[2]\n        channels = self.in_channels\n        return (batch_size, height, width, channels)\n'''\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\n'''# Custom Self-Attention Layer\nclass SelfAttention(layers.Layer):\n    def __init__(self, channels, **kwargs):\n        super(SelfAttention, self).__init__(**kwargs)\n        self.channels = channels\n        self.query_conv = layers.Conv2D(channels // 8, kernel_size=1)\n        self.key_conv = layers.Conv2D(channels // 8, kernel_size=1)\n        self.value_conv = layers.Conv2D(channels, kernel_size=1)\n        self.attn_conv = layers.Conv2D(channels, kernel_size=1)\n\n    def call(self, inputs):\n        # Generate Q, K, V matrices\n        query = self.query_conv(inputs)\n        key = self.key_conv(inputs)\n        value = self.value_conv(inputs)\n\n        # Perform dot product between query and key (self-attention)\n        attn_map = tf.matmul(query, key, transpose_b=True)\n        attn_map = tf.nn.softmax(attn_map, axis=-1)\n\n        # Weighted sum of value vectors\n        attn_out = tf.matmul(attn_map, value)\n\n        # Final attention output\n        attn_out = self.attn_conv(attn_out)\n        return attn_out + inputs  # Add residual connection\n'''\n\n\nfrom tensorflow.keras import layers\n\ndef multi_kernel_groupwise_conv2(x, filters, groups=16, strides=1):\n    # 1x1 Group-wise Convolution\n    conv1x1 = layers.DepthwiseConv2D(kernel_size=1, strides=strides, padding=\"same\")(x)\n\n    # 3x3 Group-wise Convolution\n    conv3x3 = layers.DepthwiseConv2D(kernel_size=5, strides=strides, padding=\"same\")(x)\n\n    # 5x5 Group-wise Convolution\n    conv5x5 = layers.DepthwiseConv2D(kernel_size=7, strides=strides, padding=\"same\")(x)\n\n    # Depthwise 3x3 Group-wise Convolution\n    #depthwise3x3 = layers.DepthwiseConv2D(kernel_size=3, strides=strides, padding=\"same\")(x)\n    \n    # Concatenate all outputs along the channel axis\n    x1 = layers.Concatenate()([conv1x1, conv3x3, conv5x5])\n    x1 = layers.Conv2D(filters, kernel_size=1, groups=groups, padding=\"same\")(x1)\n\n    x = layers.Conv2D(filters=filters, kernel_size=(1, 1), strides=strides, padding='same')(x)\n    x = layers.Add()([x, x1])\n    x = layers.Activation('relu')(x)\n    \n    return x\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\nclass MultiKernelGroupwiseConv2(layers.Layer):\n    def __init__(self, filters, groups=16, strides=1, **kwargs):\n        super(MultiKernelGroupwiseConv2, self).__init__(**kwargs)\n        self.filters = filters\n        self.groups = groups\n        self.strides = strides\n\n        # 1x1 Group-wise Convolution\n        self.conv1x1 = layers.DepthwiseConv2D(kernel_size=1, strides=strides, padding=\"same\")\n\n        # 3x3 Group-wise Convolution\n        self.conv3x3 = layers.DepthwiseConv2D(kernel_size=5, strides=strides, padding=\"same\")\n\n        # 5x5 Group-wise Convolution\n        self.conv5x5 = layers.DepthwiseConv2D(kernel_size=7, strides=strides, padding=\"same\")\n\n        # Final 1x1 Group-wise Convolution\n        self.final_conv = layers.Conv2D(filters, kernel_size=1, groups=groups, padding=\"same\")\n\n        # Shortcut Path\n        self.shortcut_conv = layers.Conv2D(filters=filters, kernel_size=(1, 1), strides=strides, padding='same')\n\n        # Activation Function\n        self.activation = layers.Activation('relu')\n\n    def call(self, inputs):\n        conv1x1 = self.conv1x1(inputs)\n        conv3x3 = self.conv3x3(inputs)\n        conv5x5 = self.conv5x5(inputs)\n\n        # Concatenate along the channel axis\n        x1 = layers.Concatenate()([conv1x1, conv3x3, conv5x5])\n        x1 = self.final_conv(x1)\n\n        # Shortcut connection\n        x = self.shortcut_conv(inputs)\n        x = layers.Add()([x, x1])\n        x = self.activation(x)\n\n        return x\n\n\n# Custom Attention Block with Global Pooling\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\nclass GlobalMinPooling2D(layers.Layer):\n    def __init__(self, **kwargs):\n        super(GlobalMinPooling2D, self).__init__(**kwargs)\n\n    def call(self, inputs):\n        return tf.reduce_min(inputs, axis=[1, 2])\n\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0], input_shape[-1])\n\n    def get_config(self):\n        config = super(GlobalMinPooling2D, self).get_config()\n        return config\n\n\nclass AttentionBlock(layers.Layer):\n    def __init__(self, channels, reduction_ratio=8, use_scale = True, **kwargs):\n        super(AttentionBlock, self).__init__(**kwargs)\n        self.channels = channels\n        #inputs1, inputs2 = inputs\n\n        # Global Pooling (Average, Max, and Min)\n        self.gap = layers.GlobalAveragePooling2D()\n        self.gmp = layers.GlobalMaxPooling2D()\n        self.gmin = GlobalMinPooling2D()  #layers.Lambda(lambda x: tf.reduce_min(x, axis=[1, 2], keepdims=False))  # Min pooling\n\n        # Efficient Channel Attention (Replaces Dense Layer)\n        \n        #self.channel_attn = layers.DepthwiseConv2D(kernel_size=1)\n        #self.channel_attn_out = layers.DepthwiseConv2D(kernel_size=1)\n\n        # Spatial Attention (Lightweight Conv)\n        #self.spatial_attn = layers.Conv2D(1, kernel_size=3, padding=\"same\", activation=\"sigmoid\")\n\n        # Batch Normalization\n        self.batch_norm = layers.BatchNormalization()\n        \n        '''self.weight1 = self.add_weight(\n            shape=(1,1,1,1), initializer=initializers.HeNormal(), trainable=True, name=\"weight1\"\n        )\n        self.weight2 = self.add_weight(\n            shape=(1,1,1,1), initializer=initializers.RandomNormal(mean=0.0, stddev=0.02), trainable=True, name=\"weight2\"\n        )'''\n        \n        #self.weight1 = self.add_weight(shape=(1, 1, 1, 1),  initializer=tf.keras.initializers.HeNormal(), trainable=True, name='weight1', \n         #                                     constraint=tf.keras.constraints.MaxNorm(2.0))\n        \n        self.weight1 = self.add_weight(shape=(1, 1, 1, 1),  initializer=tf.keras.initializers.HeNormal(), trainable=True, name='weight2', \n                                              constraint=tf.keras.constraints.MaxNorm(2.0))\n\n        self.weight2 = self.add_weight(shape=(1, 1, 1, 1),  initializer=tf.keras.initializers.HeNormal(), trainable=True, name='weight3', \n                                              constraint=tf.keras.constraints.MaxNorm(2.0))\n\n        self.weight3 = self.add_weight(shape=(1, 1, 1, 1), initializer='ones', trainable=True, name='alpha2')\n        \n            \n        self.dropout = layers.Dropout(0.25)\n        self.bn = layers.LayerNormalization()\n\n        self.use_scale = use_scale\n        self.local_attn = DeeperGlobalLocalAttentionLayer1(units=self.channels, activation='sigmoid', \n                                                                              dropout_rate=0.2,  # You can adjust the dropout rate\n                                                                              use_scale=self.use_scale)\n\n        self.multi_kernel_layer = MultiKernelGroupwiseConv2(filters=self.channels, groups=16, strides=1)\n        #output = multi_kernel_layer(input_tensor)\n\n        \n    def call(self, inputs):\n        inputs1, inputs2 = inputs\n\n        \n        # Compute Global Average, Max, and Min Pooling\n        gap_out = self.gap(inputs1)\n        gmp_out = self.gmp(inputs1)\n        gmin_out = self.gmin(inputs1)\n\n        # Combine pooling features\n        combined_pool1 = gap_out + gmp_out + gmin_out\n        combined_pool2 = gmp_out - (gap_out - gmin_out)\n        \n        # Normalize for stability\n        #combined_pool1 = tf.nn.l2_normalize(combined_pool1, axis=-1)\n        \n        combined_pool1 = tf.expand_dims(tf.expand_dims(combined_pool1, 1), 1) \n        combined_pool2 = tf.expand_dims(tf.expand_dims(combined_pool2, 1), 1)\n\n        combined_pool1 = self.bn(combined_pool1)\n        combined_pool1 = self.dropout(combined_pool1)\n\n        combined_pool2 = self.bn(combined_pool2)\n        combined_pool2 = self.dropout(combined_pool2)\n        \n        combined_pool1 = combined_pool1 * self.weight1\n        combined_pool2 = combined_pool2 * self.weight1\n        combined_pool = combined_pool1 + combined_pool2\n\n\n\n        gap_out2 = self.gap(inputs2)\n        gmp_out2 = self.gmp(inputs2)\n        gmin_out2 = self.gmin(inputs2)\n\n        # Combine pooling features\n        combined_pool3 = gap_out2 + gmp_out2 + gmin_out2\n        combined_pool4 = gmp_out2 - (gap_out2 - gmin_out2)\n        \n        # Normalize for stability\n        #combined_pool1 = tf.nn.l2_normalize(combined_pool1, axis=-1)\n        \n        combined_pool3 = tf.expand_dims(tf.expand_dims(combined_pool3, 1), 1) \n        combined_pool4 = tf.expand_dims(tf.expand_dims(combined_pool4, 1), 1)\n\n        combined_pool3 = self.bn(combined_pool3)\n        combined_pool3 = self.dropout(combined_pool3)\n\n        combined_pool4 = self.bn(combined_pool4)\n        combined_pool4 = self.dropout(combined_pool4)\n        \n        combined_pool3 = combined_pool3 * self.weight1\n        combined_pool4 = combined_pool4 * self.weight1\n        combined_pool5 = combined_pool3 + combined_pool4\n\n        \n        \n\n        #combined_pool = tf.sigmoid(combined_pool1 + combined_pool2) \n        \n        #combined_pool = inputs * combined_pool * self.weight3\n\n        # Compute multiscale information for input modalitiy = 1\n        channel_attn = self.multi_kernel_layer(inputs1)  #multi_kernel_groupwise_conv2(inputs1, filters=self.channels, groups=16)  # Strides=1 to avoid mismatch   #self.channel_attn(inputs2)\n        gap = tf.reduce_mean(channel_attn, axis=[1, 2], keepdims=True)\n        gmp = tf.reduce_max(channel_attn, axis=[1, 2], keepdims=True)\n        gmp_min = tf.reduce_min(channel_attn, axis=[1, 2], keepdims=True)\n\n        pooled_features1 = gap + gmp + gmp_min\n        pooled_features2 = gmp - (gap - gmp_min)\n\n        pooled_features = pooled_features1 + pooled_features2\n\n        channel_attn = channel_attn + pooled_features \n        channel_attn = self.bn(channel_attn)\n        channel_attn = self.dropout(channel_attn)\n        channel_attn = channel_attn * self.weight2\n\n        # Compute multiscale information for input modalitiy = 2\n        channel_attn2 = self.multi_kernel_layer(inputs2)     #multi_kernel_groupwise_conv2(inputs2, filters=self.channels, groups=16)  # Strides=1 to avoid mismatch   #self.channel_attn_out(pooled_features)  # Sigmoid attention map\n        #print('channel_attn shape:', channel_attn.shape)\n\n        gap2 = tf.reduce_mean(channel_attn2, axis=[1, 2], keepdims=True)\n        gmp2 = tf.reduce_max(channel_attn2, axis=[1, 2], keepdims=True)\n        gmp_min2 = tf.reduce_min(channel_attn2, axis=[1, 2], keepdims=True)\n\n        pooled_features3 = gap2 + gmp2 + gmp_min2\n        pooled_features4 = gmp2 - (gap2 - gmp_min2)\n\n        pooled_features5 = pooled_features3 + pooled_features4\n        \n        channel_attn2 = channel_attn2 + pooled_features5 \n        channel_attn2 = self.bn(channel_attn2)\n        channel_attn2 = self.dropout(channel_attn2)\n        channel_attn2 = channel_attn2 * self.weight2\n\n\n        ## Cross-modal attention\n        cross_attention1 = tf.sigmoid(combined_pool + channel_attn2)\n        cross_attention2 = tf.sigmoid(combined_pool5 + channel_attn)\n\n        multi_modal_attn = tf.sigmoid(cross_attention1 + cross_attention2)\n        \n        local1 = self.local_attn(inputs1) \n        local2 = self.local_attn(inputs2)\n\n        \n        \n        \n        # Apply Cross Attention Maps\n        attention1 =  tf.sigmoid(multi_modal_attn + local2) \n        attention2 =  tf.sigmoid(multi_modal_attn + local1) \n        \n        att1 = inputs1 * attention2 * self.weight3\n        att2 = inputs2 * attention1 * self.weight3\n        \n        print('output attention 1 shape:', att1.shape)\n        print('output attention 2 shape:', att2.shape)\n\n        return att1, att2\n\n\n# Residual Attention Block (Using Spatial Attention)\n'''class ResidualAttentionBlock(layers.Layer):\n    def __init__(self, in_channels, **kwargs):\n        super(ResidualAttentionBlock, self).__init__(**kwargs)\n        self.in_channels = in_channels\n        self.attn_block = AttentionBlock(in_channels)\n\n        # Convolution layers for feature refinement\n        #self.dwc = layers.DepthwiseConv2D(kernel_size=(3, 3), padding=\"same\", depth_multiplier=1)\n        self.conv1 = layers.Conv2D(in_channels, kernel_size=1, padding=\"same\")\n        self.relu = layers.ReLU()\n        self.conv2 = layers.Conv2D(in_channels, kernel_size=1, padding=\"same\")\n\n    def call(self, inputs):\n        # Apply attention mechanism\n        attn_out = self.attn_block(inputs)\n\n        # Refining features\n        #x = self.dwc(attn_out)\n        x = self.conv1(attn_out)\n        x = self.relu(x)\n        #x = self.dwc(x)\n        x = self.conv2(x)\n\n        # Add residual connection\n        return inputs * tf.sigmoid(x + attn_out)  # Residual connection for better feature learning\n'''\n\n\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\n'''def multi_kernel_groupwise_conv1(x, filters, groups=8, strides=2):\n    \"\"\" Advanced Multi-Kernel Groupwise Convolution Block (Without Attention, Optimized) \"\"\"\n\n    # Multi-Kernel Parallel Convolutions with Different Receptive Fields\n    conv1x1 = layers.Conv2D(filters // 4, kernel_size=1, groups=groups, padding=\"same\")(x)\n    conv3x3 = layers.Conv2D(filters // 4, kernel_size=3, padding=\"same\")(x)\n    conv5x5 = layers.Conv2D(filters // 4, kernel_size=5, padding=\"same\")(x)\n\n    # Dilated Convolutions for Larger Receptive Field (Helps Capture Global Context)\n    conv_dilated = layers.Conv2D(filters // 4, kernel_size=3, dilation_rate=2, padding=\"same\")(x)\n\n    # Feature Fusion via Concatenation\n    x1 = layers.Concatenate()([conv1x1, conv3x3, conv5x5, conv_dilated])\n\n    # Channel Shuffle for Better Feature Mixing (Inspired by ShuffleNet)\n    def channel_shuffle(x, groups):\n        batch, height, width, channels = x.shape\n        x = tf.reshape(x, [-1, height, width, groups, channels // groups])\n        x = tf.transpose(x, [0, 1, 2, 4, 3])\n        x = tf.reshape(x, [-1, height, width, channels])\n        return x\n\n    x1 = layers.Lambda(lambda x: channel_shuffle(x, groups))(x1)\n\n    # Depthwise + Grouped Convolutions Hybrid (Efficient Feature Extraction)\n    x1 = layers.DepthwiseConv2D(kernel_size=3, padding=\"same\")(x1)\n    x1 = layers.Conv2D(filters, kernel_size=1, groups=groups, padding=\"same\")(x1)\n\n    # Strided Convolution for Downsampling Instead of Extra 1x1 Conv\n    x = layers.Conv2D(filters, kernel_size=5, strides=strides, padding=\"same\")(x)\n\n    # Residual Connection for Gradient Flow\n    x = layers.Add()([x, x1])\n    x = layers.Activation(\"relu\")(x)\n    \n    return x'''\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\n'''def multi_kernel_groupwise_conv1(x, filters, groups=8, strides=2):\n    \"\"\" Advanced Multi-Kernel Groupwise Convolution Block (Without Attention, Optimized) \"\"\"\n\n    # Multi-Kernel Parallel Convolutions with Different Receptive Fields\n    conv1x1 = layers.Conv2D(filters // 4, kernel_size=1, groups=groups, padding=\"same\")(x)\n    conv3x3 = layers.DepthwiseConv2D(kernel_size=3, padding=\"same\")(x)\n    conv5x5 = layers.DepthwiseConv2D(kernel_size=5, padding=\"same\")(x)\n\n    # Dilated Convolutions for Larger Receptive Field\n    conv_dilated = layers.DepthwiseConv2D(kernel_size=3, dilation_rate=2, padding=\"same\")(x)\n\n    # Feature Fusion via Concatenation\n    x1 = layers.Concatenate()([conv1x1, conv3x3, conv5x5, conv_dilated])\n\n    # Channel Shuffle for Better Feature Mixing\n    def channel_shuffle(x, groups):\n        batch, height, width, channels = x.shape\n        x = tf.reshape(x, [-1, height, width, groups, channels // groups])\n        x = tf.transpose(x, [0, 1, 2, 4, 3])\n        x = tf.reshape(x, [-1, height, width, channels])\n        return x\n\n    x1 = layers.Lambda(lambda x: channel_shuffle(x, groups))(x1)\n\n    # Depthwise + Grouped Convolutions Hybrid\n    x1 = layers.DepthwiseConv2D(kernel_size=3, padding=\"same\")(x1)\n    x1 = layers.Conv2D(filters, kernel_size=1, groups=groups, padding=\"same\")(x1)\n\n    # **FIX: Ensure x1 is downsampled before addition**\n    x1 = layers.DepthwiseConv2D(kernel_size=3, strides=strides, padding=\"same\")(x1)\n\n    # Strided Convolution for Downsampling\n    x = layers.DepthwiseConv2D(kernel_size=3, strides=strides, padding=\"same\")(x)\n\n    # **Now x and x1 have the same shape before addition**\n    x = layers.Add()([x, x1])\n    x = layers.Activation(\"relu\")(x)\n    \n    return x'''\n\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\ndef multi_kernel_groupwise_conv1(x, filters, groups=8, strides=2):\n    \"\"\" Advanced Multi-Kernel Groupwise Convolution Block (Without Attention, Optimized) \"\"\"\n\n    # Multi-Kernel Parallel Convolutions with Different Receptive Fields\n    conv1x1 = layers.Conv2D(filters // 4, kernel_size=1, padding=\"same\")(x)\n    conv3x3 = layers.DepthwiseConv2D(kernel_size=3, padding=\"same\")(x)\n    conv5x5 = layers.DepthwiseConv2D(kernel_size=5, padding=\"same\")(x)\n    conv_dilated = layers.DepthwiseConv2D(kernel_size=3, dilation_rate=2, padding=\"same\")(x)\n\n    # Feature Fusion via Concatenation\n    x1 = layers.Concatenate()([conv1x1, conv3x3, conv5x5, conv_dilated])\n\n    # Channel Shuffle for Better Feature Mixing\n    def channel_shuffle(x, groups):\n        batch, height, width, channels = tf.unstack(tf.shape(x))\n        x = tf.reshape(x, [-1, height, width, groups, channels // groups])\n        x = tf.transpose(x, [0, 1, 2, 4, 3])\n        x = tf.reshape(x, [-1, height, width, channels])\n        return x\n\n    x1 = layers.Lambda(lambda x: channel_shuffle(x, groups))(x1)\n\n    # Depthwise + Grouped Convolutions Hybrid\n    x1 = layers.DepthwiseConv2D(kernel_size=3, padding=\"same\")(x1)\n    x1 = layers.Conv2D(filters, kernel_size=1, padding=\"same\")(x1)\n\n    # Downsampling x1\n    x1 = layers.DepthwiseConv2D(kernel_size=3, strides=strides, padding=\"same\")(x1)\n\n    # **Fix: Ensure x has the same number of channels as x1**\n    x = layers.Conv2D(filters, kernel_size=1, padding=\"same\")(x)\n    x = layers.DepthwiseConv2D(kernel_size=3, strides=strides, padding=\"same\")(x)\n\n    # Residual Connection\n    x = layers.Add()([x, x1])\n    x = layers.Activation(\"relu\")(x)\n    \n    return x\n\n\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\ndef multi_kernel_groupwise_conv(x, filters, groups=8, strides=1, use_se=True):\n    # 1x1 Group-wise Convolution (Efficient Channel Mixing)\n    conv1x1 = layers.Conv2D(filters, kernel_size=1, groups=groups, strides=strides, padding=\"same\", use_bias=False)(x)\n    conv1x1 = layers.BatchNormalization()(conv1x1)\n\n    # Depthwise Convolutions (Multi-scale receptive fields)\n    conv3x3 = layers.DepthwiseConv2D(kernel_size=5, strides=strides, padding=\"same\", use_bias=False)(x)\n    conv3x3 = layers.BatchNormalization()(conv3x3)\n\n    conv5x5 = layers.DepthwiseConv2D(kernel_size=7, strides=strides, padding=\"same\", use_bias=False)(x)\n    conv5x5 = layers.BatchNormalization()(conv5x5)\n\n    # Concatenation and 1x1 Fusion\n    x1 = layers.Concatenate()([conv1x1, conv3x3, conv5x5])\n    x1 = layers.Conv2D(filters, kernel_size=1, groups=groups, padding=\"same\", use_bias=False)(x1)\n    x1 = layers.BatchNormalization()(x1)\n\n    # Optional: Squeeze-and-Excitation (SE) block for adaptive recalibration\n    if use_se:\n        se1 = layers.GlobalAveragePooling2D()(x1)\n        se2 = layers.GlobalMaxPooling2D()(x1)\n        se = se1 + se2\n        se = layers.Dense(filters // 16, activation='relu', use_bias=False)(se)\n        se = layers.Dense(filters, activation='sigmoid', use_bias=False)(se)\n        se = layers.Reshape((1, 1, filters))(se)\n        x1 = layers.Multiply()([x1, se])\n\n    # Residual Connection\n    x = layers.Conv2D(filters=filters, kernel_size=1, strides=strides, padding='same', use_bias=False)(x)\n    x = layers.BatchNormalization()(x)\n\n    x = layers.Add()([x, x1])\n    x = layers.Activation('gelu')(x)  # GELU activation for better convergence\n\n    return x\n\n\n\n# Usage example\n'''def RGSA(x, filters, strides=(1, 1), use_projection=False):\n    shortcut = x  # Save the original input for residual connection\n\n    x = multi_kernel_groupwise_conv(x, filters=filters, groups=16, strides=strides)\n    \n    # Normalization and Activation\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation('relu')(x)\n\n    # Second Depthwise and Pointwise Convolution\n    \n    x = layers.Conv2D(filters, kernel_size=(3, 3), padding=\"same\")(x)\n    x = layers.BatchNormalization()(x)\n\n    # Adjust Shortcut if Needed\n    if strides != (1, 1) or use_projection:\n        shortcut = layers.Conv2D(filters=filters, kernel_size=(1, 1), strides=strides, padding='same')(shortcut)\n        shortcut = layers.BatchNormalization()(shortcut)\n\n    # Residual Connection\n    x = layers.Add()([x, shortcut])\n\n    # Final Activation\n    x = layers.Activation('relu')(x)\n\n    return x'''\n\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\n'''def RGSA(x, filters, strides=(1, 1), use_projection=False):\n    shortcut = x  # Save the original input for residual connection\n\n    # Multi-Kernel Groupwise Convolution\n    x = multi_kernel_groupwise_conv(x, filters=filters, groups=16, strides=strides)\n\n    # Normalization and Activation\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation('relu')(x)\n\n    # Second Depthwise and Pointwise Convolution\n    #x = layers.DepthwiseConv2D(kernel_size=(3, 3), padding=\"same\")(x)  # Depthwise conv for spatial filtering\n    x = multi_kernel_groupwise_conv2(x, filters=filters, groups=16, strides=strides)\n    x = layers.Conv2D(filters, kernel_size=(1, 1), padding=\"same\")(x)  # Pointwise conv for channel mixing\n    x = layers.BatchNormalization()(x)\n\n    # Adjust Shortcut if Needed\n    if strides != (1, 1) or use_projection:\n        shortcut = layers.DepthwiseConv2D(kernel_size=(3, 3), strides=strides, padding='same')(shortcut)  # Efficient downsampling\n        shortcut = layers.Conv2D(filters, kernel_size=(1, 1), padding=\"same\")(shortcut)  # Channel projection\n        shortcut = layers.BatchNormalization()(shortcut)\n\n    # Residual Connection\n    x = layers.Add()([x, shortcut])\n\n    # Final Activation\n    x = layers.Activation('relu')(x)\n\n    return x'''\n\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\ndef RGSA(x, filters, strides=(1, 1), use_projection=False):\n    shortcut = x  # Save the original input for residual connection\n\n    # Multi-Kernel Groupwise Convolution (First Layer)\n    x = multi_kernel_groupwise_conv(x, filters=filters, groups=16, strides=strides)\n\n    # Normalization and Activation\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation('relu')(x)\n\n    # Second Depthwise and Pointwise Convolution\n    x = multi_kernel_groupwise_conv(x, filters=filters, groups=16, strides=(1, 1))  # Strides=1 to avoid mismatch\n    x = layers.Conv2D(filters, kernel_size=(1, 1), padding=\"same\")(x)  # Pointwise conv for channel mixing\n    x = layers.BatchNormalization()(x)\n\n    # Adjust Shortcut if Needed (Ensure Matching Shape)\n    if strides != (1, 1) or use_projection:\n        shortcut = layers.Conv2D(filters, kernel_size=(1, 1), strides=strides, padding=\"same\")(shortcut)  # Downsampling shortcut\n        shortcut = layers.BatchNormalization()(shortcut)\n\n    # Residual Connection (Ensure Same Shape)\n    x = layers.Add()([x, shortcut])\n\n    # Final Activation\n    x = layers.Activation('relu')(x)\n\n    return x\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def info_fusion(x_1, x_2, filter):\n    x11 = multi_kernel_groupwise_conv1(x_1, filters=filter, groups=16, strides=(2,2))\n    x21 = multi_kernel_groupwise_conv1(x_2, filters=filter, groups=16, strides=(2,2))\n\n    print('128 x11, x21 shape:',x11.shape, x21.shape)\n\n    '''x11 = multi_kernel_groupwise_conv(x11, filters=256, groups=16, strides=(2,2))\n    x21 = multi_kernel_groupwise_conv(x21, filters=256, groups=16, strides=(2,2))\n\n    print('256 x11, x21 shape:',x11.shape, x21.shape)\n\n    x11 = multi_kernel_groupwise_conv(x11, filters=512, groups=16, strides=(2,2))\n    x21 = multi_kernel_groupwise_conv(x21, filters=512, groups=16, strides=(2,2))\n\n    print('512 x11, x21 shape:',x11.shape, x21.shape)'''\n    return x11, x21\n\n\ndef residual_GLC_branch1(inputs1, inputs2):\n    \n    x1 = Conv2D(filters=64, kernel_size=(7, 7), strides=(2, 2), padding='same')(inputs1)\n    #x1 = DeeperAttentionLayer1(units=64, use_scale=True)(x1) ## MFA ####\n    #x1 = AttentionBlock(64)(x1)\n    x1 = BatchNormalization()(x1)\n    x1 = tf.keras.layers.Activation('relu')(x1)\n    x1 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x1)\n    \n    x2 = Conv2D(filters=64, kernel_size=(7, 7), strides=(2, 2), padding='same')(inputs2)\n    #x2 = DeeperAttentionLayer1(units=64, use_scale=True)(x2) ## MFA ####\n    #x2 = AttentionBlock(64)(x2)\n    x2 = BatchNormalization()(x2)\n    x2 = tf.keras.layers.Activation('relu')(x2)\n    x2 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x2)\n    \n\n    x1 = RGSA(x1, filters=64)\n    x1 = tf.keras.layers.Dropout(0.25)(x1, training = True)  ## MCD ####\n    #x1 = DeeperAttentionLayer1(units=64, use_scale=True)(x1) ## MFA ####\n    #x1 = AttentionBlock(64)(x1)\n\n    x2 = RGSA(x2, filters=64)\n    x2 = tf.keras.layers.Dropout(0.25)(x2, training = True)  ## MCD ####\n    #x2 = DeeperAttentionLayer1(units=64, use_scale=True)(x2)\n    #x2 = AttentionBlock(64)(x2)\n    \n    x_1, x_2 = AttentionBlock(channels = 64)([x1, x2])  ## MIFA ####\n\n    print('64 x_1, x_2 shape:',x_1.shape, x_2.shape)\n\n    \n    \n    x1 = RGSA(x_1, filters=64)\n    x1 = tf.keras.layers.Dropout(0.25)(x1, training = True)  ## MCD ####\n    #x1 = DeeperAttentionLayer1(units=64, use_scale=True)(x1) ## MFA ####\n    #x1 = AttentionBlock(64)(x1)\n    \n    x2 = RGSA(x_2, filters=64)\n    x2 = tf.keras.layers.Dropout(0.25)(x2, training = True)  ## MCD ####\n    #x2 = DeeperAttentionLayer1(units=64, use_scale=True)(x2)\n    #x2 = AttentionBlock(64)(x2)\n\n    x_11, x_22 = AttentionBlock(channels = 64)([x1, x2])  ## MIFA ####\n    print('x_11, x_22 shape:',x_11.shape, x_22.shape)\n\n    \n    x11, x21 = info_fusion(x_11, x_22, 128)\n    x11, x21 = info_fusion(x11, x21, 256)\n    x11, x21 = info_fusion(x11, x21, 512)\n    \n    \n    \n    x1 = RGSA(x_11, filters=128, strides=(2, 2), use_projection=True)\n    x1 = tf.keras.layers.Dropout(0.25)(x1, training = True)  ## MCD ####\n    #x1 = DeeperAttentionLayer1(units=128, use_scale=True)(x1) ## MFA ####\n    #x1 = AttentionBlock(128)(x1)\n\n    x2 = RGSA(x_22, filters=128, strides=(2, 2), use_projection=True)\n    x2 = tf.keras.layers.Dropout(0.25)(x2, training = True)  ## MCD ####\n    #x2 = DeeperAttentionLayer1(units=128, use_scale=True)(x2)\n    #x2 = AttentionBlock(128)(x2)\n\n    x_111, x_222 = AttentionBlock(channels = 128)([x1, x2])  ## MIFA ####\n    print('x_111, x_222 shape:',x_111.shape, x_222.shape)\n    \n    x1 = RGSA(x_111, filters=128)\n    x1 = tf.keras.layers.Dropout(0.25)(x1, training = True)  ## MCD ####\n    #x1 = DeeperAttentionLayer1(units=128, use_scale=True)(x1)\n    #x1 = AttentionBlock(128)(x1)\n  \n    x2 = RGSA(x_222, filters=128)\n    x2 = tf.keras.layers.Dropout(0.25)(x2, training = True)  ## MCD ####\n    #x2 = DeeperAttentionLayer1(units=128, use_scale=True)(x2)\n    #x2 = AttentionBlock(128)(x2)\n\n    x_1111, x_2222 = AttentionBlock(channels = 128)([x1, x2])  ## MIFA ####\n    print('x_1111, x_2222 shape:',x_1111.shape, x_2222.shape)\n\n\n    x111, x211 = info_fusion(x_1111, x_2222, 256)\n    x111, x211 = info_fusion(x111, x211, 512)\n    \n    \n    x1 = RGSA(x_1111, filters=256, strides=(2, 2), use_projection=True)\n    x1 = tf.keras.layers.Dropout(0.25)(x1, training = True)  ## MCD ####\n    #x1 = DeeperAttentionLayer1(units=256, use_scale=True)(x1)\n    #x1 = AttentionBlock(256)(x1)\n    \n    x2 = RGSA(x_2222, filters=256, strides=(2, 2), use_projection=True)\n    x2 = tf.keras.layers.Dropout(0.25)(x2, training = True)  ## MCD ####\n    #x2 = DeeperAttentionLayer1(units=256, use_scale=True)(x2)\n    #x2 = AttentionBlock(256)(x2)\n\n    x_11111, x_22222 = AttentionBlock(channels = 256)([x1, x2])  ## MIFA ####\n    print('x_11111, x_22222 shape:',x_11111.shape, x_22222.shape)\n    \n    \n    x1 = RGSA(x_11111, filters=256)\n    x1 = tf.keras.layers.Dropout(0.25)(x1, training = True)  ## MCD ####\n    #x1 = DeeperAttentionLayer1(units=256, use_scale=True)(x1)\n    #x1 = AttentionBlock(256)(x1)\n    \n    x2 = RGSA(x_22222, filters=256)\n    x2 = tf.keras.layers.Dropout(0.25)(x2, training = True)  ## MCD ####\n    #x2 = DeeperAttentionLayer1(units=256, use_scale=True)(x2)\n    #x2 = AttentionBlock(256)(x2)\n    \n    x_12, x_21 = AttentionBlock(channels = 256)([x1, x2])  ## MIFA ####\n    print('x_12, x_21 shape:',x_12.shape, x_21.shape)\n\n    x1111, x2111 = info_fusion(x_12, x_21, 512)\n    \n\n    x1 = RGSA(x_12, filters=512, strides=(2, 2), use_projection=True)\n    #x1 = DeeperAttentionLayer1(units=512, use_scale=True)(x1)\n    #x1 = AttentionBlock(512)(x1)\n    \n    \n    x2 = RGSA(x_21, filters=512, strides=(2, 2), use_projection=True)\n    #x2 = DeeperAttentionLayer1(units=512, use_scale=True)(x2)\n    #x2 = AttentionBlock(512)(x2)\n\n    x_112, x_211 = AttentionBlock(channels = 512)([x1, x2])  ## MIFA ####\n    print('x_112, x_211 shape:',x_112.shape, x_211.shape)\n    \n    x1 = RGSA(x_112, filters=512)\n    x2 = RGSA(x_211, filters=512)\n    x_1112, x_2111 = AttentionBlock(channels = 512)([x1, x2])\n    print('x_1112, x_2111 shape:',x_1112.shape, x_2111.shape)\n\n    # x11, x21, x111, x211, x1111, x2111, x_1112, x_2111\n\n    x__1 = tf.keras.layers.Concatenate(axis=-1)([x11, x111, x1111, x_1112])\n    x__1 = tf.keras.layers.Dropout(0.25)(x__1, training = True)  ## MCD ####\n    x__2 = tf.keras.layers.Concatenate(axis=-1)([x21, x211, x2111, x_2111])\n    x__2 = tf.keras.layers.Dropout(0.25)(x__2, training = True)  ## MCD ####\n    \n    return x__1, x__2","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#def build_resnet18(input_shape=(128, 128, 3), num_classes=2):\ninput_shape=(128, 128, 3)\ninputs1 = Input(shape=input_shape)\ninputs2 = Input(shape=input_shape)\n\nimport tensorflow.keras.layers as L\n\n#input_data = Input(shape=input_shape, name='input_data')\n# Initial convolutional layer\n\nx1, x2 = residual_GLC_branch1(inputs1, inputs2)\n#print('x:',x.shape)\n\ncon = tf.keras.layers.Concatenate(axis=-1)([x1, x2])\n\ncon = tf.keras.layers.Dropout(0.25)(con, training = True)  ## MCD ####\n\nx = GlobalAveragePooling2D()(con)\n#print('GlobalAveragePooling2D x:',x.shape)\n\noutputs1 = Dense(5, activation='softmax')(x)\noutputs2 = Dense(7, activation='softmax')(x)\n\n# Create the model\nmodel = Model([inputs1, inputs2], [outputs1, outputs2])\n#return model\nprint(model.summary())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\n\ninitial_gamma = 0.5\nlearning_rate = 1e-2\noptimizer = Adam(learning_rate=0.001)\n#opt = Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.9, epsilon=None, amsgrad=False)\n# Compile the model with the custom optimizer\nmodel.compile(optimizer=optimizer,\n              loss=['categorical_crossentropy', 'categorical_crossentropy'],\n              loss_weights=[initial_gamma, (1 -  initial_gamma)],\n              metrics=['accuracy', 'accuracy'])\n\n\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\ndef checkpoint_callback():\n\n    checkpoint_filepath = 'best1_model_cer_skin_lung.keras'\n\n    model_checkpoint_callback= ModelCheckpoint(filepath=checkpoint_filepath,\n                           save_weights_only=False,\n                           #frequency='epoch',\n                           monitor='val_loss',\n                           save_best_only=True,\n                            mode='min',\n                           verbose=1)\n\n    return model_checkpoint_callback\n\ndef early_stopping(patience):\n    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', restore_best_weights=True, patience=60, verbose=1)\n    return es_callback\n\n\n\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n                              patience=50, verbose = 1, min_lr=0.00001)\n\ncheckpoint_callback = checkpoint_callback()\n\nearly_stopping = early_stopping(patience=100)\ncallbacks = [checkpoint_callback, early_stopping, reduce_lr]\n            \n\n# Fit the model with callbacks\nhistory = model.fit([X_train_s, X_train_h], [y_train_s, y_train_h],\n                    epochs=200,\n                    validation_data=([X_val_s, X_val_h1], [y_val_s, y_val_h1]), verbose=1,\n                    shuffle=True,\n                    callbacks=callbacks) # UpdateGammaCallback","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.evaluate([X_test_s, X_test_h1], [y_test_s, y_test_h1])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit([X_train_s, X_train_h], [y_train_s, y_train_h],\n                    epochs=200,\n                    validation_data=([X_val_s, X_val_h1], [y_val_s, y_val_h1]), verbose=1,\n                    shuffle=True,\n                    callbacks=callbacks) # UpdateGammaCallback","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.evaluate([X_test_s, X_test_h1], [y_test_s, y_test_h1])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#def build_resnet18(input_shape=(128, 128, 3), num_classes=2):\ninput_shape=(128, 128, 3)\ninputs1 = Input(shape=input_shape)\ninputs2 = Input(shape=input_shape)\n\nimport tensorflow.keras.layers as L\n\n#input_data = Input(shape=input_shape, name='input_data')\n# Initial convolutional layer\n\nx1, x2 = residual_GLC_branch1(inputs1, inputs2)\n#print('x:',x.shape)\n\ncon = tf.keras.layers.Concatenate(axis=-1)([x1, x2])\n\ncon = tf.keras.layers.Dropout(0.25)(con, training = True)  ## MCD ####\n\nx = GlobalAveragePooling2D()(con)\n#print('GlobalAveragePooling2D x:',x.shape)\n\noutputs1 = Dense(5, activation='softmax')(x)\noutputs2 = Dense(7, activation='softmax')(x)\n\n# Create the model\nmodel = Model([inputs1, inputs2], [outputs1, outputs2])\n#return model\nprint(model.summary())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\n\ninitial_gamma = 0.5\nlearning_rate = 1e-2\noptimizer = Adam(learning_rate=0.001)\n#opt = Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.9, epsilon=None, amsgrad=False)\n# Compile the model with the custom optimizer\nmodel.compile(optimizer=optimizer,\n              loss=['categorical_crossentropy', 'categorical_crossentropy'],\n              loss_weights=[initial_gamma, (1 -  initial_gamma)],\n              metrics=['accuracy', 'accuracy'])\n\n\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\ndef checkpoint_callback():\n\n    checkpoint_filepath = 'best1_model_cer_skin_lung.keras'\n\n    model_checkpoint_callback= ModelCheckpoint(filepath=checkpoint_filepath,\n                           save_weights_only=False,\n                           #frequency='epoch',\n                           monitor='val_loss',\n                           save_best_only=True,\n                            mode='min',\n                           verbose=1)\n\n    return model_checkpoint_callback\n\ndef early_stopping(patience):\n    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', restore_best_weights=True, patience=60, verbose=1)\n    return es_callback\n\n\n\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n                              patience=50, verbose = 1, min_lr=0.00001)\n\ncheckpoint_callback = checkpoint_callback()\n\nearly_stopping = early_stopping(patience=100)\ncallbacks = [checkpoint_callback, early_stopping, reduce_lr]\n            \n\n# Fit the model with callbacks\nhistory = model.fit([X_train_s, X_train_h], [y_train_s, y_train_h],\n                    epochs=200,\n                    validation_data=([X_val_s, X_val_h1], [y_val_s, y_val_h1]), verbose=1,\n                    shuffle=True,\n                    callbacks=callbacks) # UpdateGammaCallback","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.evaluate([X_test_s, X_test_h1], [y_test_s, y_test_h1])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit([X_train_s, X_train_h], [y_train_s, y_train_h],\n                    epochs=200,\n                    validation_data=([X_val_s, X_val_h1], [y_val_s, y_val_h1]), verbose=1,\n                    shuffle=True,\n                    callbacks=callbacks) # UpdateGammaCallback","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.evaluate([X_test_s, X_test_h1], [y_test_s, y_test_h1])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\n\ninitial_gamma = 0.5\nlearning_rate = 1e-2\noptimizer = Adam(learning_rate=0.001)\n#opt = Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.9, epsilon=None, amsgrad=False)\n# Compile the model with the custom optimizer\nmodel.compile(optimizer=optimizer,\n              loss=['categorical_crossentropy', 'categorical_crossentropy'],\n              loss_weights=[initial_gamma, (1 -  initial_gamma)],\n              metrics=['accuracy', 'accuracy'])\n\n\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\ndef checkpoint_callback():\n\n    checkpoint_filepath = 'best1_model_cer_skin_lung.keras'\n\n    model_checkpoint_callback= ModelCheckpoint(filepath=checkpoint_filepath,\n                           save_weights_only=False,\n                           #frequency='epoch',\n                           monitor='val_loss',\n                           save_best_only=True,\n                            mode='min',\n                           verbose=1)\n\n    return model_checkpoint_callback\n\ndef early_stopping(patience):\n    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', restore_best_weights=True, patience=60, verbose=1)\n    return es_callback\n\n\n\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n                              patience=50, verbose = 1, min_lr=0.00001)\n\ncheckpoint_callback = checkpoint_callback()\n\nearly_stopping = early_stopping(patience=100)\ncallbacks = [checkpoint_callback, early_stopping, reduce_lr]\n            \n\n# Fit the model with callbacks\nhistory = model.fit([X_train_s, X_train_h], [y_train_s, y_train_h],\n                    epochs=200,\n                    validation_data=([X_val_s, X_val_h1], [y_val_s, y_val_h1]), verbose=1,\n                    shuffle=True,\n                    callbacks=callbacks) # UpdateGammaCallback","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\n\ninitial_gamma = 0.5\nlearning_rate = 1e-2\noptimizer = Adam(learning_rate=0.001)\n#opt = Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.9, epsilon=None, amsgrad=False)\n# Compile the model with the custom optimizer\nmodel.compile(optimizer=optimizer,\n              loss=['categorical_crossentropy', 'categorical_crossentropy'],\n              loss_weights=[initial_gamma, (1 -  initial_gamma)],\n              metrics=['accuracy', 'accuracy'])\n\n\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\ndef checkpoint_callback():\n\n    checkpoint_filepath = 'best1_model_cer_skin_lung.keras'\n\n    model_checkpoint_callback= ModelCheckpoint(filepath=checkpoint_filepath,\n                           save_weights_only=False,\n                           #frequency='epoch',\n                           monitor='val_loss',\n                           save_best_only=True,\n                            mode='min',\n                           verbose=1)\n\n    return model_checkpoint_callback\n\ndef early_stopping(patience):\n    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', restore_best_weights=True, patience=60, verbose=1)\n    return es_callback\n\n\n\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n                              patience=50, verbose = 1, min_lr=0.00001)\n\ncheckpoint_callback = checkpoint_callback()\n\nearly_stopping = early_stopping(patience=100)\ncallbacks = [checkpoint_callback, early_stopping, reduce_lr]\n            \n\n# Fit the model with callbacks\nhistory = model.fit([X_train_s, X_train_h], [y_train_s, y_train_h],\n                    epochs=200,\n                    validation_data=([X_val_s, X_val_h1], [y_val_s, y_val_h1]), verbose=1,\n                    shuffle=True,\n                    callbacks=callbacks) # UpdateGammaCallback","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit([X_train_s, X_train_h], [y_train_s, y_train_h],\n                    epochs=200,\n                    validation_data=([X_val_s, X_val_h1], [y_val_s, y_val_h1]), verbose=1,\n                    shuffle=True,\n                    callbacks=callbacks) # UpdateGammaCallback","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.evaluate([X_test_s, X_test_h1], [y_test_s, y_test_h1])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit([X_train_s, X_train_h], [y_train_s, y_train_h],\n                    epochs=200,\n                    validation_data=([X_val_s, X_val_h1], [y_val_s, y_val_h1]), verbose=1,\n                    shuffle=True,\n                    callbacks=callbacks) # UpdateGammaCallback","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.evaluate([X_test_s, X_test_h1], [y_test_s, y_test_h1])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''random_indices = np.random.choice(2430, 2003, replace=False)\n\nX_test_s1 = X_test_s1[random_indices]\ny_test_s1 = y_test_s1[random_indices]\n\nX_test_s1.shape, y_test_s1.shape, X_test_s.shape, y_test_s.shape'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''#X_train.shape, y_train.shape, X_test.shape, y_test.shape, \nX_train_s.shape,X_test_s.shape, y_train_s.shape,y_test_s.shape, X_test_s1.shape, y_test_s1.shape'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''print(X_train_h.shape, y_train_h.shape, X_test_h.shape, y_test_h.shape,\n#X_train.shape, y_train.shape, X_test.shape, y_test.shape,\nX_train_s.shape,X_test_s.shape, X_test_s1.shape, y_train_s.shape,y_test_s.shape, y_test_s1.shape)'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Multi-branch fusion attention (MFA) module**","metadata":{}},{"cell_type":"code","source":"#### Multi-branch fusion attention (MFA) module #####\n\nclass DeeperGlobalLocalAttentionLayer1(layers.Layer):\n    def __init__(self, units, activation='sigmoid', dropout_rate=0.2, use_scale=True, axis=-1, **kwargs):\n        super(DeeperGlobalLocalAttentionLayer1, self).__init__(**kwargs)\n        self.units = units\n        self.activation = activation\n        self.dropout_rate = dropout_rate\n        self.use_scale = use_scale\n        self.axis = axis\n\n    def build(self, input_shape):\n        _, _, _, channels = input_shape\n        self.global_conv1 = layers.Conv2D(filters=self.units, kernel_size=(1, 1), activation=self.activation)\n        self.global_avg_pooling1 = layers.GlobalAveragePooling2D()\n        \n        self.global_conv2 = layers.Conv2D(filters=self.units, kernel_size=(1, 1), activation=self.activation)\n        self.global_avg_pooling2 = layers.GlobalMaxPooling2D()\n        \n        self.global_conv3 = layers.Conv2D(filters=self.units, kernel_size=(1, 1), activation=self.activation)\n        self.global_avg_pooling3 = layers.GlobalAveragePooling2D()\n        \n        self.global_conv4 = layers.Conv2D(filters=self.units, kernel_size=(1, 1), activation=self.activation)\n        self.global_avg_pooling4 = layers.GlobalMaxPooling2D()\n        \n        self.concat1 = layers.Add()\n        self.concat2 = layers.Add()\n        self.concat3 = layers.Add()\n        self.concat4 = layers.Add()\n        self.concat5 = layers.Concatenate(axis=-1)\n        \n        self.global_attention = layers.Dense(units=self.units, activation=self.activation)\n        \n        self.local_conv1 = layers.Conv2D(filters=self.units, kernel_size=(1, 1), activation=self.activation)\n        self.local_conv2 = layers.Conv2D(filters=self.units, kernel_size=(1, 1), activation=self.activation)\n        self.concat6 = layers.Add()\n        \n        if self.use_scale:\n            self.global_scale = self.add_weight(shape=(1, 1, 1, 1), initializer='ones', trainable=True, name='global_scale')\n            self.local_scale = self.add_weight(shape=(1, 1, 1, self.units), initializer='ones', trainable=True, name='local_scale')\n        \n        super(DeeperGlobalLocalAttentionLayer1, self).build(input_shape)\n\n    def call(self, inputs, training=None):\n        ##### Hierarchical Information Fusion Attention(HIFA) ######\n        \n        global_attention1 = self.global_conv1(inputs)\n        global_avg1 = self.global_avg_pooling1(global_attention1)\n        \n        global_attention2 = self.global_conv2(global_attention1)\n        global_avg2 = self.global_avg_pooling2(global_attention2)\n        \n        global_concat1 = self.concat1([global_avg1, global_avg2])\n        global_attention_concat1 = self.concat2([global_attention1, global_attention2])\n        \n        global_attention3 = self.global_conv3(global_attention_concat1)\n        global_avg3 = self.global_avg_pooling3(global_attention3)\n        \n        global_attention4 = self.global_conv4(global_attention3)\n        global_avg4 = self.global_avg_pooling4(global_attention4)\n        \n        global_concat2 = self.concat3([global_avg3, global_avg4])\n        global_attention_concat2 = self.concat4([global_attention3, global_attention4])\n        \n        global_avg_concat = self.concat5([global_concat1, global_concat2])\n        \n        global_attention = self.global_attention(global_avg_concat)\n        global_attention = tf.expand_dims(tf.expand_dims(global_attention, 1), 1)\n\n        ##### Channel-wise Local Information Attention (CLIA) ######\n        \n        local_attention1 = self.local_conv1(inputs)\n        local_attention1 = tf.reduce_mean(local_attention1, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        local_attention2 = self.local_conv2(local_attention1)\n        local_attention2 = tf.reduce_mean(local_attention2, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        \n        local_attention = self.concat6([local_attention1, local_attention2])\n        \n        # Scale Global and Local Attention\n        if self.use_scale:\n            global_attention *= self.global_scale\n            local_attention *= self.local_scale\n\n        # Combine Global and Local Attention\n        attention = tf.sigmoid(global_attention + local_attention)\n        return attention\n\n    def get_config(self):\n        config = super(DeeperGlobalLocalAttentionLayer1, self).get_config()\n        config.update({'units': self.units, 'activation': self.activation, 'dropout_rate': self.dropout_rate,\n                       'use_scale': self.use_scale})\n        return config\n\nclass DeeperAttentionLayer1(layers.Layer):\n    def __init__(self, units=64, use_scale=True, **kwargs):\n        super(DeeperAttentionLayer1, self).__init__(**kwargs)\n        self.units = units\n        self.use_scale = use_scale\n\n    def build(self, input_shape):\n        _, H, W, C = input_shape\n        self.alpha = self.add_weight(shape=(1, 1, 1, C), initializer='ones', trainable=True, name='alpha')\n        self.deeper_global_local_attention = DeeperGlobalLocalAttentionLayer1(units=self.units, activation='sigmoid', \n                                                                              dropout_rate=0.2,  # You can adjust the dropout rate\n                                                                              use_scale=self.use_scale)\n        super(DeeperAttentionLayer1, self).build(input_shape)\n\n    def call(self, inputs, training=None):\n        attention = self.deeper_global_local_attention(inputs, training=training)\n        attention_feature = inputs * attention * self.alpha\n        return attention_feature\n\n    def get_config(self):\n        config = super(DeeperAttentionLayer1, self).get_config()\n        config.update({'units': self.units, 'use_scale': self.use_scale})\n        return config\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Multimodal information fusion attention (MIFA)**","metadata":{}},{"cell_type":"code","source":"########## Multimodal information fusion attention (MIFA) ###############\n\n\n\nclass GlobalMinPooling2D(layers.Layer):\n    def __init__(self, **kwargs):\n        super(GlobalMinPooling2D, self).__init__(**kwargs)\n\n    def call(self, inputs):\n        return tf.reduce_min(inputs, axis=[1, 2])\n\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0], input_shape[-1])\n\n    def get_config(self):\n        config = super(GlobalMinPooling2D, self).get_config()\n        return config\n\n\nclass DeeperGlobalLocalAttentionLayer(layers.Layer):\n    def __init__(self, units, activation='sigmoid', dropout_rate=0.2, use_scale=True, axis=-1, **kwargs):\n        super(DeeperGlobalLocalAttentionLayer, self).__init__(**kwargs)\n        self.units = units\n        self.activation = activation\n        self.dropout_rate = dropout_rate\n        self.use_scale = use_scale\n        self.axis = axis\n\n    def build(self, input_shapes):\n        input_shape1, input_shape2 = input_shapes\n        _, _, _, channels1 = input_shape1\n        _, _, _, channels2 = input_shape2\n        \n        self.global_min_pooling1 = GlobalMinPooling2D()\n        self.global_avg_pooling1 = layers.GlobalAveragePooling2D()\n        self.global_max_pooling1 = layers.GlobalMaxPooling2D()\n        \n        self.global_attention = layers.Dense(units=self.units, activation=self.activation)\n        \n        self.global_min_pooling2 = GlobalMinPooling2D()\n        self.global_avg_pooling2 = layers.GlobalAveragePooling2D()\n        self.global_max_pooling2 = layers.GlobalMaxPooling2D()\n        \n        #self.global_attention2 = layers.Dense(units=self.units, activation=self.activation)\n        \n        \n        self.concat = layers.Add()\n        #self.global_attention3 = layers.Dense(units=self.units, activation=self.activation)\n        \n        self.local_conv1 = layers.Conv2D(filters=self.units, kernel_size=(1, 1), activation=self.activation)\n        self.local_conv2 = layers.Conv2D(filters=self.units, kernel_size=(1, 1), activation=self.activation)\n        \n        \n        \n        self.concat2 = layers.Add()\n        #self.local_conv5 = layers.Conv2D(filters=self.units, kernel_size=(1, 1), activation=self.activation)\n        \n        if self.use_scale:\n            self.global_scale = self.add_weight(shape=(1, 1, 1, 1), initializer='ones', trainable=True, name='global_scale')\n            self.local_scale = self.add_weight(shape=(1, 1, 1, self.units), initializer='ones', trainable=True, name='local_scale')\n        \n        super(DeeperGlobalLocalAttentionLayer, self).build(input_shapes)\n\n    def call(self, inputs, training=None):\n        inputs1, inputs2 = inputs\n\n        #########  Multimodal Global Information Fusion Attention (MGIFA) #########\n        global_min1 = self.global_min_pooling1(inputs1)\n        global_avg1 = self.global_avg_pooling1(inputs1)\n        global_max1 = self.global_max_pooling1(inputs1)\n\n        global_min2 = self.global_min_pooling2(inputs2)\n        global_avg2 = self.global_avg_pooling2(inputs2)\n        global_max2 = self.global_max_pooling2(inputs2)\n\n        concat_min = self.concat([global_min1, global_min2])\n        concat_avg = self.concat([global_avg1, global_avg2])\n        concat_max = self.concat([global_max1, global_max2])\n        \n        concat_min = self.global_attention(concat_min)\n        concat_avg = self.global_attention(concat_avg)\n        concat_max = self.global_attention(concat_max)\n        \n        concat_global_attention = self.concat([concat_min, concat_avg, concat_max])\n        \n        #global_attention = self.global_attention3(concat_global_attention)\n        \n        global_attention = tf.expand_dims(tf.expand_dims(concat_global_attention, 1), 1)\n\n        #########  Multimodal Local Information Fusion Attention (MLIFA) #########\n        \n        local_conv1 = self.local_conv1(inputs1)\n        local_min1 = tf.reduce_min(local_conv1, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        local_avg1 = tf.reduce_mean(local_conv1, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        local_max1 = tf.reduce_max(local_conv1, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        \n        local_conv2 = self.local_conv2(inputs2)\n        local_min2 = tf.reduce_min(local_conv2, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        local_avg2 = tf.reduce_mean(local_conv2, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        local_max2 = tf.reduce_max(local_conv2, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        \n        local_concat_min = self.concat2([local_min1, local_min2])\n        local_concat_avg = self.concat2([local_avg1, local_avg2])\n        local_concat_max = self.concat2([local_max1, local_max2])\n\n        local_attention = self.concat2([local_concat_min, local_concat_avg, local_concat_max])\n        \n        \n        # Scale Global and Local Attention\n        if self.use_scale:\n            global_attention *= self.global_scale\n            local_attention *= self.local_scale\n\n        # Combine Global and Local Attention\n        attention = tf.sigmoid(global_attention + local_attention)\n        return attention\n\n    def get_config(self):\n        config = super(DeeperGlobalLocalAttentionLayer, self).get_config()\n        config.update({'units': self.units, 'activation': self.activation, 'dropout_rate': self.dropout_rate,\n                       'use_scale': self.use_scale})\n        return config\n\nclass DeeperAttentionLayer(layers.Layer):\n    def __init__(self, units=64, use_scale=True,axis=-1, **kwargs):\n        super(DeeperAttentionLayer, self).__init__(**kwargs)\n        self.units = units\n        self.use_scale = use_scale\n        self.axis = axis \n\n    def build(self, input_shapes):\n        input_shape1, input_shape2 = input_shapes\n        _, H, W, C1 = input_shape1\n        _, H, W, C2 = input_shape2\n        \n        self.alpha1 = self.add_weight(shape=(1, 1, 1, C1), initializer='ones', trainable=True, name='alpha1')\n        self.alpha2 = self.add_weight(shape=(1, 1, 1, C2), initializer='ones', trainable=True, name='alpha2')\n        \n        self.deeper_global_local_attention = DeeperGlobalLocalAttentionLayer(units=self.units, activation='sigmoid', \n                                                                              dropout_rate=0.2,  # You can adjust the dropout rate\n                                                                              use_scale=self.use_scale)\n        #self.concat3 = layers.Add()\n        #self.concat4 = layers.Add()\n        \n        super(DeeperAttentionLayer, self).build(input_shapes)\n\n    def call(self, inputs, training=None):\n        inputs1, inputs2 = inputs\n        attention = self.deeper_global_local_attention([inputs1, inputs2], training=training)\n        \n        #inputs_concat = self.concat3([inputs1, inputs2])\n        #alpha_concat = self.concat4([self.alpha1, self.alpha2])\n        \n        attention_feature1 = inputs1 * attention * self.alpha1\n        attention_feature2 = inputs2 * attention * self.alpha2\n        \n        return attention_feature1, attention_feature2\n\n    def get_config(self):\n        config = super(DeeperAttentionLayer, self).get_config()\n        config.update({'units': self.units, 'use_scale': self.use_scale})\n        return config\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### RRA block ########\n\ndef RGSA(x, filters, strides=(1, 1), use_projection=False):\n    shortcut = x\n\n    # Define the first convolutional layer of the block\n    \n    x = Conv2D(filters=filters, kernel_size=(3, 3), strides=strides, padding='same', \n               #activation = 'relu'\n\n              )(x)\n    x = DeeperAttentionLayer1(units=filters, use_scale=True)(x)\n    x = BatchNormalization()(x)\n    x = tf.keras.layers.Activation('relu')(x)\n\n    # Define the second convolutional layer of the block\n    \n    x = Conv2D(filters=filters, kernel_size=(3, 3), padding='same')(x)\n    x = DeeperAttentionLayer1(units=filters, use_scale=True)(x)\n    \n    x = BatchNormalization()(x)\n\n    # If the stride is not (1, 1), the dimensions need to be adjusted\n    if strides != (1, 1) or use_projection:\n        \n        shortcut = Conv2D(filters=filters, kernel_size=(1, 1), strides=strides, padding='same')(shortcut)\n        shortcut = BatchNormalization()(shortcut)\n\n    # Add the shortcut (identity connection)\n    \n    x = tf.keras.layers.add([x, shortcut])\n    \n    x = tf.keras.layers.Activation('relu')(x)\n    return x\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**DRIFA-Net**","metadata":{}},{"cell_type":"code","source":"def residual_GLC_branch1(inputs1, inputs2):\n    \n    x1 = Conv2D(filters=64, kernel_size=(7, 7), strides=(2, 2), padding='same')(inputs1)\n    x1 = DeeperAttentionLayer1(units=64, use_scale=True)(x1) ## MFA ####\n    x1 = BatchNormalization()(x1)\n    x1 = tf.keras.layers.Activation('relu')(x1)\n    x1 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x1)\n    \n    x2 = Conv2D(filters=64, kernel_size=(7, 7), strides=(2, 2), padding='same')(inputs2)\n    x2 = DeeperAttentionLayer1(units=64, use_scale=True)(x2) ## MFA ####\n    x2 = BatchNormalization()(x2)\n    x2 = tf.keras.layers.Activation('relu')(x2)\n    x2 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x2)\n    \n\n    x1 = RGSA(x1, filters=64)\n    x1 = tf.keras.layers.Dropout(0.25)(x1, training = True)  ## MCD ####\n    x1 = DeeperAttentionLayer1(units=64, use_scale=True)(x1) ## MFA ####\n\n    x2 = RGSA(x2, filters=64)\n    x2 = tf.keras.layers.Dropout(0.25)(x2, training = True)  ## MCD ####\n    x2 = DeeperAttentionLayer1(units=64, use_scale=True)(x2)\n    \n    x1, x2 = DeeperAttentionLayer(units=64, use_scale=True)([x1, x2])  ## MIFA ####\n    \n    x1 = RGSA(x1, filters=64)\n    x1 = tf.keras.layers.Dropout(0.25)(x1, training = True)  ## MCD ####\n    x1 = DeeperAttentionLayer1(units=64, use_scale=True)(x1) ## MFA ####\n    \n    x2 = RGSA(x2, filters=64)\n    x2 = tf.keras.layers.Dropout(0.25)(x2, training = True)  ## MCD ####\n    x2 = DeeperAttentionLayer1(units=64, use_scale=True)(x2)\n\n    x1, x2 = DeeperAttentionLayer(units=64, use_scale=True)([x1, x2])  ## MIFA ####\n    \n    x1 = RGSA(x1, filters=128, strides=(2, 2), use_projection=True)\n    x1 = tf.keras.layers.Dropout(0.25)(x1, training = True)  ## MCD ####\n    x1 = DeeperAttentionLayer1(units=128, use_scale=True)(x1) ## MFA ####\n\n    x2 = RGSA(x2, filters=128, strides=(2, 2), use_projection=True)\n    x2 = tf.keras.layers.Dropout(0.25)(x2, training = True)  ## MCD ####\n    x2 = DeeperAttentionLayer1(units=128, use_scale=True)(x2)\n\n    x1, x2 = DeeperAttentionLayer(units=128, use_scale=True)([x1, x2])  ## MIFA ####\n    \n    x1 = RGSA(x1, filters=128)\n    x1 = tf.keras.layers.Dropout(0.25)(x1, training = True)  ## MCD ####\n    x1 = DeeperAttentionLayer1(units=128, use_scale=True)(x1)\n  \n    x2 = RGSA(x2, filters=128)\n    x2 = tf.keras.layers.Dropout(0.25)(x2, training = True)  ## MCD ####\n    x2 = DeeperAttentionLayer1(units=128, use_scale=True)(x2)\n\n    x1, x2 = DeeperAttentionLayer(units=128, use_scale=True)([x1, x2])  ## MIFA ####\n    \n    x1 = RGSA(x1, filters=256, strides=(2, 2), use_projection=True)\n    x1 = tf.keras.layers.Dropout(0.25)(x1, training = True)  ## MCD ####\n    x1 = DeeperAttentionLayer1(units=256, use_scale=True)(x1)\n    \n    x2 = RGSA(x2, filters=256, strides=(2, 2), use_projection=True)\n    x2 = tf.keras.layers.Dropout(0.25)(x2, training = True)  ## MCD ####\n    x2 = DeeperAttentionLayer1(units=256, use_scale=True)(x2)\n\n    x1, x2 = DeeperAttentionLayer(units=256, use_scale=True)([x1, x2])  ## MIFA ####\n    \n    \n    x1 = RGSA(x1, filters=256)\n    x1 = tf.keras.layers.Dropout(0.25)(x1, training = True)  ## MCD ####\n    x1 = DeeperAttentionLayer1(units=256, use_scale=True)(x1)\n    \n    x2 = RGSA(x2, filters=256)\n    x2 = tf.keras.layers.Dropout(0.25)(x2, training = True)  ## MCD ####\n    x2 = DeeperAttentionLayer1(units=256, use_scale=True)(x2)\n    \n    x1, x2 = DeeperAttentionLayer(units=256, use_scale=True)([x1, x2])  ## MIFA ####\n\n    x1 = RGSA(x1, filters=512, strides=(2, 2), use_projection=True)\n    x1 = DeeperAttentionLayer1(units=512, use_scale=True)(x1)\n    \n    x2 = RGSA(x2, filters=512, strides=(2, 2), use_projection=True)\n    x2 = DeeperAttentionLayer1(units=512, use_scale=True)(x2)\n\n    x1, x2 = DeeperAttentionLayer(units=512, use_scale=True)([x1, x2])  ## MIFA ####\n    \n    x1 = RGSA(x1, filters=512)\n    x2 = RGSA(x2, filters=512)\n    x1, x2 = DeeperAttentionLayer(units=512, use_scale=True)([x1, x2])\n    \n    return x1, x2","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#def build_resnet18(input_shape=(128, 128, 3), num_classes=2):\ninput_shape=(128, 128, 3)\ninputs1 = Input(shape=input_shape)\ninputs2 = Input(shape=input_shape)\n\n\n\n#input_data = Input(shape=input_shape, name='input_data')\n# Initial convolutional layer\n\nx1, x2 = residual_GLC_branch1(inputs1, inputs2)\n#print('x:',x.shape)\n\ncon = tf.keras.layers.Concatenate(axis=-1)([x1, x2])\n\ncon = tf.keras.layers.Dropout(0.25)(con, training = True)  ## MCD ####\n\nx = GlobalAveragePooling2D()(con)\nprint('GlobalAveragePooling2D x:',x.shape)\n\noutputs1 = Dense(5, activation='softmax')(x)\noutputs2 = Dense(7, activation='softmax')(x)\n\n# Create the model\nmodel = Model([inputs1, inputs2], [outputs1, outputs2])\n#return model\nprint(model.summary())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\n\ninitial_gamma = 0.5\n\noptimizer = Adam(learning_rate=0.001)\n# Compile the model with the custom optimizer\nmodel.compile(optimizer=optimizer,\n              loss=['categorical_crossentropy', 'categorical_crossentropy'],\n              loss_weights=[initial_gamma, (1 -  initial_gamma)],\n              metrics=['accuracy', 'accuracy'])\n\n\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\ndef checkpoint_callback():\n\n    checkpoint_filepath = 'best1_model_cer_skin_lung.keras'\n\n    model_checkpoint_callback= ModelCheckpoint(filepath=checkpoint_filepath,\n                           save_weights_only=False,\n                           #frequency='epoch',\n                           monitor='val_loss',\n                           save_best_only=True,\n                            mode='min',\n                           verbose=0)\n\n    return model_checkpoint_callback\n\ndef early_stopping(patience):\n    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, verbose=1)\n    return es_callback\n\n\n\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n                              patience=5, min_lr=0.00001)\n\ncheckpoint_callback = checkpoint_callback()\n\nearly_stopping = early_stopping(patience=100)\ncallbacks = [checkpoint_callback, early_stopping, reduce_lr]\n            \n\n# Fit the model with callbacks\nhistory = model.fit([X_train_s, X_train_h], [y_train_s, y_train_h],\n                    epochs=200,\n                    validation_split=0.2, verbose=1,\n                    shuffle=True,\n                    callbacks=callbacks) # UpdateGammaCallback\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.evaluate([X_test_s, X_test_h1], [y_test_s, y_test_h1])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nmodel.evaluate([X_test_s1, X_test_h], [y_test_s1, y_test_h])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, Model\nimport tensorflow.keras.backend as K\n\ndef dct_2d(x):\n    return tf.signal.dct(tf.signal.dct(x, type=2, norm='ortho'), type=2, norm='ortho', axis=2)\n\nclass DCTAttentionNonLocalBlock(Model):\n    def __init__(self, in_channels, intermediate_channels, dct_threshold=0.1):\n        super(DCTAttentionNonLocalBlock, self).__init__()\n        self.in_channels = in_channels\n        self.intermediate_channels = intermediate_channels\n        self.dct_threshold = dct_threshold\n        \n        self.theta = layers.Conv2D(intermediate_channels, kernel_size=1, use_bias=False)\n        self.phi = layers.Conv2D(intermediate_channels, kernel_size=1, use_bias=False)\n        self.g = layers.Conv2D(intermediate_channels, kernel_size=1, use_bias=False)\n        self.W = layers.Conv2D(in_channels, kernel_size=1, use_bias=False)\n        \n        self.attn_refine = tf.keras.Sequential([\n            layers.Conv2D(in_channels, kernel_size=3, padding='same', use_bias=False),\n            layers.ReLU(),\n            layers.Conv2D(in_channels, kernel_size=3, padding='same', use_bias=False)\n        ])\n        \n        self.channel_match_conv = layers.Conv2D(intermediate_channels, kernel_size=1)\n        self.dct_attention_conv = layers.Conv2D(intermediate_channels, kernel_size=1)\n        self.reduce_channels_dct = layers.Conv2D(intermediate_channels, kernel_size=1)\n        \n        self.relative_bias_dct = tf.Variable(tf.zeros((1, 1, 1, 1)), trainable=True)\n        self.relative_bias_non_local = tf.Variable(tf.zeros((1, 1, 1, 1)), trainable=True)\n    \n    def dct_transform(self, inputs):\n        dct_input = dct_2d(inputs)\n        \n        gap = tf.reduce_mean(dct_input, axis=[1, 2], keepdims=True)\n        gmp = tf.reduce_max(dct_input, axis=[1, 2], keepdims=True)\n        gmp_min = tf.reduce_min(dct_input, axis=[1, 2], keepdims=True)\n        \n        pooled_features1 = gap + gmp + gmp_min\n        pooled_features2 = gmp - (gap - gmp_min)\n        pooled_features = pooled_features1 + pooled_features2\n        \n        return pooled_features\n    \n    def frequency_attention(self, dct_input):\n        threshold_idx = int(self.dct_threshold * tf.shape(dct_input)[-1])\n        mask = tf.ones_like(dct_input)\n        #mask = tf.tensor_scatter_nd_update(mask, [[..., threshold_idx:]], tf.zeros_like(dct_input[..., threshold_idx:]))\n        mask = tf.concat([tf.ones_like(dct_input[..., :threshold_idx]), tf.zeros_like(dct_input[..., threshold_idx:])], axis=-1)\n\n        return dct_input * mask\n\n    #@tf.function\n    def call(self, inputs):\n        shape = tf.shape(inputs)  # ✅ Fix: Store shape in a variable\n        batch_size = shape[0]\n        height = shape[1]\n        width = shape[2]\n        in_channels = shape[3]\n        \n        dct_input = self.dct_transform(inputs)\n    \n        '''def call(self, inputs):\n        batch_size, height, width, in_channels = tf.shape(inputs)\n        \n        dct_input = self.dct_transform(inputs)'''\n    \n        dct_attention = self.frequency_attention(dct_input) + self.relative_bias_dct\n        \n        theta_x = tf.reshape(self.theta(inputs), [batch_size, -1, self.intermediate_channels])\n        phi_x = tf.reshape(self.phi(inputs), [batch_size, -1, self.intermediate_channels])\n        g_x = tf.reshape(self.g(inputs), [batch_size, -1, self.intermediate_channels])\n        \n        f = tf.matmul(theta_x, phi_x, transpose_b=True)\n        f_softmax = tf.nn.softmax(f, axis=-1)\n        y = tf.matmul(f_softmax, g_x)\n        \n        y = tf.reshape(y, [batch_size, height, width, self.intermediate_channels])\n        \n        refined_attention = self.attn_refine(inputs) + self.relative_bias_non_local\n        refined_attention = self.channel_match_conv(refined_attention)\n        \n        scale_factor = tf.sigmoid(refined_attention)\n        refined_dct_attention = self.reduce_channels_dct(self.dct_transform(dct_attention))\n        dct_attention = self.reduce_channels_dct(dct_attention)\n      \n        \n        y1 = y + (y * scale_factor) + refined_attention\n        y2 = y - (y * scale_factor) - refined_attention\n        y = y1 + y2 + refined_dct_attention + dct_attention\n        \n        y = self.W(y)\n        return tf.nn.relu(inputs + y)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''### RRA block ########\n\ndef RGSA(x, filters, strides=(1, 1), use_projection=False):\n    shortcut = x\n\n    # Define the first convolutional layer of the block\n    \n    #x = Conv2D(filters=filters, kernel_size=(3, 3), strides=strides, padding='same')(x)\n    x = tf.keras.layers.DepthwiseConv2D(kernel_size=(3, 3), strides=strides,  padding=\"same\", depth_multiplier=1)(x)\n    x = Conv2D(filters, 1, padding=\"same\")(x)\n    \n    #x = DeeperAttentionLayer1(units=filters, use_scale=True)(x)\n    dct_attention_block = DCTAttentionNonLocalBlock(filters, filters/2)\n    x = dct_attention_block(x)\n    print('dct 1:',x.shape)\n    x = BatchNormalization()(x)\n    x = tf.keras.layers.Activation('relu')(x)\n\n    # Define the second convolutional layer of the block\n    \n    #x = Conv2D(filters=filters, kernel_size=(3, 3), padding='same')(x)\n    x = tf.keras.layers.DepthwiseConv2D(kernel_size=(3, 3), padding=\"same\", depth_multiplier=1)(x)\n    x = Conv2D(filters, 1, padding=\"same\")(x)\n    \n    #x = DeeperAttentionLayer1(units=filters, use_scale=True)(x)\n    dct_attention_block = DCTAttentionNonLocalBlock(filters, filters/2)\n    x = dct_attention_block(x)\n    print('dct 2:',x.shape)\n    \n    \n    x = BatchNormalization()(x)\n\n    # If the stride is not (1, 1), the dimensions need to be adjusted\n    if strides != (1, 1) or use_projection:\n        \n        shortcut = Conv2D(filters=filters, kernel_size=(1, 1), strides=strides, padding='same')(shortcut)\n        shortcut = BatchNormalization()(shortcut)\n\n    # Add the shortcut (identity connection)\n    \n    x = tf.keras.layers.add([x, shortcut])\n    \n    x = tf.keras.layers.Activation('relu')(x)\n    return x'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers\n\n# Define DCT Transformation\ndef dct_2d(x):\n    return tf.signal.dct(tf.signal.dct(x, type=2, norm='ortho'), type=2, norm='ortho', axis=2)\n\n# DCT Attention Block\ndef dct_transform(inputs):\n    dct_input = dct_2d(inputs)\n    gap = tf.reduce_mean(dct_input, axis=[1, 2], keepdims=True)\n    gmp = tf.reduce_max(dct_input, axis=[1, 2], keepdims=True)\n    gmp_min = tf.reduce_min(dct_input, axis=[1, 2], keepdims=True)\n    \n    pooled_features1 = gap + gmp + gmp_min\n    pooled_features2 = gmp - (gap - gmp_min)\n    pooled_features = pooled_features1 + pooled_features2\n    \n    return pooled_features\n\n# Frequency Attention\ndef frequency_attention(dct_input, dct_threshold):\n    threshold_idx = int(dct_threshold * tf.shape(dct_input)[-1])\n    mask = tf.concat([tf.ones_like(dct_input[..., :threshold_idx]), tf.zeros_like(dct_input[..., threshold_idx:])], axis=-1)\n    return dct_input * mask\n\n# Main DCT Attention Non-Local Block\ndef dct_attention_non_local_block(inputs, in_channels, intermediate_channels, dct_threshold=0.1):\n    batch_size, height, width, _ = tf.unstack(tf.shape(inputs))\n    \n    # Layers\n    theta = layers.Conv2D(intermediate_channels, kernel_size=1, use_bias=False)(inputs)\n    phi = layers.Conv2D(intermediate_channels, kernel_size=1, use_bias=False)(inputs)\n    g = layers.Conv2D(intermediate_channels, kernel_size=1, use_bias=False)(inputs)\n    W = layers.Conv2D(in_channels, kernel_size=1, use_bias=False)\n    \n    attn_refine = tf.keras.Sequential([\n        layers.Conv2D(in_channels, kernel_size=3, padding='same', use_bias=False),\n        layers.ReLU(),\n        layers.Conv2D(in_channels, kernel_size=3, padding='same', use_bias=False)\n    ])\n    \n    channel_match_conv = layers.Conv2D(intermediate_channels, kernel_size=1)\n    reduce_channels_dct = layers.Conv2D(intermediate_channels, kernel_size=1)\n    \n    relative_bias_dct = tf.Variable(tf.zeros((1, 1, 1, 1)), trainable=True)\n    relative_bias_non_local = tf.Variable(tf.zeros((1, 1, 1, 1)), trainable=True)\n    \n    dct_input = dct_transform(inputs)\n    dct_attention = frequency_attention(dct_input, dct_threshold) + relative_bias_dct\n    \n    theta_x = tf.reshape(theta, [batch_size, -1, intermediate_channels])\n    phi_x = tf.reshape(phi, [batch_size, -1, intermediate_channels])\n    g_x = tf.reshape(g, [batch_size, -1, intermediate_channels])\n    \n    f = tf.matmul(theta_x, phi_x, transpose_b=True)\n    f_softmax = tf.nn.softmax(f, axis=-1)\n    y = tf.matmul(f_softmax, g_x)\n    y = tf.reshape(y, [batch_size, height, width, intermediate_channels])\n    \n    refined_attention = attn_refine(inputs) + relative_bias_non_local\n    refined_attention = channel_match_conv(refined_attention)\n    \n    scale_factor = tf.sigmoid(refined_attention)\n    refined_dct_attention = reduce_channels_dct(dct_transform(dct_attention))\n    dct_attention = reduce_channels_dct(dct_attention)\n    \n    y1 = y + (y * scale_factor) + refined_attention\n    y2 = y - (y * scale_factor) - refined_attention\n    y = y1 + y2 + refined_dct_attention + dct_attention\n    \n    y = W(y)\n    return tf.nn.relu(inputs + y)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def dct_attention_non_local_block(inputs, in_channels, intermediate_channels, dct_threshold=0.1):\n    batch_size = tf.shape(inputs)[0]  # Extract batch size dynamically\n    height = tf.shape(inputs)[1]\n    width = tf.shape(inputs)[2]\n\n    # Layers\n    theta = layers.Conv2D(intermediate_channels, kernel_size=1, use_bias=False)(inputs)\n    phi = layers.Conv2D(intermediate_channels, kernel_size=1, use_bias=False)(inputs)\n    g = layers.Conv2D(intermediate_channels, kernel_size=1, use_bias=False)(inputs)\n    W = layers.Conv2D(in_channels, kernel_size=1, use_bias=False)\n\n    attn_refine = tf.keras.Sequential([\n        layers.Conv2D(in_channels, kernel_size=3, padding='same', use_bias=False),\n        layers.ReLU(),\n        layers.Conv2D(in_channels, kernel_size=3, padding='same', use_bias=False)\n    ])\n\n    channel_match_conv = layers.Conv2D(intermediate_channels, kernel_size=1)\n    reduce_channels_dct = layers.Conv2D(intermediate_channels, kernel_size=1)\n\n    relative_bias_dct = tf.Variable(tf.zeros((1, 1, 1, 1)), trainable=True)\n    relative_bias_non_local = tf.Variable(tf.zeros((1, 1, 1, 1)), trainable=True)\n\n    dct_input = dct_transform(inputs)\n    dct_attention = frequency_attention(dct_input, dct_threshold) + relative_bias_dct\n\n    theta_x = tf.reshape(theta, [batch_size, -1, intermediate_channels])\n    phi_x = tf.reshape(phi, [batch_size, -1, intermediate_channels])\n    g_x = tf.reshape(g, [batch_size, -1, intermediate_channels])\n\n    f = tf.matmul(theta_x, phi_x, transpose_b=True)\n    f_softmax = tf.nn.softmax(f, axis=-1)\n    y = tf.matmul(f_softmax, g_x)\n    y = tf.reshape(y, [batch_size, height, width, intermediate_channels])\n\n    refined_attention = attn_refine(inputs) + relative_bias_non_local\n    refined_attention = channel_match_conv(refined_attention)\n\n    scale_factor = tf.sigmoid(refined_attention)\n    refined_dct_attention = reduce_channels_dct(dct_transform(dct_attention))\n    dct_attention = reduce_channels_dct(dct_attention)\n\n    y1 = y + (y * scale_factor) + refined_attention\n    y2 = y - (y * scale_factor) - refined_attention\n    y = y1 + y2 + refined_dct_attention + dct_attention\n\n    y = W(y)\n    return tf.nn.relu(inputs + y)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers\n\n# Define DCT Transformation\ndef dct_2d(x):\n    return tf.signal.dct(tf.signal.dct(x, type=2, norm='ortho'), type=2, norm='ortho', axis=2)\n\n# DCT Attention Block\ndef dct_transform(inputs):\n    dct_input = dct_2d(inputs)\n    gap = tf.reduce_mean(dct_input, axis=[1, 2], keepdims=True)\n    gmp = tf.reduce_max(dct_input, axis=[1, 2], keepdims=True)\n    gmp_min = tf.reduce_min(dct_input, axis=[1, 2], keepdims=True)\n    \n    pooled_features1 = gap + gmp + gmp_min\n    pooled_features2 = gmp - (gap - gmp_min)\n    pooled_features = pooled_features1 + pooled_features2\n    \n    return pooled_features\n\n# Frequency Attention\ndef frequency_attention(dct_input, dct_threshold):\n    threshold_idx = int(dct_threshold * tf.shape(dct_input)[-1])\n    mask = tf.concat([tf.ones_like(dct_input[..., :threshold_idx]), tf.zeros_like(dct_input[..., threshold_idx:])], axis=-1)\n    return dct_input * mask\n\n# Main DCT Attention Non-Local Block\ndef dct_attention_non_local_block(inputs, in_channels, intermediate_channels, dct_threshold=0.1):\n    shape = tf.shape(inputs)\n    batch_size, height, width = shape[0], shape[1], shape[2]\n\n    # Layers\n    theta = layers.Conv2D(intermediate_channels, kernel_size=1, use_bias=False)(inputs)\n    phi = layers.Conv2D(intermediate_channels, kernel_size=1, use_bias=False)(inputs)\n    g = layers.Conv2D(intermediate_channels, kernel_size=1, use_bias=False)(inputs)\n    W = layers.Conv2D(in_channels, kernel_size=1, use_bias=False)\n\n    attn_refine = tf.keras.Sequential([\n        layers.Conv2D(in_channels, kernel_size=3, padding='same', use_bias=False),\n        layers.ReLU(),\n        layers.Conv2D(in_channels, kernel_size=3, padding='same', use_bias=False)\n    ])\n    \n    channel_match_conv = layers.Conv2D(intermediate_channels, kernel_size=1)\n    reduce_channels_dct = layers.Conv2D(intermediate_channels, kernel_size=1)\n\n    relative_bias_dct = tf.Variable(tf.zeros((1, 1, 1, 1)), trainable=True)\n    relative_bias_non_local = tf.Variable(tf.zeros((1, 1, 1, 1)), trainable=True)\n\n    dct_input = dct_transform(inputs)\n    dct_attention = frequency_attention(dct_input, dct_threshold) + relative_bias_dct\n\n    theta_x = tf.reshape(theta, [batch_size, height * width, intermediate_channels])\n    phi_x = tf.reshape(phi, [batch_size, height * width, intermediate_channels])\n    g_x = tf.reshape(g, [batch_size, height * width, intermediate_channels])\n\n    f = tf.matmul(theta_x, phi_x, transpose_b=True)\n    f_softmax = tf.nn.softmax(f, axis=-1)\n    y = tf.matmul(f_softmax, g_x)\n    y = tf.reshape(y, [batch_size, height, width, intermediate_channels])\n\n    refined_attention = attn_refine(inputs) + relative_bias_non_local\n    refined_attention = channel_match_conv(refined_attention)\n\n    scale_factor = tf.sigmoid(refined_attention)\n    refined_dct_attention = reduce_channels_dct(dct_transform(dct_attention))\n    dct_attention = reduce_channels_dct(dct_attention)\n\n    y1 = y + (y * scale_factor) + refined_attention\n    y2 = y - (y * scale_factor) - refined_attention\n    y = y1 + y2 + refined_dct_attention + dct_attention\n\n    y = W(y)\n    return tf.nn.relu(inputs + y)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Conv2D, DepthwiseConv2D, BatchNormalization, Activation, Add\n\ndef RGSA(x, filters, strides=(1, 1), use_projection=False):\n    shortcut = x  # Save the original input for residual connection\n\n    # First Depthwise and Pointwise Convolution\n    x = DepthwiseConv2D(kernel_size=(3, 3), strides=strides, padding=\"same\", depth_multiplier=1)(x)\n    x = Conv2D(filters, kernel_size=1, padding=\"same\")(x)\n    \n    # Apply DCT Attention Block\n    x = dct_attention_non_local_block(x, filters, filters)\n    print('DCT 1:', x.shape)\n\n    # Normalization and Activation\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    # Second Depthwise and Pointwise Convolution\n    x = DepthwiseConv2D(kernel_size=(3, 3), padding=\"same\", depth_multiplier=1)(x)\n    x = Conv2D(filters, kernel_size=1, padding=\"same\")(x)\n\n    # Apply DCT Attention Block Again\n    x = dct_attention_non_local_block(x, filters, filters)\n    print('DCT 2:', x.shape)\n\n    # Normalization\n    x = BatchNormalization()(x)\n\n    # Adjust Shortcut if Needed\n    if strides != (1, 1) or use_projection:\n        shortcut = Conv2D(filters=filters, kernel_size=(1, 1), strides=strides, padding='same')(shortcut)\n        shortcut = BatchNormalization()(shortcut)\n\n    # Residual Connection\n    x = Add()([x, shortcut])\n\n    # Final Activation\n    x = Activation('relu')(x)\n\n    return x\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers\n\n# Define DCT Transformation\ndef dct_2d(x):\n    return tf.signal.dct(tf.signal.dct(x, type=2, norm='ortho'), type=2, norm='ortho', axis=2)\n\n# DCT Attention Block\ndef dct_transform(inputs):\n    dct_input = dct_2d(inputs)\n    gap = tf.reduce_mean(dct_input, axis=[1, 2], keepdims=True)\n    gmp = tf.reduce_max(dct_input, axis=[1, 2], keepdims=True)\n    gmp_min = tf.reduce_min(dct_input, axis=[1, 2], keepdims=True)\n    \n    pooled_features1 = gap + gmp + gmp_min\n    pooled_features2 = gmp - (gap - gmp_min)\n    pooled_features = pooled_features1 + pooled_features2\n    \n    return pooled_features\n\n# Frequency Attention\ndef frequency_attention(dct_input, dct_threshold):\n    threshold_idx = int(dct_threshold * tf.shape(dct_input)[-1])\n    mask = tf.concat([tf.ones_like(dct_input[..., :threshold_idx]), tf.zeros_like(dct_input[..., threshold_idx:])], axis=-1)\n    return dct_input * mask\n\n# Main DCT Attention Non-Local Block\ndef dct_attention_non_local_block(inputs, in_channels, intermediate_channels, dct_threshold=0.1):\n    shape = tf.shape(inputs)\n    batch_size, height, width = shape[0], shape[1], shape[2]\n\n    # Layers\n    theta = layers.Conv2D(intermediate_channels, kernel_size=1, use_bias=False)(inputs)\n    phi = layers.Conv2D(intermediate_channels, kernel_size=1, use_bias=False)(inputs)\n    g = layers.Conv2D(intermediate_channels, kernel_size=1, use_bias=False)(inputs)\n    W = layers.Conv2D(in_channels, kernel_size=1, use_bias=False)\n\n    attn_refine = tf.keras.Sequential([\n        layers.Conv2D(in_channels, kernel_size=3, padding='same', use_bias=False),\n        layers.ReLU(),\n        layers.Conv2D(in_channels, kernel_size=3, padding='same', use_bias=False)\n    ])\n    \n    channel_match_conv = layers.Conv2D(intermediate_channels, kernel_size=1)\n    reduce_channels_dct = layers.Conv2D(intermediate_channels, kernel_size=1)\n\n    relative_bias_dct = tf.Variable(tf.zeros((1, 1, 1, 1)), trainable=True)\n    relative_bias_non_local = tf.Variable(tf.zeros((1, 1, 1, 1)), trainable=True)\n\n    dct_input = dct_transform(inputs)\n    dct_attention = frequency_attention(dct_input, dct_threshold) + relative_bias_dct\n\n    theta_x = tf.reshape(theta, [batch_size, height * width, intermediate_channels])\n    phi_x = tf.reshape(phi, [batch_size, height * width, intermediate_channels])\n    g_x = tf.reshape(g, [batch_size, height * width, intermediate_channels])\n\n    f = tf.matmul(theta_x, phi_x, transpose_b=True)\n    f_softmax = tf.nn.softmax(f, axis=-1)\n    y = tf.matmul(f_softmax, g_x)\n    y = tf.reshape(y, [batch_size, height, width, intermediate_channels])\n\n    refined_attention = attn_refine(inputs) + relative_bias_non_local\n    refined_attention = channel_match_conv(refined_attention)\n\n    scale_factor = tf.sigmoid(refined_attention)\n    refined_dct_attention = reduce_channels_dct(dct_transform(dct_attention))\n    dct_attention = reduce_channels_dct(dct_attention)\n\n    print('refined_dct_attention shape:', refined_dct_attention.shape)\n    print('dct_attention shape:', dct_attention.shape)\n    \n    y1 = y + (y * scale_factor) + refined_attention\n    y2 = y - (y * scale_factor) - refined_attention\n    y = y1 + y2 + refined_dct_attention + dct_attention\n\n    y = W(y)\n    return tf.nn.relu(inputs + y)\n\n# RGSA Block with DCT Attention\nfrom tensorflow.keras.layers import DepthwiseConv2D, BatchNormalization, Activation, Add\n\ndef RGSA(x, filters, strides=(1, 1), use_projection=False):\n    shortcut = x  # Save the original input for residual connection\n\n    # First Depthwise and Pointwise Convolution\n    x = DepthwiseConv2D(kernel_size=(3, 3), strides=strides, padding=\"same\", depth_multiplier=1)(x)\n    x = Conv2D(filters, kernel_size=1, padding=\"same\")(x)\n    \n    # Apply DCT Attention Block\n    x = dct_attention_non_local_block(x, filters, filters)\n    print('DCT 1:', x.shape)\n\n    # Normalization and Activation\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    # Second Depthwise and Pointwise Convolution\n    x = DepthwiseConv2D(kernel_size=(3, 3), padding=\"same\", depth_multiplier=1)(x)\n    x = Conv2D(filters, kernel_size=1, padding=\"same\")(x)\n\n    # Apply DCT Attention Block Again\n    x = dct_attention_non_local_block(x, filters, filters)\n    print('DCT 2:', x.shape)\n\n    # Normalization\n    x = BatchNormalization()(x)\n\n    # Adjust Shortcut if Needed\n    if strides != (1, 1) or use_projection:\n        shortcut = Conv2D(filters=filters, kernel_size=(1, 1), strides=strides, padding='same')(shortcut)\n        shortcut = BatchNormalization()(shortcut)\n\n    # Residual Connection\n    x = Add()([x, shortcut])\n\n    # Final Activation\n    x = Activation('relu')(x)\n\n    return x\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers\n\n\n\nclass DCTAttentionNonLocalBlock(layers.Layer):\n    def __init__(self, in_channels, intermediate_channels, dct_threshold=0.1):\n        super(DCTAttentionNonLocalBlock, self).__init__()\n        self.in_channels = in_channels\n        self.intermediate_channels = intermediate_channels\n        self.dct_threshold = dct_threshold\n        \n        # Define your layers here\n        self.theta = layers.Conv2D(intermediate_channels, kernel_size=1, use_bias=False)\n        self.phi = layers.Conv2D(intermediate_channels, kernel_size=1, use_bias=False)\n        self.g = layers.Conv2D(intermediate_channels, kernel_size=1, use_bias=False)\n        self.W = layers.Conv2D(in_channels, kernel_size=1, use_bias=False)\n        \n        self.attn_refine = tf.keras.Sequential([\n            layers.Conv2D(in_channels, kernel_size=3, padding='same', use_bias=False),\n            layers.ReLU(),\n            layers.Conv2D(in_channels, kernel_size=3, padding='same', use_bias=False)\n        ])\n        \n        self.channel_match_conv = layers.Conv2D(intermediate_channels, kernel_size=1)\n        self.reduce_channels_dct = layers.Conv2D(intermediate_channels, kernel_size=1)\n\n        self.relative_bias_dct = tf.Variable(tf.zeros((1, 1, 1, 1)), trainable=True)\n        self.relative_bias_non_local = tf.Variable(tf.zeros((1, 1, 1, 1)), trainable=True)\n\n    def call(self, inputs):\n        # Get the shape of the input\n        shape = tf.shape(inputs)\n        batch_size, height, width = shape[0], shape[1], shape[2]\n\n        # Process the inputs\n        dct_input = dct_transform(inputs)  # Apply DCT transform\n        dct_attention = frequency_attention(dct_input, self.dct_threshold) + self.relative_bias_dct\n\n        theta_x = tf.reshape(self.theta(inputs), [batch_size, height * width, self.intermediate_channels])\n        phi_x = tf.reshape(self.phi(inputs), [batch_size, height * width, self.intermediate_channels])\n        g_x = tf.reshape(self.g(inputs), [batch_size, height * width, self.intermediate_channels])\n\n        f = tf.matmul(theta_x, phi_x, transpose_b=True)\n        f_softmax = tf.nn.softmax(f, axis=-1)\n        y = tf.matmul(f_softmax, g_x)\n        y = tf.reshape(y, [batch_size, height, width, self.intermediate_channels])\n\n        refined_attention = self.attn_refine(inputs) + self.relative_bias_non_local\n        refined_attention = self.channel_match_conv(refined_attention)\n\n        scale_factor = tf.sigmoid(refined_attention)\n        refined_dct_attention = self.reduce_channels_dct(dct_transform(dct_attention))\n        dct_attention = self.reduce_channels_dct(dct_attention)\n\n        y1 = y + (y * scale_factor) + refined_attention\n        y2 = y - (y * scale_factor) - refined_attention\n        y = y1 + y2 + refined_dct_attention + dct_attention\n\n        y = self.W(y)\n        return tf.nn.relu(inputs + y)\n\n# Usage example\ndef RGSA(x, filters, strides=(1, 1), use_projection=False):\n    shortcut = x  # Save the original input for residual connection\n\n    # First Depthwise and Pointwise Convolution\n    x = layers.DepthwiseConv2D(kernel_size=(3, 3), strides=strides, padding=\"same\", depth_multiplier=1)(x)\n    x = layers.Conv2D(filters, kernel_size=1, padding=\"same\")(x)\n    \n    # Apply DCT Attention Block\n    x = DCTAttentionNonLocalBlock(filters, filters)(x)\n    print('DCT 1:', x.shape)\n\n    # Normalization and Activation\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation('relu')(x)\n\n    # Second Depthwise and Pointwise Convolution\n    x = layers.DepthwiseConv2D(kernel_size=(3, 3), padding=\"same\", depth_multiplier=1)(x)\n    x = layers.Conv2D(filters, kernel_size=1, padding=\"same\")(x)\n\n    # Apply DCT Attention Block Again\n    x = DCTAttentionNonLocalBlock(filters, filters)(x)\n    print('DCT 2:', x.shape)\n\n    # Normalization\n    x = layers.BatchNormalization()(x)\n\n    # Adjust Shortcut if Needed\n    if strides != (1, 1) or use_projection:\n        shortcut = layers.Conv2D(filters=filters, kernel_size=(1, 1), strides=strides, padding='same')(shortcut)\n        shortcut = layers.BatchNormalization()(shortcut)\n\n    # Residual Connection\n    x = layers.Add()([x, shortcut])\n\n    # Final Activation\n    x = layers.Activation('relu')(x)\n\n    return x\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers\n\n'''def dct_2d(x):\n    return tf.signal.dct(tf.signal.dct(x, type=2, norm='ortho'), type=2, norm='ortho', axis=2)\n'''\n\n'''def dct_2d(x):\n    \"\"\"\n    Applies the 2D Discrete Cosine Transform (DCT) along the spatial dimensions (height, width).\n    \"\"\"\n    batch_size, height, width, channels = tf.shape(x)[0], tf.shape(x)[1], tf.shape(x)[2], tf.shape(x)[3]\n\n    # Reshape to make height and width combined in the last dimension\n    x_reshaped = tf.reshape(x, (batch_size, height * width, channels))\n\n    # Apply DCT along the last dimension (axis=-1) for both height and width\n    dct_input = tf.signal.dct(x_reshaped, type=2, norm='ortho', axis=-1)\n\n    # Reshape back to the original shape\n    dct_input = tf.reshape(dct_input, (batch_size, height, width, channels))\n    \n    return dct_input'''\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, Input\n\n\nclass FrequencyTransformLayer(layers.Layer):\n    def __init__(self, transform_type='dct', **kwargs):\n        super(FrequencyTransformLayer, self).__init__(**kwargs)\n        self.transform_type = transform_type\n\n    def call(self, inputs):\n        # Ensure inputs are float32 for DCT and FFT operations\n        inputs = tf.cast(inputs, tf.float32)\n\n        if self.transform_type == 'dct':\n            # Apply 2D DCT along the last axis\n            input_shape = tf.shape(inputs)\n            flattened_inputs = tf.reshape(inputs, [-1, input_shape[-1]])  # Flatten along the last axis\n            dct_transformed = tf.signal.dct(flattened_inputs, type=2, norm='ortho')\n            return tf.reshape(dct_transformed, input_shape)  # Reshape back to original dimensions\n        elif self.transform_type == 'fft':\n            # Apply 2D FFT and return the magnitude\n            fft_transformed = tf.signal.fft2d(tf.cast(inputs, tf.complex64))\n            return tf.math.abs(fft_transformed)\n        else:\n            raise ValueError(\"Unsupported transform type. Choose 'dct' or 'fft'.\")\n\n    def get_config(self):\n        config = super(FrequencyTransformLayer, self).get_config()\n        config.update({'transform_type': self.transform_type})\n        return config\n\n    \n# DCT Attention Block\ndef dct_transform(inputs):\n    #self.dct_transform = FrequencyTransformLayer(transform_type='dct')\n    dct_input = FrequencyTransformLayer(transform_type='dct')(inputs)\n    #dct_input = dct_2d(inputs)\n    gap = tf.reduce_mean(dct_input, axis=[1, 2], keepdims=True)\n    gmp = tf.reduce_max(dct_input, axis=[1, 2], keepdims=True)\n    gmp_min = tf.reduce_min(dct_input, axis=[1, 2], keepdims=True)\n    \n    pooled_features1 = gap + gmp + gmp_min\n    pooled_features2 = gmp - (gap - gmp_min)\n    pooled_features = pooled_features1 + pooled_features2\n    \n    return pooled_features\n# Frequency Attention\ndef frequency_attention(dct_input, dct_threshold):\n    threshold_idx = int(dct_threshold * tf.shape(dct_input)[-1])\n    mask = tf.concat([tf.ones_like(dct_input[..., :threshold_idx]), tf.zeros_like(dct_input[..., threshold_idx:])], axis=-1)\n    return dct_input * mask\n\n\nclass DCTAttentionNonLocalBlock(layers.Layer):\n    def __init__(self, in_channels, intermediate_channels, dct_threshold=0.1):\n        super(DCTAttentionNonLocalBlock, self).__init__()\n        self.in_channels = in_channels\n        self.intermediate_channels = intermediate_channels\n        self.dct_threshold = dct_threshold\n        \n        # Define your layers here\n        self.theta = layers.Conv2D(intermediate_channels, kernel_size=1, use_bias=False)\n        self.phi = layers.Conv2D(intermediate_channels, kernel_size=1, use_bias=False)\n        self.g = layers.Conv2D(intermediate_channels, kernel_size=1, use_bias=False)\n        self.W = layers.Conv2D(in_channels, kernel_size=1, use_bias=False)\n        \n        self.attn_refine = tf.keras.Sequential([\n            layers.Conv2D(in_channels, kernel_size=3, padding='same', use_bias=False),\n            layers.ReLU(),\n            layers.Conv2D(in_channels, kernel_size=3, padding='same', use_bias=False)\n        ])\n        \n        self.channel_match_conv = layers.Conv2D(intermediate_channels, kernel_size=1)\n        self.reduce_channels_dct = layers.Conv2D(intermediate_channels, kernel_size=1)\n\n        self.relative_bias_dct = tf.Variable(tf.zeros((1, 1, 1, 1)), trainable=True)\n        self.relative_bias_non_local = tf.Variable(tf.zeros((1, 1, 1, 1)), trainable=True)\n\n    def call(self, inputs):\n        # Get the shape of the input\n        shape = tf.shape(inputs)\n        batch_size, height, width = shape[0], shape[1], shape[2]\n\n        # Process the inputs\n        dct_input = dct_transform(inputs)  # Apply DCT transform\n        dct_attention = frequency_attention(dct_input, self.dct_threshold) + self.relative_bias_dct\n\n        theta_x = tf.reshape(self.theta(inputs), [batch_size, height * width, self.intermediate_channels])\n        phi_x = tf.reshape(self.phi(inputs), [batch_size, height * width, self.intermediate_channels])\n        g_x = tf.reshape(self.g(inputs), [batch_size, height * width, self.intermediate_channels])\n\n        f = tf.matmul(theta_x, phi_x, transpose_b=True)\n        f_softmax = tf.nn.softmax(f, axis=-1)\n        y = tf.matmul(f_softmax, g_x)\n        y = tf.reshape(y, [batch_size, height, width, self.intermediate_channels])\n\n        refined_attention = self.attn_refine(inputs) + self.relative_bias_non_local\n        refined_attention = self.channel_match_conv(refined_attention)\n\n        scale_factor = tf.sigmoid(refined_attention)\n        refined_dct_attention = self.reduce_channels_dct(dct_transform(dct_attention))\n        dct_attention = self.reduce_channels_dct(dct_attention)\n\n        y1 = y + (y * scale_factor) + refined_attention\n        y2 = y - (y * scale_factor) - refined_attention\n        y = y1 + y2 + refined_dct_attention + dct_attention\n\n        y = self.W(y)\n        return tf.nn.relu(inputs + y)\n\n    def compute_output_shape(self, input_shape):\n        \"\"\"\n        Manually compute the output shape of the layer.\n        The output shape will be the same as the input shape.\n        \"\"\"\n        # The output will have the same dimensions as the input (batch_size, height, width, channels)\n        batch_size = input_shape[0]\n        height = input_shape[1]\n        width = input_shape[2]\n        channels = self.in_channels\n        return (batch_size, height, width, channels)\n\n# Usage example\ndef RGSA(x, filters, strides=(1, 1), use_projection=False):\n    shortcut = x  # Save the original input for residual connection\n\n    # First Depthwise and Pointwise Convolution\n    x = layers.DepthwiseConv2D(kernel_size=(3, 3), strides=strides, padding=\"same\", depth_multiplier=1)(x)\n    x = layers.Conv2D(filters, kernel_size=1, padding=\"same\")(x)\n    \n    # Apply DCT Attention Block\n    x = DCTAttentionNonLocalBlock(filters, filters)(x)\n    #print('DCT 1:', x.shape)\n\n    # Normalization and Activation\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation('relu')(x)\n\n    # Second Depthwise and Pointwise Convolution\n    x = layers.DepthwiseConv2D(kernel_size=(3, 3), padding=\"same\", depth_multiplier=1)(x)\n    x = layers.Conv2D(filters, kernel_size=1, padding=\"same\")(x)\n\n    # Apply DCT Attention Block Again\n    x = DCTAttentionNonLocalBlock(filters, filters)(x)\n    #print('DCT 2:', x.shape)\n\n    # Normalization\n    x = layers.BatchNormalization()(x)\n\n    # Adjust Shortcut if Needed\n    if strides != (1, 1) or use_projection:\n        shortcut = layers.Conv2D(filters=filters, kernel_size=(1, 1), strides=strides, padding='same')(shortcut)\n        shortcut = layers.BatchNormalization()(shortcut)\n\n    # Residual Connection\n    x = layers.Add()([x, shortcut])\n\n    # Final Activation\n    x = layers.Activation('relu')(x)\n\n    return x\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''#### Multi-branch fusion attention (MFA) module #####\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.signal import fft2d, dct\n\nclass GlobalMinPooling2D(layers.Layer):\n    def __init__(self, **kwargs):\n        super(GlobalMinPooling2D, self).__init__(**kwargs)\n\n    def call(self, inputs):\n        return tf.reduce_min(inputs, axis=[1, 2])\n\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0], input_shape[-1])\n\n    def get_config(self):\n        config = super(GlobalMinPooling2D, self).get_config()\n        return config\n\nclass DeeperGlobalLocalAttentionLayer1(layers.Layer):\n    def __init__(self, units, activation='sigmoid', dropout_rate=0.2, use_scale=True, axis=-1, **kwargs):\n        super(DeeperGlobalLocalAttentionLayer1, self).__init__(**kwargs)\n        self.units = units\n        self.activation = activation\n        self.dropout_rate = dropout_rate\n        self.use_scale = use_scale\n        self.axis = axis\n\n    def build(self, input_shape):\n        _, _, _, channels = input_shape\n        \n        \n        self.global_conv1 = layers.Conv2D(filters=self.units, kernel_size=(1, 1), activation=self.activation)\n        self.global_avg_pooling1 = layers.GlobalAveragePooling2D()\n        \n        self.global_conv2 = layers.Conv2D(filters=self.units, kernel_size=(1, 1), activation=self.activation)\n        self.global_avg_pooling2 = layers.GlobalMaxPooling2D()\n        \n        self.global_conv_3 = layers.Conv2D(filters=self.units, kernel_size=(1, 1), activation=self.activation)\n        self.global_avg_pooling_3 = GlobalMinPooling2D()\n        \n        \n        self.global_conv3 = layers.Conv2D(filters=self.units, kernel_size=(1, 1), activation=self.activation)\n        self.global_avg_pooling3 = layers.GlobalAveragePooling2D()\n        \n        self.global_conv4 = layers.Conv2D(filters=self.units, kernel_size=(1, 1), activation=self.activation)\n        self.global_avg_pooling4 = layers.GlobalMaxPooling2D()\n\n        self.global_conv_4 = layers.Conv2D(filters=self.units, kernel_size=(1, 1), activation=self.activation)\n        self.global_avg_pooling_4 = GlobalMinPooling2D()\n        \n        \n        self.concat1 = layers.Add()\n        self.concat2 = layers.Add()\n        self.concat3 = layers.Add()\n        self.concat4 = layers.Add()\n        self.concat_3 = layers.Add()\n        self.concat_4 = layers.Add()\n        \n        self.concat5 = layers.Concatenate(axis=-1)\n        \n        self.global_attention = layers.Dense(units=self.units, activation=self.activation)\n\n        #self.local_dsc = layers.DepthwiseConv2D(kernel_size=(3, 3), padding=\"same\", depth_multiplier=1)\n        self.local_conv1 = layers.Conv2D(filters=self.units, kernel_size=(1, 1), activation=self.activation)\n        self.local_conv2 = layers.Conv2D(filters=self.units, kernel_size=(1, 1), activation=self.activation)\n        self.concat6 = layers.Add()\n        \n        if self.use_scale:\n            self.global_scale = self.add_weight(shape=(1, 1, 1, 1), initializer='ones', trainable=True, name='global_scale')\n            self.local_scale = self.add_weight(shape=(1, 1, 1, self.units), initializer='ones', trainable=True, name='local_scale')\n        \n        super(DeeperGlobalLocalAttentionLayer1, self).build(input_shape)\n\n    def call(self, inputs, training=None):\n        ##### Hierarchical Information Fusion Attention(HIFA) ######\n        \n        global_attention1 = self.global_conv1(inputs)\n        global_avg1 = self.global_avg_pooling1(global_attention1)\n        \n        global_attention2 = self.global_conv2(global_attention1)\n        global_avg2 = self.global_avg_pooling2(global_attention2)\n\n        global_attention_3 = self.global_conv_3(global_attention1)\n        global_avg_3 = self.global_avg_pooling_3(global_attention_3)\n        \n        \n        global_concat1 = self.concat1([global_avg1, global_avg2, global_avg_3])\n        global_sub1 = global_avg2 - global_avg1 - global_avg_3\n        global_concat1 = global_concat1 + global_sub1\n        \n        global_attention_concat1 = self.concat2([global_attention1, global_attention2, global_attention_3])\n        \n        global_sub_1 = global_attention2 - global_attention1 - global_attention_3\n\n        global_attention_concat1 = global_attention_concat1 + global_sub_1\n        \n        global_attention3 = self.global_conv3(global_attention_concat1)\n        global_avg3 = self.global_avg_pooling3(global_attention3)\n        \n        global_attention4 = self.global_conv4(global_attention3)\n        global_avg4 = self.global_avg_pooling4(global_attention4)\n\n        global_attention_4 = self.global_conv_3(global_attention3)\n        global_avg_4 = self.global_avg_pooling_3(global_attention_4)\n        \n        \n        global_concat2 = self.concat3([global_avg3, global_avg4, global_avg_4])\n        global_sub2 = global_avg4 - global_avg3 - global_avg_4\n        global_concat2 = global_concat2 + global_sub2\n\n        \n        #global_attention_concat2 = self.concat4([global_attention3, global_attention4, global_attention_4])\n        #global_sub_2 = global_attention4 - global_attention3 - global_attention_4\n\n        #global_attention_concat2 = global_attention_concat2 + global_sub_2\n\n        \n        \n        global_avg_concat = self.concat5([global_concat1, global_concat2])\n        \n        global_attention = self.global_attention(global_avg_concat)\n        global_attention = tf.expand_dims(tf.expand_dims(global_attention, 1), 1)\n\n        ##### frequency domain Local Information learning Attention ######\n        \n        input_shape = tf.shape(inputs)\n\n        \n        flattened_inputs = tf.reshape(inputs, [-1, input_shape[-1]])  # Flatten along the last axis\n        dct_transformed = tf.signal.dct(flattened_inputs, type=2, norm='ortho')\n        dct_transformed = tf.reshape(dct_transformed, input_shape)  # Reshape back to original dimensions\n        \n        local_attention1 = self.local_conv1(dct_transformed)\n        local_attention1 = tf.reduce_mean(local_attention1, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        local_attention2 = self.local_conv2(local_attention1)\n        local_attention2 = tf.reduce_mean(local_attention2, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n\n        local_attention3 = self.local_conv1(inputs)\n        local_attention3 = tf.reduce_max(local_attention3, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        local_attention4 = self.local_conv2(local_attention3)\n        local_attention4 = tf.reduce_max(local_attention4, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n\n        local_attention5 = self.local_conv1(inputs)\n        local_attention5 = tf.reduce_mean(local_attention5, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        local_attention6 = self.local_conv2(local_attention5)\n        local_attention6 = tf.reduce_mean(local_attention6, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n\n        local_attention7 = self.local_conv1(dct_transformed)\n        local_attention7 = tf.reduce_max(local_attention7, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        local_attention8 = self.local_conv2(local_attention7)\n        local_attention8 = tf.reduce_max(local_attention8, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n\n        \n        \n        #local_attention = self.concat6([local_attention1, local_attention2, local_attention3, local_attention4, \n         #                              local_attention5, local_attention6, local_attention7, local_attention8])\n        \n        local_attention_avg = (self.concat6([local_attention1, local_attention2, local_attention5, local_attention6])) \n        local_attention_max = self.concat6([local_attention3, local_attention4, local_attention7, local_attention8]) \n\n        local_attention = L.Activation(\"relu\")(self.concat6([local_attention_avg, local_attention_max])) \n        \n        # Scale Global and Local Attention\n        if self.use_scale:\n            global_attention *= self.global_scale\n            local_attention *= self.local_scale\n\n        # Combine Global and Local Attention\n        attention = tf.sigmoid(global_attention + local_attention)\n        return attention\n\n    def get_config(self):\n        config = super(DeeperGlobalLocalAttentionLayer1, self).get_config()\n        config.update({'units': self.units, 'activation': self.activation, 'dropout_rate': self.dropout_rate,\n                       'use_scale': self.use_scale})\n        return config\n\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Code strating here for MICCAI","metadata":{}},{"cell_type":"code","source":"#### Multi-branch fusion attention (MFA) module #####\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.signal import fft2d, dct\n\nclass GlobalMinPooling2D(layers.Layer):\n    def __init__(self, **kwargs):\n        super(GlobalMinPooling2D, self).__init__(**kwargs)\n\n    def call(self, inputs):\n        return tf.reduce_min(inputs, axis=[1, 2])\n\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0], input_shape[-1])\n\n    def get_config(self):\n        config = super(GlobalMinPooling2D, self).get_config()\n        return config\n\nclass DeeperGlobalLocalAttentionLayer1(layers.Layer):\n    def __init__(self, units, activation='sigmoid', dropout_rate=0.2, use_scale=True, axis=-1, **kwargs):\n        super(DeeperGlobalLocalAttentionLayer1, self).__init__(**kwargs)\n        self.units = units\n        self.activation = activation\n        self.dropout_rate = dropout_rate\n        self.use_scale = use_scale\n        self.axis = axis\n\n    def build(self, input_shape):\n        _, _, _, channels = input_shape\n        \n        \n        self.global_conv1 = layers.DepthwiseConv2D(kernel_size=(1, 1), activation=self.activation)\n        self.global_avg_pooling1 = layers.GlobalAveragePooling2D()\n        \n        self.global_conv2 = layers.DepthwiseConv2D(kernel_size=(1, 1), activation=self.activation)\n        self.global_avg_pooling2 = layers.GlobalMaxPooling2D()\n        \n        self.global_conv_3 = layers.DepthwiseConv2D(kernel_size=(1, 1), activation=self.activation)\n        self.global_avg_pooling_3 = GlobalMinPooling2D()\n        \n        \n        self.global_conv3 = layers.DepthwiseConv2D(kernel_size=(1, 1), activation=self.activation)\n        self.global_avg_pooling3 = layers.GlobalAveragePooling2D()\n        \n        self.global_conv4 = layers.DepthwiseConv2D(kernel_size=(1, 1), activation=self.activation)\n        self.global_avg_pooling4 = layers.GlobalMaxPooling2D()\n\n        self.global_conv_4 = layers.DepthwiseConv2D(kernel_size=(1, 1), activation=self.activation)\n        self.global_avg_pooling_4 = GlobalMinPooling2D()\n        \n        \n        self.concat1 = layers.Add()\n        self.concat2 = layers.Add()\n        self.concat3 = layers.Add()\n        self.concat4 = layers.Add()\n        self.concat_3 = layers.Add()\n        self.concat_4 = layers.Add()\n        \n        self.concat5 = layers.Concatenate(axis=-1)\n        \n        self.global_attention = layers.Dense(units=self.units, activation=self.activation)\n\n        #self.local_dsc = layers.DepthwiseConv2D(kernel_size=(3, 3), padding=\"same\", depth_multiplier=1)\n        self.local_conv1 = layers.DepthwiseConv2D(kernel_size=(1, 1), activation=self.activation)\n        self.local_conv2 = layers.DepthwiseConv2D(kernel_size=(1, 1), activation=self.activation)\n        \n        self.concat6 = layers.Add()\n        \n        if self.use_scale:\n            self.global_scale = self.add_weight(shape=(1, 1, 1, 1), initializer=tf.keras.initializers.HeNormal(), trainable=True, name='global_scale')\n            self.local_scale = self.add_weight(shape=(1, 1, 1, self.units), initializer=tf.keras.initializers.HeNormal(), trainable=True, name='local_scale')\n        \n        super(DeeperGlobalLocalAttentionLayer1, self).build(input_shape)\n\n    def call(self, inputs, training=None):\n        ##### Hierarchical Information Fusion Attention(HIFA) ######\n        \n        global_attention1 = self.global_conv1(inputs)\n        global_avg1 = self.global_avg_pooling1(global_attention1)\n        \n        global_attention2 = self.global_conv2(global_attention1)\n        global_avg2 = self.global_avg_pooling2(global_attention2)\n\n        global_attention_3 = self.global_conv_3(global_attention1)\n        global_avg_3 = self.global_avg_pooling_3(global_attention_3)\n        \n        \n        global_concat1 = self.concat1([global_avg1, global_avg2, global_avg_3])\n        global_sub1 = global_avg2 - global_avg1 - global_avg_3\n        global_concat1 = global_concat1 + global_sub1\n        \n        global_attention_concat1 = self.concat2([global_attention1, global_attention2, global_attention_3])\n        \n        global_sub_1 = global_attention2 - global_attention1 - global_attention_3\n\n        global_attention_concat1 = global_attention_concat1 + global_sub_1\n        \n        global_attention3 = self.global_conv3(global_attention_concat1)\n        global_avg3 = self.global_avg_pooling3(global_attention3)\n        \n        global_attention4 = self.global_conv4(global_attention3)\n        global_avg4 = self.global_avg_pooling4(global_attention4)\n\n        global_attention_4 = self.global_conv_3(global_attention3)\n        global_avg_4 = self.global_avg_pooling_3(global_attention_4)\n        \n        \n        global_concat2 = self.concat3([global_avg3, global_avg4, global_avg_4])\n        global_sub2 = global_avg4 - global_avg3 - global_avg_4\n        global_concat2 = global_concat2 + global_sub2\n\n        \n        #global_attention_concat2 = self.concat4([global_attention3, global_attention4, global_attention_4])\n        #global_sub_2 = global_attention4 - global_attention3 - global_attention_4\n\n        #global_attention_concat2 = global_attention_concat2 + global_sub_2\n\n        \n        \n        global_avg_concat = self.concat5([global_concat1, global_concat2])\n        \n        global_attention = self.global_attention(global_avg_concat)\n        #global_attention = L.Activation(\"relu\")(tf.expand_dims(tf.expand_dims(global_attention, 1), 1))\n        \n        global_attention = tf.expand_dims(tf.expand_dims(global_attention, 1), 1)\n\n        ##### frequency domain Local Information learning Attention ######\n        \n        input_shape = tf.shape(inputs)\n\n        \n        flattened_inputs = tf.reshape(inputs, [-1, input_shape[-1]])  # Flatten along the last axis\n        dct_transformed = tf.signal.dct(flattened_inputs, type=2, norm='ortho')\n        dct_transformed = tf.reshape(dct_transformed, input_shape)  # Reshape back to original dimensions\n        \n        local_attention1 = self.local_conv1(dct_transformed)\n        local_attention1 = tf.reduce_mean(local_attention1, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        local_attention2 = self.local_conv2(local_attention1)\n        local_attention2 = tf.reduce_mean(local_attention2, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n\n        '''local_attention3 = self.local_conv1(inputs)\n        local_attention3 = tf.reduce_max(local_attention3, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        local_attention4 = self.local_conv2(local_attention3)\n        local_attention4 = tf.reduce_max(local_attention4, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n\n        local_attention5 = self.local_conv1(inputs)\n        local_attention5 = tf.reduce_mean(local_attention5, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        local_attention6 = self.local_conv2(local_attention5)\n        local_attention6 = tf.reduce_mean(local_attention6, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions'''\n\n        local_attention7 = self.local_conv1(dct_transformed)\n        local_attention7 = tf.reduce_max(local_attention7, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        local_attention8 = self.local_conv2(local_attention7)\n        local_attention8 = tf.reduce_max(local_attention8, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n\n        local_attention9 = self.local_conv1(dct_transformed)\n        local_attention9 = tf.reduce_min(local_attention9, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n        local_attention10 = self.local_conv2(local_attention9)\n        local_attention10 = tf.reduce_min(local_attention10, axis=[1, 2], keepdims=True)  # Reduce spatial dimensions\n\n\n        \n        \n        #local_attention = self.concat6([local_attention1, local_attention2, local_attention3, local_attention4, \n         #                              local_attention5, local_attention6, local_attention7, local_attention8])\n        \n        local_attention_avg = (self.concat6([local_attention1, local_attention2])) \n        local_attention_max = self.concat6([local_attention7, local_attention8]) \n        local_attention_min = self.concat6([local_attention9, local_attention10]) \n\n        local_attention = self.concat6([local_attention_avg, local_attention_max, local_attention_min]) \n        \n        # Scale Global and Local Attention\n        if self.use_scale:\n            global_attention *= self.global_scale\n            local_attention *= self.local_scale\n\n        # Combine Global and Local Attention\n        attention = tf.sigmoid(global_attention + local_attention)\n        return attention\n\n    def get_config(self):\n        config = super(DeeperGlobalLocalAttentionLayer1, self).get_config()\n        config.update({'units': self.units, 'activation': self.activation, 'dropout_rate': self.dropout_rate,\n                       'use_scale': self.use_scale})\n        return config\n\n'''class DeeperAttentionLayer1(layers.Layer):\n    def __init__(self, units=64, use_scale=True, **kwargs):\n        super(DeeperAttentionLayer1, self).__init__(**kwargs)\n        self.units = units\n        self.use_scale = use_scale\n\n    def build(self, input_shape):\n        _, H, W, C = input_shape\n        self.alpha = self.add_weight(shape=(1, 1, 1, C), initializer='ones', trainable=True, name='alpha')\n        self.deeper_global_local_attention = DeeperGlobalLocalAttentionLayer1(units=self.units, activation='sigmoid', \n                                                                              dropout_rate=0.2,  # You can adjust the dropout rate\n                                                                              use_scale=self.use_scale)\n        super(DeeperAttentionLayer1, self).build(input_shape)\n\n    def call(self, inputs, training=None):\n        attention = self.deeper_global_local_attention(inputs, training=training)\n        attention_feature = inputs * attention * self.alpha\n        return attention_feature\n\n    def get_config(self):\n        config = super(DeeperAttentionLayer1, self).get_config()\n        config.update({'units': self.units, 'use_scale': self.use_scale})\n        return config'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers\n\n# Frequency Transform Layer\n'''class FrequencyTransformLayer(layers.Layer):\n    def __init__(self, transform_type='dct', **kwargs):\n        super(FrequencyTransformLayer, self).__init__(**kwargs)\n        self.transform_type = transform_type\n\n    def call(self, inputs):\n        # Ensure inputs are float32 for DCT and FFT operations\n        #inputs = tf.cast(inputs, tf.float32)\n\n        if self.transform_type == 'dct':\n            # Apply 2D DCT along the last axis\n            input_shape = tf.shape(inputs)\n            flattened_inputs = tf.reshape(inputs, [-1, input_shape[-1]])  # Flatten along the last axis\n            dct_transformed = tf.signal.dct(flattened_inputs, type=2, norm='ortho', axis = -1)\n            return tf.reshape(dct_transformed, input_shape)  # Reshape back to original dimensions\n        elif self.transform_type == 'fft':\n            # Apply 2D FFT and return the magnitude\n            fft_transformed = tf.signal.fft2d(tf.cast(inputs, tf.complex64))\n            return tf.math.abs(fft_transformed)\n        else:\n            raise ValueError(\"Unsupported transform type. Choose 'dct' or 'fft'.\")\n\n    def get_config(self):\n        config = super(FrequencyTransformLayer, self).get_config()\n        config.update({'transform_type': self.transform_type})\n        return config'''\n\n# DCT Transform Function\n'''def dct_transform(inputs):\n    # Apply DCT transform to the inputs\n    dct_input = FrequencyTransformLayer(transform_type='dct')(inputs)  # Use FrequencyTransformLayer for DCT\n    \n    # Apply global average pooling (GAP), global max pooling (GMP), and global min pooling (GMP_MIN)\n    gap = tf.reduce_mean(dct_input, axis=[1, 2], keepdims=True)\n    gmp = tf.reduce_max(dct_input, axis=[1, 2], keepdims=True)\n    gmp_min = tf.reduce_min(dct_input, axis=[1, 2], keepdims=True)\n    \n    # Combine the pooled features\n    pooled_features1 = gap + gmp + gmp_min\n    pooled_features2 = gmp - (gap - gmp_min)\n    pooled_features = pooled_features1 + pooled_features2\n    \n    return pooled_features'''\n\n\n\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\n# 1D DCT from scratch using TensorFlow\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\n# 1D DCT from scratch using TensorFlow\n'''def dct1d(x):\n    \"\"\"1D Discrete Cosine Transform.\"\"\"\n    N = tf.shape(x)[0]\n    k = tf.range(N, dtype=tf.float32)  # Create range for k\n    n = tf.range(N, dtype=tf.float32)  # Create range for n\n    # Using broadcasting to calculate the DCT\n    X = tf.reduce_sum(x * tf.cos(np.pi / N * (n + 0.5)[:, None] * k[None, :]), axis=0)\n    return X\n\n# 2D DCT from scratch using TensorFlow\ndef dct2d(x):\n    \"\"\"2D Discrete Cosine Transform (DCT). Applies DCT row-wise and column-wise.\"\"\"\n    # Apply DCT to each row (along the last axis)\n    x_dct_rows = tf.map_fn(dct1d, x, dtype=tf.float32)\n    # Apply DCT to each column (along the second last axis)\n    x_dct = tf.map_fn(lambda x: dct1d(tf.transpose(x)), x_dct_rows, dtype=tf.float32)\n    return x_dct\n\n\nimport tensorflow as tf\n\n# 1D DCT from scratch using TensorFlow\ndef dct1d(x):\n    \"\"\"1D Discrete Cosine Transform.\"\"\"\n    N = tf.shape(x)[0]\n    k = tf.range(N, dtype=tf.int32)  # Create range for k\n    n = tf.range(N, dtype=tf.int32)  # Create range for n\n    pi = tf.constant(3, dtype=tf.int32)  # Explicitly use TensorFlow constant for pi\n    \n    # Using broadcasting to calculate the DCT\n    X = tf.reduce_sum(x * tf.cos(3 / N * (n + 0.5)[:, None] * k[None, :]), axis=0)\n    return X\n\n# 2D DCT from scratch using TensorFlow\ndef dct2d(x):\n    \"\"\"2D Discrete Cosine Transform (DCT). Applies DCT row-wise and column-wise.\"\"\"\n    # Apply DCT to each row (along the last axis)\n    x_dct_rows = tf.map_fn(dct1d, x, dtype=tf.int32)\n    # Apply DCT to each column (along the second last axis)\n    x_dct = tf.map_fn(lambda x: dct1d(tf.transpose(x)), x_dct_rows, dtype=tf.int32)\n    return x_dct\n\n# Custom FrequencyTransformLayer class\nclass FrequencyTransformLayer(layers.Layer):\n    def __init__(self, transform_type='dct', **kwargs):\n        super(FrequencyTransformLayer, self).__init__(**kwargs)\n        self.transform_type = transform_type\n\n    def call(self, inputs):\n        # Ensure inputs are float32 for DCT and FFT operations\n        inputs = tf.cast(inputs, tf.float32)\n\n        if self.transform_type == 'dct':\n            # Apply 2D DCT from scratch (using custom dct2d function)\n            input_shape = tf.shape(inputs)\n            flattened_inputs = tf.reshape(inputs, [-1, input_shape[-1]])  # Flatten along the last axis\n            \n            # Perform the 2D DCT using the custom scratch implementation\n            dct_transformed = dct2d(flattened_inputs)  # Perform 2D DCT\n            \n            # Reshape back to the original dimensions\n            return tf.reshape(dct_transformed, input_shape)  # Reshape back to original dimensions\n\n        elif self.transform_type == 'fft':\n            # Apply 2D FFT and return the magnitude\n            fft_transformed = tf.signal.fft2d(tf.cast(inputs, tf.complex64))\n            return tf.math.abs(fft_transformed)\n\n        else:\n            raise ValueError(\"Unsupported transform type. Choose 'dct' or 'fft'.\")\n\n    def get_config(self):\n        config = super(FrequencyTransformLayer, self).get_config()\n        config.update({'transform_type': self.transform_type})\n        return config\n'''\n\n\n\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Layer\n\n'''class DCTTransformLayer(Layer):\n    def __init__(self, **kwargs):\n        super(DCTTransformLayer, self).__init__(**kwargs)\n        # Optionally initialize any additional parameters here\n        #self.frequency_transform = FrequencyTransformLayer(transform_type='dct')\n\n    def build(self, input_shape):\n        # This method is used for any initialization specific to the layer, such as creating weights or other components\n        pass\n\n    def call(self, inputs):\n        # Apply DCT transform to the inputs\n        #dct_input = self.frequency_transform(inputs)  # Using self to reference the FrequencyTransformLayer instance\n        \n        # Apply global average pooling (GAP), global max pooling (GMP), and global min pooling (GMP_MIN)\n        gap = tf.reduce_mean(inputs, axis=[1, 2], keepdims=True)\n        gmp = tf.reduce_max(inputs, axis=[1, 2], keepdims=True)\n        gmp_min = tf.reduce_min(inputs, axis=[1, 2], keepdims=True)\n        \n        # Combine the pooled features\n        pooled_features1 = gap + gmp + gmp_min\n        pooled_features2 = gmp - (gap - gmp_min)\n        pooled_features = pooled_features1 + pooled_features2\n        \n        return pooled_features\n\n\n\n\n# Frequency Attention\nimport tensorflow as tf\n\n\n# DCT Attention Non-Local Block\nclass DCTAttentionNonLocalBlock(layers.Layer):\n    def __init__(self, in_channels, intermediate_channels, dct_threshold=0.1):\n        super(DCTAttentionNonLocalBlock, self).__init__()\n        self.in_channels = in_channels\n        self.intermediate_channels = intermediate_channels\n        self.dct_threshold = dct_threshold\n\n\n        #self.dct_transform_layer = DCTTransformLayer()\n\n        \n        \n        # Define your layers here\n        self.theta = layers.Conv2D(intermediate_channels, kernel_size=1, use_bias=False)\n        self.phi = layers.Conv2D(intermediate_channels, kernel_size=1, use_bias=False)\n        self.g = layers.Conv2D(intermediate_channels, kernel_size=1, use_bias=False)\n        self.W = layers.Conv2D(in_channels, kernel_size=1, use_bias=False)\n        \n        self.attn_refine = tf.keras.Sequential([\n            layers.Conv2D(in_channels, kernel_size=3, padding='same', use_bias=False),\n            layers.ReLU(),\n            layers.Conv2D(in_channels, kernel_size=3, padding='same', use_bias=False)\n        ])\n        \n        self.channel_match_conv = layers.Conv2D(intermediate_channels, kernel_size=1)\n        self.reduce_channels_dct = layers.Conv2D(intermediate_channels, kernel_size=1)\n\n        self.relative_bias_dct = tf.Variable(tf.zeros((1, 1, 1, 1)), trainable=True)\n        self.relative_bias_non_local = tf.Variable(tf.zeros((1, 1, 1, 1)), trainable=True)\n\n    def call(self, inputs):\n        # Get the shape of the input\n        shape = tf.shape(inputs)\n        batch_size, height, width = shape[0], shape[1], shape[2]\n\n        # Process the inputs\n        # Global attention\n        gap = tf.reduce_mean(inputs, axis=[1, 2], keepdims=True)\n        gmp = tf.reduce_max(inputs, axis=[1, 2], keepdims=True)\n        gmp_min = tf.reduce_min(inputs, axis=[1, 2], keepdims=True)\n        \n        # Combine the pooled features\n        pooled_features1 = gap + gmp + gmp_min\n        pooled_features2 = gmp - (gap - gmp_min)\n        \n        #pooled_features = pooled_features1 + pooled_features2\n        \n        #dct_input = self.dct_transform_layer(inputs)  # Apply DCT transform\n        #dct_input = dct_transform(inputs)  # Apply DCT transform\n\n        \n        dct_attention = tf.sigmoid((pooled_features1 + pooled_features2) * self.relative_bias_dct) #frequency_attention(dct_input, self.dct_threshold) + self.relative_bias_dct\n\n        theta_x = tf.reshape(self.theta(inputs), [batch_size, height * width, self.intermediate_channels])\n        phi_x = tf.reshape(self.phi(inputs), [batch_size, height * width, self.intermediate_channels])\n        g_x = tf.reshape(self.g(inputs), [batch_size, height * width, self.intermediate_channels])\n\n        f = tf.matmul(theta_x, phi_x, transpose_b=True)\n        f_softmax = tf.nn.softmax(f, axis=-1)\n        y = tf.matmul(f_softmax, g_x)\n        y = tf.reshape(y, [batch_size, height, width, self.intermediate_channels])\n\n        refined_attention = self.attn_refine(inputs) + self.relative_bias_non_local\n        refined_attention = self.channel_match_conv(refined_attention)\n\n        scale_factor = tf.sigmoid(refined_attention)\n        #refined_dct_attention = self.reduce_channels_dct(dct_transform(dct_attention))\n        dct_attention = self.reduce_channels_dct(dct_attention)\n\n        y1 = y + (y * scale_factor) + refined_attention\n        y2 = y - (y * scale_factor) - refined_attention\n        y = tf.sigmoid(y1 + y2 + dct_attention)\n\n        y = self.W(y)\n        return tf.nn.relu(inputs + y)\n\n    def compute_output_shape(self, input_shape):\n        \"\"\"\n        Manually compute the output shape of the layer.\n        The output shape will be the same as the input shape.\n        \"\"\"\n        # The output will have the same dimensions as the input (batch_size, height, width, channels)\n        batch_size = input_shape[0]\n        height = input_shape[1]\n        width = input_shape[2]\n        channels = self.in_channels\n        return (batch_size, height, width, channels)\n'''\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\n'''# Custom Self-Attention Layer\nclass SelfAttention(layers.Layer):\n    def __init__(self, channels, **kwargs):\n        super(SelfAttention, self).__init__(**kwargs)\n        self.channels = channels\n        self.query_conv = layers.Conv2D(channels // 8, kernel_size=1)\n        self.key_conv = layers.Conv2D(channels // 8, kernel_size=1)\n        self.value_conv = layers.Conv2D(channels, kernel_size=1)\n        self.attn_conv = layers.Conv2D(channels, kernel_size=1)\n\n    def call(self, inputs):\n        # Generate Q, K, V matrices\n        query = self.query_conv(inputs)\n        key = self.key_conv(inputs)\n        value = self.value_conv(inputs)\n\n        # Perform dot product between query and key (self-attention)\n        attn_map = tf.matmul(query, key, transpose_b=True)\n        attn_map = tf.nn.softmax(attn_map, axis=-1)\n\n        # Weighted sum of value vectors\n        attn_out = tf.matmul(attn_map, value)\n\n        # Final attention output\n        attn_out = self.attn_conv(attn_out)\n        return attn_out + inputs  # Add residual connection\n'''\n# Custom Attention Block with Global Pooling\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\nclass GlobalMinPooling2D(layers.Layer):\n    def __init__(self, **kwargs):\n        super(GlobalMinPooling2D, self).__init__(**kwargs)\n\n    def call(self, inputs):\n        return tf.reduce_min(inputs, axis=[1, 2])\n\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0], input_shape[-1])\n\n    def get_config(self):\n        config = super(GlobalMinPooling2D, self).get_config()\n        return config\n\n\nclass AttentionBlock(layers.Layer):\n    def __init__(self, channels, reduction_ratio=8, use_scale = True, **kwargs):\n        super(AttentionBlock, self).__init__(**kwargs)\n        self.channels = channels\n\n        # Global Pooling (Average, Max, and Min)\n        self.gap = layers.GlobalAveragePooling2D()\n        self.gmp = layers.GlobalMaxPooling2D()\n        self.gmin = GlobalMinPooling2D()  #layers.Lambda(lambda x: tf.reduce_min(x, axis=[1, 2], keepdims=False))  # Min pooling\n\n        # Efficient Channel Attention (Replaces Dense Layer)\n        self.channel_attn = layers.DepthwiseConv2D(kernel_size=1)\n        self.channel_attn_out = layers.DepthwiseConv2D(kernel_size=1)\n\n        # Spatial Attention (Lightweight Conv)\n        #self.spatial_attn = layers.Conv2D(1, kernel_size=3, padding=\"same\", activation=\"sigmoid\")\n\n        # Batch Normalization\n        self.batch_norm = layers.BatchNormalization()\n        \n        '''self.weight1 = self.add_weight(\n            shape=(1,1,1,1), initializer=initializers.HeNormal(), trainable=True, name=\"weight1\"\n        )\n        self.weight2 = self.add_weight(\n            shape=(1,1,1,1), initializer=initializers.RandomNormal(mean=0.0, stddev=0.02), trainable=True, name=\"weight2\"\n        )'''\n        \n        #self.weight1 = self.add_weight(shape=(1, 1, 1, 1),  initializer=tf.keras.initializers.HeNormal(), trainable=True, name='weight1', \n         #                                     constraint=tf.keras.constraints.MaxNorm(2.0))\n        \n        self.weight1 = self.add_weight(shape=(1, 1, 1, 1),  initializer=tf.keras.initializers.HeNormal(), trainable=True, name='weight2', \n                                              constraint=tf.keras.constraints.MaxNorm(2.0))\n\n        self.weight2 = self.add_weight(shape=(1, 1, 1, 1),  initializer=tf.keras.initializers.RandomNormal(mean=1.0, stddev=0.02), trainable=True, name='weight3', \n                                              constraint=tf.keras.constraints.MaxNorm(2.0))\n\n        self.weight3 = self.add_weight(shape=(1, 1, 1, 1), initializer='ones', trainable=True, name='alpha2')\n        \n            \n        self.dropout = layers.Dropout(0.25)\n        self.bn = layers.BatchNormalization()\n\n        self.use_scale = use_scale\n        self.local_attn = DeeperGlobalLocalAttentionLayer1(units=self.channels, activation='sigmoid', \n                                                                              dropout_rate=0.2,  # You can adjust the dropout rate\n                                                                              use_scale=self.use_scale)\n\n        \n    def call(self, inputs):\n        inputs1, inputs2 = inputs\n        \n        # Compute Global Average, Max, and Min Pooling\n        gap_out = self.gap(inputs1)\n        gmp_out = self.gmp(inputs1)\n        gmin_out = self.gmin(inputs1)\n\n        # Combine pooling features\n        combined_pool1 = gap_out + gmp_out + gmin_out\n        combined_pool2 = gmp_out - (gap_out - gmin_out)\n        \n        # Normalize for stability\n        #combined_pool1 = tf.nn.l2_normalize(combined_pool1, axis=-1)\n        \n        combined_pool1 = tf.expand_dims(tf.expand_dims(combined_pool1, 1), 1) \n        combined_pool2 = tf.expand_dims(tf.expand_dims(combined_pool2, 1), 1)\n\n        combined_pool1 = self.bn(combined_pool1)\n        combined_pool1 = self.dropout(combined_pool1)\n\n        combined_pool2 = self.bn(combined_pool2)\n        combined_pool2 = self.dropout(combined_pool2)\n        \n        combined_pool1 = combined_pool1 * self.weight1\n        combined_pool2 = combined_pool2 * self.weight1\n        combined_pool = tf.sigmoid(combined_pool1 + combined_pool2) \n        \n\n        #combined_pool = tf.sigmoid(combined_pool1 + combined_pool2) \n        \n        #combined_pool = inputs * combined_pool * self.weight3\n\n        # Compute Channel Attention\n        channel_attn = self.channel_attn(inputs2)\n        gap = tf.reduce_mean(channel_attn, axis=[1, 2], keepdims=True)\n        gmp = tf.reduce_max(channel_attn, axis=[1, 2], keepdims=True)\n        gmp_min = tf.reduce_min(channel_attn, axis=[1, 2], keepdims=True)\n\n        pooled_features1 = gap + gmp + gmp_min\n        pooled_features2 = gmp - (gap - gmp_min)\n\n        pooled_features = pooled_features1 + pooled_features2\n        \n        channel_attn = self.channel_attn_out(pooled_features)  # Sigmoid attention map\n        #print('channel_attn shape:', channel_attn.shape)\n        channel_attn = channel_attn + pooled_features\n        channel_attn = self.batch_norm(channel_attn)\n        \n\n        channel_attn = self.bn(channel_attn)\n        channel_attn = self.dropout(channel_attn)\n\n        channel_attn = channel_attn * self.weight2\n        channel_attn = tf.sigmoid(channel_attn) \n\n        \n        local1 = self.local_attn(inputs1)\n        local2 = self.local_attn(inputs2)\n        \n        \n        # Apply Cross Attention Maps\n        attention1 =  tf.sigmoid(combined_pool + local2) \n        attention2 =  tf.sigmoid(channel_attn + local1) \n        \n        att1 = inputs1 * attention2 * self.weight3\n        att2 = inputs2 * attention1 * self.weight3\n        #print('output attention shape:', output.shape)\n\n        return att1, att2\n\n\n# Residual Attention Block (Using Spatial Attention)\n'''class ResidualAttentionBlock(layers.Layer):\n    def __init__(self, in_channels, **kwargs):\n        super(ResidualAttentionBlock, self).__init__(**kwargs)\n        self.in_channels = in_channels\n        self.attn_block = AttentionBlock(in_channels)\n\n        # Convolution layers for feature refinement\n        #self.dwc = layers.DepthwiseConv2D(kernel_size=(3, 3), padding=\"same\", depth_multiplier=1)\n        self.conv1 = layers.Conv2D(in_channels, kernel_size=1, padding=\"same\")\n        self.relu = layers.ReLU()\n        self.conv2 = layers.Conv2D(in_channels, kernel_size=1, padding=\"same\")\n\n    def call(self, inputs):\n        # Apply attention mechanism\n        attn_out = self.attn_block(inputs)\n\n        # Refining features\n        #x = self.dwc(attn_out)\n        x = self.conv1(attn_out)\n        x = self.relu(x)\n        #x = self.dwc(x)\n        x = self.conv2(x)\n\n        # Add residual connection\n        return inputs * tf.sigmoid(x + attn_out)  # Residual connection for better feature learning\n'''\n\nfrom tensorflow.keras import layers\n\ndef multi_kernel_groupwise_conv(x, filters, groups=8, strides=1):\n    # 1x1 Group-wise Convolution\n    conv1x1 = layers.Conv2D(filters, kernel_size=1, groups=groups, strides=strides, padding=\"same\")(x)\n\n    # 3x3 Group-wise Convolution\n    conv3x3 = layers.DepthwiseConv2D(kernel_size=3, strides=strides, padding=\"same\")(x)\n\n    # 5x5 Group-wise Convolution\n    conv5x5 = layers.DepthwiseConv2D(kernel_size=5, strides=strides, padding=\"same\")(x)\n\n    # Depthwise 3x3 Group-wise Convolution\n    #depthwise3x3 = layers.DepthwiseConv2D(kernel_size=3, strides=strides, padding=\"same\")(x)\n    \n    # Concatenate all outputs along the channel axis\n    x1 = layers.Concatenate()([conv1x1, conv3x3, conv5x5])\n    x1 = layers.Conv2D(filters, kernel_size=1, groups=groups, padding=\"same\")(x1)\n\n    x = layers.Conv2D(filters=filters, kernel_size=(1, 1), strides=strides, padding='same')(x)\n    x = layers.Add()([x, x1])\n    x = layers.Activation('relu')(x)\n    \n    return x\n\n\n# Usage example\ndef RGSA(x, filters, strides=(1, 1), use_projection=False):\n    shortcut = x  # Save the original input for residual connection\n\n    # First Depthwise and Pointwise Convolution\n    #x = layers.DepthwiseConv2D(kernel_size=(3, 3), strides=strides, padding=\"same\", depth_multiplier=1)(x)\n    \n    #x = layers.Conv2D(filters, kernel_size=3, strides=strides, padding=\"same\")(x)\n\n    #x = layers.DepthwiseConv2D(kernel_size=3, strides=strides, padding=\"same\")(x)\n\n    #x = layers.Conv2D(filters=x.shape[-1], kernel_size=1, groups=4, padding=\"same\")(x)\n\n    # Depthwise Convolution\n    #x = layers.DepthwiseConv2D(kernel_size=3, strides=strides, padding=\"same\")(x)\n    \n    # Apply DCT Attention Block\n    #x = DCTAttentionNonLocalBlock(filters, filters)(x)\n    #x = AttentionBlock(filters)(x)\n\n    x = multi_kernel_groupwise_conv(x, filters=filters, groups=8, strides=strides)\n    \n    # Normalization and Activation\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation('relu')(x)\n\n    # Second Depthwise and Pointwise Convolution\n    x = layers.Conv2D(filters, kernel_size=(3, 3), padding=\"same\")(x)\n    #x = layers.Conv2D(filters, kernel_size=3, padding=\"same\")(x)\n\n    # Apply DCT Attention Block Again\n    #x = DCTAttentionNonLocalBlock(filters, filters)(x)\n    #x = AttentionBlock(filters)(x)\n\n    # Normalization\n    x = layers.BatchNormalization()(x)\n\n    # Adjust Shortcut if Needed\n    if strides != (1, 1) or use_projection:\n        shortcut = layers.Conv2D(filters=filters, kernel_size=(1, 1), strides=strides, padding='same')(shortcut)\n        shortcut = layers.BatchNormalization()(shortcut)\n\n    # Residual Connection\n    x = layers.Add()([x, shortcut])\n\n    # Final Activation\n    x = layers.Activation('relu')(x)\n\n    return x\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def residual_GLC_branch1(inputs1, inputs2):\n    \n    x1 = Conv2D(filters=64, kernel_size=(7, 7), strides=(2, 2), padding='same')(inputs1)\n    #x1 = DeeperAttentionLayer1(units=64, use_scale=True)(x1) ## MFA ####\n    #x1 = AttentionBlock(64)(x1)\n    x1 = BatchNormalization()(x1)\n    x1 = tf.keras.layers.Activation('relu')(x1)\n    x1 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x1)\n    \n    x2 = Conv2D(filters=64, kernel_size=(7, 7), strides=(2, 2), padding='same')(inputs2)\n    #x2 = DeeperAttentionLayer1(units=64, use_scale=True)(x2) ## MFA ####\n    #x2 = AttentionBlock(64)(x2)\n    x2 = BatchNormalization()(x2)\n    x2 = tf.keras.layers.Activation('relu')(x2)\n    x2 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x2)\n    \n\n    x1 = RGSA(x1, filters=64)\n    x1 = tf.keras.layers.Dropout(0.25)(x1, training = True)  ## MCD ####\n    #x1 = DeeperAttentionLayer1(units=64, use_scale=True)(x1) ## MFA ####\n    #x1 = AttentionBlock(64)(x1)\n\n    x2 = RGSA(x2, filters=64)\n    x2 = tf.keras.layers.Dropout(0.25)(x2, training = True)  ## MCD ####\n    #x2 = DeeperAttentionLayer1(units=64, use_scale=True)(x2)\n    #x2 = AttentionBlock(64)(x2)\n    \n    x1, x2 = AttentionBlock(channels = 64)([x1, x2])  ## MIFA ####\n    \n    x1 = RGSA(x1, filters=64)\n    x1 = tf.keras.layers.Dropout(0.25)(x1, training = True)  ## MCD ####\n    #x1 = DeeperAttentionLayer1(units=64, use_scale=True)(x1) ## MFA ####\n    #x1 = AttentionBlock(64)(x1)\n    \n    x2 = RGSA(x2, filters=64)\n    x2 = tf.keras.layers.Dropout(0.25)(x2, training = True)  ## MCD ####\n    #x2 = DeeperAttentionLayer1(units=64, use_scale=True)(x2)\n    #x2 = AttentionBlock(64)(x2)\n\n    x1, x2 = AttentionBlock(channels = 64)([x1, x2])  ## MIFA ####\n    \n    x1 = RGSA(x1, filters=128, strides=(2, 2), use_projection=True)\n    x1 = tf.keras.layers.Dropout(0.25)(x1, training = True)  ## MCD ####\n    #x1 = DeeperAttentionLayer1(units=128, use_scale=True)(x1) ## MFA ####\n    #x1 = AttentionBlock(128)(x1)\n\n    x2 = RGSA(x2, filters=128, strides=(2, 2), use_projection=True)\n    x2 = tf.keras.layers.Dropout(0.25)(x2, training = True)  ## MCD ####\n    #x2 = DeeperAttentionLayer1(units=128, use_scale=True)(x2)\n    #x2 = AttentionBlock(128)(x2)\n\n    x1, x2 = AttentionBlock(channels = 128)([x1, x2])  ## MIFA ####\n    \n    x1 = RGSA(x1, filters=128)\n    x1 = tf.keras.layers.Dropout(0.25)(x1, training = True)  ## MCD ####\n    #x1 = DeeperAttentionLayer1(units=128, use_scale=True)(x1)\n    #x1 = AttentionBlock(128)(x1)\n  \n    x2 = RGSA(x2, filters=128)\n    x2 = tf.keras.layers.Dropout(0.25)(x2, training = True)  ## MCD ####\n    #x2 = DeeperAttentionLayer1(units=128, use_scale=True)(x2)\n    #x2 = AttentionBlock(128)(x2)\n\n    x1, x2 = AttentionBlock(channels = 128)([x1, x2])  ## MIFA ####\n    \n    x1 = RGSA(x1, filters=256, strides=(2, 2), use_projection=True)\n    x1 = tf.keras.layers.Dropout(0.25)(x1, training = True)  ## MCD ####\n    #x1 = DeeperAttentionLayer1(units=256, use_scale=True)(x1)\n    #x1 = AttentionBlock(256)(x1)\n    \n    x2 = RGSA(x2, filters=256, strides=(2, 2), use_projection=True)\n    x2 = tf.keras.layers.Dropout(0.25)(x2, training = True)  ## MCD ####\n    #x2 = DeeperAttentionLayer1(units=256, use_scale=True)(x2)\n    #x2 = AttentionBlock(256)(x2)\n\n    x1, x2 = AttentionBlock(channels = 256)([x1, x2])  ## MIFA ####\n    \n    \n    x1 = RGSA(x1, filters=256)\n    x1 = tf.keras.layers.Dropout(0.25)(x1, training = True)  ## MCD ####\n    #x1 = DeeperAttentionLayer1(units=256, use_scale=True)(x1)\n    #x1 = AttentionBlock(256)(x1)\n    \n    x2 = RGSA(x2, filters=256)\n    x2 = tf.keras.layers.Dropout(0.25)(x2, training = True)  ## MCD ####\n    #x2 = DeeperAttentionLayer1(units=256, use_scale=True)(x2)\n    #x2 = AttentionBlock(256)(x2)\n    \n    x1, x2 = AttentionBlock(channels = 256)([x1, x2])  ## MIFA ####\n\n    x1 = RGSA(x1, filters=512, strides=(2, 2), use_projection=True)\n    #x1 = DeeperAttentionLayer1(units=512, use_scale=True)(x1)\n    #x1 = AttentionBlock(512)(x1)\n    \n    \n    x2 = RGSA(x2, filters=512, strides=(2, 2), use_projection=True)\n    #x2 = DeeperAttentionLayer1(units=512, use_scale=True)(x2)\n    #x2 = AttentionBlock(512)(x2)\n\n    x1, x2 = AttentionBlock(channels = 512)([x1, x2])  ## MIFA ####\n    \n    x1 = RGSA(x1, filters=512)\n    x2 = RGSA(x2, filters=512)\n    x1, x2 = AttentionBlock(channels = 512)([x1, x2])\n    \n    return x1, x2","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#def build_resnet18(input_shape=(128, 128, 3), num_classes=2):\ninput_shape=(128, 128, 3)\ninputs1 = Input(shape=input_shape)\ninputs2 = Input(shape=input_shape)\n\nimport tensorflow.keras.layers as L\n\n#input_data = Input(shape=input_shape, name='input_data')\n# Initial convolutional layer\n\nx1, x2 = residual_GLC_branch1(inputs1, inputs2)\n#print('x:',x.shape)\n\ncon = tf.keras.layers.Concatenate(axis=-1)([x1, x2])\n\ncon = tf.keras.layers.Dropout(0.25)(con, training = True)  ## MCD ####\n\nx = GlobalAveragePooling2D()(con)\nprint('GlobalAveragePooling2D x:',x.shape)\n\noutputs1 = Dense(5, activation='softmax')(x)\noutputs2 = Dense(7, activation='softmax')(x)\n\n# Create the model\nmodel = Model([inputs1, inputs2], [outputs1, outputs2])\n#return model\nprint(model.summary())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nimport tensorflow as tf\n\n# Hyperparameters\ninitial_gamma = 0.5\nlearning_rate = 1e-2\noptimizer = Adam(learning_rate=0.001)\n\n# Compile the model initially\nmodel.compile(optimizer=optimizer,\n              loss=['categorical_crossentropy', 'categorical_crossentropy'],\n              loss_weights=[initial_gamma, (1 - initial_gamma)],\n              metrics=['accuracy', 'accuracy'])\n\ndef checkpoint_callback():\n    checkpoint_filepath = 'best1_model_cer_skin_lung.keras'\n    return ModelCheckpoint(filepath=checkpoint_filepath,\n                           save_weights_only=False,\n                           monitor='val_loss',\n                           save_best_only=True,\n                           mode='min',\n                           verbose=1)\n\ndef early_stopping(patience):\n    return EarlyStopping(monitor='val_loss', restore_best_weights=True, patience=60, verbose=1)\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n                              patience=50, verbose=1, min_lr=0.00001)\n\ncallbacks = [checkpoint_callback(), early_stopping(patience=100), reduce_lr]\n\n# Alternating Training Loop\nnum_epochs = 200\nfor epoch in range(num_epochs):\n    print(f\"Epoch {epoch+1}/{num_epochs}\")\n    \n    if epoch % 2 == 0:\n        # Train on first modality only (freeze second modality)\n        for layer in model.layers:\n            layer.trainable = True  # Unfreeze all layers\n        \n        # Set loss weights: train skin modality (set lung modality loss weight to 0)\n        model.compile(optimizer=optimizer,\n                      loss=['categorical_crossentropy', 'categorical_crossentropy'],\n                      loss_weights=[1.0, 0.0],\n                      metrics=['accuracy', 'accuracy'])\n        \n        history = model.fit([X_train_s, X_train_h], [y_train_s, y_train_h],\n                            epochs=10,\n                            validation_data=([X_val_s, X_val_h1], [y_val_s, y_val_h1]),\n                            verbose=1,\n                            shuffle=True,\n                            callbacks=callbacks)\n    \n    else:\n        # Train on second modality only (freeze first modality)\n        for layer in model.layers:\n            layer.trainable = True  # Unfreeze all layers\n        \n        # Set loss weights: train lung modality (set skin modality loss weight to 0)\n        model.compile(optimizer=optimizer,\n                      loss=['categorical_crossentropy', 'categorical_crossentropy'],\n                      loss_weights=[0.0, 1.0],\n                      metrics=['accuracy', 'accuracy'])\n        \n        history = model.fit([X_train_s, X_train_h], [y_train_s, y_train_h],\n                            epochs=1,\n                            validation_data=([X_val_s, X_val_h1], [y_val_s, y_val_h1]),\n                            verbose=1,\n                            shuffle=True,\n                            callbacks=callbacks)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train_ct = X_train_s\nX_train_mri = X_train_h\ny_train_ct = y_train_s \ny_train_mri = y_train_h\n\nX_val_ct = X_val_s\nX_val_mri = X_val_h\ny_val_ct = y_val_s\ny_val_mri = y_val_h","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n\n# Optimizer and learning rate setup\ninitial_gamma1 = 0.25\ninitial_gamma2 = 0.25\ninitial_gamma3 = 0.25\nlearning_rate = 1e-2\noptimizer = Adam(learning_rate=learning_rate)\n\n# Compile the model with the custom optimizer\nmodel.compile(optimizer=optimizer,\n              loss=['categorical_crossentropy', 'categorical_crossentropy', 'categorical_crossentropy', 'categorical_crossentropy'],\n              loss_weights=[initial_gamma1, initial_gamma2, initial_gamma3, (1 - (initial_gamma1 + initial_gamma2 + initial_gamma3))],\n              metrics=['accuracy', 'accuracy', 'accuracy', 'accuracy'])\n\n# Checkpoint callback\ndef checkpoint_callback():\n    checkpoint_filepath = 'best1_model_cer_skin_lung.keras'\n    model_checkpoint_callback = ModelCheckpoint(filepath=checkpoint_filepath,\n                                                save_weights_only=False,\n                                                monitor='val_loss',\n                                                save_best_only=True,\n                                                mode='min',\n                                                verbose=1)\n    return model_checkpoint_callback\n\n# Early stopping callback\ndef early_stopping(patience):\n    es_callback = EarlyStopping(monitor='val_loss', restore_best_weights=True, patience=patience, verbose=1)\n    return es_callback\n\n# Reduce learning rate callback\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n                              patience=50, verbose=1, min_lr=0.00001)\n\n# Callbacks list\ncheckpoint_callback = checkpoint_callback()\nearly_stopping = early_stopping(patience=100)\ncallbacks = [checkpoint_callback, early_stopping, reduce_lr]\n\nimport tensorflow as tf\nimport numpy as np\n\n# Updated generator to ensure outputs are TensorFlow-compatible\ndef alternating_training_data_generator(X_train_s, X_train_h, X_train_ct, X_train_mri, y_train_s, y_train_h, y_train_ct, y_train_mri, batch_size):\n    while True:\n        # Shuffle training data (if required)\n        indices_s = np.random.permutation(len(X_train_s))\n        indices_h = np.random.permutation(len(X_train_h))\n        indices_ct = np.random.permutation(len(X_train_ct))\n        indices_mri = np.random.permutation(len(X_train_mri))\n\n        for i in range(0, len(X_train_s), batch_size):\n            # Alternating between modalities\n            if i % 4 == 0:\n                # Train with skin modality (X_train_s, y_train_s)\n                X_batch = X_train_s[indices_s[i:i+batch_size]]\n                y_batch = y_train_s[indices_s[i:i+batch_size]]\n                # Feed all modalities, fill others with zeros\n                X_batch_all = [X_batch, np.zeros_like(X_batch), np.zeros_like(X_batch), np.zeros_like(X_batch)]\n                y_batch_all = [y_batch, np.zeros_like(y_batch), np.zeros_like(y_batch), np.zeros_like(y_batch)]\n            elif i % 4 == 1:\n                # Train with lung modality (X_train_h, y_train_h)\n                X_batch = X_train_h[indices_h[i:i+batch_size]]\n                y_batch = y_train_h[indices_h[i:i+batch_size]]\n                # Fill others with zeros\n                X_batch_all = [np.zeros_like(X_batch), X_batch, np.zeros_like(X_batch), np.zeros_like(X_batch)]\n                y_batch_all = [np.zeros_like(y_batch), y_batch, np.zeros_like(y_batch), np.zeros_like(y_batch)]\n            elif i % 4 == 2:\n                # Train with CT modality (X_train_ct, y_train_ct)\n                X_batch = X_train_ct[indices_ct[i:i+batch_size]]\n                y_batch = y_train_ct[indices_ct[i:i+batch_size]]\n                # Fill others with zeros\n                X_batch_all = [np.zeros_like(X_batch), np.zeros_like(X_batch), X_batch, np.zeros_like(X_batch)]\n                y_batch_all = [np.zeros_like(y_batch), np.zeros_like(y_batch), y_batch, np.zeros_like(y_batch)]\n            else:\n                # Train with MRI modality (X_train_mri, y_train_mri)\n                X_batch = X_train_mri[indices_mri[i:i+batch_size]]\n                y_batch = y_train_mri[indices_mri[i:i+batch_size]]\n                # Fill others with zeros\n                X_batch_all = [np.zeros_like(X_batch), np.zeros_like(X_batch), np.zeros_like(X_batch), X_batch]\n                y_batch_all = [np.zeros_like(y_batch), np.zeros_like(y_batch), np.zeros_like(y_batch), y_batch]\n\n            # Convert to TensorFlow Tensors before yielding\n            yield (tf.convert_to_tensor(X_batch_all), tf.convert_to_tensor(y_batch_all))  # Return as tuple of tensors\n\n\n# Updated validation data generator with same structure\ndef alternating_validation_data_generator(X_val_s, X_val_h, X_val_ct, X_val_mri, y_val_s, y_val_h, y_val_ct, y_val_mri, batch_size):\n    while True:\n        # Shuffle validation data (if required)\n        indices_s = np.random.permutation(len(X_val_s))\n        indices_h = np.random.permutation(len(X_val_h))\n        indices_ct = np.random.permutation(len(X_val_ct))\n        indices_mri = np.random.permutation(len(X_val_mri))\n\n        for i in range(0, len(X_val_s), batch_size):\n            # Alternating between modalities\n            if i % 4 == 0:\n                # Validation with skin modality (X_val_s, y_val_s)\n                X_batch = X_val_s[indices_s[i:i+batch_size]]\n                y_batch = y_val_s[indices_s[i:i+batch_size]]\n                # You need to feed all modalities, fill others with zeros\n                X_batch_all = [X_batch, np.zeros_like(X_batch), np.zeros_like(X_batch), np.zeros_like(X_batch)]\n                y_batch_all = [y_batch, np.zeros_like(y_batch), np.zeros_like(y_batch), np.zeros_like(y_batch)]\n            elif i % 4 == 1:\n                # Validation with lung modality (X_val_h, y_val_h)\n                X_batch = X_val_h[indices_h[i:i+batch_size]]\n                y_batch = y_val_h[indices_h[i:i+batch_size]]\n                # Fill others with zeros\n                X_batch_all = [np.zeros_like(X_batch), X_batch, np.zeros_like(X_batch), np.zeros_like(X_batch)]\n                y_batch_all = [np.zeros_like(y_batch), y_batch, np.zeros_like(y_batch), np.zeros_like(y_batch)]\n            elif i % 4 == 2:\n                # Validation with CT modality (X_val_ct, y_val_ct)\n                X_batch = X_val_ct[indices_ct[i:i+batch_size]]\n                y_batch = y_val_ct[indices_ct[i:i+batch_size]]\n                # Fill others with zeros\n                X_batch_all = [np.zeros_like(X_batch), np.zeros_like(X_batch), X_batch, np.zeros_like(X_batch)]\n                y_batch_all = [np.zeros_like(y_batch), np.zeros_like(y_batch), y_batch, np.zeros_like(y_batch)]\n            else:\n                # Validation with MRI modality (X_val_mri, y_val_mri)\n                X_batch = X_val_mri[indices_mri[i:i+batch_size]]\n                y_batch = y_val_mri[indices_mri[i:i+batch_size]]\n                # Fill others with zeros\n                X_batch_all = [np.zeros_like(X_batch), np.zeros_like(X_batch), np.zeros_like(X_batch), X_batch]\n                y_batch_all = [np.zeros_like(y_batch), np.zeros_like(y_batch), np.zeros_like(y_batch), y_batch]\n\n            # Convert to TensorFlow Tensors before yielding\n            yield (tf.convert_to_tensor(X_batch_all), tf.convert_to_tensor(y_batch_all))  # Return as tuple of tensors\n\n\n# In model.fit, update the validation data to use the validation data generator\nhistory = model.fit(\n    alternating_training_data_generator(X_train_s, X_train_h, X_train_ct, X_train_mri, y_train_s, y_train_h, y_train_ct, y_train_mri, batch_size=32),\n    epochs=200,\n    validation_data=alternating_validation_data_generator(X_val_s, X_val_h, X_val_ct, X_val_mri, y_val_s, y_val_h, y_val_ct, y_val_mri, batch_size=32),\n    steps_per_epoch=len(X_train_s) // 32,  # Adjust based on your batch size and dataset length\n    validation_steps=len(X_val_s) // 32,  # Same logic for validation\n    callbacks=callbacks\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\n\n# Updated generator for two modalities (Skin and Lung)\ndef alternating_training_data_generator(X_train_s, X_train_h, y_train_s, y_train_h, batch_size):\n    while True:\n        # Shuffle training data (if required)\n        indices_s = np.random.permutation(len(X_train_s))\n        indices_h = np.random.permutation(len(X_train_h))\n\n        for i in range(0, len(X_train_s), batch_size):\n            # Alternating between two modalities\n            if i % 2 == 0:\n                # Train with skin modality (X_train_s, y_train_s)\n                X_batch = X_train_s[indices_s[i:i+batch_size]]\n                y_batch = y_train_s[indices_s[i:i+batch_size]]\n                # Feed all modalities, fill the second with zeros\n                X_batch_all = [X_batch, np.zeros_like(X_batch)]  # Skin and empty lung\n                y_batch_all = [y_batch, np.zeros_like(y_batch)]  # Skin and empty lung\n            else:\n                # Train with lung modality (X_train_h, y_train_h)\n                X_batch = X_train_h[indices_h[i:i+batch_size]]\n                y_batch = y_train_h[indices_h[i:i+batch_size]]\n                # Fill the first modality with zeros\n                X_batch_all = [np.zeros_like(X_batch), X_batch]  # Empty skin and lung\n                y_batch_all = [np.zeros_like(y_batch), y_batch]  # Empty skin and lung\n\n            # Convert to TensorFlow tensors before yielding\n            yield (tf.convert_to_tensor(X_batch_all), tf.convert_to_tensor(y_batch_all))  # Return as tuple of tensors\n\n\n# Updated validation data generator for two modalities\ndef alternating_validation_data_generator(X_val_s, X_val_h, y_val_s, y_val_h, batch_size):\n    while True:\n        # Shuffle validation data (if required)\n        indices_s = np.random.permutation(len(X_val_s))\n        indices_h = np.random.permutation(len(X_val_h))\n\n        for i in range(0, len(X_val_s), batch_size):\n            # Alternating between two modalities\n            if i % 2 == 0:\n                # Validation with skin modality (X_val_s, y_val_s)\n                X_batch = X_val_s[indices_s[i:i+batch_size]]\n                y_batch = y_val_s[indices_s[i:i+batch_size]]\n                # Feed all modalities, fill the second with zeros\n                X_batch_all = [X_batch, np.zeros_like(X_batch)]  # Skin and empty lung\n                y_batch_all = [y_batch, np.zeros_like(y_batch)]  # Skin and empty lung\n            else:\n                # Validation with lung modality (X_val_h, y_val_h)\n                X_batch = X_val_h[indices_h[i:i+batch_size]]\n                y_batch = y_val_h[indices_h[i:i+batch_size]]\n                # Fill the first modality with zeros\n                X_batch_all = [np.zeros_like(X_batch), X_batch]  # Empty skin and lung\n                y_batch_all = [np.zeros_like(y_batch), y_batch]  # Empty skin and lung\n\n            # Convert to TensorFlow tensors before yielding\n            yield (tf.convert_to_tensor(X_batch_all), tf.convert_to_tensor(y_batch_all))  # Return as tuple of tensors\n\n\n# In model.fit, update the validation data to use the validation data generator\nhistory = model.fit(\n    alternating_training_data_generator(X_train_s, X_train_h, y_train_s, y_train_h, batch_size=32),\n    epochs=200,\n    validation_data=alternating_validation_data_generator(X_val_s, X_val_h, y_val_s, y_val_h, batch_size=32),\n    steps_per_epoch=len(X_train_s) // 32,  # Adjust based on your batch size and dataset length\n    validation_steps=len(X_val_s) // 32,  # Same logic for validation\n    callbacks=callbacks\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nimport tensorflow as tf\n\ninitial_gamma = 0.5\nlearning_rate = 1e-2\noptimizer = Adam(learning_rate=0.001)\n\n# Compile the model initially\nmodel.compile(optimizer=optimizer,\n              loss=['categorical_crossentropy', 'categorical_crossentropy'],\n              loss_weights=[initial_gamma, (1 - initial_gamma)],\n              metrics=['accuracy', 'accuracy'])\n\ndef checkpoint_callback():\n    checkpoint_filepath = 'best1_model_cer_skin_lung.keras'\n    return ModelCheckpoint(filepath=checkpoint_filepath,\n                           save_weights_only=False,\n                           monitor='val_loss',\n                           save_best_only=True,\n                           mode='min',\n                           verbose=1)\n\ndef early_stopping(patience):\n    return EarlyStopping(monitor='val_loss', restore_best_weights=True, patience=60, verbose=1)\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n                              patience=50, verbose=1, min_lr=0.00001)\n\ncallbacks = [checkpoint_callback(), early_stopping(patience=100), reduce_lr]\n\n# Alternating Training Loop\nnum_epochs = 200\nfor epoch in range(num_epochs):\n    print(f\"Epoch {epoch+1}/{num_epochs}\")\n    if epoch % 2 == 0:\n        # Train on first modality only (freeze second modality)\n        for layer in model.layers:\n            layer.trainable = True  # Unfreeze all layers\n        model.compile(optimizer=optimizer,\n                      loss=['categorical_crossentropy', 'categorical_crossentropy'],\n                      loss_weights=[1.0, 0.0],\n                      metrics=['accuracy', 'accuracy'])\n        history = model.fit([X_train_s, X_train_h], [y_train_s, y_train_h],\n                            epochs=1,\n                            validation_data=([X_val_s, X_val_h1], [y_val_s, y_val_h1]), verbose=1,\n                            shuffle=True,\n                            callbacks=callbacks)\n    else:\n        # Train on second modality only (freeze first modality)\n        for layer in model.layers:\n            layer.trainable = True  # Unfreeze all layers\n        model.compile(optimizer=optimizer,\n                      loss=['categorical_crossentropy', 'categorical_crossentropy'],\n                      loss_weights=[0.0, 1.0],\n                      metrics=['accuracy', 'accuracy'])\n        history = model.fit([X_train_s, X_train_h], [y_train_s, y_train_h],\n                            epochs=1,\n                            validation_data=([X_val_s, X_val_h1], [y_val_s, y_val_h1]), verbose=1,\n                            shuffle=True,\n                            callbacks=callbacks)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\n\ninitial_gamma1 = 0.25\ninitial_gamma2 = 0.25\ninitial_gamma3 = 0.25\nlearning_rate = 1e-2\noptimizer = Adam(learning_rate=0.001)\n#opt = Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.9, epsilon=None, amsgrad=False)\n# Compile the model with the custom optimizer\nmodel.compile(optimizer=optimizer,\n              loss=['categorical_crossentropy', 'categorical_crossentropy', 'categorical_crossentropy', 'categorical_crossentropy'],\n              loss_weights=[initial_gamma1, initial_gamma2, initial_gamma3, (1 -  (initial_gamma1 + initial_gamma2 + initial_gamma3))],\n              metrics=['accuracy', 'accuracy', 'accuracy', 'accuracy'])\n\n\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\ndef checkpoint_callback():\n\n    checkpoint_filepath = 'best1_model_cer_skin_lung.keras'\n\n    model_checkpoint_callback= ModelCheckpoint(filepath=checkpoint_filepath,\n                           save_weights_only=False,\n                           #frequency='epoch',\n                           monitor='val_loss',\n                           save_best_only=True,\n                            mode='min',\n                           verbose=1)\n\n    return model_checkpoint_callback\n\ndef early_stopping(patience):\n    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', restore_best_weights=True, patience=60, verbose=1)\n    return es_callback\n\n\n\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n                              patience=50, verbose = 1, min_lr=0.00001)\n\ncheckpoint_callback = checkpoint_callback()\n\nearly_stopping = early_stopping(patience=100)\ncallbacks = [checkpoint_callback, early_stopping, reduce_lr]\n            \n\n# Fit the model with callbacks\nhistory = model.fit([X_train_s, X_train_h, X_train_ct, X_train_mri], [y_train_s, y_train_h, y_train_ct, y_train_mri],\n                    epochs=200,\n                    validation_data=([X_val_s, X_val_h1, X_val_ct, X_val_mri], [y_val_s, y_val_h1, y_val_ct, y_val_mri]), verbose=1,\n                    shuffle=True,\n                    callbacks=callbacks) # UpdateGammaCallback","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\n\ninitial_gamma = 0.5\nlearning_rate = 1e-2\noptimizer = Adam(learning_rate=0.001)\n#opt = Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.9, epsilon=None, amsgrad=False)\n# Compile the model with the custom optimizer\nmodel.compile(optimizer=optimizer,\n              loss=['categorical_crossentropy', 'categorical_crossentropy'],\n              loss_weights=[initial_gamma, (1 -  initial_gamma)],\n              metrics=['accuracy', 'accuracy'])\n\n\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\ndef checkpoint_callback():\n\n    checkpoint_filepath = 'best1_model_cer_skin_lung.keras'\n\n    model_checkpoint_callback= ModelCheckpoint(filepath=checkpoint_filepath,\n                           save_weights_only=False,\n                           #frequency='epoch',\n                           monitor='val_loss',\n                           save_best_only=True,\n                            mode='min',\n                           verbose=1)\n\n    return model_checkpoint_callback\n\ndef early_stopping(patience):\n    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', restore_best_weights=True, patience=60, verbose=1)\n    return es_callback\n\n\n\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n                              patience=50, verbose = 1, min_lr=0.00001)\n\ncheckpoint_callback = checkpoint_callback()\n\nearly_stopping = early_stopping(patience=100)\ncallbacks = [checkpoint_callback, early_stopping, reduce_lr]\n            \n\n# Fit the model with callbacks\nhistory = model.fit([X_train_s, X_train_h], [y_train_s, y_train_h],\n                    epochs=200,\n                    validation_data=([X_val_s, X_val_h1], [y_val_s, y_val_h1]), verbose=1,\n                    shuffle=True,\n                    callbacks=callbacks) # UpdateGammaCallback","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.evaluate([X_test_s, X_test_h1], [y_test_s, y_test_h1])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit([X_train_s, X_train_h], [y_train_s, y_train_h],\n                    epochs=200,\n                    validation_data=([X_val_s, X_val_h1], [y_val_s, y_val_h1]), verbose=1,\n                    shuffle=True,\n                    callbacks=callbacks) # UpdateGammaCallback","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### best result till date","metadata":{}},{"cell_type":"code","source":"model.evaluate([X_test_s, X_test_h1], [y_test_s, y_test_h1])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit([X_train_s, X_train_h], [y_train_s, y_train_h],\n                    epochs=200,\n                    validation_data=([X_val_s, X_val_h1], [y_val_s, y_val_h1]), verbose=1,\n                    shuffle=True,\n                    callbacks=callbacks) # UpdateGammaCallback","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.evaluate([X_test_s, X_test_h1], [y_test_s, y_test_h1])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.evaluate([X_test_s, X_test_h1], [y_test_s, y_test_h1])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit([X_train_s, X_train_h], [y_train_s, y_train_h],\n                    epochs=200,\n                    validation_data=([X_val_s, X_val_h1], [y_val_s, y_val_h1]), verbose=1,\n                    shuffle=True,\n                    callbacks=callbacks) # UpdateGammaCallback","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.evaluate([X_test_s, X_test_h1], [y_test_s, y_test_h1])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#def build_resnet18(input_shape=(128, 128, 3), num_classes=2):\ninput_shape=(128, 128, 3)\ninputs1 = Input(shape=input_shape)\ninputs2 = Input(shape=input_shape)\n\nimport tensorflow.keras.layers as L\n\n#input_data = Input(shape=input_shape, name='input_data')\n# Initial convolutional layer\n\nx1, x2 = residual_GLC_branch1(inputs1, inputs2)\n#print('x:',x.shape)\n\ncon = tf.keras.layers.Concatenate(axis=-1)([x1, x2])\n\ncon = tf.keras.layers.Dropout(0.25)(con, training = True)  ## MCD ####\n\nx = GlobalAveragePooling2D()(con)\nprint('GlobalAveragePooling2D x:',x.shape)\n\noutputs1 = Dense(5, activation='softmax')(x)\noutputs2 = Dense(7, activation='softmax')(x)\n\n# Create the model\nmodel = Model([inputs1, inputs2], [outputs1, outputs2])\n#return model\nprint(model.summary())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\n\ninitial_gamma = 0.5\n\noptimizer = Adam(learning_rate=0.001)\n# Compile the model with the custom optimizer\nmodel.compile(optimizer=optimizer,\n              loss=['categorical_crossentropy', 'categorical_crossentropy'],\n              loss_weights=[initial_gamma, (1 -  initial_gamma)],\n              metrics=['accuracy', 'accuracy'])\n\n\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\ndef checkpoint_callback():\n\n    checkpoint_filepath = 'best1_model_cer_skin_lung.keras'\n\n    model_checkpoint_callback= ModelCheckpoint(filepath=checkpoint_filepath,\n                           save_weights_only=False,\n                           #frequency='epoch',\n                           monitor='val_loss',\n                           save_best_only=True,\n                            mode='min',\n                           verbose=1)\n\n    return model_checkpoint_callback\n\ndef early_stopping(patience):\n    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, verbose=1)\n    return es_callback\n\n\n\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n                              patience=5, min_lr=0.00001)\n\ncheckpoint_callback = checkpoint_callback()\n\nearly_stopping = early_stopping(patience=15)\ncallbacks = [checkpoint_callback, early_stopping, reduce_lr]\n            \n\n# Fit the model with callbacks\nhistory = model.fit([X_train_s, X_train_h], [y_train_s, y_train_h],\n                    epochs=200,\n                    validation_split=0.2, verbose=1,\n                    shuffle=True,\n                    callbacks=callbacks) # UpdateGammaCallback","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.evaluate([X_test_s, X_test_h1], [y_test_s, y_test_h1])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.evaluate([X_test_s, X_test_h1], [y_test_s, y_test_h1])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.evaluate([X_test_s1, X_test_h], [y_test_s1, y_test_h])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred = model.predict([X_test_s1, X_test_h]) \n\ny_pred_binary1 = y_pred[0] >= 0.5\ny_pred_binary_pgd_test1 = np.array(y_pred_binary1, dtype='int32')\n\nprint('y_pred_binary_pgd_test1:', y_pred_binary_pgd_test1.shape)\n\ny_pred_binary2 = y_pred[1] >= 0.5\ny_pred_binary_pgd_test2 = np.array(y_pred_binary2, dtype='int32')\n\nprint('y_pred_binary_pgd_test2:', y_pred_binary_pgd_test2.shape)\n\n#y_test_s, y_test_h\n# Calculate evaluation metrics for the current epsilon\ny_test_categorical1 = y_test_s1\ny_test_categorical2 = y_test_h\n\n## Task 1:\nprint('skin cancer classification:')\naccuracy = accuracy_score(y_pred_binary_pgd_test1, y_test_categorical1) * 100\nprecision = precision_score(y_pred_binary_pgd_test1, y_test_categorical1, average='macro') * 100\nrecall = recall_score(y_pred_binary_pgd_test1, y_test_categorical1, average='macro') * 100\nf1 = f1_score(y_pred_binary_pgd_test1, y_test_categorical1, average='macro') * 100\n#auc = roc_auc_score(y_pred, y_train_categorical, multi_class='ovr') * 100\nprint('accuracy:', accuracy)\nprint('precision:', precision)\nprint('recall:', recall)\nprint('f1:', f1)\n\n## Task 2:\nprint('Cervical cancer classification:')\naccuracy = accuracy_score(y_pred_binary_pgd_test2, y_test_categorical2) * 100\nprecision = precision_score(y_pred_binary_pgd_test2, y_test_categorical2, average='macro') * 100\nrecall = recall_score(y_pred_binary_pgd_test2, y_test_categorical2, average='macro') * 100\nf1 = f1_score(y_pred_binary_pgd_test2, y_test_categorical2, average='macro') * 100\nprint('accuracy:', accuracy)\nprint('precision:', precision)\nprint('recall:', recall)\nprint('f1:', f1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit([X_train_s, X_train_h], [y_train_s, y_train_h],\n                    epochs=100,\n                    validation_split=0.2, verbose=1,\n                    shuffle=True,\n                   callbacks=callbacks)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.evaluate([X_test_s, X_test_h1], [y_test_s, y_test_h1])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\n\nmodel1 = load_model('/kaggle/working/best1_model_cer_skin_lung.keras', custom_objects={'DeeperAttentionLayer1': DeeperAttentionLayer1,\n                                                                         'DeeperAttentionLayer': DeeperAttentionLayer\n                                                                  })\nmodel1.evaluate([X_test_s, X_test_h1], [y_test_s, y_test_h1])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.evaluate([X_test_s1, X_test_h], [y_test_s1, y_test_h])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred = model.predict([X_test_s1, X_test_h]) \n\ny_pred_binary1 = y_pred[0] >= 0.5\ny_pred_binary_pgd_test1 = np.array(y_pred_binary1, dtype='int32')\n\nprint('y_pred_binary_pgd_test1:', y_pred_binary_pgd_test1.shape)\n\ny_pred_binary2 = y_pred[1] >= 0.5\ny_pred_binary_pgd_test2 = np.array(y_pred_binary2, dtype='int32')\n\nprint('y_pred_binary_pgd_test2:', y_pred_binary_pgd_test2.shape)\n\n#y_test_s, y_test_h\n# Calculate evaluation metrics for the current epsilon\ny_test_categorical1 = y_test_s1\ny_test_categorical2 = y_test_h\n\n## Task 1:\nprint('skin cancer classification:')\naccuracy = accuracy_score(y_pred_binary_pgd_test1, y_test_categorical1) * 100\nprecision = precision_score(y_pred_binary_pgd_test1, y_test_categorical1, average='macro') * 100\nrecall = recall_score(y_pred_binary_pgd_test1, y_test_categorical1, average='macro') * 100\nf1 = f1_score(y_pred_binary_pgd_test1, y_test_categorical1, average='macro') * 100\n#auc = roc_auc_score(y_pred, y_train_categorical, multi_class='ovr') * 100\nprint('accuracy:', accuracy)\nprint('precision:', precision)\nprint('recall:', recall)\nprint('f1:', f1)\n\n## Task 2:\nprint('Cervical cancer classification:')\naccuracy = accuracy_score(y_pred_binary_pgd_test2, y_test_categorical2) * 100\nprecision = precision_score(y_pred_binary_pgd_test2, y_test_categorical2, average='macro') * 100\nrecall = recall_score(y_pred_binary_pgd_test2, y_test_categorical2, average='macro') * 100\nf1 = f1_score(y_pred_binary_pgd_test2, y_test_categorical2, average='macro') * 100\nprint('accuracy:', accuracy)\nprint('precision:', precision)\nprint('recall:', recall)\nprint('f1:', f1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred = model.predict([X_test_s, X_test_h1]) \n\ny_pred_binary1 = y_pred[0] >= 0.5\ny_pred_binary_pgd_test1 = np.array(y_pred_binary1, dtype='int32')\n\nprint('y_pred_binary_pgd_test1:', y_pred_binary_pgd_test1.shape)\n\ny_pred_binary2 = y_pred[1] >= 0.5\ny_pred_binary_pgd_test2 = np.array(y_pred_binary2, dtype='int32')\n\nprint('y_pred_binary_pgd_test2:', y_pred_binary_pgd_test2.shape)\n\n#y_test_s, y_test_h\n# Calculate evaluation metrics for the current epsilon\ny_test_categorical1 = y_test_s\ny_test_categorical2 = y_test_h1\n\n## Task 1:\nprint('skin cancer classification:')\naccuracy = accuracy_score(y_pred_binary_pgd_test1, y_test_categorical1) * 100\nprecision = precision_score(y_pred_binary_pgd_test1, y_test_categorical1, average='macro') * 100\nrecall = recall_score(y_pred_binary_pgd_test1, y_test_categorical1, average='macro') * 100\nf1 = f1_score(y_pred_binary_pgd_test1, y_test_categorical1, average='macro') * 100\n#auc = roc_auc_score(y_pred, y_train_categorical, multi_class='ovr') * 100\nprint('accuracy:', accuracy)\nprint('precision:', precision)\nprint('recall:', recall)\nprint('f1:', f1)\n\n## Task 2:\nprint('Cervical cancer classification:')\naccuracy = accuracy_score(y_pred_binary_pgd_test2, y_test_categorical2) * 100\nprecision = precision_score(y_pred_binary_pgd_test2, y_test_categorical2, average='macro') * 100\nrecall = recall_score(y_pred_binary_pgd_test2, y_test_categorical2, average='macro') * 100\nf1 = f1_score(y_pred_binary_pgd_test2, y_test_categorical2, average='macro') * 100\nprint('accuracy:', accuracy)\nprint('precision:', precision)\nprint('recall:', recall)\nprint('f1:', f1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save('best_model_ever.keras')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}